%% ================================================================================
%% This LaTeX file was created by AbiWord.                                         
%% AbiWord is a free, Open Source word processor.                                  
%% More information about AbiWord is available at http://www.abisource.com/        
%% ================================================================================

\documentclass[a4paper,portrait,12pt]{article}
\usepackage[latin1]{inputenc}
\usepackage{calc}
\usepackage{setspace}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage[normalem]{ulem}
%% Please revise the following command, if your babel
%% package does not support en-US
\usepackage[en]{babel}
\usepackage{color}
\usepackage{hyperref}
 
\begin{document}


\begin{flushleft}
Convex Optimization
\end{flushleft}


\begin{flushleft}
Solutions Manual
\end{flushleft}





\begin{flushleft}
Stephen Boyd
\end{flushleft}





\begin{flushleft}
January 4, 2006
\end{flushleft}





\begin{flushleft}
Lieven Vandenberghe
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 2
\end{flushleft}





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Definition of convexity
\end{flushleft}


\begin{flushleft}
2.1 Let C $\subseteq$ Rn be a convex set, with x1 , . . . , xk $\in$ C, and let $\theta$1 , . . . , $\theta$k $\in$ R satisfy $\theta$i $\geq$ 0,
\end{flushleft}


\begin{flushleft}
$\theta$1 + · · · + $\theta$k = 1. Show that $\theta$1 x1 + · · · + $\theta$k xk $\in$ C. (The definition of convexity is that
\end{flushleft}


\begin{flushleft}
this holds for k = 2; you must show it for arbitrary k.) Hint. Use induction on k.
\end{flushleft}


\begin{flushleft}
Solution. This is readily shown by induction from the definition of convex set. We illustrate the idea for k = 3, leaving the general case to the reader. Suppose that x 1 , x2 , x3 $\in$ C,
\end{flushleft}


\begin{flushleft}
and $\theta$1 + $\theta$2 + $\theta$3 = 1 with $\theta$1 , $\theta$2 , $\theta$3 $\geq$ 0. We will show that y = $\theta$1 x1 + $\theta$2 x2 + $\theta$3 x3 $\in$ C.
\end{flushleft}


\begin{flushleft}
At least one of the $\theta$i is not equal to one; without loss of generality we can assume that
\end{flushleft}


\begin{flushleft}
$\theta$1 = 1. Then we can write
\end{flushleft}


\begin{flushleft}
y = $\theta$1 x1 + (1 $-$ $\theta$1 )($\mu$2 x2 + $\mu$3 x3 )
\end{flushleft}





\begin{flushleft}
where $\mu$2 = $\theta$2 /(1 $-$ $\theta$1 ) and $\mu$2 = $\theta$3 /(1 $-$ $\theta$1 ). Note that $\mu$2 , $\mu$3 $\geq$ 0 and
\end{flushleft}





\begin{flushleft}
1 $-$ $\theta$1
\end{flushleft}


\begin{flushleft}
$\theta$2 + $\theta$ 3
\end{flushleft}


=


= 1.


\begin{flushleft}
1 $-$ $\theta$1
\end{flushleft}


\begin{flushleft}
1 $-$ $\theta$1
\end{flushleft}


\begin{flushleft}
Since C is convex and x2 , x3 $\in$ C, we conclude that $\mu$2 x2 + $\mu$3 x3 $\in$ C. Since this point
\end{flushleft}


\begin{flushleft}
and x1 are in C, y $\in$ C.
\end{flushleft}


\begin{flushleft}
2.2 Show that a set is convex if and only if its intersection with any line is convex. Show that
\end{flushleft}


\begin{flushleft}
a set is affine if and only if its intersection with any line is affine.
\end{flushleft}


\begin{flushleft}
Solution. We prove the first part. The intersection of two convex sets is convex. Therefore if S is a convex set, the intersection of S with a line is convex.
\end{flushleft}


\begin{flushleft}
Conversely, suppose the intersection of S with any line is convex. Take any two distinct
\end{flushleft}


\begin{flushleft}
points x1 and x2 $\in$ S. The intersection of S with the line through x1 and x2 is convex.
\end{flushleft}


\begin{flushleft}
Therefore convex combinations of x1 and x2 belong to the intersection, hence also to S.
\end{flushleft}


\begin{flushleft}
2.3 Midpoint convexity. A set C is midpoint convex if whenever two points a, b are in C, the
\end{flushleft}


\begin{flushleft}
average or midpoint (a + b)/2 is in C. Obviously a convex set is midpoint convex. It can
\end{flushleft}


\begin{flushleft}
be proved that under mild conditions midpoint convexity implies convexity. As a simple
\end{flushleft}


\begin{flushleft}
case, prove that if C is closed and midpoint convex, then C is convex.
\end{flushleft}


\begin{flushleft}
Solution. We have to show that $\theta$x + (1 $-$ $\theta$)y $\in$ C for all $\theta$ $\in$ [0, 1] and x, y $\in$ C. Let
\end{flushleft}


\begin{flushleft}
$\theta$(k) be the binary number of length k, i.e., a number of the form
\end{flushleft}


\begin{flushleft}
$\mu$1 + $\mu$ 2 =
\end{flushleft}





\begin{flushleft}
$\theta$(k) = c1 2$-$1 + c2 2$-$2 + · · · + ck 2$-$k
\end{flushleft}





\begin{flushleft}
with ci $\in$ \{0, 1\}, closest to $\theta$. By midpoint convexity (applied k times, recursively),
\end{flushleft}


\begin{flushleft}
$\theta$(k) x + (1 $-$ $\theta$ (k) )y $\in$ C. Because C is closed,
\end{flushleft}


\begin{flushleft}
lim ($\theta$(k) x + (1 $-$ $\theta$ (k) )y) = $\theta$x + (1 $-$ $\theta$)y $\in$ C.
\end{flushleft}





\begin{flushleft}
k$\rightarrow$$\infty$
\end{flushleft}





\begin{flushleft}
2.4 Show that the convex hull of a set S is the intersection of all convex sets that contain S.
\end{flushleft}


\begin{flushleft}
(The same method can be used to show that the conic, or affine, or linear hull of a set S
\end{flushleft}


\begin{flushleft}
is the intersection of all conic sets, or affine sets, or subspaces that contain S.)
\end{flushleft}


\begin{flushleft}
Solution. Let H be the convex hull of S and let D be the intersection of all convex sets
\end{flushleft}


\begin{flushleft}
that contain S, i.e.,
\end{flushleft}


\begin{flushleft}
D=
\end{flushleft}


\begin{flushleft}
\{D | D convex, D $\supseteq$ S\}.
\end{flushleft}





\begin{flushleft}
We will show that H = D by showing that H $\subseteq$ D and D $\subseteq$ H.
\end{flushleft}


\begin{flushleft}
First we show that H $\subseteq$ D. Suppose x $\in$ H, i.e., x is a convex combination of some
\end{flushleft}


\begin{flushleft}
points x1 , . . . , xn $\in$ S. Now let D be any convex set such that D $\supseteq$ S. Evidently, we have
\end{flushleft}


\begin{flushleft}
x1 , . . . , xn $\in$ D. Since D is convex, and x is a convex combination of x1 , . . . , xn , it follows
\end{flushleft}


\begin{flushleft}
that x $\in$ D. We have shown that for any convex set D that contains S, we have x $\in$ D.
\end{flushleft}


\begin{flushleft}
This means that x is in the intersection of all convex sets that contain S, i.e., x $\in$ D.
\end{flushleft}


\begin{flushleft}
Now let us show that D $\subseteq$ H. Since H is convex (by definition) and contains S, we must
\end{flushleft}


\begin{flushleft}
have H = D for some D in the construction of D, proving the claim.
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
Examples
\end{flushleft}


\begin{flushleft}
2.5 What is the distance between two parallel hyperplanes \{x $\in$ Rn | aT x = b1 \} and \{x $\in$
\end{flushleft}


\begin{flushleft}
Rn | aT x = b2 \}?
\end{flushleft}


\begin{flushleft}
Solution. The distance between the two hyperplanes is |b1 $-$ b2 |/ a 2 . To see this,
\end{flushleft}


\begin{flushleft}
consider the construction in the figure below.
\end{flushleft}





\begin{flushleft}
x2 = (b2 / a 2 )a
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
a
\end{flushleft}





\begin{flushleft}
x1 = (b1 / a 2 )a
\end{flushleft}





\begin{flushleft}
aT x = b 2
\end{flushleft}





\begin{flushleft}
aT x = b 1
\end{flushleft}


\begin{flushleft}
The distance between the two hyperplanes is also the distance between the two points
\end{flushleft}


\begin{flushleft}
x1 and x2 where the hyperplane intersects the line through the origin and parallel to the
\end{flushleft}


\begin{flushleft}
normal vector a. These points are given by
\end{flushleft}


\begin{flushleft}
x1 = (b1 / a 22 )a,
\end{flushleft}





\begin{flushleft}
x2 = (b2 / a 22 )a,
\end{flushleft}





\begin{flushleft}
and the distance is
\end{flushleft}


\begin{flushleft}
x1 $-$ x2
\end{flushleft}





2





\begin{flushleft}
= |b1 $-$ b2 |/ a 2 .
\end{flushleft}





\begin{flushleft}
2.6 When does one halfspace contain another? Give conditions under which
\end{flushleft}


\begin{flushleft}
\{x | aT x $\leq$ b\} $\subseteq$ \{x | a
\end{flushleft}


\begin{flushleft}
˜T x $\leq$ ˜b\}
\end{flushleft}


\begin{flushleft}
(where a = 0, a
\end{flushleft}


\begin{flushleft}
˜ = 0). Also find the conditions under which the two halfspaces are equal.
\end{flushleft}


\begin{flushleft}
˜ = \{x | a
\end{flushleft}


\begin{flushleft}
Solution. Let H = \{x | aT x $\leq$ b\} and H
\end{flushleft}


\begin{flushleft}
˜T x $\leq$ ˜b\}. The conditions are:
\end{flushleft}


\begin{flushleft}
˜ if and only if there exists a $\lambda$ $>$ 0 such that a
\end{flushleft}


\begin{flushleft}
$\bullet$ H$\subseteq$H
\end{flushleft}


\begin{flushleft}
˜ = $\lambda$a and ˜b $\geq$ $\lambda$b.
\end{flushleft}


\begin{flushleft}
˜ if and only if there exists a $\lambda$ $>$ 0 such that a
\end{flushleft}


\begin{flushleft}
$\bullet$ H=H
\end{flushleft}


\begin{flushleft}
˜ = $\lambda$a and ˜b = $\lambda$b.
\end{flushleft}





\begin{flushleft}
Let us prove the first condition. The condition is clearly sufficient: if a
\end{flushleft}


\begin{flushleft}
˜ = $\lambda$a and ˜b $\geq$ $\lambda$b
\end{flushleft}


\begin{flushleft}
for some $\lambda$ $>$ 0, then
\end{flushleft}


\begin{flushleft}
aT x $\leq$ b =$\Rightarrow$ $\lambda$aT x $\leq$ $\lambda$b =$\Rightarrow$ a
\end{flushleft}


\begin{flushleft}
˜T x $\leq$ ˜b,
\end{flushleft}


˜


\begin{flushleft}
i.e., H $\subseteq$ H.
\end{flushleft}


\begin{flushleft}
To prove necessity, we distinguish three cases. First suppose a and a
\end{flushleft}


\begin{flushleft}
˜ are not parallel. This
\end{flushleft}


\begin{flushleft}
means we can find a v with a
\end{flushleft}


\begin{flushleft}
˜T v = 0 and aT v = 0. Let x
\end{flushleft}


\begin{flushleft}
ˆ be any point in the intersection
\end{flushleft}


\begin{flushleft}
˜ i.e., aT x
\end{flushleft}


\begin{flushleft}
of H and H,
\end{flushleft}


\begin{flushleft}
ˆ $\leq$ b and a
\end{flushleft}


\begin{flushleft}
˜T x $\leq$ ˜b. We have aT (ˆ
\end{flushleft}


\begin{flushleft}
x + tv) = aT x
\end{flushleft}


\begin{flushleft}
ˆ $\leq$ b for all t $\in$ R.
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
However a
\end{flushleft}


˜ (ˆ


\begin{flushleft}
x + tv) = a
\end{flushleft}


\begin{flushleft}
˜ x
\end{flushleft}


\begin{flushleft}
ˆ + t˜
\end{flushleft}


\begin{flushleft}
a v, and since a
\end{flushleft}


\begin{flushleft}
˜ v = 0, we will have a
\end{flushleft}


\begin{flushleft}
˜T (ˆ
\end{flushleft}


\begin{flushleft}
x + tv) $>$ ˜b for
\end{flushleft}


\begin{flushleft}
sufficiently large t $>$ 0 or sufficiently small t $<$ 0. In other words, if a and a
\end{flushleft}


\begin{flushleft}
˜ are not
\end{flushleft}


\begin{flushleft}
˜ i.e., H $\subseteq$ H.
\end{flushleft}


˜


\begin{flushleft}
parallel, we can find a point x
\end{flushleft}


\begin{flushleft}
ˆ + tv $\in$ H that is not in H,
\end{flushleft}


\begin{flushleft}
Next suppose a and a
\end{flushleft}


\begin{flushleft}
˜ are parallel, but point in opposite directions, i.e., a
\end{flushleft}


\begin{flushleft}
˜ = $\lambda$a for some
\end{flushleft}


\begin{flushleft}
$\lambda$ $<$ 0. Let x
\end{flushleft}


\begin{flushleft}
ˆ be any point in H. Then x
\end{flushleft}


\begin{flushleft}
ˆ $-$ ta $\in$ H for all t $\geq$ 0. However for t large enough
\end{flushleft}


\begin{flushleft}
˜ Again, this shows H $\subseteq$ H.
\end{flushleft}


˜


\begin{flushleft}
we will have a
\end{flushleft}


\begin{flushleft}
˜T (ˆ
\end{flushleft}


\begin{flushleft}
x $-$ ta) = a
\end{flushleft}


\begin{flushleft}
˜T x
\end{flushleft}


\begin{flushleft}
ˆ + t$\lambda$ a 22 $>$ ˜b, so x
\end{flushleft}


\begin{flushleft}
ˆ $-$ ta $\in$ H.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Finally, we assume a
\end{flushleft}


\begin{flushleft}
˜ = $\lambda$a for some $\lambda$ $>$ 0 but ˜b $<$ $\lambda$b. Consider any point x
\end{flushleft}


\begin{flushleft}
ˆ that satisfies
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


˜


\begin{flushleft}
a x
\end{flushleft}


\begin{flushleft}
ˆ = b. Then a
\end{flushleft}


\begin{flushleft}
˜ x
\end{flushleft}


\begin{flushleft}
ˆ = $\lambda$aT x
\end{flushleft}


\begin{flushleft}
ˆ = $\lambda$b $>$ ˜b, so x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ H.
\end{flushleft}


\begin{flushleft}
The proof for the second part of the problem is similar.
\end{flushleft}


\begin{flushleft}
2.7 Voronoi description of halfspace. Let a and b be distinct points in R n . Show that the set
\end{flushleft}


\begin{flushleft}
of all points that are closer (in Euclidean norm) to a than b, i.e., \{x | x$-$a 2 $\leq$ x$-$b 2 \},
\end{flushleft}


\begin{flushleft}
is a halfspace. Describe it explicitly as an inequality of the form cT x $\leq$ d. Draw a picture.
\end{flushleft}


\begin{flushleft}
Solution. Since a norm is always nonnegative, we have x $-$ a 2 $\leq$ x $-$ b 2 if and only
\end{flushleft}


\begin{flushleft}
if x $-$ a 22 $\leq$ x $-$ b 22 , so
\end{flushleft}


\begin{flushleft}
x$-$a
\end{flushleft}





2


2





\begin{flushleft}
$\leq$ x$-$b
\end{flushleft}





2


2





$\Leftarrow$$\Rightarrow$


$\Leftarrow$$\Rightarrow$


$\Leftarrow$$\Rightarrow$





\begin{flushleft}
(x $-$ a)T (x $-$ a) $\leq$ (x $-$ b)T (x $-$ b)
\end{flushleft}


\begin{flushleft}
xT x $-$ 2aT x + aT a $\leq$ xT x $-$ 2bT x + bT b
\end{flushleft}


\begin{flushleft}
2(b $-$ a)T x $\leq$ bT b $-$ aT a.
\end{flushleft}





\begin{flushleft}
Therefore, the set is indeed a halfspace. We can take c = 2(b $-$ a) and d = b T b $-$ aT a.
\end{flushleft}


\begin{flushleft}
This makes good geometric sense: the points that are equidistant to a and b are given by
\end{flushleft}


\begin{flushleft}
a hyperplane whose normal is in the direction b $-$ a.
\end{flushleft}


\begin{flushleft}
2.8 Which of the following sets S are polyhedra? If possible, express S in the form S =
\end{flushleft}


\begin{flushleft}
\{x | Ax b, F x = g\}.
\end{flushleft}


\begin{flushleft}
(a) S = \{y1 a1 + y2 a2 | $-$ 1 $\leq$ y1 $\leq$ 1, $-$ 1 $\leq$ y2 $\leq$ 1\}, where a1 , a2 $\in$ Rn .
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
(b) S = \{x $\in$ Rn | x
\end{flushleft}


\begin{flushleft}
0, 1T x = 1,
\end{flushleft}


\begin{flushleft}
x a = b1 ,
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
a1 , . . . , an $\in$ R and b1 , b2 $\in$ R.
\end{flushleft}


\begin{flushleft}
(c) S = \{x $\in$ Rn | x 0, xT y $\leq$ 1 for all y with y 2 = 1\}.
\end{flushleft}





\begin{flushleft}
(d) S = \{x $\in$ Rn | x
\end{flushleft}





\begin{flushleft}
0, xT y $\leq$ 1 for all y with
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
xi a2i = b2 \}, where
\end{flushleft}





\begin{flushleft}
|yi | = 1\}.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) S is a polyhedron. It is the parallelogram with corners a1 + a2 , a1 $-$ a2 , $-$a1 + a2 ,
\end{flushleft}


\begin{flushleft}
$-$a1 $-$ a2 , as shown below for an example in R2 .
\end{flushleft}





\begin{flushleft}
c2
\end{flushleft}





\begin{flushleft}
a2
\end{flushleft}





\begin{flushleft}
a1
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}


\begin{flushleft}
c1
\end{flushleft}





\begin{flushleft}
For simplicity we assume that a1 and a2 are independent. We can express S as the
\end{flushleft}


\begin{flushleft}
intersection of three sets:
\end{flushleft}


\begin{flushleft}
$\bullet$ S1 : the plane defined by a1 and a2
\end{flushleft}


\begin{flushleft}
$\bullet$ S2 = \{z + y1 a1 + y2 a2 | aT1 z = aT2 z = 0, $-$1 $\leq$ y1 $\leq$ 1\}. This is a slab parallel to
\end{flushleft}


\begin{flushleft}
a2 and orthogonal to S1
\end{flushleft}


\begin{flushleft}
$\bullet$ S3 = \{z + y1 a1 + y2 a2 | aT1 z = aT2 z = 0, $-$1 $\leq$ y2 $\leq$ 1\}. This is a slab parallel to
\end{flushleft}


\begin{flushleft}
a1 and orthogonal to S1
\end{flushleft}


\begin{flushleft}
Each of these sets can be described with linear inequalities.
\end{flushleft}


\begin{flushleft}
$\bullet$ S1 can be described as
\end{flushleft}


\begin{flushleft}
vkT x = 0, k = 1, . . . , n $-$ 2
\end{flushleft}





\begin{flushleft}
where vk are n $-$ 2 independent vectors that are orthogonal to a1 and a2 (which
\end{flushleft}


\begin{flushleft}
form a basis for the nullspace of the matrix [a1 a2 ]T ).
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
$\bullet$ Let c1 be a vector in the plane defined by a1 and a2 , and orthogonal to a2 . For
\end{flushleft}


\begin{flushleft}
example, we can take
\end{flushleft}


\begin{flushleft}
aT a2
\end{flushleft}


\begin{flushleft}
c 1 = a 1 $-$ 1 2 a2 .
\end{flushleft}


\begin{flushleft}
a2 2
\end{flushleft}


\begin{flushleft}
Then x $\in$ S2 if and only if
\end{flushleft}


\begin{flushleft}
$-$|cT1 a1 | $\leq$ cT1 x $\leq$ |cT1 a1 |.
\end{flushleft}


\begin{flushleft}
$\bullet$ Similarly, let c2 be a vector in the plane defined by a1 and a2 , and orthogonal
\end{flushleft}


\begin{flushleft}
to a1 , e.g.,
\end{flushleft}


\begin{flushleft}
aT a1
\end{flushleft}


\begin{flushleft}
c 2 = a 2 $-$ 2 2 a1 .
\end{flushleft}


\begin{flushleft}
a1 2
\end{flushleft}


\begin{flushleft}
Then x $\in$ S3 if and only if
\end{flushleft}


\begin{flushleft}
$-$|cT2 a2 | $\leq$ cT2 x $\leq$ |cT2 a2 |.
\end{flushleft}


\begin{flushleft}
Putting it all together, we can describe S as the solution set of 2n linear inequalities
\end{flushleft}


\begin{flushleft}
vkT x
\end{flushleft}


\begin{flushleft}
$-$vkT x
\end{flushleft}


\begin{flushleft}
cT1 x
\end{flushleft}


\begin{flushleft}
$-$cT1 x
\end{flushleft}


\begin{flushleft}
cT2 x
\end{flushleft}


\begin{flushleft}
$-$cT2 x
\end{flushleft}





$\leq$


$\leq$


$\leq$


$\leq$


$\leq$


$\leq$





\begin{flushleft}
0, k = 1, . . . , n $-$ 2
\end{flushleft}


\begin{flushleft}
0, k = 1, . . . , n $-$ 2
\end{flushleft}


\begin{flushleft}
|cT1 a1 |
\end{flushleft}


\begin{flushleft}
|cT1 a1 |
\end{flushleft}


\begin{flushleft}
|cT2 a2 |
\end{flushleft}


\begin{flushleft}
|cT2 a2 |.
\end{flushleft}





\begin{flushleft}
(b) S is a polyhedron, defined by linear inequalities xk $\geq$ 0 and three equality constraints.
\end{flushleft}


\begin{flushleft}
(c) S is not a polyhedron. It is the intersection of the unit ball \{x | x 2 $\leq$ 1\} and the
\end{flushleft}


\begin{flushleft}
nonnegative orthant Rn
\end{flushleft}


\begin{flushleft}
+ . This follows from the following fact, which follows from
\end{flushleft}


\begin{flushleft}
the Cauchy-Schwarz inequality:
\end{flushleft}


\begin{flushleft}
xT y $\leq$ 1 for all y with y
\end{flushleft}





2





= 1 $\Leftarrow$$\Rightarrow$





\begin{flushleft}
x
\end{flushleft}





2





$\leq$ 1.





\begin{flushleft}
Although in this example we define S as an intersection of halfspaces, it is not a
\end{flushleft}


\begin{flushleft}
polyhedron, because the definition requires infinitely many halfspaces.
\end{flushleft}


\begin{flushleft}
(d) S is a polyhedron. S is the intersection of the set \{x | |xk | $\leq$ 1, k = 1, . . . , n\} and
\end{flushleft}


\begin{flushleft}
the nonnegative orthant Rn
\end{flushleft}


\begin{flushleft}
+ . This follows from the following fact:
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
xT y $\leq$ 1 for all y with
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|yi | = 1 $\Leftarrow$$\Rightarrow$ |xi | $\leq$ 1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
We can prove this as follows. First suppose that |xi | $\leq$ 1 for all i. Then
\end{flushleft}


\begin{flushleft}
xT y =
\end{flushleft}





\begin{flushleft}
xi y i $\leq$
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
if
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
|xi ||yi | $\leq$
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
|yi | = 1
\end{flushleft}





\begin{flushleft}
|yi | = 1.
\end{flushleft}





\begin{flushleft}
Conversely, suppose that x is a nonzero vector that satisfies xT y $\leq$ 1 for all y with
\end{flushleft}


\begin{flushleft}
|yi | = 1. In particular we can make the following choice for y: let k be an index
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
for which |xk | = maxi |xi |, and take yk = 1 if xk $>$ 0, yk = $-$1 if xk $<$ 0, and yi = 0
\end{flushleft}


\begin{flushleft}
for i = k. With this choice of y we have
\end{flushleft}


\begin{flushleft}
xT y =
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
xi yi = yk xk = |xk | = max |xi |.
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Therefore we must have maxi |xi | $\leq$ 1.
\end{flushleft}


\begin{flushleft}
All this implies that we can describe S by a finite number of linear inequalities: it
\end{flushleft}


\begin{flushleft}
is the intersection of the nonnegative orthant with the set \{x | $-$ 1 x 1\}, i.e.,
\end{flushleft}


\begin{flushleft}
the solution of 2n linear inequalities
\end{flushleft}


\begin{flushleft}
$-$xi
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





$\leq$


$\leq$





\begin{flushleft}
0, i = 1, . . . , n
\end{flushleft}


\begin{flushleft}
1, i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
Note that as in part (c) the set S was given as an intersection of an infinite number of
\end{flushleft}


\begin{flushleft}
halfspaces. The difference is that here most of the linear inequalities are redundant,
\end{flushleft}


\begin{flushleft}
and only a finite number are needed to characterize S.
\end{flushleft}


\begin{flushleft}
None of these sets are affine sets or subspaces, except in some trivial cases. For example,
\end{flushleft}


\begin{flushleft}
the set defined in part (a) is a subspace (hence an affine set), if a1 = a2 = 0; the set
\end{flushleft}


\begin{flushleft}
defined in part (b) is an affine set if n = 1 and S = \{1\}; etc.
\end{flushleft}





\begin{flushleft}
2.9 Voronoi sets and polyhedral decomposition. Let x0 , . . . , xK $\in$ Rn . Consider the set of
\end{flushleft}


\begin{flushleft}
points that are closer (in Euclidean norm) to x0 than the other xi , i.e.,
\end{flushleft}


\begin{flushleft}
V = \{x $\in$ Rn | x $-$ x0
\end{flushleft}





2





\begin{flushleft}
$\leq$ x $-$ xi
\end{flushleft}





2,





\begin{flushleft}
i = 1, . . . , K\}.
\end{flushleft}





\begin{flushleft}
V is called the Voronoi region around x0 with respect to x1 , . . . , xK .
\end{flushleft}


\begin{flushleft}
(a) Show that V is a polyhedron. Express V in the form V = \{x | Ax
\end{flushleft}





\begin{flushleft}
b\}.
\end{flushleft}





\begin{flushleft}
(b) Conversely, given a polyhedron P with nonempty interior, show how to find x0 , . . . , xK
\end{flushleft}


\begin{flushleft}
so that the polyhedron is the Voronoi region of x0 with respect to x1 , . . . , xK .
\end{flushleft}


\begin{flushleft}
(c) We can also consider the sets
\end{flushleft}


\begin{flushleft}
Vk = \{x $\in$ Rn | x $-$ xk
\end{flushleft}





2





\begin{flushleft}
$\leq$ x $-$ xi
\end{flushleft}





2,





\begin{flushleft}
i = k\}.
\end{flushleft}





\begin{flushleft}
The set Vk consists of points in Rn for which the closest point in the set \{x0 , . . . , xK \}
\end{flushleft}


\begin{flushleft}
is xk .
\end{flushleft}


\begin{flushleft}
The sets V0 , . . . , VK give a polyhedral decomposition of Rn . More precisely, the sets
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
Vk are polyhedra, k=0 Vk = Rn , and int Vi $\cap$ int Vj = $\emptyset$ for i = j, i.e., Vi and Vj
\end{flushleft}


\begin{flushleft}
intersect at most along a boundary.
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
Suppose that P1 , . . . , Pm are polyhedra such that i=1 Pi = Rn , and int Pi $\cap$
\end{flushleft}


\begin{flushleft}
int Pj = $\emptyset$ for i = j. Can this polyhedral decomposition of Rn be described as
\end{flushleft}


\begin{flushleft}
the Voronoi regions generated by an appropriate set of points?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) x is closer to x0 than to xi if and only if
\end{flushleft}


\begin{flushleft}
x $-$ x0
\end{flushleft}





2





\begin{flushleft}
$\leq$ x $-$ xi
\end{flushleft}





2





$\Leftarrow$$\Rightarrow$





$\Leftarrow$$\Rightarrow$





$\Leftarrow$$\Rightarrow$





\begin{flushleft}
(x $-$ x0 )T (x $-$ x0 ) $\leq$ (x $-$ xi )T (x $-$ xi )
\end{flushleft}





\begin{flushleft}
xT x $-$ 2xT0 x + xT0 x0 $\leq$ xT x $-$ 2xTi x + xTi xi
\end{flushleft}





\begin{flushleft}
2(xi $-$ x0 )T x $\leq$ xTi xi $-$ xT0 x0 ,
\end{flushleft}





\begin{flushleft}
which defines a halfspace. We can express V as V = \{x | Ax
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
x1 $-$ x 0
\end{flushleft}


\begin{flushleft}
 x2 $-$ x 0 
\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}
A = 2
\end{flushleft}


..


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


.


\begin{flushleft}
xK $-$ x 0
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
b\} with
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
xT1 x1 $-$ xT0 x0
\end{flushleft}


\begin{flushleft}
 xT2 x2 $-$ xT0 x0 
\end{flushleft}


\begin{flushleft}
.
\end{flushleft}


\begin{flushleft}
b=
\end{flushleft}


..


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


.


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
xK xK $-$ x 0 x0
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
(b) Conversely, suppose V = \{x | Ax b\} with A $\in$ RK×n and b $\in$ RK . We can pick
\end{flushleft}


\begin{flushleft}
any x0 $\in$ \{x | Ax ≺ b\}, and then construct K points xi by taking the mirror image
\end{flushleft}


\begin{flushleft}
of x0 with respect to the hyperplanes \{x | aTi x = bi \}. In other words, we choose xi
\end{flushleft}


\begin{flushleft}
of the form xi = x0 + $\lambda$ai , where $\lambda$ is chosen in such a way that the distance of xi to
\end{flushleft}


\begin{flushleft}
the hyperplane defined by aTi x = bi is equal to the distance of x0 to the hyperplane:
\end{flushleft}


\begin{flushleft}
bi $-$ aTi x0 = aTi xi $-$ bi .
\end{flushleft}


\begin{flushleft}
Solving for $\lambda$, we obtain $\lambda$ = 2(bi $-$ aTi x0 )/ ai
\end{flushleft}


\begin{flushleft}
xi = x 0 +
\end{flushleft}





2


2,





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
2(bi $-$ aTi x0 )
\end{flushleft}


\begin{flushleft}
ai .
\end{flushleft}


\begin{flushleft}
ai 2
\end{flushleft}





\begin{flushleft}
(c) A polyhedral decomposition of Rn can not always be described as Voronoi regions
\end{flushleft}


\begin{flushleft}
generated by a set of points \{x1 , . . . , xm \}. The figure shows a counterexample in
\end{flushleft}


\begin{flushleft}
R2 .
\end{flushleft}





\begin{flushleft}
P1
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}


\begin{flushleft}
P4
\end{flushleft}





\begin{flushleft}
P3
\end{flushleft}





\begin{flushleft}
P2
\end{flushleft}


\begin{flushleft}
H2
\end{flushleft}





\begin{flushleft}
P˜1
\end{flushleft}





\begin{flushleft}
P˜2
\end{flushleft}


\begin{flushleft}
H1
\end{flushleft}





2





\begin{flushleft}
R is decomposed into 4 polyhedra P1 , . . . , P4 by 2 hyperplanes H1 , H2 . Suppose
\end{flushleft}


\begin{flushleft}
we arbitrarily pick x1 $\in$ P1 and x2 $\in$ P2 . x3 $\in$ P3 must be the mirror image of x1
\end{flushleft}


\begin{flushleft}
and x2 with respect to H2 and H1 , respectively. However, the mirror image of x1
\end{flushleft}


\begin{flushleft}
with respect to H2 lies in P˜1 , and the mirror image of x2 with respect to H1 lies in
\end{flushleft}


\begin{flushleft}
P˜2 , so it is impossible to find such an x3 .
\end{flushleft}


\begin{flushleft}
2.10 Solution set of a quadratic inequality. Let C $\subseteq$ Rn be the solution set of a quadratic
\end{flushleft}


\begin{flushleft}
inequality,
\end{flushleft}


\begin{flushleft}
C = \{x $\in$ Rn | xT Ax + bT x + c $\leq$ 0\},
\end{flushleft}


\begin{flushleft}
with A $\in$ Sn , b $\in$ Rn , and c $\in$ R.
\end{flushleft}


\begin{flushleft}
(a) Show that C is convex if A
\end{flushleft}





0.





\begin{flushleft}
(b) Show that the intersection of C and the hyperplane defined by g T x + h = 0 (where
\end{flushleft}


\begin{flushleft}
g = 0) is convex if A + $\lambda$gg T
\end{flushleft}


\begin{flushleft}
0 for some $\lambda$ $\in$ R.
\end{flushleft}


\begin{flushleft}
Are the converses of these statements true?
\end{flushleft}


\begin{flushleft}
Solution. A set is convex if and only if its intersection with an arbitrary line \{ˆ
\end{flushleft}


\begin{flushleft}
x + tv | t $\in$
\end{flushleft}


\begin{flushleft}
R\} is convex.
\end{flushleft}


\begin{flushleft}
(a) We have
\end{flushleft}


(ˆ


\begin{flushleft}
x + tv)T A(ˆ
\end{flushleft}


\begin{flushleft}
x + tv) + bT (ˆ
\end{flushleft}


\begin{flushleft}
x + tv) + c = $\alpha$t2 + $\beta$t + $\gamma$
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
$\alpha$ = v T Av,
\end{flushleft}





\begin{flushleft}
$\beta$ = bT v + 2ˆ
\end{flushleft}


\begin{flushleft}
xT Av,
\end{flushleft}





\begin{flushleft}
$\gamma$ = c + bT x
\end{flushleft}


\begin{flushleft}
ˆ+x
\end{flushleft}


\begin{flushleft}
ˆT Aˆ
\end{flushleft}


\begin{flushleft}
x.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
The intersection of C with the line defined by x
\end{flushleft}


\begin{flushleft}
ˆ and v is the set
\end{flushleft}


\{ˆ


\begin{flushleft}
x + tv | $\alpha$t2 + $\beta$t + $\gamma$ $\leq$ 0\},
\end{flushleft}


\begin{flushleft}
which is convex if $\alpha$ $\geq$ 0. This is true for any v, if v T Av $\geq$ 0 for all v, i.e., A 0.
\end{flushleft}


\begin{flushleft}
The converse does not hold; for example, take A = $-$1, b = 0, c = $-$1. Then A 0,
\end{flushleft}


\begin{flushleft}
but C = R is convex.
\end{flushleft}


\begin{flushleft}
(b) Let H = \{x | g T x + h = 0\}. We define $\alpha$, $\beta$, and $\gamma$ as in the solution of part (a),
\end{flushleft}


\begin{flushleft}
and, in addition,
\end{flushleft}


\begin{flushleft}
$\delta$ = g T v,
\end{flushleft}


\begin{flushleft}
= gT x
\end{flushleft}


\begin{flushleft}
ˆ + h.
\end{flushleft}


\begin{flushleft}
Without loss of generality we can assume that x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ H, i.e.,
\end{flushleft}


\begin{flushleft}
of C $\cap$ H with the line defined by x
\end{flushleft}


\begin{flushleft}
ˆ and v is
\end{flushleft}





\begin{flushleft}
= 0. The intersection
\end{flushleft}





\{ˆ


\begin{flushleft}
x + tv | $\alpha$t2 + $\beta$t + $\gamma$ $\leq$ 0, $\delta$t = 0\}.
\end{flushleft}


\begin{flushleft}
If $\delta$ = g T v = 0, the intersection is the singleton \{ˆ
\end{flushleft}


\begin{flushleft}
x\}, if $\gamma$ $\leq$ 0, or it is empty. In
\end{flushleft}


\begin{flushleft}
either case it is a convex set. If $\delta$ = g T v = 0, the set reduces to
\end{flushleft}


\{ˆ


\begin{flushleft}
x + tv | $\alpha$t2 + $\beta$t + $\gamma$ $\leq$ 0\},
\end{flushleft}


\begin{flushleft}
which is convex if $\alpha$ $\geq$ 0. Therefore C $\cap$ H is convex if
\end{flushleft}


\begin{flushleft}
g T v = 0 =$\Rightarrow$ v T Av $\geq$ 0.
\end{flushleft}





\begin{flushleft}
(2.10.A)
\end{flushleft}





\begin{flushleft}
This is true if there exists $\lambda$ such that A + $\lambda$gg T
\end{flushleft}


\begin{flushleft}
0; then (2.10.A) holds, because
\end{flushleft}


\begin{flushleft}
then
\end{flushleft}


\begin{flushleft}
v T Av = v T (A + $\lambda$gg T )v $\geq$ 0
\end{flushleft}





\begin{flushleft}
for all v satisfying g T v = 0.
\end{flushleft}


\begin{flushleft}
Again, the converse is not true.
\end{flushleft}





\begin{flushleft}
2.11 Hyperbolic sets. Show that the hyperbolic set \{x $\in$ R2+ | x1 x2 $\geq$ 1\} is convex. As a
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
generalization, show that \{x $\in$ Rn
\end{flushleft}


\begin{flushleft}
x $\geq$ 1\} is convex. Hint. If a, b $\geq$ 0 and
\end{flushleft}


+ |


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
$\theta$ 1$-$$\theta$
\end{flushleft}


\begin{flushleft}
0 $\leq$ $\theta$ $\leq$ 1, then a b
\end{flushleft}


\begin{flushleft}
$\leq$ $\theta$a + (1 $-$ $\theta$)b; see §3.1.9.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We prove the first part without using the hint. Consider a convex combination z of
\end{flushleft}


\begin{flushleft}
two points (x1 , x2 ) and (y1 , y2 ) in the set. If x y, then z = $\theta$x + (1 $-$ $\theta$)y y and
\end{flushleft}


\begin{flushleft}
obviously z1 z2 $\geq$ y1 y2 $\geq$ 1. Similar proof if y x.
\end{flushleft}


\begin{flushleft}
Suppose y 0 and x y, i.e., (y1 $-$ x1 )(y2 $-$ x2 ) $<$ 0. Then
\end{flushleft}


\begin{flushleft}
($\theta$x1 + (1 $-$ $\theta$)y1 )($\theta$x2 + (1 $-$ $\theta$)y2 )
\end{flushleft}


=


=


$\geq$





\begin{flushleft}
(b) Assume that
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\theta$2 x1 x2 + (1 $-$ $\theta$)2 y1 y2 + $\theta$(1 $-$ $\theta$)x1 y2 + $\theta$(1 $-$ $\theta$)x2 y1
\end{flushleft}


\begin{flushleft}
$\theta$x1 x2 + (1 $-$ $\theta$)y1 y2 $-$ $\theta$(1 $-$ $\theta$)(y1 $-$ x1 )(y2 $-$ x2 )
\end{flushleft}


1.





\begin{flushleft}
xi $\geq$ 1 and
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
yi $\geq$ 1. Using the inequality in the hint, we have
\end{flushleft}





\begin{flushleft}
($\theta$xi + (1 $-$ $\theta$)yi ) $\geq$
\end{flushleft}





\begin{flushleft}
x$\theta$i yi1$-$$\theta$ = (
\end{flushleft}





\begin{flushleft}
xi ) $\theta$ (
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
yi )1$-$$\theta$ $\geq$ 1.
\end{flushleft}





\begin{flushleft}
2.12 Which of the following sets are convex?
\end{flushleft}


\begin{flushleft}
(a) A slab, i.e., a set of the form \{x $\in$ Rn | $\alpha$ $\leq$ aT x $\leq$ $\beta$\}.
\end{flushleft}





\begin{flushleft}
(b) A rectangle, i.e., a set of the form \{x $\in$ Rn | $\alpha$i $\leq$ xi $\leq$ $\beta$i , i = 1, . . . , n\}. A rectangle
\end{flushleft}


\begin{flushleft}
is sometimes called a hyperrectangle when n $>$ 2.
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
(c) A wedge, i.e., \{x $\in$ Rn | aT1 x $\leq$ b1 , aT2 x $\leq$ b2 \}.
\end{flushleft}





\begin{flushleft}
(d) The set of points closer to a given point than a given set, i.e.,
\end{flushleft}


\begin{flushleft}
\{x | x $-$ x0
\end{flushleft}





2





\begin{flushleft}
$\leq$ x$-$y
\end{flushleft}





2





\begin{flushleft}
for all y $\in$ S\}
\end{flushleft}





\begin{flushleft}
where S $\subseteq$ Rn .
\end{flushleft}





\begin{flushleft}
(e) The set of points closer to one set than another, i.e.,
\end{flushleft}


\begin{flushleft}
\{x | dist(x, S) $\leq$ dist(x, T )\},
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
where S, T $\subseteq$ R , and
\end{flushleft}


\begin{flushleft}
dist(x, S) = inf\{ x $-$ z
\end{flushleft}





2





\begin{flushleft}
| z $\in$ S\}.
\end{flushleft}





\begin{flushleft}
(f) [HUL93, volume 1, page 93] The set \{x | x + S2 $\subseteq$ S1 \}, where S1 , S2 $\subseteq$ Rn with S1
\end{flushleft}


\begin{flushleft}
convex.
\end{flushleft}


\begin{flushleft}
(g) The set of points whose distance to a does not exceed a fixed fraction $\theta$ of the
\end{flushleft}


\begin{flushleft}
distance to b, i.e., the set \{x | x $-$ a 2 $\leq$ $\theta$ x $-$ b 2 \}. You can assume a = b and
\end{flushleft}


\begin{flushleft}
0 $\leq$ $\theta$ $\leq$ 1.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) A slab is an intersection of two halfspaces, hence it is a convex set (and a polyhedron).
\end{flushleft}


\begin{flushleft}
(b) As in part (a), a rectangle is a convex set and a polyhedron because it is a finite
\end{flushleft}


\begin{flushleft}
intersection of halfspaces.
\end{flushleft}


\begin{flushleft}
(c) A wedge is an intersection of two halfspaces, so it is convex set. It is also a polyhedron. It is a cone if b1 = 0 and b2 = 0.
\end{flushleft}


\begin{flushleft}
(d) This set is convex because it can be expressed as
\end{flushleft}





\begin{flushleft}
y$\in$S
\end{flushleft}





\begin{flushleft}
\{x | x $-$ x0
\end{flushleft}





2





\begin{flushleft}
$\leq$ x$-$y
\end{flushleft}





2 \},





\begin{flushleft}
i.e., an intersection of halfspaces. (For fixed y, the set
\end{flushleft}


\begin{flushleft}
\{x | x $-$ x0
\end{flushleft}





2





\begin{flushleft}
$\leq$ x$-$y
\end{flushleft}





2\}





\begin{flushleft}
is a halfspace; see exercise 2.9).
\end{flushleft}


\begin{flushleft}
(e) In general this set is not convex, as the following example in R shows. With S =
\end{flushleft}


\begin{flushleft}
\{$-$1, 1\} and T = \{0\}, we have
\end{flushleft}


\begin{flushleft}
\{x | dist(x, S) $\leq$ dist(x, T )\} = \{x $\in$ R | x $\leq$ $-$1/2 or x $\geq$ 1/2\}
\end{flushleft}


\begin{flushleft}
which clearly is not convex.
\end{flushleft}


\begin{flushleft}
(f) This set is convex. x + S2 $\subseteq$ S1 if x + y $\in$ S1 for all y $\in$ S2 . Therefore
\end{flushleft}


\begin{flushleft}
\{x | x + S2 $\subseteq$ S1 \} =
\end{flushleft}





\begin{flushleft}
y$\in$S2
\end{flushleft}





\begin{flushleft}
\{x | x + y $\in$ S1 \} =
\end{flushleft}





\begin{flushleft}
y$\in$S2
\end{flushleft}





\begin{flushleft}
(S1 $-$ y),
\end{flushleft}





\begin{flushleft}
the intersection of convex sets S1 $-$ y.
\end{flushleft}





\begin{flushleft}
(g) The set is convex, in fact a ball.
\end{flushleft}


\begin{flushleft}
\{x | x $-$ a
\end{flushleft}


=





=





2





\begin{flushleft}
$\leq$ $\theta$ x $-$ b 2\}
\end{flushleft}





\begin{flushleft}
\{x | x $-$ a
\end{flushleft}





2


2





\begin{flushleft}
$\leq$ $\theta$2 x $-$ b 22 \}
\end{flushleft}





\begin{flushleft}
\{x | (1 $-$ $\theta$ 2 )xT x $-$ 2(a $-$ $\theta$ 2 b)T x + (aT a $-$ $\theta$2 bT b) $\leq$ 0\}
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
If $\theta$ = 1, this is a halfspace. If $\theta$ $<$ 1, it is a ball
\end{flushleft}


\begin{flushleft}
\{x | (x $-$ x0 )T (x $-$ x0 ) $\leq$ R2 \},
\end{flushleft}


\begin{flushleft}
with center x0 and radius R given by
\end{flushleft}


\begin{flushleft}
x0 =
\end{flushleft}





\begin{flushleft}
a $-$ $\theta$2 b
\end{flushleft}


,


\begin{flushleft}
1 $-$ $\theta$2
\end{flushleft}





\begin{flushleft}
$\theta$2 b 22 $-$ a
\end{flushleft}


\begin{flushleft}
1 $-$ $\theta$2
\end{flushleft}





\begin{flushleft}
R=
\end{flushleft}





2


2





1/2





\begin{flushleft}
$-$ x0
\end{flushleft}





2


2





.





\begin{flushleft}
2.13 Conic hull of outer products. Consider the set of rank-k outer products, defined as
\end{flushleft}


\begin{flushleft}
\{XX T | X $\in$ Rn×k , rank X = k\}. Describe its conic hull in simple terms.
\end{flushleft}


\begin{flushleft}
Solution. We have XX T
\end{flushleft}


\begin{flushleft}
0 and rank(XX T ) = k. A positive combination of such
\end{flushleft}


\begin{flushleft}
matrices can have rank up to n, but never less than k. Indeed, Let A and B be positive
\end{flushleft}


\begin{flushleft}
semidefinite matrices of rank k, with rank(A + B) = r $<$ k. Let V $\in$ Rn×(n$-$r) be a
\end{flushleft}


\begin{flushleft}
matrix with R(V ) = N (A + B), i.e.,
\end{flushleft}


\begin{flushleft}
V T (A + B)V = V T AV + V T BV = 0.
\end{flushleft}


\begin{flushleft}
Since A, B
\end{flushleft}





\begin{flushleft}
0, this means
\end{flushleft}





\begin{flushleft}
V T AV = V T BV = 0,
\end{flushleft}


\begin{flushleft}
which implies that rank A $\leq$ r and rank B $\leq$ r. We conclude that rank(A + B) $\geq$ k for
\end{flushleft}


\begin{flushleft}
any A, B such that rank(A, B) = k and A, B 0.
\end{flushleft}


\begin{flushleft}
It follows that the conic hull of the set of rank-k outer products is the set of positive
\end{flushleft}


\begin{flushleft}
semidefinite matrices of rank greater than or equal to k, along with the zero matrix.
\end{flushleft}


\begin{flushleft}
2.14 Expanded and restricted sets. Let S $\subseteq$ Rn , and let · be a norm on Rn .
\end{flushleft}





\begin{flushleft}
(a) For a $\geq$ 0 we define Sa as \{x | dist(x, S) $\leq$ a\}, where dist(x, S) = inf y$\in$S x $-$ y .
\end{flushleft}


\begin{flushleft}
We refer to Sa as S expanded or extended by a. Show that if S is convex, then Sa
\end{flushleft}


\begin{flushleft}
is convex.
\end{flushleft}


\begin{flushleft}
(b) For a $\geq$ 0 we define S$-$a = \{x | B(x, a) $\subseteq$ S\}, where B(x, a) is the ball (in the norm
\end{flushleft}


\begin{flushleft}
· ), centered at x, with radius a. We refer to S$-$a as S shrunk or restricted by a,
\end{flushleft}


\begin{flushleft}
since S$-$a consists of all points that are at least a distance a from Rn \ensuremath{\backslash}S. Show that
\end{flushleft}


\begin{flushleft}
if S is convex, then S$-$a is convex.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Consider two points x1 , x2 $\in$ Sa . For 0 $\leq$ $\theta$ $\leq$ 1,
\end{flushleft}


\begin{flushleft}
dist($\theta$x1 + (1 $-$ $\theta$)x2 , X)
\end{flushleft}





=


=


=


$\leq$





\begin{flushleft}
inf $\theta$x1 + (1 $-$ $\theta$)x2 $-$ y
\end{flushleft}





\begin{flushleft}
y$\in$S
\end{flushleft}





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
$\theta$x1 + (1 $-$ $\theta$)x2 $-$ $\theta$y1 $-$ (1 $-$ $\theta$)y2
\end{flushleft}





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
$\theta$(x1 $-$ y1 ) + (1 $-$ $\theta$)(x2 $-$ y2 )
\end{flushleft}





\begin{flushleft}
y1 ,y2 $\in$S
\end{flushleft}


\begin{flushleft}
y1 ,y2 $\in$S
\end{flushleft}





\begin{flushleft}
inf ($\theta$ x1 $-$ y1 + (1 $-$ $\theta$) x2 $-$ y2 )
\end{flushleft}





\begin{flushleft}
y1 ,y2 $\in$S
\end{flushleft}





=





\begin{flushleft}
$\theta$ inf
\end{flushleft}





$\leq$





\begin{flushleft}
a,
\end{flushleft}





\begin{flushleft}
y1 $\in$S
\end{flushleft}





\begin{flushleft}
x1 $-$ y1 + (1 $-$ $\theta$) inf
\end{flushleft}





\begin{flushleft}
y2 $\in$s
\end{flushleft}





\begin{flushleft}
x2 $-$ y 2 )
\end{flushleft}





\begin{flushleft}
so $\theta$x1 + (1 $-$ $\theta$)x2 $\in$ Sa , proving convexity.
\end{flushleft}


\begin{flushleft}
(b) Consider two points x1 , x2 $\in$ S$-$a , so for all u with u $\leq$ a,
\end{flushleft}


\begin{flushleft}
x1 + u $\in$ S,
\end{flushleft}





\begin{flushleft}
x2 + u $\in$ S.
\end{flushleft}





\begin{flushleft}
For 0 $\leq$ $\theta$ $\leq$ 1 and u $\leq$ a,
\end{flushleft}


\begin{flushleft}
$\theta$x1 + (1 $-$ $\theta$)x2 + u = $\theta$(x1 + u) + (1 $-$ $\theta$)(x2 + u) $\in$ S,
\end{flushleft}


\begin{flushleft}
because S is convex. We conclude that $\theta$x1 + (1 $-$ $\theta$)x2 $\in$ S$-$a .
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
2.15 Some sets of probability distributions. Let x be a real-valued random variable with
\end{flushleft}


\begin{flushleft}
prob(x = ai ) = pi , i = 1, . . . , n, where a1 $<$ a2 $<$ · · · $<$ an . Of course p $\in$ Rn lies
\end{flushleft}


\begin{flushleft}
in the standard probability simplex P = \{p | 1T p = 1, p
\end{flushleft}


\begin{flushleft}
0\}. Which of the following
\end{flushleft}


\begin{flushleft}
conditions are convex in p? (That is, for which of the following conditions is the set of
\end{flushleft}


\begin{flushleft}
p $\in$ P that satisfy the condition convex?)
\end{flushleft}


\begin{flushleft}
(a) $\alpha$ $\leq$ E f (x) $\leq$ $\beta$, where E f (x) is the expected value of f (x), i.e., E f (x) =
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
p f (ai ). (The function f : R $\rightarrow$ R is given.)
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
(b) prob(x $>$ $\alpha$) $\leq$ $\beta$.
\end{flushleft}


\begin{flushleft}
(c) E |x3 | $\leq$ $\alpha$ E |x|.
\end{flushleft}





\begin{flushleft}
(d) E x2 $\leq$ $\alpha$.
\end{flushleft}


\begin{flushleft}
(e) E x2 $\geq$ $\alpha$.
\end{flushleft}





\begin{flushleft}
(f) var(x) $\leq$ $\alpha$, where var(x) = E(x $-$ E x)2 is the variance of x.
\end{flushleft}





\begin{flushleft}
(g) var(x) $\geq$ $\alpha$.
\end{flushleft}





\begin{flushleft}
(h) quartile(x) $\geq$ $\alpha$, where quartile(x) = inf\{$\beta$ | prob(x $\leq$ $\beta$) $\geq$ 0.25\}.
\end{flushleft}


\begin{flushleft}
(i) quartile(x) $\leq$ $\alpha$.
\end{flushleft}





\begin{flushleft}
Solution. We first note that the constraints pi $\geq$ 0, i = 1, . . . , n, define halfspaces, and
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
p = 1 defines a hyperplane, so P is a polyhedron.
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
The first five constraints are, in fact, linear inequalities in the probabilities pi .
\end{flushleft}


\begin{flushleft}
(a) E f (x) =
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi f (ai ), so the constraint is equivalent to two linear inequalities
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\alpha$$\leq$
\end{flushleft}


\begin{flushleft}
(b) prob(x $\geq$ $\alpha$) =
\end{flushleft}





\begin{flushleft}
i: ai $\geq$$\alpha$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi f (ai ) $\leq$ $\beta$.
\end{flushleft}





\begin{flushleft}
pi , so the constraint is equivalent to a linear inequality
\end{flushleft}





\begin{flushleft}
i: ai $\geq$$\alpha$
\end{flushleft}





\begin{flushleft}
pi $\leq$ $\beta$.
\end{flushleft}





\begin{flushleft}
(c) The constraint is equivalent to a linear inequality
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi (|a3i | $-$ $\alpha$|ai |) $\leq$ 0.
\end{flushleft}





\begin{flushleft}
(d) The constraint is equivalent to a linear inequality
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi a2i $\leq$ $\alpha$.
\end{flushleft}





\begin{flushleft}
(e) The constraint is equivalent to a linear inequality
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi a2i $\geq$ $\alpha$.
\end{flushleft}





\begin{flushleft}
The first five constraints therefore define convex sets.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(f) The constraint
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
var(x) = E x2 $-$ (E x)2 =
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi a2i $-$ (
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
p i ai ) 2 $\leq$ $\alpha$
\end{flushleft}





\begin{flushleft}
is not convex in general. As a counterexample, we can take n = 2, a1 = 0, a2 = 1,
\end{flushleft}


\begin{flushleft}
and $\alpha$ = 1/5. p = (1, 0) and p = (0, 1) are two points that satisfy var(x) $\leq$ $\alpha$, but
\end{flushleft}


\begin{flushleft}
the convex combination p = (1/2, 1/2) does not.
\end{flushleft}


\begin{flushleft}
(g) This constraint is equivalent to
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
a2i pi + (
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ai pi )2 = bT p + pT Ap $\leq$ $\alpha$
\end{flushleft}





\begin{flushleft}
where bi = a2i and A = aaT . This defines a convex set, since the matrix aaT is
\end{flushleft}


\begin{flushleft}
positive semidefinite.
\end{flushleft}


\begin{flushleft}
Let us denote quartile(x) = f (p) to emphasize it is a function of p. The figure illustrates
\end{flushleft}


\begin{flushleft}
the definition. It shows the cumulative distribution for a distribution p with f (p) = a2 .
\end{flushleft}


\begin{flushleft}
prob(x $\leq$ $\beta$)
\end{flushleft}


\begin{flushleft}
PSfrag replacements
\end{flushleft}


1


\begin{flushleft}
p1 + p2 + · · · + pn$-$1
\end{flushleft}


\begin{flushleft}
p1 + p 2
\end{flushleft}


0.25


\begin{flushleft}
p1
\end{flushleft}


\begin{flushleft}
a1
\end{flushleft}





\begin{flushleft}
an
\end{flushleft}





\begin{flushleft}
a2
\end{flushleft}





\begin{flushleft}
$\beta$
\end{flushleft}





\begin{flushleft}
(h) The constraint f (p) $\geq$ $\alpha$ is equivalent to
\end{flushleft}


\begin{flushleft}
prob(x $\leq$ $\beta$) $<$ 0.25 for all $\beta$ $<$ $\alpha$.
\end{flushleft}


\begin{flushleft}
If $\alpha$ $\leq$ a1 , this is always true. Otherwise, define k = max\{i | ai $<$ $\alpha$\}. This is a fixed
\end{flushleft}


\begin{flushleft}
integer, independent of p. The constraint f (p) $\geq$ $\alpha$ holds if and only if
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
prob(x $\leq$ ak ) =
\end{flushleft}





\begin{flushleft}
pi $<$ 0.25.
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
This is a strict linear inequality in p, which defines an open halfspace.
\end{flushleft}


\begin{flushleft}
(i) The constraint f (p) $\leq$ $\alpha$ is equivalent to
\end{flushleft}


\begin{flushleft}
prob(x $\leq$ $\beta$) $\geq$ 0.25 for all $\beta$ $\geq$ $\alpha$.
\end{flushleft}


\begin{flushleft}
This can be expressed as a linear inequality
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
(If $\alpha$ $\leq$ a1 , we define k = 0.)
\end{flushleft}





\begin{flushleft}
pi $\geq$ 0.25.
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
Operations that preserve convexity
\end{flushleft}


\begin{flushleft}
2.16 Show that if S1 and S2 are convex sets in Rm×n , then so is their partial sum
\end{flushleft}


\begin{flushleft}
S = \{(x, y1 + y2 ) | x $\in$ Rm , y1 , y2 $\in$ Rn , (x, y1 ) $\in$ S1 , (x, y2 ) $\in$ S2 \}.
\end{flushleft}


\begin{flushleft}
Solution. We consider two points (¯
\end{flushleft}


\begin{flushleft}
x, y¯1 + y¯2 ), (˜
\end{flushleft}


\begin{flushleft}
x, y˜1 + y˜2 ) $\in$ S, i.e., with
\end{flushleft}


(¯


\begin{flushleft}
x, y¯1 ) $\in$ S1 ,
\end{flushleft}





(¯


\begin{flushleft}
x, y¯2 ) $\in$ S2 ,
\end{flushleft}





\begin{flushleft}
For 0 $\leq$ $\theta$ $\leq$ 1,
\end{flushleft}





(˜


\begin{flushleft}
x, y˜1 ) $\in$ S1 ,
\end{flushleft}





(˜


\begin{flushleft}
x, y˜2 ) $\in$ S2 .
\end{flushleft}





\begin{flushleft}
$\theta$(¯
\end{flushleft}


\begin{flushleft}
x, y¯1 + y¯2 ) + (1 $-$ $\theta$)(˜
\end{flushleft}


\begin{flushleft}
x, y˜1 + y˜2 ) = ($\theta$¯
\end{flushleft}


\begin{flushleft}
x + (1 $-$ $\theta$)˜
\end{flushleft}


\begin{flushleft}
x, ($\theta$¯
\end{flushleft}


\begin{flushleft}
y1 + (1 $-$ $\theta$)˜
\end{flushleft}


\begin{flushleft}
y1 ) + ($\theta$¯
\end{flushleft}


\begin{flushleft}
y2 + (1 $-$ $\theta$)˜
\end{flushleft}


\begin{flushleft}
y2 ))
\end{flushleft}





\begin{flushleft}
is in S because, by convexity of S1 and S2 ,
\end{flushleft}


\begin{flushleft}
($\theta$¯
\end{flushleft}


\begin{flushleft}
x + (1 $-$ $\theta$)˜
\end{flushleft}


\begin{flushleft}
x, $\theta$¯
\end{flushleft}


\begin{flushleft}
y1 + (1 $-$ $\theta$)˜
\end{flushleft}


\begin{flushleft}
y 1 ) $\in$ S1 ,
\end{flushleft}





\begin{flushleft}
($\theta$¯
\end{flushleft}


\begin{flushleft}
x + (1 $-$ $\theta$)˜
\end{flushleft}


\begin{flushleft}
x, $\theta$¯
\end{flushleft}


\begin{flushleft}
y2 + (1 $-$ $\theta$)˜
\end{flushleft}


\begin{flushleft}
y 2 ) $\in$ S2 .
\end{flushleft}





\begin{flushleft}
2.17 Image of polyhedral sets under perspective function. In this problem we study the image
\end{flushleft}


\begin{flushleft}
of hyperplanes, halfspaces, and polyhedra under the perspective function P (x, t) = x/t,
\end{flushleft}


\begin{flushleft}
with dom P = Rn × R++ . For each of the following sets C, give a simple description of
\end{flushleft}


\begin{flushleft}
P (C) = \{v/t | (v, t) $\in$ C, t $>$ 0\}.
\end{flushleft}





\begin{flushleft}
(a) The polyhedron C = conv\{(v1 , t1 ), . . . , (vK , tK )\} where vi $\in$ Rn and ti $>$ 0.
\end{flushleft}


\begin{flushleft}
Solution. The polyhedron
\end{flushleft}


\begin{flushleft}
P (C) = conv\{v1 /t1 , . . . , vK /tK \}.
\end{flushleft}


\begin{flushleft}
We first show that P (C) $\subseteq$ conv\{v1 /t1 , . . . , vK /tK \}. Let x = (v, t) $\in$ C, with
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
v=
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\theta$i vi ,
\end{flushleft}





\begin{flushleft}
t=
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
and $\theta$
\end{flushleft}





\begin{flushleft}
$\theta$ i ti ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
0, 1T $\theta$ = 1. The image P (x) can be expressed as
\end{flushleft}


\begin{flushleft}
P (x) = v/t =
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
$\theta$v
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
$\theta$t
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}





\begin{flushleft}
$\theta$ i ti
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
$\theta$ t
\end{flushleft}


\begin{flushleft}
k=1 k k
\end{flushleft}





\begin{flushleft}
$\mu$i =
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





=





\begin{flushleft}
$\mu$i vi /ti
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
, i = 1, . . . , K.
\end{flushleft}





\begin{flushleft}
It is clear that $\mu$ 0, 1T $\mu$ = 1, so we can conclude that P (x) $\in$ conv\{v1 /t1 , . . . , vK /tK \}
\end{flushleft}


\begin{flushleft}
for all x $\in$ C.
\end{flushleft}


\begin{flushleft}
Next, we show that P (C) $\supseteq$ conv\{v1 /t1 , . . . , vK /tK \}. Consider a point
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\mu$i vi /ti
\end{flushleft}





\begin{flushleft}
z=
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
with $\mu$
\end{flushleft}





\begin{flushleft}
0, 1T $\mu$ = 1. Define
\end{flushleft}


\begin{flushleft}
$\theta$i =
\end{flushleft}





\begin{flushleft}
It is clear that $\theta$
\end{flushleft}





\begin{flushleft}
$\mu$i
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
$\mu$j /tj
\end{flushleft}





\begin{flushleft}
, i = 1, . . . , K.
\end{flushleft}





\begin{flushleft}
0 and 1T $\theta$ = 1. Moreover, z = P (v, t) where
\end{flushleft}





\begin{flushleft}
t=
\end{flushleft}





\begin{flushleft}
$\theta$ i ti =
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i.e., (v, t) $\in$ C.
\end{flushleft}





\begin{flushleft}
ti
\end{flushleft}





\begin{flushleft}
$\mu$
\end{flushleft}


\begin{flushleft}
i i
\end{flushleft}


=


\begin{flushleft}
$\mu$
\end{flushleft}


\begin{flushleft}
/t
\end{flushleft}


\begin{flushleft}
j j j
\end{flushleft}





1


,


\begin{flushleft}
$\mu$
\end{flushleft}


\begin{flushleft}
/t
\end{flushleft}


\begin{flushleft}
j j j
\end{flushleft}





\begin{flushleft}
v=
\end{flushleft}





\begin{flushleft}
$\theta$i vi ,
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) The hyperplane C = \{(v, t) | f T v + gt = h\} (with f and g not both zero).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
P (C)
\end{flushleft}





=





\begin{flushleft}
\{z | f T z + g = h/t for some t $>$ 0\}
\end{flushleft}





=





\begin{flushleft}
\{z | f T z + g $>$ 0\}
\end{flushleft}


\begin{flushleft}
 \{z | f T z + g $<$ 0\}
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}
 \{z | f T z + g = 0\}
\end{flushleft}





\begin{flushleft}
h=0
\end{flushleft}


\begin{flushleft}
h$>$0
\end{flushleft}


\begin{flushleft}
h $<$ 0.
\end{flushleft}





\begin{flushleft}
(c) The halfspace C = \{(v, t) | f T v + gt $\leq$ h\} (with f and g not both zero).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
P (C)
\end{flushleft}





=





\begin{flushleft}
\{z | f T z + g $\leq$ h/t for some t $>$ 0\}
\end{flushleft}





=





\begin{flushleft}
Rn
\end{flushleft}


\begin{flushleft}
 \{z | f T z + g $<$ 0\}
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}
 \{z | f T z + g $\leq$ 0\}
\end{flushleft}





\begin{flushleft}
h=0
\end{flushleft}


\begin{flushleft}
h$>$0
\end{flushleft}


\begin{flushleft}
h $<$ 0.
\end{flushleft}





\begin{flushleft}
(d) The polyhedron C = \{(v, t) | F v + gt h\}.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
P (C) = \{z | F z + g (1/t)h for some t $>$ 0\}.
\end{flushleft}





\begin{flushleft}
More explicitly, z $\in$ P (C) if and only if it satisfies the following conditions:
\end{flushleft}


\begin{flushleft}
$\bullet$ fiT z + gi $\leq$ 0 if hi = 0
\end{flushleft}


\begin{flushleft}
$\bullet$ fiT z + gi $<$ 0 if hi $<$ 0
\end{flushleft}


\begin{flushleft}
$\bullet$ (fiT z + gi )/hi $\leq$ (fkT z + gk )/hk if hi $>$ 0 and hk $<$ 0.
\end{flushleft}





\begin{flushleft}
2.18 Invertible linear-fractional functions. Let f : Rn $\rightarrow$ Rn be the linear-fractional function
\end{flushleft}


\begin{flushleft}
f (x) = (Ax + b)/(cT x + d),
\end{flushleft}





\begin{flushleft}
dom f = \{x | cT x + d $>$ 0\}.
\end{flushleft}





\begin{flushleft}
Suppose the matrix
\end{flushleft}


\begin{flushleft}
Q=
\end{flushleft}





\begin{flushleft}
A
\end{flushleft}


\begin{flushleft}
cT
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}





\begin{flushleft}
is nonsingular. Show that f is invertible and that f $-$1 is a linear-fractional mapping.
\end{flushleft}


\begin{flushleft}
Give an explicit expression for f $-$1 and its domain in terms of A, b, c, and d. Hint. It
\end{flushleft}


\begin{flushleft}
may be easier to express f $-$1 in terms of Q.
\end{flushleft}


\begin{flushleft}
Solution. This follows from remark 2.2 on page 41. The inverse of f is given by
\end{flushleft}


\begin{flushleft}
f $-$1 (x) = P $-$1 (Q$-$1 P(x)),
\end{flushleft}


\begin{flushleft}
so f $-$1 is the projective transformation associated with Q$-$1 .
\end{flushleft}


\begin{flushleft}
2.19 Linear-fractional functions and convex sets. Let f : Rm $\rightarrow$ Rn be the linear-fractional
\end{flushleft}


\begin{flushleft}
function
\end{flushleft}


\begin{flushleft}
f (x) = (Ax + b)/(cT x + d),
\end{flushleft}


\begin{flushleft}
dom f = \{x | cT x + d $>$ 0\}.
\end{flushleft}


\begin{flushleft}
In this problem we study the inverse image of a convex set C under f , i.e.,
\end{flushleft}


\begin{flushleft}
f $-$1 (C) = \{x $\in$ dom f | f (x) $\in$ C\}.
\end{flushleft}


\begin{flushleft}
For each of the following sets C $\subseteq$ Rn , give a simple description of f $-$1 (C).
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
(a) The halfspace C = \{y | g T y $\leq$ h\} (with g = 0).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
f $-$1 (C)
\end{flushleft}





=


=


=





\begin{flushleft}
\{x $\in$ dom f | g T f (x) $\leq$ h\}
\end{flushleft}





\begin{flushleft}
\{x | g T (Ax + b)/(cT x + d) $\leq$ h, cT x + d $>$ 0\}
\end{flushleft}





\begin{flushleft}
\{x | (AT g $-$ hc)T x $\leq$ hd $-$ g T b, cT x + d $>$ 0\},
\end{flushleft}





\begin{flushleft}
which is another halfspace, intersected with dom f .
\end{flushleft}


\begin{flushleft}
(b) The polyhedron C = \{y | Gy
\end{flushleft}


\begin{flushleft}
Solution. The polyhedron
\end{flushleft}


\begin{flushleft}
f $-$1 (C)
\end{flushleft}





=


=


=





\begin{flushleft}
h\}.
\end{flushleft}





\begin{flushleft}
\{x $\in$ dom f | Gf (x)
\end{flushleft}





\begin{flushleft}
h\}
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
\{x | G(Ax + b)/(c x + d)
\end{flushleft}





\begin{flushleft}
h, cT x + d $>$ 0\}
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
\{x | (GA $-$ hc )x $\leq$ hd $-$ Gb, cT x + d $>$ 0\},
\end{flushleft}





\begin{flushleft}
a polyhedron intersected with dom f .
\end{flushleft}


\begin{flushleft}
(c) The ellipsoid \{y | y T P $-$1 y $\leq$ 1\} (where P $\in$ Sn
\end{flushleft}


++ ).


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
f $-$1 (C)
\end{flushleft}





=


=


=





\begin{flushleft}
\{x $\in$ dom f | f (x)T P $-$1 f (x) $\leq$ 1\}
\end{flushleft}





\begin{flushleft}
\{x $\in$ dom f | (Ax + b)T P $-$1 (Ax + b) $\leq$ (cT x + d)2 \},
\end{flushleft}





\begin{flushleft}
\{x | xT Qx + 2q T x $\leq$ r, cT x + d $>$ 0\}.
\end{flushleft}





\begin{flushleft}
where Q = AT P $-$1 A $-$ ccT , q = bT P $-$1 A + dc, r = d2 $-$ bT P $-$1 b. If AT P $-$1 A
\end{flushleft}


\begin{flushleft}
this is an ellipsoid intersected with dom f .
\end{flushleft}


\begin{flushleft}
(d) The solution set of a linear matrix inequality, C = \{y | y1 A1 + · · · + yn An
\end{flushleft}


\begin{flushleft}
where A1 , . . . , An , B $\in$ Sp .
\end{flushleft}


\begin{flushleft}
Solution. We denote by aTi the ith row of A.
\end{flushleft}


\begin{flushleft}
f $-$1 (C)
\end{flushleft}





=


=


=





\begin{flushleft}
\{x $\in$ dom f | f1 (x)A1 + f2 (x)A2 + · · · + fn (x)An
\end{flushleft}


\begin{flushleft}
(aT1 x
\end{flushleft}





\begin{flushleft}
\{x $\in$ dom f |
\end{flushleft}





\begin{flushleft}
+ b1 )A1 + · · · +
\end{flushleft}





\begin{flushleft}
\{x $\in$ dom f | G1 x1 + · · · + Gm xm
\end{flushleft}





\begin{flushleft}
(aTn x
\end{flushleft}





\begin{flushleft}
+ bn )An
\end{flushleft}





\begin{flushleft}
ccT
\end{flushleft}


\begin{flushleft}
B\},
\end{flushleft}





\begin{flushleft}
B\}
\end{flushleft}


\begin{flushleft}
(cT x + d)B\}
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
H, c x + d $>$ 0\}
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
Gi = a1i A1 + a2i A2 + · · · + ani An $-$ ci B,
\end{flushleft}





\begin{flushleft}
H = dB $-$ b1 A1 $-$ b2 A2 $-$ · · · $-$ bn An .
\end{flushleft}





\begin{flushleft}
f $-$1 (C) is the intersection of dom f with the solution set of an LMI.
\end{flushleft}





\begin{flushleft}
Separation theorems and supporting hyperplanes
\end{flushleft}


\begin{flushleft}
2.20 Strictly positive solution of linear equations. Suppose A $\in$ Rm×n , b $\in$ Rm , with b $\in$ R(A).
\end{flushleft}


\begin{flushleft}
Show that there exists an x satisfying
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





0,





\begin{flushleft}
Ax = b
\end{flushleft}





\begin{flushleft}
if and only if there exists no $\lambda$ with
\end{flushleft}


\begin{flushleft}
AT $\lambda$
\end{flushleft}





0,





\begin{flushleft}
AT $\lambda$ = 0,
\end{flushleft}





\begin{flushleft}
bT $\lambda$ $\leq$ 0.
\end{flushleft}





\begin{flushleft}
Hint. First prove the following fact from linear algebra: cT x = d for all x satisfying
\end{flushleft}


\begin{flushleft}
Ax = b if and only if there is a vector $\lambda$ such that c = AT $\lambda$, d = bT $\lambda$.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Solution. We first prove the result in the hint. Suppose that there exists a $\lambda$ such that
\end{flushleft}


\begin{flushleft}
c = AT $\lambda$, d = bT $\lambda$. It is clear that if Ax = b then
\end{flushleft}


\begin{flushleft}
cT x = $\lambda$T Ax = $\lambda$T b = d.
\end{flushleft}


\begin{flushleft}
Conversely, suppose Ax = b implies cT x = d, and that rank A = r. Let F $\in$ Rn×(n$-$r)
\end{flushleft}


\begin{flushleft}
be a matrix with R(F ) = N (A), and let x0 be a solution of Ax = b. Then Ax = b if and
\end{flushleft}


\begin{flushleft}
only if x = F y + x0 for some y, and cT x = d for all x = F y + x0 implies
\end{flushleft}


\begin{flushleft}
c T F y + c T x0 = d
\end{flushleft}


\begin{flushleft}
for all y. This is only possible if F T c = 0, i.e., c $\in$ N (F T ) = R(AT ), i.e., there exists
\end{flushleft}


\begin{flushleft}
a $\lambda$ such that c = AT $\lambda$. The condition cT F y + cT x0 = d then reduces to cT x0 = d, i.e.,
\end{flushleft}


\begin{flushleft}
$\lambda$T Ax0 = $\lambda$T b = d. In conclusion, if cT x = d for all x with Ax = b, then there there exists
\end{flushleft}


\begin{flushleft}
a $\lambda$ such that c = AT $\lambda$ and d = bT $\lambda$.
\end{flushleft}


\begin{flushleft}
To prove the main result, we use a standard separating hyperplane argument, applied to
\end{flushleft}


\begin{flushleft}
the sets C = Rn
\end{flushleft}


\begin{flushleft}
++ and D = \{x | Ax = b\}. If they are disjoint, there exists c = 0 and d
\end{flushleft}


\begin{flushleft}
such that cT x $\geq$ d for all x $\in$ C and cT x $\leq$ d for all x $\in$ D. The first condition means that
\end{flushleft}


\begin{flushleft}
c 0 and d $\leq$ 0. Since cT x $\leq$ d on D, which is an affine set, we must have cT x constant
\end{flushleft}


\begin{flushleft}
on D. (If cT x weren't constant on D, it would take on all values.) We can relabel d to be
\end{flushleft}


\begin{flushleft}
this constant value, so we have cT x = d on D. Now using the hint, there is some $\lambda$ such
\end{flushleft}


\begin{flushleft}
that c = AT $\lambda$, d = bT $\lambda$.
\end{flushleft}


\begin{flushleft}
2.21 The set of separating hyperplanes. Suppose that C and D are disjoint subsets of R n .
\end{flushleft}


\begin{flushleft}
Consider the set of (a, b) $\in$ Rn+1 for which aT x $\leq$ b for all x $\in$ C, and aT x $\geq$ b for all
\end{flushleft}


\begin{flushleft}
x $\in$ D. Show that this set is a convex cone (which is the singleton \{0\} if there is no
\end{flushleft}


\begin{flushleft}
hyperplane that separates C and D).
\end{flushleft}


\begin{flushleft}
Solution. The conditions aT x $\leq$ b for all x $\in$ C and aT x $\geq$ b for all x $\in$ D, form a set
\end{flushleft}


\begin{flushleft}
of homogeneous linear inequalities in (a, b). Therefore K is the intersection of halfspaces
\end{flushleft}


\begin{flushleft}
that pass through the origin. Hence it is a convex cone.
\end{flushleft}


\begin{flushleft}
Note that this does not require convexity of C or D.
\end{flushleft}


\begin{flushleft}
2.22 Finish the proof of the separating hyperplane theorem in §2.5.1: Show that a separating
\end{flushleft}


\begin{flushleft}
hyperplane exists for two disjoint convex sets C and D. You can use the result proved
\end{flushleft}


\begin{flushleft}
in §2.5.1, i.e., that a separating hyperplane exists when there exist points in the two sets
\end{flushleft}


\begin{flushleft}
whose distance is equal to the distance between the two sets.
\end{flushleft}


\begin{flushleft}
Hint. If C and D are disjoint convex sets, then the set \{x $-$ y | x $\in$ C, y $\in$ D\} is convex
\end{flushleft}


\begin{flushleft}
and does not contain the origin.
\end{flushleft}


\begin{flushleft}
Solution. Following the hint, we first confirm that
\end{flushleft}


\begin{flushleft}
S = \{x $-$ y | x $\in$ C, y $\in$ D\},
\end{flushleft}


\begin{flushleft}
is convex, since it is the sum of two convex sets.
\end{flushleft}


\begin{flushleft}
Since C and D are disjoint, 0 $\in$ S. We distinguish two cases. First suppose 0 $\in$ cl S. The
\end{flushleft}


\begin{flushleft}
partial separating hyperplane in §2.5.1 applies to the sets \{0\} and cl S, so there exists an
\end{flushleft}


\begin{flushleft}
a = 0 such that
\end{flushleft}


\begin{flushleft}
aT (x $-$ y) $>$ 0
\end{flushleft}





\begin{flushleft}
for all x $-$ y $\in$ cl S. In particular this also holds for all x $-$ y $\in$ S, i.e., aT x $>$ aT y for all
\end{flushleft}


\begin{flushleft}
x $\in$ C and y $\in$ D.
\end{flushleft}


\begin{flushleft}
Next, assume 0 $\in$ cl S. Since 0 $\in$ S, it must be in the boundary of S. If S has empty
\end{flushleft}


\begin{flushleft}
interior, it is contained in a hyperplane \{z | aT z = b\}, which must include the origin,
\end{flushleft}


\begin{flushleft}
hence b = 0. In other words, aT x = aT y for all x $\in$ C and all y $\in$ D, so we have a trivial
\end{flushleft}


\begin{flushleft}
separating hyperplane.
\end{flushleft}


\begin{flushleft}
If S has nonempty interior, we consider the set
\end{flushleft}


\begin{flushleft}
S$-$ = \{z | B(z, ) $\subseteq$ S\},
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
where B(z, ) is the Euclidean ball with center z and radius $>$ 0. S$-$ is the set S,
\end{flushleft}


\begin{flushleft}
shrunk by (see exercise 2.14). cl S$-$ is closed and convex, and does not contain 0, so
\end{flushleft}


\begin{flushleft}
by the partial separating hyperplane result, it is strictly separated from \{0\} by at least
\end{flushleft}


\begin{flushleft}
one hyperplane with normal vector a( ):
\end{flushleft}


\begin{flushleft}
a( )T z $>$ 0 for all z $\in$ S$-$ .
\end{flushleft}


\begin{flushleft}
Without loss of generality we assume a( ) 2 = 1. Now let k , k = 1, 2, . . . be a sequence
\end{flushleft}


\begin{flushleft}
of positive values of k with limk$\rightarrow$$\infty$ k = 0. Since a( k ) 2 = 1 for all k, the sequence
\end{flushleft}


\begin{flushleft}
a( k ) contains a convergent subsequence, and we will denote its limit by a
\end{flushleft}


\begin{flushleft}
¯. We have
\end{flushleft}


\begin{flushleft}
a( k )T z $>$ 0 for all z $\in$ S$-$
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
for all k, and therefore a
\end{flushleft}


\begin{flushleft}
¯T z $>$ 0 for all z $\in$ int S, and a
\end{flushleft}


\begin{flushleft}
¯T z $\geq$ 0 for all z $\in$ S, i.e.,
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
¯T x $\geq$ a
\end{flushleft}


\begin{flushleft}
¯T y
\end{flushleft}


\begin{flushleft}
for all x $\in$ C, y $\in$ D.
\end{flushleft}





\begin{flushleft}
2.23 Give an example of two closed convex sets that are disjoint but cannot be strictly separated.
\end{flushleft}


\begin{flushleft}
Solution. Take C = \{x $\in$ R2 | x2 $\leq$ 0\} and D = \{x $\in$ R2+ | x1 x2 $\geq$ 1\}.
\end{flushleft}





\begin{flushleft}
2.24 Supporting hyperplanes.
\end{flushleft}





\begin{flushleft}
(a) Express the closed convex set \{x $\in$ R2+ | x1 x2 $\geq$ 1\} as an intersection of halfspaces.
\end{flushleft}


\begin{flushleft}
Solution. The set is the intersection of all supporting halfspaces at points in its
\end{flushleft}


\begin{flushleft}
boundary, which is given by \{x $\in$ R2+ | x1 x2 = 1\}. The supporting hyperplane at
\end{flushleft}


\begin{flushleft}
x = (t, 1/t) is given by
\end{flushleft}


\begin{flushleft}
x1 /t2 + x2 = 2/t,
\end{flushleft}


\begin{flushleft}
so we can express the set as
\end{flushleft}


\begin{flushleft}
\{x $\in$ R2 | x1 /t2 + x2 $\geq$ 2/t\}.
\end{flushleft}





\begin{flushleft}
t$>$0
\end{flushleft}





\begin{flushleft}
(b) Let C = \{x $\in$ Rn | x $\infty$ $\leq$ 1\}, the $\infty$ -norm unit ball in Rn , and let x
\end{flushleft}


\begin{flushleft}
ˆ be a point
\end{flushleft}


\begin{flushleft}
in the boundary of C. Identify the supporting hyperplanes of C at x
\end{flushleft}


\begin{flushleft}
ˆ explicitly.
\end{flushleft}


\begin{flushleft}
Solution. sT x $\geq$ sT x
\end{flushleft}


\begin{flushleft}
ˆ for all x $\in$ C if and only if
\end{flushleft}


\begin{flushleft}
si $<$ 0
\end{flushleft}


\begin{flushleft}
si $>$ 0
\end{flushleft}


\begin{flushleft}
si = 0
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
ˆi = 1
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
ˆi = $-$1
\end{flushleft}


\begin{flushleft}
$-$1 $<$ x
\end{flushleft}


\begin{flushleft}
ˆi $<$ 1.
\end{flushleft}





\begin{flushleft}
2.25 Inner and outer polyhedral approximations. Let C $\subseteq$ Rn be a closed convex set, and
\end{flushleft}


\begin{flushleft}
suppose that x1 , . . . , xK are on the boundary of C. Suppose that for each i, aTi (x$-$xi ) = 0
\end{flushleft}


\begin{flushleft}
defines a supporting hyperplane for C at xi , i.e., C $\subseteq$ \{x | aTi (x $-$ xi ) $\leq$ 0\}. Consider the
\end{flushleft}


\begin{flushleft}
two polyhedra
\end{flushleft}


\begin{flushleft}
Pinner = conv\{x1 , . . . , xK \},
\end{flushleft}





\begin{flushleft}
Pouter = \{x | aTi (x $-$ xi ) $\leq$ 0, i = 1, . . . , K\}.
\end{flushleft}





\begin{flushleft}
Show that Pinner $\subseteq$ C $\subseteq$ Pouter . Draw a picture illustrating this.
\end{flushleft}


\begin{flushleft}
Solution. The points xi are in C because C is closed. Any point in Pinner = conv\{x1 , . . . , xK \}
\end{flushleft}


\begin{flushleft}
is also in C because C is convex. Therefore Pinner $\subseteq$ C.
\end{flushleft}


\begin{flushleft}
If x $\in$ C then aTi (x $-$ xi ) $\leq$ 0 for i = 1, . . . , K, i.e., x $\in$ Pouter . Therefore C $\subseteq$ Pouter .
\end{flushleft}


\begin{flushleft}
The figure shows an example with K = 4.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
2.26 Support function. The support function of a set C $\subseteq$ Rn is defined as
\end{flushleft}


\begin{flushleft}
SC (y) = sup\{y T x | x $\in$ C\}.
\end{flushleft}


\begin{flushleft}
(We allow SC (y) to take on the value +$\infty$.) Suppose that C and D are closed convex sets
\end{flushleft}


\begin{flushleft}
in Rn . Show that C = D if and only if their support functions are equal.
\end{flushleft}


\begin{flushleft}
Solution. Obviously if C = D the support functions are equal. We show that if the
\end{flushleft}


\begin{flushleft}
support functions are equal, then C = D, by showing that D $\subseteq$ C and C $\subseteq$ D.
\end{flushleft}


\begin{flushleft}
We first show that D $\subseteq$ C. Suppose there exists a point x0 $\in$ D, x $\in$ C. Since C is
\end{flushleft}


\begin{flushleft}
closed, x0 can be strictly separated from C, i.e., there exists an a = 0 with aT x0 $>$ b and
\end{flushleft}


\begin{flushleft}
aT x $<$ b for all x $\in$ C. This means that
\end{flushleft}


\begin{flushleft}
sup aT x $\leq$ b $<$ aT x0 $\leq$ sup aT x,
\end{flushleft}





\begin{flushleft}
x$\in$C
\end{flushleft}





\begin{flushleft}
x$\in$D
\end{flushleft}





\begin{flushleft}
which implies that SC (a) = SD (a). By repeating the argument with the roles of C and
\end{flushleft}


\begin{flushleft}
D reversed, we can show that C $\subseteq$ D.
\end{flushleft}





\begin{flushleft}
2.27 Converse supporting hyperplane theorem. Suppose the set C is closed, has nonempty
\end{flushleft}


\begin{flushleft}
interior, and has a supporting hyperplane at every point in its boundary. Show that C is
\end{flushleft}


\begin{flushleft}
convex.
\end{flushleft}


\begin{flushleft}
Solution. Let H be the set of all halfspaces that contain C. H is a closed convex set,
\end{flushleft}


\begin{flushleft}
and contains C by definition.
\end{flushleft}


\begin{flushleft}
The support function SC of a set C is defined as SC (y) = supx$\in$C y T x. The set H and its
\end{flushleft}


\begin{flushleft}
interior can be defined in terms of the support function as
\end{flushleft}


\begin{flushleft}
H=
\end{flushleft}


\begin{flushleft}
y=0
\end{flushleft}





\begin{flushleft}
\{x | y T x $\leq$ SC (y)\},
\end{flushleft}





\begin{flushleft}
int H =
\end{flushleft}


\begin{flushleft}
y=0
\end{flushleft}





\begin{flushleft}
\{x | y T x $<$ SC (y)\},
\end{flushleft}





\begin{flushleft}
and the boundary of H is the set of all points in H with y T x = SC (y) for at least one
\end{flushleft}


\begin{flushleft}
y = 0.
\end{flushleft}


\begin{flushleft}
By definition int C $\subseteq$ int H. We also have bd C $\subseteq$ bd H: if x
\end{flushleft}


\begin{flushleft}
¯ $\in$ bd C, then there exists
\end{flushleft}


\begin{flushleft}
a supporting hyperplane at x
\end{flushleft}


\begin{flushleft}
¯, i.e., a vector a = 0 such that aT x
\end{flushleft}


\begin{flushleft}
¯ = SC (a), i.e., x
\end{flushleft}


\begin{flushleft}
¯ $\in$ bd H.
\end{flushleft}


\begin{flushleft}
We now show that these properties imply that C is convex. Consider an arbitrary line
\end{flushleft}


\begin{flushleft}
intersecting int C. The intersection is a union of disjoint open intervals Ik , with endpoints
\end{flushleft}


\begin{flushleft}
in bd C (hence also in bd H), and interior points in int C (hence also in int H). Now
\end{flushleft}


\begin{flushleft}
int H is a convex set, so the interior points of two different intervals I1 and I2 can not
\end{flushleft}


\begin{flushleft}
be separated by boundary points (since boundary points are in bd H, not in int H).
\end{flushleft}


\begin{flushleft}
Therefore there can be at most one interval, i.e., int C is convex.
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
Convex cones and generalized inequalities
\end{flushleft}


\begin{flushleft}
2.28 Positive semidefinite cone for n = 1, 2, 3. Give an explicit description of the positive
\end{flushleft}


\begin{flushleft}
semidefinite cone Sn
\end{flushleft}


\begin{flushleft}
+ , in terms of the matrix coefficients and ordinary inequalities, for
\end{flushleft}


\begin{flushleft}
n = 1, 2, 3. To describe a general element of Sn , for n = 1, 2, 3, use the notation
\end{flushleft}


\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
x2
\end{flushleft}





\begin{flushleft}
x1 ,
\end{flushleft}





\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x3
\end{flushleft}





\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x3
\end{flushleft}





,





\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x4
\end{flushleft}


\begin{flushleft}
x5
\end{flushleft}





\begin{flushleft}
x3
\end{flushleft}


\begin{flushleft}
x5
\end{flushleft}


\begin{flushleft}
x6
\end{flushleft}





.





\begin{flushleft}
Solution. For n = 1 the condition is x1 $\geq$ 0. For n = 2 the condition is
\end{flushleft}


\begin{flushleft}
x1 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x1 x3 $-$ x22 $\geq$ 0.
\end{flushleft}





\begin{flushleft}
x3 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
For n = 3 the condition is
\end{flushleft}


\begin{flushleft}
x1 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x2 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x1 x4 $-$ x22 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x3 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x4 x6 $-$ x25 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x1 x6 $-$ x23 $\geq$ 0
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
x1 x4 x6 + 2x2 x3 x5 $-$ x1 x25 $-$ x6 x22 $-$ x4 x23 $\geq$ 0,
\end{flushleft}


\begin{flushleft}
i.e., all principal minors must be nonnegative.
\end{flushleft}


\begin{flushleft}
We give the proof for n = 3, assuming the result is true for n = 2. The matrix
\end{flushleft}


\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x3
\end{flushleft}





\begin{flushleft}
X=
\end{flushleft}





\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x4
\end{flushleft}


\begin{flushleft}
x5
\end{flushleft}





\begin{flushleft}
x3
\end{flushleft}


\begin{flushleft}
x5
\end{flushleft}


\begin{flushleft}
x6
\end{flushleft}





\begin{flushleft}
is positive semidefinite if and only if
\end{flushleft}


\begin{flushleft}
z T Xz = x1 z12 + 2x2 z1 z2 + 2x3 z1 z3 + x4 z22 + 2x5 z2 z3 + x6 z32 $\geq$ 0
\end{flushleft}


\begin{flushleft}
for all z.
\end{flushleft}


\begin{flushleft}
If x1 = 0, we must have x2 = x3 = 0, so X
\end{flushleft}


\begin{flushleft}
x4
\end{flushleft}


\begin{flushleft}
x5
\end{flushleft}





\begin{flushleft}
0 if and only if
\end{flushleft}


\begin{flushleft}
x5
\end{flushleft}


\begin{flushleft}
x6
\end{flushleft}





0.





\begin{flushleft}
Applying the result for the 2 × 2-case, we conclude that if x1 = 0, X
\end{flushleft}


\begin{flushleft}
x4 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x2 = x3 = 0,
\end{flushleft}





\begin{flushleft}
x6 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x 4 x6 $-$
\end{flushleft}





\begin{flushleft}
x25
\end{flushleft}





\begin{flushleft}
0 if and only if
\end{flushleft}





$\geq$ 0.





\begin{flushleft}
Now assume x1 = 0. We have
\end{flushleft}


\begin{flushleft}
z T Xz = x1 (z1 +(x2 /x1 )z2 +(x3 /x1 )z3 )2 +(x4 $-$x22 /x1 )z22 +(x6 $-$x23 /x1 )z32 +2(x5 $-$x2 x3 /x1 )z2 z3 ,
\end{flushleft}


\begin{flushleft}
so it is clear that we must have x1 $>$ 0 and
\end{flushleft}


\begin{flushleft}
x4 $-$ x22 /x1
\end{flushleft}


\begin{flushleft}
x5 $-$ x2 x3 /x1
\end{flushleft}





\begin{flushleft}
x5 $-$ x2 x3 /x1
\end{flushleft}


\begin{flushleft}
x6 $-$ x23 /x1
\end{flushleft}





0.





\begin{flushleft}
By the result for 2 × 2-case studied above, this is equivalent to
\end{flushleft}


\begin{flushleft}
x1 x4 $-$ x22 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x1 x6 $-$ x23 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
(x4 $-$ x22 /x1 )(x6 $-$ x23 /x1 ) $-$ (x5 $-$ x2 x3 /x1 )2 $\geq$ 0.
\end{flushleft}





\begin{flushleft}
The third inequality simplifies to
\end{flushleft}


\begin{flushleft}
(x1 x4 x6 $-$ 2x2 x3 x5 $-$ x1 x25 $-$ x6 x22 $-$ x4 x23 )/x1 $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Therefore, if x1 $>$ 0, then X
\end{flushleft}


\begin{flushleft}
x1 x4 $-$ x22
\end{flushleft}





$\geq$ 0,





\begin{flushleft}
x1 x6 $-$ x23
\end{flushleft}





\begin{flushleft}
0 if and only if
\end{flushleft}


$\geq$ 0,





\begin{flushleft}
(x1 x4 x6 $-$ 2x2 x3 x5 $-$ x1 x25 $-$ x6 x22 $-$ x4 x23 )/x1 $\geq$ 0.
\end{flushleft}





\begin{flushleft}
We can combine the conditions for x1 = 0 and x1 $>$ 0 by saying that all 7 principal minors
\end{flushleft}


\begin{flushleft}
must be nonnegative.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
2.29 Cones in R2 . Suppose K $\subseteq$ R2 is a closed convex cone.
\end{flushleft}


\begin{flushleft}
(a) Give a simple description of K in terms of the polar coordinates of its elements
\end{flushleft}


\begin{flushleft}
(x = r(cos $\phi$, sin $\phi$) with r $\geq$ 0).
\end{flushleft}


\begin{flushleft}
(b) Give a simple description of K ∗ , and draw a plot illustrating the relation between
\end{flushleft}


\begin{flushleft}
K and K ∗ .
\end{flushleft}


\begin{flushleft}
(c) When is K pointed?
\end{flushleft}


\begin{flushleft}
(d) When is K proper (hence, defines a generalized inequality)? Draw a plot illustrating
\end{flushleft}


\begin{flushleft}
what x K y means when K is proper.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) In R2 a cone K is a {``}pie slice'' (see figure).
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
PSfrag replacements
\end{flushleft}


\begin{flushleft}
$\beta$
\end{flushleft}





\begin{flushleft}
$\alpha$
\end{flushleft}





\begin{flushleft}
In terms of polar coordinates, a pointed closed convex cone K can be expressed
\end{flushleft}


\begin{flushleft}
K = \{(r cos $\phi$, r sin $\phi$) | r $\geq$ 0, $\alpha$ $\leq$ $\phi$ $\leq$ $\beta$\}
\end{flushleft}


\begin{flushleft}
where 0 $\leq$ $\beta$ $-$ $\alpha$ $<$ 180◦ . When $\beta$ $-$ $\alpha$ = 180◦ , this gives a non-pointed cone (a
\end{flushleft}


\begin{flushleft}
halfspace). Other possible non-pointed cones are the entire plane
\end{flushleft}


\begin{flushleft}
K = \{(r cos $\phi$, r sin $\phi$) | r $\geq$ 0, 0 $\leq$ $\phi$ $\leq$ 2$\pi$\} = R2 ,
\end{flushleft}


\begin{flushleft}
and lines through the origin
\end{flushleft}


\begin{flushleft}
K = \{(r cos $\alpha$, r sin $\alpha$) | r $\in$ R\}.
\end{flushleft}


\begin{flushleft}
(b) By definition, K ∗ is the intersection of all halfspaces xT y $\geq$ 0 where x $\in$ K. However,
\end{flushleft}


\begin{flushleft}
as can be seen from the figure, if K is pointed, the two halfspaces defined by the
\end{flushleft}


\begin{flushleft}
extreme rays are sufficient to define K ∗ , i.e.,
\end{flushleft}


\begin{flushleft}
K ∗ = \{y | y1 cos $\alpha$ + y2 sin $\alpha$ $\geq$ 0, y1 cos $\beta$ + y2 sin $\beta$ $\geq$ 0\}.
\end{flushleft}





\newpage
2





\begin{flushleft}
K∗
\end{flushleft}





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
If K is a halfspace, K = \{x | v T x $\geq$ 0\}, the dual cone is the ray
\end{flushleft}


\begin{flushleft}
K ∗ = \{tv | t $\geq$ 0\}.
\end{flushleft}


\begin{flushleft}
If K = R2 , the dual cone is K ∗ = \{0\}. If K is a line \{tv | t $\in$ R\} through the origin,
\end{flushleft}


\begin{flushleft}
the dual cone is the line perpendicular to v
\end{flushleft}


\begin{flushleft}
K ∗ = \{y | v T y = 0\}.
\end{flushleft}


\begin{flushleft}
(c) See part (a).
\end{flushleft}


\begin{flushleft}
(d) K must be closed convex and pointed, and have nonempty interior. From part (a),
\end{flushleft}


\begin{flushleft}
this means K can be expressed as
\end{flushleft}


\begin{flushleft}
K = \{(r cos $\phi$, r sin $\phi$) | r $\geq$ 0, $\alpha$ $\leq$ $\phi$ $\leq$ $\beta$\}
\end{flushleft}


\begin{flushleft}
where 0 $<$ $\beta$ $-$ $\alpha$ $<$ 180◦ .
\end{flushleft}


\begin{flushleft}
x K y means y $\in$ x + K.
\end{flushleft}


\begin{flushleft}
2.30 Properties of generalized inequalities. Prove the properties of (nonstrict and strict) generalized inequalities listed in §2.4.1.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
Properties of generalized inequalities.
\end{flushleft}


\begin{flushleft}
(a)
\end{flushleft}


\begin{flushleft}
(b)
\end{flushleft}


\begin{flushleft}
(c)
\end{flushleft}


\begin{flushleft}
(d)
\end{flushleft}


\begin{flushleft}
(e)
\end{flushleft}


\begin{flushleft}
(f)
\end{flushleft}





\begin{flushleft}
K is preserved under addition. If y $-$ x $\in$ K and v $-$ u $\in$ K, where K is a convex
\end{flushleft}


\begin{flushleft}
cone, then the conic combination (y $-$ x) + (v $-$ u) $\in$ K, i.e., x + u K y + v.
\end{flushleft}





\begin{flushleft}
K is transitive. If y $-$ x $\in$ K and z $-$ y $\in$ K then the conic combination (y $-$ x) +
\end{flushleft}


\begin{flushleft}
(z $-$ y) = z $-$ x $\in$ K, i.e., x K z.
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
is preserved under nonnegative scaling. Follows from the fact that K is a cone.
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
is reflexive. Any cone contains the origin.
\end{flushleft}





\begin{flushleft}
K is antisymmetric. If y $-$ x $\in$ K and x $-$ y $\in$ K, then y $-$ x = 0 because K is
\end{flushleft}


\begin{flushleft}
pointed.
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
K.
\end{flushleft}





\begin{flushleft}
is preserved under limits. If yi $-$ xi $\in$ K and K is closed, then limi$\rightarrow$$\infty$ (yi $-$ xi ) $\in$
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Properties of strict inequality.
\end{flushleft}


\begin{flushleft}
(a) If x ≺K y then x
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
y. Every set contains its interior.
\end{flushleft}





\begin{flushleft}
(b) If x ≺K y and u K v then x + u ≺K y + v. If y $-$ x $\in$ int K, then (y $-$ x) + z $\in$ K
\end{flushleft}


\begin{flushleft}
for all sufficiently small nonzero z. Since K is a convex cone and v $-$ u $\in$ K,
\end{flushleft}


\begin{flushleft}
(y $-$ x) + z + (u $-$ v) $\in$ K for all sufficiently small u, i.e., x + u ≺K y + v.
\end{flushleft}





\begin{flushleft}
(c) If x ≺K y and $\alpha$ $>$ 0 then $\alpha$x ≺K $\alpha$y. If y $-$ x + z $\in$ K for sufficiently small nonzero
\end{flushleft}


\begin{flushleft}
z, then $\alpha$(y $-$ x + z) $\in$ K for all $\alpha$ $>$ 0, i.e., $\alpha$(y $-$ x) + z˜ $\in$ K for all sufficiently
\end{flushleft}


\begin{flushleft}
small nonzero z˜.
\end{flushleft}





\begin{flushleft}
(d) x ≺K x. 0 $\in$ int K because K is a pointed cone.
\end{flushleft}





\begin{flushleft}
(e) If x ≺K y, then for u and v small enough, x + u ≺K y + v. If y $-$ x $\in$ int K, then
\end{flushleft}


\begin{flushleft}
(y $-$ x) + (v $-$ u) $\in$ int K for sufficiently small u and v.
\end{flushleft}





\begin{flushleft}
2.31 Properties of dual cones. Let K ∗ be the dual cone of a convex cone K, as defined in (2.19).
\end{flushleft}


\begin{flushleft}
Prove the following.
\end{flushleft}


\begin{flushleft}
(a) K ∗ is indeed a convex cone.
\end{flushleft}


\begin{flushleft}
Solution. K ∗ is the intersection of a set of homogeneous halfspaces (meaning,
\end{flushleft}


\begin{flushleft}
halfspaces that include the origin as a boundary point). Hence it is a closed convex
\end{flushleft}


\begin{flushleft}
cone.
\end{flushleft}





\begin{flushleft}
(b) K1 $\subseteq$ K2 implies K2∗ $\subseteq$ K1∗ .
\end{flushleft}


\begin{flushleft}
Solution. y $\in$ K2∗ means xT y $\geq$ 0 for all x $\in$ K2 , which is includes K1 , therefore
\end{flushleft}


\begin{flushleft}
xT y $\geq$ 0 for all x $\in$ K1 .
\end{flushleft}


\begin{flushleft}
(c) K ∗ is closed.
\end{flushleft}


\begin{flushleft}
Solution. See part (a).
\end{flushleft}





\begin{flushleft}
(d) The interior of K ∗ is given by int K ∗ = \{y | y T x $>$ 0 for all x $\in$ K\}.
\end{flushleft}


\begin{flushleft}
Solution. If y T x $>$ 0 for all x $\in$ K then (y + u)T x $>$ 0 for all x $\in$ K and all
\end{flushleft}


\begin{flushleft}
sufficiently small u; hence y $\in$ int K.
\end{flushleft}


\begin{flushleft}
Conversely if y $\in$ K ∗ and y T x = 0 for some x $\in$ K, then y $\in$ int K ∗ because
\end{flushleft}


\begin{flushleft}
(y $-$ tx)T x $<$ 0 for all t $>$ 0.
\end{flushleft}





\begin{flushleft}
(e) If K has nonempty interior then K ∗ is pointed.
\end{flushleft}


\begin{flushleft}
Solution. Suppose K ∗ is not pointed, i.e., there exists a nonzero y $\in$ K ∗ such that
\end{flushleft}


\begin{flushleft}
$-$y $\in$ K ∗ . This means y T x $\geq$ 0 and $-$y T x $\geq$ 0 for all x $\in$ K, i.e., y T x = 0 for all
\end{flushleft}


\begin{flushleft}
x $\in$ K, hence K has empty interior.
\end{flushleft}


\begin{flushleft}
(f) K ∗∗ is the closure of K. (Hence if K is closed, K ∗∗ = K.)
\end{flushleft}


\begin{flushleft}
Solution. By definition of K ∗ , y = 0 is the normal vector of a (homogeneous)
\end{flushleft}


\begin{flushleft}
halfspace containing K if and only if y $\in$ K ∗ . The intersection of all homogeneous
\end{flushleft}


\begin{flushleft}
halfspaces containing a convex cone K is the closure of K. Therefore the closure of
\end{flushleft}


\begin{flushleft}
K is
\end{flushleft}


\begin{flushleft}
cl K =
\end{flushleft}


\begin{flushleft}
\{x | y T x $\geq$ 0\} = \{x | y T x $\geq$ 0 for all y $\in$ K ∗ \} = K ∗∗ .
\end{flushleft}


\begin{flushleft}
y$\in$K ∗
\end{flushleft}





\begin{flushleft}
(g) If the closure of K is pointed then K ∗ has nonempty interior.
\end{flushleft}


\begin{flushleft}
Solution. If K ∗ has empty interior, there exists an a = 0 such that aT y = 0 for all
\end{flushleft}


\begin{flushleft}
y $\in$ K ∗ . This means a and $-$a are both in K ∗∗ , which contradicts the fact that K ∗∗
\end{flushleft}


\begin{flushleft}
is pointed.
\end{flushleft}


\begin{flushleft}
As an example that shows that it is not sufficient that K is pointed, consider K =
\end{flushleft}


\begin{flushleft}
\{0\} $\cup$ \{(x1 , x2 ) | x1 $>$ 0\}. This is a pointed cone, but its dual has empty interior.
\end{flushleft}





\begin{flushleft}
2.32 Find the dual cone of \{Ax | x 0\}, where A $\in$ Rm×n .
\end{flushleft}


\begin{flushleft}
Solution. K ∗ = \{y | AT y 0\}.
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
2.33 The monotone nonnegative cone. We define the monotone nonnegative cone as
\end{flushleft}


\begin{flushleft}
Km+ = \{x $\in$ Rn | x1 $\geq$ x2 $\geq$ · · · $\geq$ xn $\geq$ 0\}.
\end{flushleft}


\begin{flushleft}
i.e., all nonnegative vectors with components sorted in nonincreasing order.
\end{flushleft}


\begin{flushleft}
(a) Show that Km+ is a proper cone.
\end{flushleft}


∗


\begin{flushleft}
(b) Find the dual cone Km+
\end{flushleft}


\begin{flushleft}
. Hint. Use the identity
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
xi y i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





=





\begin{flushleft}
(x1 $-$ x2 )y1 + (x2 $-$ x3 )(y1 + y2 ) + (x3 $-$ x4 )(y1 + y2 + y3 ) + · · ·
\end{flushleft}


\begin{flushleft}
+ (xn$-$1 $-$ xn )(y1 + · · · + yn$-$1 ) + xn (y1 + · · · + yn ).
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The set Km+ is defined by n homogeneous linear inequalities, hence it is a closed
\end{flushleft}


\begin{flushleft}
(polyhedral) cone.
\end{flushleft}


\begin{flushleft}
The interior of Km+ is nonempty, because there are points that satisfy the inequalities with strict inequality, for example, x = (n, n $-$ 1, n $-$ 2, . . . , 1).
\end{flushleft}


\begin{flushleft}
To show that Km+ is pointed, we note that if x $\in$ Km+ , then $-$x $\in$ Km+ only if
\end{flushleft}


\begin{flushleft}
x = 0. This implies that the cone does not contain an entire line.
\end{flushleft}


\begin{flushleft}
(b) Using the hint, we see that y T x $\geq$ 0 for all x $\in$ Km+ if and only if
\end{flushleft}


\begin{flushleft}
y1 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
y1 + y2 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
Therefore
\end{flushleft}





\begin{flushleft}
. . . , y1 + y2 + · · · + yn $\geq$ 0.
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}


∗


\begin{flushleft}
Km+
\end{flushleft}


\begin{flushleft}
= \{y |
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
yi $\geq$ 0, k = 1, . . . , n\}.
\end{flushleft}





\begin{flushleft}
2.34 The lexicographic cone and ordering. The lexicographic cone is defined as
\end{flushleft}


\begin{flushleft}
Klex = \{0\} $\cup$ \{x $\in$ Rn | x1 = · · · = xk = 0, xk+1 $>$ 0, for some k, 0 $\leq$ k $<$ n\},
\end{flushleft}


\begin{flushleft}
i.e., all vectors whose first nonzero coefficient (if any) is positive.
\end{flushleft}


\begin{flushleft}
(a) Verify that Klex is a cone, but not a proper cone.
\end{flushleft}


\begin{flushleft}
(b) We define the lexicographic ordering on Rn as follows: x $\leq$lex y if and only if
\end{flushleft}


\begin{flushleft}
y $-$ x $\in$ Klex . (Since Klex is not a proper cone, the lexicographic ordering is not a
\end{flushleft}


\begin{flushleft}
generalized inequality.) Show that the lexicographic ordering is a linear ordering:
\end{flushleft}


\begin{flushleft}
for any x, y $\in$ Rn , either x $\leq$lex y or y $\leq$lex x. Therefore any set of vectors can be
\end{flushleft}


\begin{flushleft}
sorted with respect to the lexicographic cone, which yields the familiar sorting used
\end{flushleft}


\begin{flushleft}
in dictionaries.
\end{flushleft}


∗


.


\begin{flushleft}
(c) Find Klex
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Klex is not closed. For example, ( , $-$1, 0, . . . , 0) $\in$ Klex for all
\end{flushleft}


= 0.





\begin{flushleft}
$>$ 0, but not for
\end{flushleft}





\begin{flushleft}
(b) If x = y then x $\leq$lex y and y $\leq$lex x. If not, let k = min\{i $\in$ \{1, . . . , n\} | xi = yi \},
\end{flushleft}


\begin{flushleft}
be the index of the first component in which x and y differ. If xk $<$ yk , we have
\end{flushleft}


\begin{flushleft}
x $\leq$lex y. If xk $>$ yk , we have x $\geq$lex y.
\end{flushleft}


∗


\begin{flushleft}
= R+ e1 = \{(t, 0, . . . , 0) | t $\geq$ 0\}. To prove this, first note that if y = (t, 0, . . . , 0)
\end{flushleft}


\begin{flushleft}
(c) Klex
\end{flushleft}


\begin{flushleft}
with t $\geq$ 0, then obviously y T x = tx1 $\geq$ 0 for all x $\in$ Klex .
\end{flushleft}


\begin{flushleft}
Conversely, suppose y T x $\geq$ 0 for all x $\in$ Klex . In particular y T e1 $\geq$ 0, so y1 $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Furthermore, by considering x = ( , $-$1, 0, . . . , 0), we have y1 $-$ y2 $\geq$ 0 for all $>$ 0,
\end{flushleft}


\begin{flushleft}
which is only possible if y2 = 0. Similarly, one can prove that y3 = · · · = yn = 0.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
2.35 Copositive matrices. A matrix X $\in$ Sn is called copositive if z T Xz $\geq$ 0 for all z
\end{flushleft}


0.


\begin{flushleft}
Verify that the set of copositive matrices is a proper cone. Find its dual cone.
\end{flushleft}


\begin{flushleft}
Solution. We denote by K the set of copositive matrices in Sn . K is a closed convex
\end{flushleft}


\begin{flushleft}
cone because it is the intersection of (infinitely many) halfspaces defined by homogeneous
\end{flushleft}


\begin{flushleft}
inequalities
\end{flushleft}


\begin{flushleft}
zi zj Xij $\geq$ 0.
\end{flushleft}


\begin{flushleft}
z T Xz =
\end{flushleft}


\begin{flushleft}
i,j
\end{flushleft}





\begin{flushleft}
K has nonempty interior, because it includes the cone of positive semidefinite matrices,
\end{flushleft}


\begin{flushleft}
which has nonempty interior. K is pointed because X $\in$ K, $-$X $\in$ K means z T Xz = 0
\end{flushleft}


\begin{flushleft}
for all z 0, hence X = 0.
\end{flushleft}


\begin{flushleft}
By definition, the dual cone of a cone K is the set of normal vectors of all homogeneous
\end{flushleft}


\begin{flushleft}
halfspaces containing K (plus the origin). Therefore,
\end{flushleft}


\begin{flushleft}
K ∗ = conv\{zz T | z
\end{flushleft}





0\}.





\begin{flushleft}
2.36 Euclidean distance matrices. Let x1 , . . . , xn $\in$ Rk . The matrix D $\in$ Sn defined by Dij =
\end{flushleft}


\begin{flushleft}
xi $-$ xj 22 is called a Euclidean distance matrix. It satisfies some obvious properties such
\end{flushleft}


1/2


1/2


1/2


\begin{flushleft}
as Dij = Dji , Dii = 0, Dij $\geq$ 0, and (from the triangle inequality) Dik $\leq$ Dij + Djk .
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
We now pose the question: When is a matrix D $\in$ S a Euclidean distance matrix (for
\end{flushleft}


\begin{flushleft}
some points in Rk , for some k)? A famous result answers this question: D $\in$ Sn is a
\end{flushleft}


\begin{flushleft}
Euclidean distance matrix if and only if Dii = 0 and xT Dx $\leq$ 0 for all x with 1T x = 0.
\end{flushleft}


\begin{flushleft}
(See §8.3.3.)
\end{flushleft}


\begin{flushleft}
Show that the set of Euclidean distance matrices is a convex cone. Find the dual cone.
\end{flushleft}


\begin{flushleft}
Solution. The set of Euclidean distance matrices in Sn is a closed convex cone because
\end{flushleft}


\begin{flushleft}
it is the intersection of (infinitely many) halfspaces defined by the following homogeneous
\end{flushleft}


\begin{flushleft}
inequalities:
\end{flushleft}


\begin{flushleft}
eTi Dei $\leq$ 0,
\end{flushleft}





\begin{flushleft}
eTi Dei $\geq$ 0,
\end{flushleft}





\begin{flushleft}
xT Dx =
\end{flushleft}


\begin{flushleft}
j,k
\end{flushleft}





\begin{flushleft}
xj xk Djk $\leq$ 0,
\end{flushleft}





\begin{flushleft}
for all i = 1, . . . , n, and all x with 1T x = 1.
\end{flushleft}


\begin{flushleft}
It follows that dual cone is given by
\end{flushleft}


\begin{flushleft}
K ∗ = conv(\{$-$xxT | 1T x = 1\}
\end{flushleft}





\begin{flushleft}
\{e1 eT1 , $-$e1 eT1 , . . . , en eTn , $-$en eTn \}).
\end{flushleft}





\begin{flushleft}
This can be made more explicit as follows. Define V $\in$ Rn×(n$-$1) as
\end{flushleft}


\begin{flushleft}
Vij =
\end{flushleft}





\begin{flushleft}
1 $-$ 1/n
\end{flushleft}


\begin{flushleft}
$-$1/n
\end{flushleft}





\begin{flushleft}
i=j
\end{flushleft}


\begin{flushleft}
i = j.
\end{flushleft}





\begin{flushleft}
The columns of V form a basis for the set of vectors orthogonal to 1, i.e., a vector x
\end{flushleft}


\begin{flushleft}
satisfies 1T x = 0 if and only if x = V y for some y. The dual cone is
\end{flushleft}


\begin{flushleft}
K ∗ = \{V W V T + diag(u) | W
\end{flushleft}





\begin{flushleft}
0, u $\in$ Rn \}.
\end{flushleft}





\begin{flushleft}
2.37 Nonnegative polynomials and Hankel LMIs. Let Kpol be the set of (coefficients of) nonnegative polynomials of degree 2k on R:
\end{flushleft}


\begin{flushleft}
Kpol = \{x $\in$ R2k+1 | x1 + x2 t + x3 t2 + · · · + x2k+1 t2k $\geq$ 0 for all t $\in$ R\}.
\end{flushleft}


\begin{flushleft}
(a) Show that Kpol is a proper cone.
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
(b) A basic result states that a polynomial of degree 2k is nonnegative on R if and only
\end{flushleft}


\begin{flushleft}
if it can be expressed as the sum of squares of two polynomials of degree k or less.
\end{flushleft}


\begin{flushleft}
In other words, x $\in$ Kpol if and only if the polynomial
\end{flushleft}


\begin{flushleft}
p(t) = x1 + x2 t + x3 t2 + · · · + x2k+1 t2k
\end{flushleft}


\begin{flushleft}
can be expressed as
\end{flushleft}





\begin{flushleft}
p(t) = r(t)2 + s(t)2 ,
\end{flushleft}


\begin{flushleft}
where r and s are polynomials of degree k.
\end{flushleft}


\begin{flushleft}
Use this result to show that
\end{flushleft}


\begin{flushleft}
Kpol =
\end{flushleft}





\begin{flushleft}
x $\in$ R2k+1
\end{flushleft}





\begin{flushleft}
xi =
\end{flushleft}


\begin{flushleft}
m+n=i+1
\end{flushleft}





\begin{flushleft}
Ymn for some Y $\in$ Sk+1
\end{flushleft}


+





.





\begin{flushleft}
In other words, p(t) = x1 + x2 t + x3 t2 + · · · + x2k+1 t2k is nonnegative if and only if
\end{flushleft}


\begin{flushleft}
there exists a matrix Y $\in$ Sk+1
\end{flushleft}


\begin{flushleft}
such that
\end{flushleft}


+


\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x3
\end{flushleft}





=


=


=


..


.


=





\begin{flushleft}
x2k+1
\end{flushleft}


\begin{flushleft}
(c) Show that
\end{flushleft}





∗


\begin{flushleft}
Kpol
\end{flushleft}





\begin{flushleft}
Y11
\end{flushleft}


\begin{flushleft}
Y12 + Y21
\end{flushleft}


\begin{flushleft}
Y13 + Y22 + Y31
\end{flushleft}





\begin{flushleft}
Yk+1,k+1 .
\end{flushleft}





\begin{flushleft}
= Khan where
\end{flushleft}


\begin{flushleft}
Khan = \{z $\in$ R2k+1 | H(z)
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
H(z) = 
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
z1
\end{flushleft}


\begin{flushleft}
z2
\end{flushleft}


\begin{flushleft}
z3
\end{flushleft}


..


.


\begin{flushleft}
zk
\end{flushleft}


\begin{flushleft}
zk+1
\end{flushleft}





\begin{flushleft}
z2
\end{flushleft}


\begin{flushleft}
z3
\end{flushleft}


\begin{flushleft}
z4
\end{flushleft}


..


.





\begin{flushleft}
z3
\end{flushleft}


\begin{flushleft}
z4
\end{flushleft}


\begin{flushleft}
z5
\end{flushleft}


..


.





\begin{flushleft}
zk+1
\end{flushleft}


\begin{flushleft}
zk+2
\end{flushleft}





\begin{flushleft}
zk+2
\end{flushleft}


\begin{flushleft}
zk+3
\end{flushleft}





···


···


···


..


.


···


···





0\}


\begin{flushleft}
zk
\end{flushleft}





\begin{flushleft}
zk+1
\end{flushleft}


\begin{flushleft}
zk+2
\end{flushleft}


..


.


\begin{flushleft}
z2k$-$1
\end{flushleft}


\begin{flushleft}
z2k
\end{flushleft}





\begin{flushleft}
(This is the Hankel matrix with coefficients z1 , . . . , z2k+1 .)
\end{flushleft}





\begin{flushleft}
zk+1
\end{flushleft}


\begin{flushleft}
zk+2
\end{flushleft}


\begin{flushleft}
zk+4
\end{flushleft}


..


.


\begin{flushleft}
z2k
\end{flushleft}


\begin{flushleft}
z2k+1
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
.
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
(d) Let Kmom be the conic hull of the set of all vectors of the form (1, t, t2 , . . . , t2k ),
\end{flushleft}


\begin{flushleft}
where t $\in$ R. Show that y $\in$ Kmom if and only if y1 $\geq$ 0 and
\end{flushleft}


\begin{flushleft}
y = y1 (1, E u, E u2 , . . . , E u2k )
\end{flushleft}


\begin{flushleft}
for some random variable u. In other words, the elements of Kmom are nonnegative
\end{flushleft}


\begin{flushleft}
multiples of the moment vectors of all possible distributions on R. Show that Kpol =
\end{flushleft}


∗


\begin{flushleft}
Kmom
\end{flushleft}


.


\begin{flushleft}
(e) Combining the results of (c) and (d), conclude that Khan = cl Kmom .
\end{flushleft}


\begin{flushleft}
As an example illustrating the relation between Kmom and Khan , take k = 2 and
\end{flushleft}


\begin{flushleft}
z = (1, 0, 0, 0, 1). Show that z $\in$ Khan , z $\in$ Kmom . Find an explicit sequence of
\end{flushleft}


\begin{flushleft}
points in Kmom which converge to z.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) It is a closed convex cone, because it is the intersection of (infinitely many) closed
\end{flushleft}


\begin{flushleft}
halfspaces, and also obviously a cone.
\end{flushleft}


\begin{flushleft}
It has nonempty interior because (1, 0, 1, 0, . . . , 0, 1) $\in$ int Kpol (i.e., the polynomial
\end{flushleft}


\begin{flushleft}
1 + t2 + t4 + · · · + t2k ). It is pointed because p(t) $\geq$ 0 and $-$p(t) $\geq$ 0 imply p(t) = 0.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) First assume that xi =
\end{flushleft}


\begin{flushleft}
all t $\in$ R,
\end{flushleft}





\begin{flushleft}
m+n=i+1
\end{flushleft}





\begin{flushleft}
Ymn for some Y
\end{flushleft}





\begin{flushleft}
0. It easily verified that, for
\end{flushleft}





\begin{flushleft}
2k+1
\end{flushleft}





\begin{flushleft}
p(t) = x1 + x2 t + · · · + x2k+1 t2k
\end{flushleft}





\begin{flushleft}
Ymn ti$-$1
\end{flushleft}





=


\begin{flushleft}
i=1 m+n=i+1
\end{flushleft}


\begin{flushleft}
k+1
\end{flushleft}





\begin{flushleft}
Ymn tm+n$-$2
\end{flushleft}





=


\begin{flushleft}
m,n=1
\end{flushleft}


\begin{flushleft}
k+1
\end{flushleft}





\begin{flushleft}
Ymn tm$-$1 tn$-$1
\end{flushleft}





=


\begin{flushleft}
m,n=1
\end{flushleft}





=





\begin{flushleft}
vT Y v
\end{flushleft}





\begin{flushleft}
where v = (1, t, t2 , . . . , tk ). Therefore p(t) $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Conversely, assume x $\in$ Kpol . By the theorem, we can express the corresponding
\end{flushleft}


\begin{flushleft}
polynomial p(t) as p(t) = r(t)2 + s(t)2 , where
\end{flushleft}


\begin{flushleft}
r(t) = a1 + a2 t + · · · + ak+1 tk ,
\end{flushleft}





\begin{flushleft}
The coefficient of ti$-$1 in r(t)2 + s(t)2 is
\end{flushleft}





\begin{flushleft}
s(t) = b1 + b2 t + · · · + bk+1 tk ,
\end{flushleft}


\begin{flushleft}
m+n=i+1
\end{flushleft}





\begin{flushleft}
(am an + bm bn ). Therefore,
\end{flushleft}


\begin{flushleft}
Ymn
\end{flushleft}





\begin{flushleft}
(am an + bm bn ) =
\end{flushleft}





\begin{flushleft}
xi =
\end{flushleft}





\begin{flushleft}
m+n=i+1
\end{flushleft}





\begin{flushleft}
m+n=i+1
\end{flushleft}





\begin{flushleft}
for Y = aaT + bbT .
\end{flushleft}


∗


\begin{flushleft}
if and only if xT z $\geq$ 0 for all x $\in$ Kpol . Using the previous result, this is
\end{flushleft}


\begin{flushleft}
(c) z $\in$ Kpol
\end{flushleft}


\begin{flushleft}
equivalent to the condition that for all Y
\end{flushleft}


0,


\begin{flushleft}
k+1
\end{flushleft}





\begin{flushleft}
2k+1
\end{flushleft}





\begin{flushleft}
Ymn =
\end{flushleft}





\begin{flushleft}
zi
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i.e., H(z)
\end{flushleft}





\begin{flushleft}
m+n=i+1
\end{flushleft}





\begin{flushleft}
m,n=1
\end{flushleft}





\begin{flushleft}
Ymn zm+n$-$1 = tr(Y H(z)) $\geq$ 0,
\end{flushleft}





0.





\begin{flushleft}
(d) The conic hull of the vectors of the form (1, t, . . . , t2k ) is the set of nonnegative multiples of all convex combinations of vectors of the form (1, t, . . . , t2k ), i.e., nonnegative
\end{flushleft}


\begin{flushleft}
multiples of vectors of the form
\end{flushleft}


\begin{flushleft}
E(1, t, t2 , . . . , t2k ).
\end{flushleft}


\begin{flushleft}
xT z $\geq$ 0 for all z $\in$ Kmom if and only if
\end{flushleft}





\begin{flushleft}
E(x1 + x2 t + x3 t2 + · · · + x2k+1 t2k ) $\geq$ 0
\end{flushleft}





\begin{flushleft}
for all distributions on R. This is true if and only if
\end{flushleft}


\begin{flushleft}
x1 + x2 t + x3 t2 + · · · + x2k+1 t2k $\geq$ 0
\end{flushleft}


\begin{flushleft}
for all t.
\end{flushleft}


\begin{flushleft}
(e) This follows from the last result in §2.6.1, and the fact that we have shown that
\end{flushleft}


∗


∗∗


\begin{flushleft}
Khan = Kpol
\end{flushleft}


\begin{flushleft}
= Kmom
\end{flushleft}


.


\begin{flushleft}
For the example, note that E t2 = 0 means that the distribution concentrates probability one at t = 0. But then we cannot have E t4 = 1. The associated Hankel
\end{flushleft}


\begin{flushleft}
matrix is H = diag(1, 0, 1), which is clearly positive semidefinite.
\end{flushleft}


\begin{flushleft}
Let's put probability pk at t = 0, and (1 $-$ pk )/2 at each of the points t = $\pm$k.
\end{flushleft}


\begin{flushleft}
Then we have, for all k, E t = E t3 = 0. We also have E t2 = (1 $-$ pk )k 2 and
\end{flushleft}


\begin{flushleft}
E t4 = (1 $-$ pk )k 4 . Let's now choose pk = 1 $-$ 1/k 4 , so we have E t4 = 1, and
\end{flushleft}


\begin{flushleft}
E t2 = 1/k 2 . Thus, the moments of this sequence of measures converge to 1, 0, 0, 1.
\end{flushleft}





\newpage
2





\begin{flushleft}
Convex sets
\end{flushleft}





\begin{flushleft}
2.38 [Roc70, pages 15, 61] Convex cones constructed from sets.
\end{flushleft}


\begin{flushleft}
(a) The barrier cone of a set C is defined as the set of all vectors y such that y T x is
\end{flushleft}


\begin{flushleft}
bounded above over x $\in$ C. In other words, a nonzero vector y is in the barrier cone
\end{flushleft}


\begin{flushleft}
if and only if it is the normal vector of a halfspace \{x | y T x $\leq$ $\alpha$\} that contains C.
\end{flushleft}


\begin{flushleft}
Verify that the barrier cone is a convex cone (with no assumptions on C).
\end{flushleft}


\begin{flushleft}
Solution. Take two points x1 , x2 in the barrier cone. We have
\end{flushleft}


\begin{flushleft}
sup xT1 y $<$ $\infty$,
\end{flushleft}





\begin{flushleft}
y$\in$C
\end{flushleft}





\begin{flushleft}
sup xT2 y $<$ $\infty$,
\end{flushleft}





\begin{flushleft}
y$\in$C
\end{flushleft}





\begin{flushleft}
so for all $\theta$1 , $\theta$2 $\geq$ 0,
\end{flushleft}


\begin{flushleft}
sup ($\theta$1 x1 + $\theta$2 x2 )T y $\leq$ sup ($\theta$1 xT1 y) + sup ($\theta$2 xT2 y) $<$ $\infty$.
\end{flushleft}





\begin{flushleft}
y$\in$C
\end{flushleft}





\begin{flushleft}
y$\in$C
\end{flushleft}





\begin{flushleft}
y$\in$C
\end{flushleft}





\begin{flushleft}
Therefore $\theta$x1 + $\theta$2 x2 is also in the barrier cone.
\end{flushleft}


\begin{flushleft}
(b) The recession cone (also called asymptotic cone) of a set C is defined as the set of
\end{flushleft}


\begin{flushleft}
all vectors y such that for each x $\in$ C, x $-$ ty $\in$ C for all t $\geq$ 0. Show that the
\end{flushleft}


\begin{flushleft}
recession cone of a convex set is a convex cone. Show that if C is nonempty, closed,
\end{flushleft}


\begin{flushleft}
and convex, then the recession cone of C is the dual of the barrier cone.
\end{flushleft}


\begin{flushleft}
Solution. It is clear that the recession cone is a cone. We show that it is convex if
\end{flushleft}


\begin{flushleft}
C is convex.
\end{flushleft}


\begin{flushleft}
Let y1 , y2 be in the recession cone, and suppose 0 $\leq$ $\theta$ $\leq$ 1. Then if x $\in$ C
\end{flushleft}


\begin{flushleft}
x $-$ t($\theta$y1 + (1 $-$ $\theta$)y2 ) = $\theta$(x $-$ ty1 ) + (1 $-$ $\theta$)(x $-$ ty2 ) $\in$ C,
\end{flushleft}


\begin{flushleft}
for all t $\geq$ 0, because C is convex and x $-$ ty1 $\in$ C, x $-$ ty2 $\in$ C for all t $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Therefore $\theta$y1 + (1 $-$ $\theta$)y2 is in the recession cone.
\end{flushleft}


\begin{flushleft}
Before establishing the second claim, we note that if C is closed and convex, then
\end{flushleft}


\begin{flushleft}
its recession cone RC can be defined by choosing any arbitrary point x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ C, and
\end{flushleft}


\begin{flushleft}
letting
\end{flushleft}


\begin{flushleft}
RC = \{y | x
\end{flushleft}


\begin{flushleft}
ˆ $-$ ty $\in$ C $\forall$t $\geq$ 0\}.
\end{flushleft}


\begin{flushleft}
This follows from the following observation. For x $\in$ C, define
\end{flushleft}


\begin{flushleft}
RC (x) = \{y | x $-$ ty $\in$ C $\forall$t $\geq$ 0\}.
\end{flushleft}


\begin{flushleft}
We want to show that RC (x1 ) = RC (x2 ) for any x1 , x2 $\in$ C. We first show RC (x1 ) $\subseteq$
\end{flushleft}


\begin{flushleft}
RC (x2 ). If y $\in$ RC (x1 ), then x1 $-$(t/$\theta$)y $\in$ C for all t $\geq$ 0, 0 $<$ $\theta$ $<$ 1, so by convexity
\end{flushleft}


\begin{flushleft}
of C,
\end{flushleft}


\begin{flushleft}
$\theta$(x1 $-$ (t/$\theta$)y) + (1 $-$ $\theta$)x2 $\in$ C.
\end{flushleft}


\begin{flushleft}
Since C is closed,
\end{flushleft}





\begin{flushleft}
x2 $-$ ty = lim ($\theta$(x1 $-$ (t/$\theta$)y) + (1 $-$ $\theta$)x2 ) $\in$ C.
\end{flushleft}


\begin{flushleft}
$\theta$
\end{flushleft}





0





\begin{flushleft}
This holds for any t $\geq$ 0, i.e., y $\in$ RC (x2 ). The reverse inclusion RC (x2 ) $\subseteq$ RC (x1 )
\end{flushleft}


\begin{flushleft}
follows similarly.
\end{flushleft}


\begin{flushleft}
We now show that the recession cone is the dual of the barrier cone. Let SC (y) =
\end{flushleft}


\begin{flushleft}
supx$\in$C y T x. By definition of the barrier cone, SC (y) is finite if and only if y is in
\end{flushleft}


\begin{flushleft}
the barrier cone, and every halfspace that contains C can be expressed as
\end{flushleft}


\begin{flushleft}
y T x $\leq$ SC (y)
\end{flushleft}


\begin{flushleft}
for some nonzero y in the barrier cone. A closed convex set C is the intersection of
\end{flushleft}


\begin{flushleft}
all halfspaces that contain it. Therefore
\end{flushleft}


\begin{flushleft}
C = \{x | y T x $\leq$ SC (y) for all y $\in$ BC \},
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Let x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ C. A vector v is in the recession cone if and only if x
\end{flushleft}


\begin{flushleft}
ˆ $-$ tv $\in$ C for all t $\geq$ 0,
\end{flushleft}


\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
y T (ˆ
\end{flushleft}


\begin{flushleft}
x $-$ tv) $\leq$ SC (y) for all y $\in$ BC .
\end{flushleft}





\begin{flushleft}
This is true if and only if y T v $\geq$ 0 for all y $\in$ BC , i.e., if and only if v is in the dual
\end{flushleft}


\begin{flushleft}
cone of BC .
\end{flushleft}


\begin{flushleft}
(c) The normal cone of a set C at a boundary point x0 is the set of all vectors y such
\end{flushleft}


\begin{flushleft}
that y T (x $-$ x0 ) $\leq$ 0 for all x $\in$ C (i.e., the set of vectors that define a supporting
\end{flushleft}


\begin{flushleft}
hyperplane to C at x0 ). Show that the normal cone is a convex cone (with no
\end{flushleft}


\begin{flushleft}
assumptions on C). Give a simple description of the normal cone of a polyhedron
\end{flushleft}


\begin{flushleft}
\{x | Ax b\} at a point in its boundary.
\end{flushleft}


\begin{flushleft}
Solution. The normal cone is defined by a set of homogeneous linear inequalities
\end{flushleft}


\begin{flushleft}
in y, so it is a closed convex cone.
\end{flushleft}


\begin{flushleft}
Let x0 be a boundary point of \{x | Ax b\}. Suppose A and b are partitioned as
\end{flushleft}


\begin{flushleft}
A=
\end{flushleft}





\begin{flushleft}
AT1
\end{flushleft}


\begin{flushleft}
AT2
\end{flushleft}





,





\begin{flushleft}
b=
\end{flushleft}





\begin{flushleft}
b1
\end{flushleft}


\begin{flushleft}
b2
\end{flushleft}





\begin{flushleft}
in such a way that
\end{flushleft}


\begin{flushleft}
A 2 x0 ≺ b 2 .
\end{flushleft}





\begin{flushleft}
A 1 x0 = b 1 ,
\end{flushleft}


\begin{flushleft}
Then the normal at x0 is
\end{flushleft}





\begin{flushleft}
\{AT1 $\lambda$ | $\lambda$
\end{flushleft}





0\},





\begin{flushleft}
i.e., it is the conic hull of the normal vectors of the constraints that are active at x 0 .
\end{flushleft}


\begin{flushleft}
˜ be two convex cones whose interiors are nonempty and
\end{flushleft}


\begin{flushleft}
2.39 Separation of cones. Let K and K
\end{flushleft}


˜ ∗.


\begin{flushleft}
disjoint. Show that there is a nonzero y such that y $\in$ K ∗ , $-$y $\in$ K
\end{flushleft}


\begin{flushleft}
Solution. Let y = 0 be the normal vector of a separating hyperplane separating the
\end{flushleft}


\begin{flushleft}
interiors: y T x $\geq$ $\alpha$ for x $\in$ int K1 and y T x $\leq$ $\alpha$ for x $\in$ int K2 . We must have $\alpha$ = 0
\end{flushleft}


\begin{flushleft}
because K1 and K2 are cones, so if x $\in$ int K1 , then tx $\in$ int K1 for all t $>$ 0.
\end{flushleft}


\begin{flushleft}
This means that
\end{flushleft}


\begin{flushleft}
y $\in$ (int K1 )∗ = K1∗ ,
\end{flushleft}





\begin{flushleft}
$-$y $\in$ (int K2 )∗ = K2∗ .
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 3
\end{flushleft}





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Definition of convexity
\end{flushleft}


\begin{flushleft}
3.1 Suppose f : R $\rightarrow$ R is convex, and a, b $\in$ dom f with a $<$ b.
\end{flushleft}


\begin{flushleft}
(a) Show that
\end{flushleft}


\begin{flushleft}
f (x) $\leq$
\end{flushleft}





\begin{flushleft}
x$-$a
\end{flushleft}


\begin{flushleft}
b$-$x
\end{flushleft}


\begin{flushleft}
f (a) +
\end{flushleft}


\begin{flushleft}
f (b)
\end{flushleft}


\begin{flushleft}
b$-$a
\end{flushleft}


\begin{flushleft}
b$-$a
\end{flushleft}





\begin{flushleft}
for all x $\in$ [a, b].
\end{flushleft}


\begin{flushleft}
Solution. This is Jensen's inequality with $\lambda$ = (b $-$ x)/(b $-$ a).
\end{flushleft}





\begin{flushleft}
(b) Show that
\end{flushleft}





\begin{flushleft}
f (x) $-$ f (a)
\end{flushleft}


\begin{flushleft}
f (b) $-$ f (a)
\end{flushleft}


\begin{flushleft}
f (b) $-$ f (x)
\end{flushleft}


$\leq$


$\leq$


\begin{flushleft}
x$-$a
\end{flushleft}


\begin{flushleft}
b$-$a
\end{flushleft}


\begin{flushleft}
b$-$x
\end{flushleft}


\begin{flushleft}
for all x $\in$ (a, b). Draw a sketch that illustrates this inequality.
\end{flushleft}


\begin{flushleft}
Solution. We obtain the first inequality by subtracting f (a) from both sides of the
\end{flushleft}


\begin{flushleft}
inequality in (a). The second inequality follows from subtracting f (b). Geometrically, the inequalities mean that the slope of the line segment between (a, f (a)) and
\end{flushleft}


\begin{flushleft}
(b, f (b)) is larger than the slope of the segment between (a, f (a)) and (x, f (x)), and
\end{flushleft}


\begin{flushleft}
smaller than the slope of the segment between (x, f (x)) and (b, f (b)).
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
(c) Suppose f is differentiable. Use the result in (b) to show that
\end{flushleft}


\begin{flushleft}
f (a) $\leq$
\end{flushleft}





\begin{flushleft}
f (b) $-$ f (a)
\end{flushleft}


\begin{flushleft}
$\leq$ f (b).
\end{flushleft}


\begin{flushleft}
b$-$a
\end{flushleft}





\begin{flushleft}
Note that these inequalities also follow from (3.2):
\end{flushleft}


\begin{flushleft}
f (b) $\geq$ f (a) + f (a)(b $-$ a),
\end{flushleft}





\begin{flushleft}
f (a) $\geq$ f (b) + f (b)(a $-$ b).
\end{flushleft}





\begin{flushleft}
Solution. This follows from (b) by taking the limit for x $\rightarrow$ a on both sides of
\end{flushleft}


\begin{flushleft}
the first inequality, and by taking the limit for x $\rightarrow$ b on both sides of the second
\end{flushleft}


\begin{flushleft}
inequality.
\end{flushleft}


\begin{flushleft}
(d) Suppose f is twice differentiable. Use the result in (c) to show that f (a) $\geq$ 0 and
\end{flushleft}


\begin{flushleft}
f (b) $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Solution. From part (c),
\end{flushleft}


\begin{flushleft}
f (b) $-$ f (a)
\end{flushleft}


$\geq$ 0,


\begin{flushleft}
b$-$a
\end{flushleft}


\begin{flushleft}
and taking the limit for b $\rightarrow$ a shows that f (a) $\geq$ 0.
\end{flushleft}





\begin{flushleft}
3.2 Level sets of convex, concave, quasiconvex, and quasiconcave functions. Some level sets
\end{flushleft}


\begin{flushleft}
of a function f are shown below. The curve labeled 1 shows \{x | f (x) = 1\}, etc.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





3


2


1





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
Could f be convex (concave, quasiconvex, quasiconcave)? Explain your answer. Repeat
\end{flushleft}


\begin{flushleft}
for the level curves shown below.
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}





1 2 3





4





5





6





\begin{flushleft}
Solution. The first function could be quasiconvex because the sublevel sets appear to be
\end{flushleft}


\begin{flushleft}
convex. It is definitely not concave or quasiconcave because the superlevel sets are not
\end{flushleft}


\begin{flushleft}
convex.
\end{flushleft}


\begin{flushleft}
It is also not convex, for the following reason. We plot the function values along the
\end{flushleft}


\begin{flushleft}
dashed line labeled I.
\end{flushleft}


3


2


1





\begin{flushleft}
I
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
II
\end{flushleft}


\begin{flushleft}
Along this line the function passes through the points marked as black dots in the figure
\end{flushleft}


\begin{flushleft}
below. Clearly along this line segment, the function is not convex.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





3


2


1





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
If we repeat the same analysis for the second function, we see that it could be concave
\end{flushleft}


\begin{flushleft}
(and therefore it could be quasiconcave). It cannot be convex or quasiconvex, because
\end{flushleft}


\begin{flushleft}
the sublevel sets are not convex.
\end{flushleft}


\begin{flushleft}
3.3 Inverse of an increasing convex function. Suppose f : R $\rightarrow$ R is increasing and convex
\end{flushleft}


\begin{flushleft}
on its domain (a, b). Let g denote its inverse, i.e., the function with domain (f (a), f (b))
\end{flushleft}


\begin{flushleft}
and g(f (x)) = x for a $<$ x $<$ b. What can you say about convexity or concavity of g?
\end{flushleft}


\begin{flushleft}
Solution. g is concave. Its hypograph is
\end{flushleft}


\begin{flushleft}
hypo g
\end{flushleft}





=


=


=





\begin{flushleft}
\{(y, t) | t $\leq$ g(y)\}
\end{flushleft}


\begin{flushleft}
\{(y, t) | f (t) $\leq$ f (g(y))\}
\end{flushleft}


\begin{flushleft}
\{(y, t) | f (t) $\leq$ y)\}
\end{flushleft}


0


1





=





1


0





\begin{flushleft}
(because f is increasing)
\end{flushleft}





\begin{flushleft}
epi f.
\end{flushleft}





\begin{flushleft}
For differentiable g, f , we can also prove the result as follows. Differentiate g(f (x)) = x
\end{flushleft}


\begin{flushleft}
once to get
\end{flushleft}


\begin{flushleft}
g (f (x)) = 1/f (x).
\end{flushleft}


\begin{flushleft}
so g is increasing. Differentiate again to get
\end{flushleft}


\begin{flushleft}
g (f (x)) = $-$
\end{flushleft}





\begin{flushleft}
f (x)
\end{flushleft}


,


\begin{flushleft}
f (x)3
\end{flushleft}





\begin{flushleft}
so g is concave.
\end{flushleft}


\begin{flushleft}
3.4 [RV73, page 15] Show that a continuous function f : Rn $\rightarrow$ R is convex if and only if for
\end{flushleft}


\begin{flushleft}
every line segment, its average value on the segment is less than or equal to the average
\end{flushleft}


\begin{flushleft}
of its values at the endpoints of the segment: For every x, y $\in$ Rn ,
\end{flushleft}


1


0





\begin{flushleft}
f (x + $\lambda$(y $-$ x)) d$\lambda$ $\leq$
\end{flushleft}





\begin{flushleft}
f (x) + f (y)
\end{flushleft}


.


2





\begin{flushleft}
Solution. First suppose that f is convex. Jensen's inequality can be written as
\end{flushleft}


\begin{flushleft}
f (x + $\lambda$(y $-$ x)) $\leq$ f (x) + $\lambda$(f (y) $-$ f (x))
\end{flushleft}


\begin{flushleft}
for 0 $\leq$ $\lambda$ $\leq$ 1. Integrating both sides from 0 to 1 we get
\end{flushleft}


1





1


0





\begin{flushleft}
f (x + $\lambda$(y $-$ x)) d$\lambda$ $\leq$
\end{flushleft}





0





\begin{flushleft}
(f (x) + $\lambda$(f (y) $-$ f (x))) d$\lambda$ =
\end{flushleft}





\begin{flushleft}
f (x) + f (y)
\end{flushleft}


.


2





\begin{flushleft}
Now we show the converse. Suppose f is not convex. Then there are x and y and
\end{flushleft}


\begin{flushleft}
$\theta$0 $\in$ (0, 1) such that
\end{flushleft}


\begin{flushleft}
f ($\theta$0 x + (1 $-$ $\theta$0 )y) $>$ $\theta$0 f (x) + (1 $-$ $\theta$0 )f (y).
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
Consider the function of $\theta$ given by
\end{flushleft}


\begin{flushleft}
F ($\theta$) = f ($\theta$x + (1 $-$ $\theta$)y) $-$ $\theta$f (x) $-$ (1 $-$ $\theta$)f (y),
\end{flushleft}


\begin{flushleft}
which is continuous since f is. Note that F is zero for $\theta$ = 0 and $\theta$ = 1, and positive at $\theta$ 0 .
\end{flushleft}


\begin{flushleft}
Let $\alpha$ be the largest zero crossing of F below $\theta$0 and let $\beta$ be the smallest zero crossing
\end{flushleft}


\begin{flushleft}
of F above $\theta$0 . Define u = $\alpha$x + (1 $-$ $\alpha$)y and v = $\beta$x + (1 $-$ $\beta$)y. On the interval ($\alpha$, $\beta$),
\end{flushleft}


\begin{flushleft}
we have
\end{flushleft}


\begin{flushleft}
F ($\theta$) = f ($\theta$x + (1 $-$ $\theta$)y) $>$ $\theta$f (x) + (1 $-$ $\theta$)f (y),
\end{flushleft}


\begin{flushleft}
so for $\theta$ $\in$ (0, 1),
\end{flushleft}


\begin{flushleft}
f ($\theta$u + (1 $-$ $\theta$)v) $>$ $\theta$f (u) + (1 $-$ $\theta$)f (v).
\end{flushleft}


\begin{flushleft}
Integrating this expression from $\theta$ = 0 to $\theta$ = 1 yields
\end{flushleft}


1





1


0





\begin{flushleft}
f (u + $\theta$(u $-$ v)) d$\theta$ $>$
\end{flushleft}





0





\begin{flushleft}
(f (u) + $\theta$(f (u) $-$ f (v))) d$\theta$ =
\end{flushleft}





\begin{flushleft}
f (u) + f (v)
\end{flushleft}


.


2





\begin{flushleft}
In other words, the average of f over the interval [u, v] exceeds the average of its values
\end{flushleft}


\begin{flushleft}
at the endpoints. This proves the converse.
\end{flushleft}


\begin{flushleft}
3.5 [RV73, page 22] Running average of a convex function. Suppose f : R $\rightarrow$ R is convex,
\end{flushleft}


\begin{flushleft}
with R+ $\subseteq$ dom f . Show that its running average F , defined as
\end{flushleft}


\begin{flushleft}
F (x) =
\end{flushleft}





1


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
f (t) dt,
\end{flushleft}





\begin{flushleft}
dom F = R++ ,
\end{flushleft}





0





\begin{flushleft}
is convex. You can assume f is differentiable.
\end{flushleft}


\begin{flushleft}
Solution. F is differentiable with
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
F (x)
\end{flushleft}


\begin{flushleft}
F (x)
\end{flushleft}





=


=





\begin{flushleft}
$-$(1/x2 )
\end{flushleft}





\begin{flushleft}
f (t) dt + f (x)/x
\end{flushleft}


0


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
f (t) dt $-$ 2f (x)/x2 + f (x)/x
\end{flushleft}





\begin{flushleft}
(2/x3 )
\end{flushleft}


0


\begin{flushleft}
x
\end{flushleft}





=





\begin{flushleft}
(2/x3 )
\end{flushleft}


0





\begin{flushleft}
(f (t) $-$ f (x) $-$ f (x)(t $-$ x)) dt.
\end{flushleft}





\begin{flushleft}
Convexity now follows from the fact that
\end{flushleft}


\begin{flushleft}
f (t) $\geq$ f (x) + f (x)(t $-$ x)
\end{flushleft}


\begin{flushleft}
for all x, t $\in$ dom f , which implies F (x) $\geq$ 0.
\end{flushleft}


\begin{flushleft}
3.6 Functions and epigraphs. When is the epigraph of a function a halfspace? When is the
\end{flushleft}


\begin{flushleft}
epigraph of a function a convex cone? When is the epigraph of a function a polyhedron?
\end{flushleft}


\begin{flushleft}
Solution. If the function is affine, positively homogeneous (f ($\alpha$x) = $\alpha$f (x) for $\alpha$ $\geq$ 0),
\end{flushleft}


\begin{flushleft}
and piecewise-affine, respectively.
\end{flushleft}


\begin{flushleft}
3.7 Suppose f : Rn $\rightarrow$ R is convex with dom f = Rn , and bounded above on Rn . Show that
\end{flushleft}


\begin{flushleft}
f is constant.
\end{flushleft}


\begin{flushleft}
Solution. Suppose f is not constant, i.e., there exist x, y with f (x) $<$ f (y). The function
\end{flushleft}


\begin{flushleft}
g(t) = f (x + t(y $-$ x))
\end{flushleft}


\begin{flushleft}
is convex, with g(0) $<$ g(1). By Jensen's inequality
\end{flushleft}


\begin{flushleft}
g(1) $\leq$
\end{flushleft}





1


\begin{flushleft}
t$-$1
\end{flushleft}


\begin{flushleft}
g(0) + g(t)
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
for all t $>$ 1, and therefore
\end{flushleft}


\begin{flushleft}
g(t) $\geq$ tg(1) $-$ (t $-$ 1)g(0) = g(0) + t(g(1) $-$ g(0)),
\end{flushleft}


\begin{flushleft}
so g grows unboundedly as t $\rightarrow$ $\infty$. This contradicts our assumption that f is bounded.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
3.8 Second-order condition for convexity. Prove that a twice differentiable function f is convex
\end{flushleft}


\begin{flushleft}
if and only if its domain is convex and $\nabla$2 f (x) 0 for all x $\in$ dom f . Hint. First consider
\end{flushleft}


\begin{flushleft}
the case f : R $\rightarrow$ R. You can use the first-order condition for convexity (which was proved
\end{flushleft}


\begin{flushleft}
on page 70).
\end{flushleft}


\begin{flushleft}
Solution. We first assume n = 1. Suppose f : R $\rightarrow$ R is convex. Let x, y $\in$ dom f with
\end{flushleft}


\begin{flushleft}
y $>$ x. By the first-order condition,
\end{flushleft}


\begin{flushleft}
f (x)(y $-$ x) $\leq$ f (y) $-$ f (x) $\leq$ f (y)(y $-$ x).
\end{flushleft}


\begin{flushleft}
Subtracting the righthand side from the lefthand side and dividing by (y $-$ x)2 gives
\end{flushleft}


\begin{flushleft}
f (y) $-$ f (x)
\end{flushleft}


$\geq$ 0.


\begin{flushleft}
y$-$x
\end{flushleft}


\begin{flushleft}
Taking the limit for y $\rightarrow$ x yields f (x) $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Conversely, suppose f (z) $\geq$ 0 for all z $\in$ dom f . Consider two arbitrary points x, y $\in$
\end{flushleft}


\begin{flushleft}
dom f with x $<$ y. We have
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}





0





$\leq$


=


=





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
f (z)(y $-$ z) dz
\end{flushleft}





\begin{flushleft}
(f (z)(y $-$ z))
\end{flushleft}





\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
z=y
\end{flushleft}





+


\begin{flushleft}
z=x
\end{flushleft}





\begin{flushleft}
f (z) dz
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
$-$f (x)(y $-$ x) + f (y) $-$ f (x),
\end{flushleft}





\begin{flushleft}
i.e., f (y) $\geq$ f (x) + f (x)(y $-$ x). This shows that f is convex.
\end{flushleft}


\begin{flushleft}
To generalize to n $>$ 1, we note that a function is convex if and only if it is convex on
\end{flushleft}


\begin{flushleft}
all lines, i.e., the function g(t) = f (x0 + tv) is convex in t for all x0 $\in$ dom f and all v.
\end{flushleft}


\begin{flushleft}
Therefore f is convex if and only if
\end{flushleft}


\begin{flushleft}
g (t) = v T $\nabla$2 f (x0 + tv)v $\geq$ 0
\end{flushleft}


\begin{flushleft}
for all x0 $\in$ dom f , v $\in$ Rn , and t satisfying x0 + tv $\in$ dom f . In other words it is
\end{flushleft}


\begin{flushleft}
necessary and sufficient that $\nabla$2 f (x) 0 for all x $\in$ dom f .
\end{flushleft}





\begin{flushleft}
3.9 Second-order conditions for convexity on an affine set. Let F $\in$ Rn×m , x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ Rn . The
\end{flushleft}


\begin{flushleft}
restriction of f : Rn $\rightarrow$ R to the affine set \{F z + x
\end{flushleft}


\begin{flushleft}
ˆ | z $\in$ Rm \} is defined as the function
\end{flushleft}


\begin{flushleft}
f˜ : Rm $\rightarrow$ R with
\end{flushleft}


\begin{flushleft}
f˜(z) = f (F z + x
\end{flushleft}


ˆ),





\begin{flushleft}
dom f˜ = \{z | F z + x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ dom f \}.
\end{flushleft}





\begin{flushleft}
Suppose f is twice differentiable with a convex domain.
\end{flushleft}


\begin{flushleft}
(a) Show that f˜ is convex if and only if for all z $\in$ dom f˜
\end{flushleft}


\begin{flushleft}
F T $\nabla$2 f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ)F
\end{flushleft}





0.





\begin{flushleft}
(b) Suppose A $\in$ Rp×n is a matrix whose nullspace is equal to the range of F , i.e.,
\end{flushleft}


\begin{flushleft}
AF = 0 and rank A = n $-$ rank F . Show that f˜ is convex if and only if for all
\end{flushleft}


\begin{flushleft}
z $\in$ dom f˜ there exists a $\lambda$ $\in$ R such that
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ) + $\lambda$AT A
\end{flushleft}





0.





\begin{flushleft}
Hint. Use the following result: If B $\in$ Sn and A $\in$ Rp×n , then xT Bx $\geq$ 0 for all
\end{flushleft}


\begin{flushleft}
x $\in$ N (A) if and only if there exists a $\lambda$ such that B + $\lambda$AT A 0.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
(a) The Hessian of f˜ must be positive semidefinite everywhere:
\end{flushleft}


\begin{flushleft}
ˆ)F
\end{flushleft}


\begin{flushleft}
$\nabla$2 f˜(z) = F T $\nabla$2 f (F z + x
\end{flushleft}





0.





\begin{flushleft}
(b) The condition in (a) means that v T $\nabla$2 f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ)v $\geq$ 0 for all v with Av = 0, i.e.,
\end{flushleft}


\begin{flushleft}
v T AT Av = 0 =$\Rightarrow$ v T $\nabla$2 f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ)v $\geq$ 0.
\end{flushleft}


\begin{flushleft}
The result immediately follows from the hint.
\end{flushleft}


\begin{flushleft}
3.10 An extension of Jensen's inequality. One interpretation of Jensen's inequality is that
\end{flushleft}


\begin{flushleft}
randomization or dithering hurts, i.e., raises the average value of a convex function: For
\end{flushleft}


\begin{flushleft}
f convex and v a zero mean random variable, we have E f (x0 + v) $\geq$ f (x0 ). This leads
\end{flushleft}


\begin{flushleft}
to the following conjecture. If f0 is convex, then the larger the variance of v, the larger
\end{flushleft}


\begin{flushleft}
E f (x0 + v).
\end{flushleft}


\begin{flushleft}
(a) Give a counterexample that shows that this conjecture is false. Find zero mean
\end{flushleft}


\begin{flushleft}
random variables v and w, with var(v) $>$ var(w), a convex function f , and a point
\end{flushleft}


\begin{flushleft}
x0 , such that E f (x0 + v) $<$ E f (x0 + w).
\end{flushleft}


\begin{flushleft}
(b) The conjecture is true when v and w are scaled versions of each other. Show that
\end{flushleft}


\begin{flushleft}
E f (x0 + tv) is monotone increasing in t $\geq$ 0, when f is convex and v is zero mean.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Define f : R $\rightarrow$ R as
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





0,


\begin{flushleft}
x,
\end{flushleft}





\begin{flushleft}
x$\leq$0
\end{flushleft}


\begin{flushleft}
x $>$ 0,
\end{flushleft}





\begin{flushleft}
x0 = 0, and scalar random variables
\end{flushleft}


\begin{flushleft}
w=
\end{flushleft}





1


$-$1





\begin{flushleft}
with probability 1/2
\end{flushleft}


\begin{flushleft}
with probability 1/2
\end{flushleft}





\begin{flushleft}
v=
\end{flushleft}





4


$-$4/9





\begin{flushleft}
with probability 1/10
\end{flushleft}


\begin{flushleft}
with probability 9/10.
\end{flushleft}





\begin{flushleft}
w and v are zero-mean and
\end{flushleft}


\begin{flushleft}
var(v) = 16/9 $>$ 1 = var(w).
\end{flushleft}


\begin{flushleft}
However,
\end{flushleft}


\begin{flushleft}
E f (v) = 2/5 $<$ 1/2 = E f (w).
\end{flushleft}


\begin{flushleft}
(b) f (x0 +tv) is convex in t for fixed v, hence if v is a random variable, g(t) = E f (x0 +tv)
\end{flushleft}


\begin{flushleft}
is a convex function of t. From Jensen's inequality,
\end{flushleft}


\begin{flushleft}
g(t) = E f (x0 + tv) $\geq$ f (x0 ) = g(0).
\end{flushleft}


\begin{flushleft}
Now consider two points a, b, with 0 $<$ a $<$ b. If g(b) $<$ g(a), then
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
b$-$a
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
b$-$a
\end{flushleft}


\begin{flushleft}
g(0) + g(b) $<$
\end{flushleft}


\begin{flushleft}
g(a) + g(a) = g(a)
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
which contradicts Jensen's inequality. Therefore we must have g(b) $\geq$ g(a).
\end{flushleft}





\begin{flushleft}
3.11 Monotone mappings. A function $\psi$ : Rn $\rightarrow$ Rn is called monotone if for all x, y $\in$ dom $\psi$,
\end{flushleft}


\begin{flushleft}
($\psi$(x) $-$ $\psi$(y))T (x $-$ y) $\geq$ 0.
\end{flushleft}


\begin{flushleft}
(Note that {`}monotone' as defined here is not the same as the definition given in §3.6.1.
\end{flushleft}


\begin{flushleft}
Both definitions are widely used.) Suppose f : Rn $\rightarrow$ R is a differentiable convex function.
\end{flushleft}


\begin{flushleft}
Show that its gradient $\nabla$f is monotone. Is the converse true, i.e., is every monotone
\end{flushleft}


\begin{flushleft}
mapping the gradient of a convex function?
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Solution. Convexity of f implies
\end{flushleft}


\begin{flushleft}
f (x) $\geq$ f (y) + $\nabla$f (y)T (x $-$ y),
\end{flushleft}





\begin{flushleft}
f (y) $\geq$ f (x) + $\nabla$f (x)T (y $-$ x)
\end{flushleft}





\begin{flushleft}
for arbitrary x, y $\in$ dom f . Combining the two inequalities gives
\end{flushleft}


\begin{flushleft}
($\nabla$f (x) $-$ $\nabla$f (y))T (x $-$ y) $\geq$ 0,
\end{flushleft}


\begin{flushleft}
which shows that $\nabla$f is monotone.
\end{flushleft}


\begin{flushleft}
The converse not true in general. As a counterexample, consider
\end{flushleft}


\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
x1 /2 + x2
\end{flushleft}





\begin{flushleft}
$\psi$(x) =
\end{flushleft}





=





1


1/2





0


1





\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
x2
\end{flushleft}





.





\begin{flushleft}
$\psi$ is monotone because
\end{flushleft}


\begin{flushleft}
(x $-$ y)T
\end{flushleft}





1


1/2





0


1





\begin{flushleft}
(x $-$ y) = (x $-$ y)T
\end{flushleft}





1


1/4





1/4


1





\begin{flushleft}
(x $-$ y) $\geq$ 0
\end{flushleft}





\begin{flushleft}
for all x, y.
\end{flushleft}


\begin{flushleft}
However, there does not exist a function f : R2 $\rightarrow$ R such that $\psi$(x) = $\nabla$f (x), because
\end{flushleft}


\begin{flushleft}
such a function would have to satisfy
\end{flushleft}


\begin{flushleft}
$\partial$$\psi$1
\end{flushleft}


\begin{flushleft}
$\partial$2f
\end{flushleft}


=


= 0,


\begin{flushleft}
$\partial$x1 $\partial$x2
\end{flushleft}


\begin{flushleft}
$\partial$x2
\end{flushleft}





\begin{flushleft}
$\partial$2f
\end{flushleft}


\begin{flushleft}
$\partial$$\psi$2
\end{flushleft}


=


= 1/2.


\begin{flushleft}
$\partial$x1 $\partial$x2
\end{flushleft}


\begin{flushleft}
$\partial$x1
\end{flushleft}





\begin{flushleft}
3.12 Suppose f : Rn $\rightarrow$ R is convex, g : Rn $\rightarrow$ R is concave, dom f = dom g = Rn , and
\end{flushleft}


\begin{flushleft}
for all x, g(x) $\leq$ f (x). Show that there exists an affine function h such that for all x,
\end{flushleft}


\begin{flushleft}
g(x) $\leq$ h(x) $\leq$ f (x). In other words, if a concave function g is an underestimator of a
\end{flushleft}


\begin{flushleft}
convex function f , then we can fit an affine function between f and g.
\end{flushleft}


\begin{flushleft}
Solution. We first note that int epi f is nonempty (since dom f = Rn ), and does not
\end{flushleft}


\begin{flushleft}
intersect hypo g (since f (x) $<$ t for (x, t) $\in$ int epi f and t $\geq$ g(x) for (x, t) $\in$ hypo g).
\end{flushleft}


\begin{flushleft}
The two sets can therefore be separated by a hyperplane, i.e., there exist a $\in$ R n , b $\in$ R,
\end{flushleft}


\begin{flushleft}
not both zero, and c $\in$ R such that
\end{flushleft}


\begin{flushleft}
aT x + bt $\geq$ c $\geq$ aT y + bv
\end{flushleft}


\begin{flushleft}
if t $>$ f (x) and v $\leq$ g(y). We must have b = 0, since otherwise the condition would reduce
\end{flushleft}


\begin{flushleft}
to aT x $\geq$ aT y for all x and y, which is only possible if a = 0. Choosing x = y, and using
\end{flushleft}


\begin{flushleft}
the fact that f (x) $\geq$ g(x), we also see that b $>$ 0.
\end{flushleft}


\begin{flushleft}
Now we apply the separating hyperplane conditions to a point (x, t) $\in$ int epi f , and
\end{flushleft}


\begin{flushleft}
(y, v) = (x, g(x)) $\in$ hypo g, and obtain
\end{flushleft}


\begin{flushleft}
aT x + bt $\geq$ c $\geq$ aT x + bg(x),
\end{flushleft}


\begin{flushleft}
and dividing by b,
\end{flushleft}


\begin{flushleft}
t $\geq$ (c $-$ aT x)/b $\geq$ g(x),
\end{flushleft}





\begin{flushleft}
for all t $>$ f (x). Therefore the affine function h(x) = (c $-$ aT x)/b lies between f and g.
\end{flushleft}





\begin{flushleft}
3.13 Kullback-Leibler divergence and the information inequality. Let D kl be the KullbackLeibler divergence, as defined in (3.17). Prove the information inequality: Dkl (u, v) $\geq$ 0
\end{flushleft}


\begin{flushleft}
for all u, v $\in$ Rn
\end{flushleft}


\begin{flushleft}
++ . Also show that Dkl (u, v) = 0 if and only if u = v.
\end{flushleft}


\begin{flushleft}
Hint. The Kullback-Leibler divergence can be expressed as
\end{flushleft}


\begin{flushleft}
Dkl (u, v) = f (u) $-$ f (v) $-$ $\nabla$f (v)T (u $-$ v),
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
where f (v) = i=1 vi log vi is the negative entropy of v.
\end{flushleft}


\begin{flushleft}
Solution. The negative entropy is strictly convex and differentiable on Rn
\end{flushleft}


\begin{flushleft}
++ , hence
\end{flushleft}


\begin{flushleft}
f (u) $>$ f (v) + $\nabla$f (v)T (u $-$ v)
\end{flushleft}





\begin{flushleft}
for all u, v $\in$ Rn
\end{flushleft}


\begin{flushleft}
++ with u = v. Evaluating both sides of the inequality, we obtain
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
ui log ui
\end{flushleft}





$>$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
vi log vi +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(log vi + 1)(ui $-$ vi )
\end{flushleft}





\begin{flushleft}
ui log vi + 1T (u $-$ v).
\end{flushleft}





\begin{flushleft}
Re-arranging this inequality gives the desired result.
\end{flushleft}


\begin{flushleft}
3.14 Convex-concave functions and saddle-points. We say the function f : R n × Rm $\rightarrow$ R
\end{flushleft}


\begin{flushleft}
is convex-concave if f (x, z) is a concave function of z, for each fixed x, and a convex
\end{flushleft}


\begin{flushleft}
function of x, for each fixed z. We also require its domain to have the product form
\end{flushleft}


\begin{flushleft}
dom f = A × B, where A $\subseteq$ Rn and B $\subseteq$ Rm are convex.
\end{flushleft}





\begin{flushleft}
(a) Give a second-order condition for a twice differentiable function f : Rn × Rm $\rightarrow$ R
\end{flushleft}


\begin{flushleft}
to be convex-concave, in terms of its Hessian $\nabla$2 f (x, z).
\end{flushleft}


\begin{flushleft}
(b) Suppose that f : Rn ×Rm $\rightarrow$ R is convex-concave and differentiable, with $\nabla$f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) =
\end{flushleft}


\begin{flushleft}
0. Show that the saddle-point property holds: for all x, z, we have
\end{flushleft}


\begin{flushleft}
f (˜
\end{flushleft}


\begin{flushleft}
x, z) $\leq$ f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) $\leq$ f (x, z˜).
\end{flushleft}


\begin{flushleft}
Show that this implies that f satisfies the strong max-min property:
\end{flushleft}


\begin{flushleft}
sup inf f (x, z) = inf sup f (x, z)
\end{flushleft}


\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
(and their common value is f (˜
\end{flushleft}


\begin{flushleft}
x, z˜)).
\end{flushleft}


\begin{flushleft}
(c) Now suppose that f : Rn × Rm $\rightarrow$ R is differentiable, but not necessarily convexconcave, and the saddle-point property holds at x
\end{flushleft}


\begin{flushleft}
˜, z˜:
\end{flushleft}


\begin{flushleft}
f (˜
\end{flushleft}


\begin{flushleft}
x, z) $\leq$ f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) $\leq$ f (x, z˜)
\end{flushleft}


\begin{flushleft}
for all x, z. Show that $\nabla$f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) = 0.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The condition follows directly from the second-order conditions for convexity and
\end{flushleft}


\begin{flushleft}
concavity: it is
\end{flushleft}


\begin{flushleft}
$\nabla$2xx f (x, z) 0,
\end{flushleft}


\begin{flushleft}
$\nabla$2zz f (x, z) 0,
\end{flushleft}


\begin{flushleft}
for all x, z. In terms of $\nabla$2 f , this means that its 1, 1 block is positive semidefinite,
\end{flushleft}


\begin{flushleft}
and its 2, 2 block is negative semidefinite.
\end{flushleft}


\begin{flushleft}
(b) Let us fix z˜. Since $\nabla$x f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) = 0 and f (x, z˜) is convex in x, we conclude that x
\end{flushleft}


˜


\begin{flushleft}
minimizes f (x, z˜) over x, i.e., for all z, we have
\end{flushleft}


\begin{flushleft}
f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) $\leq$ f (x, z˜).
\end{flushleft}


\begin{flushleft}
This is one of the inequalities in the saddle-point condition. We can argue in the
\end{flushleft}


\begin{flushleft}
same way about z˜. Fix x
\end{flushleft}


\begin{flushleft}
˜, and note that $\nabla$z f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) = 0, together with concavity of
\end{flushleft}


\begin{flushleft}
this function in z, means that z˜ maximizes the function, i.e., for any x we have
\end{flushleft}


\begin{flushleft}
f (˜
\end{flushleft}


\begin{flushleft}
x, z˜) $\geq$ f (˜
\end{flushleft}


\begin{flushleft}
x, z).
\end{flushleft}


\begin{flushleft}
(c) To establish this we argue the same way. If the saddle-point condition holds, then
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜ minimizes f (x, z˜) over all x. Therefore we have $\nabla$fx (˜
\end{flushleft}


\begin{flushleft}
x, z˜) = 0. Similarly, since z˜
\end{flushleft}


\begin{flushleft}
maximizes f (˜
\end{flushleft}


\begin{flushleft}
x, z) over all z, we have $\nabla$fz (˜
\end{flushleft}


\begin{flushleft}
x, z˜) = 0.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Examples
\end{flushleft}


\begin{flushleft}
3.15 A family of concave utility functions. For 0 $<$ $\alpha$ $\leq$ 1 let
\end{flushleft}


\begin{flushleft}
u$\alpha$ (x) =
\end{flushleft}





\begin{flushleft}
x$\alpha$ $-$ 1
\end{flushleft}


,


\begin{flushleft}
$\alpha$
\end{flushleft}





\begin{flushleft}
with dom u$\alpha$ = R+ . We also define u0 (x) = log x (with dom u0 = R++ ).
\end{flushleft}


\begin{flushleft}
(a) Show that for x $>$ 0, u0 (x) = lim$\alpha$$\rightarrow$0 u$\alpha$ (x).
\end{flushleft}


\begin{flushleft}
(b) Show that u$\alpha$ are concave, monotone increasing, and all satisfy u$\alpha$ (1) = 0.
\end{flushleft}


\begin{flushleft}
These functions are often used in economics to model the benefit or utility of some quantity
\end{flushleft}


\begin{flushleft}
of goods or money. Concavity of u$\alpha$ means that the marginal utility (i.e., the increase
\end{flushleft}


\begin{flushleft}
in utility obtained for a fixed increase in the goods) decreases as the amount of goods
\end{flushleft}


\begin{flushleft}
increases. In other words, concavity models the effect of satiation.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) In this limit, both the numerator and denominator go to zero, so we use l'Hopital's
\end{flushleft}


\begin{flushleft}
rule:
\end{flushleft}


\begin{flushleft}
(d/d$\alpha$)(x$\alpha$ $-$ 1)
\end{flushleft}


\begin{flushleft}
x$\alpha$ log x
\end{flushleft}


\begin{flushleft}
= lim
\end{flushleft}


\begin{flushleft}
= log x.
\end{flushleft}


\begin{flushleft}
lim u$\alpha$ (x) = lim
\end{flushleft}


\begin{flushleft}
$\alpha$$\rightarrow$0
\end{flushleft}


\begin{flushleft}
$\alpha$$\rightarrow$0
\end{flushleft}


\begin{flushleft}
$\alpha$$\rightarrow$0
\end{flushleft}


\begin{flushleft}
(d/d$\alpha$)$\alpha$
\end{flushleft}


1


\begin{flushleft}
(b) By inspection we have
\end{flushleft}


\begin{flushleft}
u$\alpha$ (1) =
\end{flushleft}





\begin{flushleft}
1$\alpha$ $-$ 1
\end{flushleft}


= 0.


\begin{flushleft}
$\alpha$
\end{flushleft}





\begin{flushleft}
The derivative is given by
\end{flushleft}





\begin{flushleft}
u$\alpha$ (x) = x$\alpha$$-$1 ,
\end{flushleft}


\begin{flushleft}
which is positive for all x (since 0 $<$ $\alpha$ $<$ 1), so these functions are increasing. To
\end{flushleft}


\begin{flushleft}
show concavity, we examine the second derivative:
\end{flushleft}


\begin{flushleft}
u$\alpha$ (x) = ($\alpha$ $-$ 1)x$\alpha$$-$2 .
\end{flushleft}


\begin{flushleft}
Since this is negative for all x, we conclude that u$\alpha$ is strictly concave.
\end{flushleft}


\begin{flushleft}
3.16 For each of the following functions determine whether it is convex, concave, quasiconvex,
\end{flushleft}


\begin{flushleft}
or quasiconcave.
\end{flushleft}


\begin{flushleft}
(a) f (x) = ex $-$ 1 on R.
\end{flushleft}


\begin{flushleft}
Solution. Strictly convex, and therefore quasiconvex. Also quasiconcave but not
\end{flushleft}


\begin{flushleft}
concave.
\end{flushleft}


\begin{flushleft}
(b) f (x1 , x2 ) = x1 x2 on R2++ .
\end{flushleft}


\begin{flushleft}
Solution. The Hessian of f is
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) =
\end{flushleft}





0


1





1


0





,





\begin{flushleft}
which is neither positive semidefinite nor negative semidefinite. Therefore, f is
\end{flushleft}


\begin{flushleft}
neither convex nor concave. It is quasiconcave, since its superlevel sets
\end{flushleft}


\begin{flushleft}
\{(x1 , x2 ) $\in$ R2++ | x1 x2 $\geq$ $\alpha$\}
\end{flushleft}


\begin{flushleft}
are convex. It is not quasiconvex.
\end{flushleft}


\begin{flushleft}
(c) f (x1 , x2 ) = 1/(x1 x2 ) on R2++ .
\end{flushleft}


\begin{flushleft}
Solution. The Hessian of f is
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) =
\end{flushleft}





1


\begin{flushleft}
x1 x2
\end{flushleft}





\begin{flushleft}
2/(x21 )
\end{flushleft}


\begin{flushleft}
1/(x1 x2 )
\end{flushleft}





\begin{flushleft}
1/(x1 x2 )
\end{flushleft}


\begin{flushleft}
2/x22
\end{flushleft}





0





\begin{flushleft}
Therefore, f is convex and quasiconvex. It is not quasiconcave or concave.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
(d) f (x1 , x2 ) = x1 /x2 on R2++ .
\end{flushleft}


\begin{flushleft}
Solution. The Hessian of f is
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) =
\end{flushleft}





\begin{flushleft}
$-$1/x22
\end{flushleft}


\begin{flushleft}
2x1 /x32
\end{flushleft}





0


\begin{flushleft}
$-$1/x22
\end{flushleft}





\begin{flushleft}
which is not positive or negative semidefinite. Therefore, f is not convex or concave.
\end{flushleft}


\begin{flushleft}
It is quasiconvex and quasiconcave (i.e., quasilinear), since the sublevel and superlevel sets are halfspaces.
\end{flushleft}


\begin{flushleft}
(e) f (x1 , x2 ) = x21 /x2 on R × R++ .
\end{flushleft}


\begin{flushleft}
Solution. f is convex, as mentioned on page 72. (See also figure 3.3). This is easily
\end{flushleft}


\begin{flushleft}
verified by working out the Hessian:
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) =
\end{flushleft}





\begin{flushleft}
2/x2
\end{flushleft}


\begin{flushleft}
$-$2x1 /x22
\end{flushleft}





\begin{flushleft}
$-$2x1 /x22
\end{flushleft}


\begin{flushleft}
2x21 /x32
\end{flushleft}





1


\begin{flushleft}
$-$2x1 /x2
\end{flushleft}





\begin{flushleft}
= (2/x2 )
\end{flushleft}





\begin{flushleft}
$-$2x1 /x2
\end{flushleft}





1





0.





\begin{flushleft}
Therefore, f is convex and quasiconvex. It is not concave or quasiconcave (see the
\end{flushleft}


\begin{flushleft}
figure).
\end{flushleft}


\begin{flushleft}
1$-$$\alpha$
\end{flushleft}


\begin{flushleft}
(f) f (x1 , x2 ) = x$\alpha$
\end{flushleft}


\begin{flushleft}
, where 0 $\leq$ $\alpha$ $\leq$ 1, on R2++ .
\end{flushleft}


\begin{flushleft}
1 x2
\end{flushleft}


\begin{flushleft}
Solution. Concave and quasiconcave. The Hessian is
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}





\begin{flushleft}
$\alpha$($\alpha$ $-$ 1)x1$\alpha$$-$2 x1$-$$\alpha$
\end{flushleft}


2


\begin{flushleft}
$\alpha$(1 $-$ $\alpha$)x1$\alpha$$-$1 x$-$$\alpha$
\end{flushleft}


2





=





\begin{flushleft}
$\alpha$(1 $-$ $\alpha$)x1$\alpha$$-$1 x$-$$\alpha$
\end{flushleft}


2


\begin{flushleft}
$-$$\alpha$$-$1
\end{flushleft}


\begin{flushleft}
(1 $-$ $\alpha$)($-$$\alpha$)x$\alpha$
\end{flushleft}


\begin{flushleft}
1 x2
\end{flushleft}





\begin{flushleft}
$-$1/x21
\end{flushleft}


\begin{flushleft}
1/x1 x2
\end{flushleft}





=





\begin{flushleft}
1$-$$\alpha$
\end{flushleft}


\begin{flushleft}
$\alpha$(1 $-$ $\alpha$)x$\alpha$
\end{flushleft}


\begin{flushleft}
1 x2
\end{flushleft}





=





\begin{flushleft}
1$-$$\alpha$
\end{flushleft}


\begin{flushleft}
$-$$\alpha$(1 $-$ $\alpha$)x$\alpha$
\end{flushleft}


\begin{flushleft}
1 x2
\end{flushleft}





\begin{flushleft}
1/x1 x2
\end{flushleft}


\begin{flushleft}
$-$1/x22
\end{flushleft}





0.





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
1/x1
\end{flushleft}


\begin{flushleft}
$-$1/x2
\end{flushleft}





\begin{flushleft}
1/x1
\end{flushleft}


\begin{flushleft}
$-$1/x2
\end{flushleft}





\begin{flushleft}
f is not convex or quasiconvex.
\end{flushleft}


\begin{flushleft}
3.17 Suppose p $<$ 1, p = 0. Show that the function
\end{flushleft}


\begin{flushleft}
1/p
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
xpi
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





1/2





\begin{flushleft}
with dom f = Rn
\end{flushleft}


\begin{flushleft}
x )2 and
\end{flushleft}


\begin{flushleft}
++ is concave. This includes as special cases f (x) = (
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
the harmonic mean f (x) = ( i=1 1/xi )$-$1 . Hint. Adapt the proofs for the log-sum-exp
\end{flushleft}


\begin{flushleft}
function and the geometric mean in §3.1.5.
\end{flushleft}


\begin{flushleft}
Solution. The first derivatives of f are given by
\end{flushleft}


\begin{flushleft}
$\partial$f (x)
\end{flushleft}


=(


\begin{flushleft}
$\partial$xi
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





\begin{flushleft}
xpi )(1$-$p)/p xp$-$1
\end{flushleft}


=


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
1$-$p
\end{flushleft}





.





\begin{flushleft}
The second derivatives are
\end{flushleft}


\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
1$-$p
\end{flushleft}


=


\begin{flushleft}
$\partial$xi $\partial$xj
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





\begin{flushleft}
$-$p
\end{flushleft}





\begin{flushleft}
for i = j, and
\end{flushleft}


\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
1$-$p
\end{flushleft}


=


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x2i
\end{flushleft}





\begin{flushleft}
f (x)2
\end{flushleft}


\begin{flushleft}
x2i
\end{flushleft}





\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
xj
\end{flushleft}





\begin{flushleft}
1$-$p
\end{flushleft}





=





\begin{flushleft}
1$-$p
\end{flushleft}





$-$





\begin{flushleft}
1$-$p
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





\begin{flushleft}
1$-$p
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





\begin{flushleft}
f (x)2
\end{flushleft}


\begin{flushleft}
xi xj
\end{flushleft}


\begin{flushleft}
1$-$p
\end{flushleft}





.





\begin{flushleft}
1$-$p
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We need to show that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
1$-$p
\end{flushleft}


\begin{flushleft}
y $\nabla$ f (x)y =
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





2





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
yi f (x)1$-$p
\end{flushleft}


\begin{flushleft}
xi1$-$p
\end{flushleft}





2





\begin{flushleft}
n
\end{flushleft}





$-$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
yi2 f (x)2$-$p
\end{flushleft}


\begin{flushleft}
x2$-$p
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
This follows by applying the Cauchy-Schwarz inequality aT b $\leq$ a
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





\begin{flushleft}
ai =
\end{flushleft}





\begin{flushleft}
$-$p/2
\end{flushleft}





,





\begin{flushleft}
bi = y i
\end{flushleft}





\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





2





\begin{flushleft}
b
\end{flushleft}





$\leq$0


2





\begin{flushleft}
with
\end{flushleft}





\begin{flushleft}
1$-$p/2
\end{flushleft}





,





\begin{flushleft}
and noting that
\end{flushleft}


\begin{flushleft}
a2 = 1.
\end{flushleft}


\begin{flushleft}
i i
\end{flushleft}


\begin{flushleft}
3.18 Adapt the proof of concavity of the log-determinant function in §3.1.5 to show the following.
\end{flushleft}


\begin{flushleft}
(a) f (X) = tr X $-$1 is convex on dom f = Sn
\end{flushleft}


++ .


\begin{flushleft}
(b) f (X) = (det X)1/n is concave on dom f = Sn
\end{flushleft}


++ .


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
0 and V $\in$ Sn .
\end{flushleft}





\begin{flushleft}
(a) Define g(t) = f (Z + tV ), where Z
\end{flushleft}


\begin{flushleft}
g(t)
\end{flushleft}





=





\begin{flushleft}
tr((Z + tV )$-$1 )
\end{flushleft}





=





\begin{flushleft}
tr Z $-$1 (I + tZ $-$1/2 V Z $-$1/2 )$-$1
\end{flushleft}





=





\begin{flushleft}
tr Z $-$1 Q(I + t$\Lambda$)$-$1 QT
\end{flushleft}





=





\begin{flushleft}
tr QT Z $-$1 Q(I + t$\Lambda$)$-$1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
(QT Z $-$1 Q)ii (1 + t$\lambda$i )$-$1 ,
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where we used the eigenvalue decomposition Z $-$1/2 V Z $-$1/2 = Q$\Lambda$QT . In the last
\end{flushleft}


\begin{flushleft}
equality we express g as a positive weighted sum of convex functions 1/(1 + t$\lambda$i ),
\end{flushleft}


\begin{flushleft}
hence it is convex.
\end{flushleft}


\begin{flushleft}
(b) Define g(t) = f (Z + tV ), where Z 0 and V $\in$ Sn .
\end{flushleft}


\begin{flushleft}
g(t)
\end{flushleft}





\begin{flushleft}
(det(Z + tV ))1/n
\end{flushleft}





=





\begin{flushleft}
det Z 1/2 det(I + tZ $-$1/2 V Z $-$1/2 ) det Z 1/2
\end{flushleft}





=





\begin{flushleft}
1/n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
(det Z)1/n
\end{flushleft}





=





\begin{flushleft}
1/n
\end{flushleft}





\begin{flushleft}
(1 + t$\lambda$i )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where $\lambda$i , i = 1, . . . , n, are the eigenvalues of Z $-$1/2 V Z $-$1/2 . From the last equality
\end{flushleft}


\begin{flushleft}
we see that g is a concave function of t on \{t | Z + tV
\end{flushleft}


\begin{flushleft}
0\}, since det Z $>$ 0 and the
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
geometric mean ( i=1 xi )1/n is concave on Rn
\end{flushleft}


.


++


\begin{flushleft}
3.19 Nonnegative weighted sums and integrals.
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
(a) Show that f (x) =
\end{flushleft}


\begin{flushleft}
$\alpha$ x is a convex function of x, where $\alpha$1 $\geq$ $\alpha$2 $\geq$ · · · $\geq$
\end{flushleft}


\begin{flushleft}
i=1 i [i]
\end{flushleft}


\begin{flushleft}
$\alpha$r $\geq$ 0, and x[i] denotes the ith largest component of x. (You can use the fact that
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
f (x) = i=1 x[i] is convex on Rn .)
\end{flushleft}


\begin{flushleft}
Solution. We can express f as
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}





=





\begin{flushleft}
$\alpha$r (x[1] + x[2] + · · · + x[r] ) + ($\alpha$r$-$1 $-$ $\alpha$r )(x[1] + x[2] + · · · + x[r$-$1] )
\end{flushleft}


\begin{flushleft}
+($\alpha$r$-$2 $-$ $\alpha$r$-$1 )(x[1] + x[2] + · · · + x[r$-$2] ) + · · · + ($\alpha$1 $-$ $\alpha$2 )x[1] ,
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
which is a nonnegative sum of the convex functions
\end{flushleft}


\begin{flushleft}
x[1] ,
\end{flushleft}





\begin{flushleft}
x[1] + x[2] ,
\end{flushleft}





\begin{flushleft}
x[1] + x[2] + x[3] ,
\end{flushleft}





...,





\begin{flushleft}
x[1] + x[2] + · · · + x[r] .
\end{flushleft}





\begin{flushleft}
(b) Let T (x, $\omega$) denote the trigonometric polynomial
\end{flushleft}


\begin{flushleft}
T (x, $\omega$) = x1 + x2 cos $\omega$ + x3 cos 2$\omega$ + · · · + xn cos(n $-$ 1)$\omega$.
\end{flushleft}


\begin{flushleft}
Show that the function
\end{flushleft}


\begin{flushleft}
2$\pi$
\end{flushleft}





\begin{flushleft}
f (x) = $-$
\end{flushleft}





\begin{flushleft}
log T (x, $\omega$) d$\omega$
\end{flushleft}


0





\begin{flushleft}
is convex on \{x $\in$ Rn | T (x, $\omega$) $>$ 0, 0 $\leq$ $\omega$ $\leq$ 2$\pi$\}.
\end{flushleft}


\begin{flushleft}
Solution. The function
\end{flushleft}


\begin{flushleft}
g(x, $\omega$) = $-$ log(x1 + x2 cos $\omega$ + x3 cos 2$\omega$ + · · · + +xn cos(n $-$ 1)$\omega$)
\end{flushleft}


\begin{flushleft}
is convex in x for fixed $\omega$. Therefore
\end{flushleft}


\begin{flushleft}
2$\pi$
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
g(x, $\omega$)d$\omega$
\end{flushleft}


0





\begin{flushleft}
is convex in x.
\end{flushleft}


\begin{flushleft}
3.20 Composition with an affine function. Show that the following functions f : Rn $\rightarrow$ R are
\end{flushleft}


\begin{flushleft}
convex.
\end{flushleft}


\begin{flushleft}
(a) f (x) = Ax $-$ b , where A $\in$ Rm×n , b $\in$ Rm , and · is a norm on Rm .
\end{flushleft}


\begin{flushleft}
Solution. f is the composition of a norm, which is convex, and an affine function.
\end{flushleft}


\begin{flushleft}
(b) f (x) = $-$ (det(A0 + x1 A1 + · · · + xn An ))1/m , on \{x | A0 + x1 A1 + · · · + xn An 0\},
\end{flushleft}


\begin{flushleft}
where Ai $\in$ Sm .
\end{flushleft}


\begin{flushleft}
Solution. f is the composition of the convex function h(X) = $-$(det X)1/m and an
\end{flushleft}


\begin{flushleft}
affine transformation. To see that h is convex on Sm
\end{flushleft}


\begin{flushleft}
++ , we restrict h to a line and
\end{flushleft}


\begin{flushleft}
prove that g(t) = $-$ det(Z + tV )1/m is convex:
\end{flushleft}


\begin{flushleft}
g(t)
\end{flushleft}





=


=





\begin{flushleft}
$-$(det(Z + tV ))1/m
\end{flushleft}





\begin{flushleft}
$-$(det Z)1/m (det(I + tZ $-$1/2 V Z $-$1/2 ))1/m
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
$-$(det Z)1/m (
\end{flushleft}





\begin{flushleft}
(1 + t$\lambda$i ))1/m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where $\lambda$1 , . . . , $\lambda$m denote the eigenvalues of Z $-$1/2 V Z $-$1/2 . We have expressed g as
\end{flushleft}


\begin{flushleft}
the product of a negative constant and the geometric mean of 1 + t$\lambda$i , i = 1, . . . , m.
\end{flushleft}


\begin{flushleft}
Therefore g is convex. (See also exercise 3.18.)
\end{flushleft}


\begin{flushleft}
(c) f (X) = tr (A0 + x1 A1 + · · · + xn An )$-$1 , on \{x | A0 +x1 A1 +· · ·+xn An 0\}, where
\end{flushleft}


\begin{flushleft}
Ai $\in$ Sm . (Use the fact that tr(X $-$1 ) is convex on Sm
\end{flushleft}


\begin{flushleft}
++ ; see exercise 3.18.)
\end{flushleft}


$-$1


\begin{flushleft}
Solution. f is the composition of tr X
\end{flushleft}


\begin{flushleft}
and an affine transformation
\end{flushleft}


\begin{flushleft}
x $\rightarrow$ A 0 + x 1 A1 + · · · + x n An .
\end{flushleft}


\begin{flushleft}
3.21 Pointwise maximum and supremum. Show that the following functions f : Rn $\rightarrow$ R are
\end{flushleft}


\begin{flushleft}
convex.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) f (x) = maxi=1,...,k A(i) x $-$ b(i) , where A(i) $\in$ Rm×n , b(i) $\in$ Rm and · is a norm
\end{flushleft}


\begin{flushleft}
on Rm .
\end{flushleft}


\begin{flushleft}
Solution. f is the pointwise maximum of k functions A(i) x $-$ b(i) . Each of those
\end{flushleft}


\begin{flushleft}
functions is convex because it is the composition of an affine transformation and a
\end{flushleft}


\begin{flushleft}
norm.
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
(b) f (x) =
\end{flushleft}


\begin{flushleft}
|x|[i] on Rn , where |x| denotes the vector with |x|i = |xi | (i.e., |x| is
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
the absolute value of x, componentwise), and |x|[i] is the ith largest component of
\end{flushleft}


\begin{flushleft}
|x|. In other words, |x|[1] , |x|[2] , . . . , |x|[n] are the absolute values of the components
\end{flushleft}


\begin{flushleft}
of x, sorted in nonincreasing order.
\end{flushleft}


\begin{flushleft}
Solution. Write f as
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|x|[i] =
\end{flushleft}





\begin{flushleft}
max
\end{flushleft}





\begin{flushleft}
1$\leq$i1 $<$i2 $<$···$<$ir $\leq$n
\end{flushleft}





\begin{flushleft}
|xi1 | + · · · + |xir |
\end{flushleft}





\begin{flushleft}
which is the pointwise maximum of n!/(r!(n $-$ r)!) convex functions.
\end{flushleft}


\begin{flushleft}
3.22 Composition rules. Show that the following functions are convex.
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
eai x+bi $<$
\end{flushleft}


\begin{flushleft}
eai x+bi )) on dom f = \{x |
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
eyi ) is convex.
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
log( i=1 eai x+bi ) is convex (composition of the
\end{flushleft}





\begin{flushleft}
(a) f (x) = $-$ log($-$ log(
\end{flushleft}


\begin{flushleft}
use the fact that log(
\end{flushleft}





\begin{flushleft}
1\}. You can
\end{flushleft}





\begin{flushleft}
log-sum-exp
\end{flushleft}


\begin{flushleft}
Solution. g(x) =
\end{flushleft}


\begin{flushleft}
function and an affine mapping), so $-$g is concave. The function h(y) = $-$ log y is
\end{flushleft}


\begin{flushleft}
convex and decreasing. Therefore f (x) = h($-$g(x)) is convex.
\end{flushleft}


$\surd$


\begin{flushleft}
(b) f (x, u, v) = $-$ uv $-$ xT x on dom f = \{(x, u, v) | uv $>$ xT x, u, v $>$ 0\}. Use the
\end{flushleft}


$\surd$


\begin{flushleft}
fact that xT x/u is convex in (x, u) for u $>$ 0, and that $-$ x1 x2 is convex on R2++ .
\end{flushleft}





\begin{flushleft}
Solution. We can express f as f (x, u, v) = $-$ u(v $-$ xT x/u). The function
\end{flushleft}


$\surd$


\begin{flushleft}
h(x1 , x2 ) = $-$ x1 x2 is convex on R2++ , and decreasing in each argument. The
\end{flushleft}


\begin{flushleft}
functions g1 (u, v, x) = u and g2 (u, v, x) = v $-$ xT x/u are concave. Therefore
\end{flushleft}


\begin{flushleft}
f (u, v, x) = h(g(u, v, x)) is convex.
\end{flushleft}





\begin{flushleft}
(c) f (x, u, v) = $-$ log(uv $-$ xT x) on dom f = \{(x, u, v) | uv $>$ xT x, u, v $>$ 0\}.
\end{flushleft}


\begin{flushleft}
Solution. We can express f as
\end{flushleft}


\begin{flushleft}
f (x, u, v) = $-$ log u $-$ log(v $-$ xT x/u).
\end{flushleft}


\begin{flushleft}
The first term is convex. The function v $-$ xT x/u is concave because v is linear and
\end{flushleft}


\begin{flushleft}
xT x/u is convex on \{(x, u) | u $>$ 0\}. Therefore the second term in f is convex: it is
\end{flushleft}


\begin{flushleft}
the composition of a convex decreasing function $-$ log t and a concave function.
\end{flushleft}





\begin{flushleft}
(d) f (x, t) = $-$(tp $-$ x pp )1/p where p $>$ 1 and dom f = \{(x, t) | t $\geq$ x p \}. You can use
\end{flushleft}


\begin{flushleft}
the fact that x pp /up$-$1 is convex in (x, u) for u $>$ 0 (see exercise 3.23), and that
\end{flushleft}


\begin{flushleft}
$-$x1/p y 1$-$1/p is convex on R2+ (see exercise 3.16).
\end{flushleft}


\begin{flushleft}
Solution. We can express f as
\end{flushleft}


\begin{flushleft}
f (x, t) = $-$ tp$-$1
\end{flushleft}





\begin{flushleft}
t$-$
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
tp$-$1
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
1/p
\end{flushleft}





\begin{flushleft}
= $-$t1$-$1/p
\end{flushleft}


\begin{flushleft}
1/p 1$-$1/p
\end{flushleft}





\begin{flushleft}
This is the composition of h(y1 , y2 ) = $-$y1 y2
\end{flushleft}


\begin{flushleft}
argument) and two concave functions
\end{flushleft}


\begin{flushleft}
g1 (x, t) = t1$-$1/p ,
\end{flushleft}





\begin{flushleft}
t$-$
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
tp$-$1
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
1/p
\end{flushleft}





.





\begin{flushleft}
(convex and decreasing in each
\end{flushleft}





\begin{flushleft}
g2 (x, t) = t $-$
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


.


\begin{flushleft}
tp$-$1
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
(e) f (x, t) = $-$ log(tp $-$ x pp ) where p $>$ 1 and dom f = \{(x, t) | t $>$ x p \}. You can
\end{flushleft}


\begin{flushleft}
use the fact that x pp /up$-$1 is convex in (x, u) for u $>$ 0 (see exercise 3.23).
\end{flushleft}


\begin{flushleft}
Solution. Express f as
\end{flushleft}


\begin{flushleft}
f (x, t)
\end{flushleft}





=


=





\begin{flushleft}
$-$ log tp$-$1 $-$ log(t $-$ x
\end{flushleft}





\begin{flushleft}
p p$-$1
\end{flushleft}


)


\begin{flushleft}
p /t
\end{flushleft}





\begin{flushleft}
$-$(p $-$ 1) log t $-$ log(t $-$ x
\end{flushleft}





\begin{flushleft}
p p$-$1
\end{flushleft}


).


\begin{flushleft}
p /t
\end{flushleft}





\begin{flushleft}
The first term is convex. The second term is the composition of a decreasing convex
\end{flushleft}


\begin{flushleft}
function and a concave function, and is also convex.
\end{flushleft}


\begin{flushleft}
3.23 Perspective of a function.
\end{flushleft}


\begin{flushleft}
(a) Show that for p $>$ 1,
\end{flushleft}


\begin{flushleft}
f (x, t) =
\end{flushleft}





\begin{flushleft}
x pp
\end{flushleft}


\begin{flushleft}
|x1 |p + · · · + |xn |p
\end{flushleft}


\begin{flushleft}
= p$-$1
\end{flushleft}


\begin{flushleft}
p$-$1
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
is convex on \{(x, t) | t $>$ 0\}.
\end{flushleft}


\begin{flushleft}
Solution. This is the perspective function of x
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
(b) Show that
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
= |x1 |p + · · · + |xn |p .
\end{flushleft}





\begin{flushleft}
Ax + b 22
\end{flushleft}


\begin{flushleft}
cT x + d
\end{flushleft}





\begin{flushleft}
is convex on \{x | cT x + d $>$ 0\}, where A $\in$ Rm×n , b $\in$ Rm , c $\in$ Rn and d $\in$ R.
\end{flushleft}


\begin{flushleft}
Solution. This function is the composition of the function g(y, t) = y T y/t with an
\end{flushleft}


\begin{flushleft}
affine transformation (y, t) = (Ax + b, cT x + d). Therefore convexity of f follows
\end{flushleft}


\begin{flushleft}
from the fact that g is convex on \{(y, t) | t $>$ 0\}.
\end{flushleft}


\begin{flushleft}
For convexity of g one can note that it is the perspective of xT x, or directly verify
\end{flushleft}


\begin{flushleft}
that the Hessian
\end{flushleft}


\begin{flushleft}
I/t
\end{flushleft}


\begin{flushleft}
$-$y/t2
\end{flushleft}


\begin{flushleft}
$\nabla$2 g(y, t) =
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
$-$y /t y T y/t3
\end{flushleft}


\begin{flushleft}
is positive semidefinite, since
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
I/t
\end{flushleft}


\begin{flushleft}
$-$y T /t
\end{flushleft}





\begin{flushleft}
$-$y/t2
\end{flushleft}


\begin{flushleft}
y T y/t3
\end{flushleft}





\begin{flushleft}
v
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
= tv $-$ yw
\end{flushleft}





2 3


\begin{flushleft}
2 /t
\end{flushleft}





$\geq$0





\begin{flushleft}
for all v and w.
\end{flushleft}


\begin{flushleft}
3.24 Some functions on the probability simplex. Let x be a real-valued random variable which
\end{flushleft}


\begin{flushleft}
takes values in \{a1 , . . . , an \} where a1 $<$ a2 $<$ · · · $<$ an , with prob(x = ai ) = pi ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , n. For each of the following functions of p (on the probability simplex \{p $\in$
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
Rn
\end{flushleft}


\begin{flushleft}
+ | 1 p = 1\}), determine if the function is convex, concave, quasiconvex, or quasiconcave.
\end{flushleft}


\begin{flushleft}
(a) E x.
\end{flushleft}


\begin{flushleft}
Solution. E x = p1 a1 + · · · + pn an is linear, hence convex, concave, quasiconvex,
\end{flushleft}


\begin{flushleft}
and quasiconcave
\end{flushleft}


\begin{flushleft}
(b) prob(x $\geq$ $\alpha$).
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
Solution. Let j = min\{i | ai $\geq$ $\alpha$\}. Then prob(x $\geq$ $\alpha$) = i=j pi , This is a linear
\end{flushleft}


\begin{flushleft}
function of p, hence convex, concave, quasiconvex, and quasiconcave.
\end{flushleft}


\begin{flushleft}
(c) prob($\alpha$ $\leq$ x $\leq$ $\beta$).
\end{flushleft}


\begin{flushleft}
Solution. Let j = min\{i | ai $\geq$ $\alpha$\} and k = max\{i | ai $\leq$ $\beta$\}. Then prob($\alpha$ $\leq$ x $\leq$
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
$\beta$) =
\end{flushleft}


\begin{flushleft}
p . This is a linear function of p, hence convex, concave, quasiconvex,
\end{flushleft}


\begin{flushleft}
i=j i
\end{flushleft}


\begin{flushleft}
and quasiconcave.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(d)
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi log pi , the negative entropy of the distribution.
\end{flushleft}





\begin{flushleft}
Solution. p log p is a convex function on R+ (assuming 0 log 0 = 0), so
\end{flushleft}


\begin{flushleft}
p log pi
\end{flushleft}


\begin{flushleft}
i i
\end{flushleft}


\begin{flushleft}
is convex (and hence quasiconvex).
\end{flushleft}


\begin{flushleft}
The function is not concave or quasiconcave. Consider, for example, n = 2, p1 =
\end{flushleft}


\begin{flushleft}
(1, 0) and p2 = (0, 1). Both p1 and p2 have function value zero, but the convex combination (0.5, 0.5) has function value log(1/2) $<$ 0. This shows that the superlevel
\end{flushleft}


\begin{flushleft}
sets are not convex.
\end{flushleft}


\begin{flushleft}
(e) var x = E(x $-$ E x)2 .
\end{flushleft}


\begin{flushleft}
Solution. We have
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
var x = E x2 $-$ (E x)2 =
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pi a2i $-$ (
\end{flushleft}





\begin{flushleft}
p i ai ) 2 ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
so var x is a concave quadratic function of p.
\end{flushleft}


\begin{flushleft}
The function is not convex or quasiconvex. Consider the example with n = 2, a1 = 0,
\end{flushleft}


\begin{flushleft}
a2 = 1. Both (p1 , p2 ) = (1/4, 3/4) and (p1 , p2 ) = (3/4, 1/4) lie in the probability
\end{flushleft}


\begin{flushleft}
simplex and have var x = 3/16, but the convex combination (p1 , p2 ) = (1/2, 1/2) has
\end{flushleft}


\begin{flushleft}
a variance var x = 1/4 $>$ 3/16. This shows that the sublevel sets are not convex.
\end{flushleft}


\begin{flushleft}
(f) quartile(x) = inf\{$\beta$ | prob(x $\leq$ $\beta$) $\geq$ 0.25\}.
\end{flushleft}


\begin{flushleft}
Solution. The sublevel and the superlevel sets of quartile(x) are convex (see
\end{flushleft}


\begin{flushleft}
problem 2.15), so it is quasiconvex and quasiconcave.
\end{flushleft}


\begin{flushleft}
quartile(x) is not continuous (it takes values in a discrete set \{a1 , . . . , an \}, so it is
\end{flushleft}


\begin{flushleft}
not convex or concave. (A convex or a concave function is always continuous on the
\end{flushleft}


\begin{flushleft}
relative interior of its domain.)
\end{flushleft}


\begin{flushleft}
(g) The cardinality of the smallest set A $\subseteq$ \{a1 , . . . , an \} with probability $\geq$ 90\%. (By
\end{flushleft}


\begin{flushleft}
cardinality we mean the number of elements in A.)
\end{flushleft}


\begin{flushleft}
Solution. f is integer-valued, so it can not be convex or concave. (A convex or a
\end{flushleft}


\begin{flushleft}
concave function is always continuous on the relative interior of its domain.)
\end{flushleft}


\begin{flushleft}
f is quasiconcave because its superlevel sets are convex. We have f (p) $\geq$ $\alpha$ if and
\end{flushleft}


\begin{flushleft}
only if
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
p[i] $<$ 0.9,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where k = max\{i = 1, . . . , n | i $<$ $\alpha$\} is the largest integer less than $\alpha$, and p[i] is
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
the ith largest component of p. We know that
\end{flushleft}


\begin{flushleft}
p is a convex function of p,
\end{flushleft}


\begin{flushleft}
i=1 [i]
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
so the inequality
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


$<$


0.9


\begin{flushleft}
defines
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
convex
\end{flushleft}


\begin{flushleft}
set.
\end{flushleft}


\begin{flushleft}
[i]
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
In general, f (p) is not quasiconvex. For example, we can take n = 2, a1 = 0 and
\end{flushleft}


\begin{flushleft}
a2 = 1, and p1 = (0.1, 0.9) and p2 = (0.9, 0.1). Then f (p1 ) = f (p2 ) = 1, but
\end{flushleft}


\begin{flushleft}
f ((p1 + p2 )/2) = f (0.5, 0.5) = 2.
\end{flushleft}


\begin{flushleft}
(h) The minimum width interval that contains 90\% of the probability, i.e.,
\end{flushleft}


\begin{flushleft}
inf \{$\beta$ $-$ $\alpha$ | prob($\alpha$ $\leq$ x $\leq$ $\beta$) $\geq$ 0.9\} .
\end{flushleft}


\begin{flushleft}
Solution. The minimum width interval that contains 90\% of the probability must
\end{flushleft}


\begin{flushleft}
be of the form [ai , aj ] with 1 $\leq$ i $\leq$ j $\leq$ n, because
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
prob($\alpha$ $\leq$ x $\leq$ $\beta$) =
\end{flushleft}





\begin{flushleft}
k=i
\end{flushleft}





\begin{flushleft}
pk = prob(ai $\leq$ x $\leq$ ak )
\end{flushleft}





\begin{flushleft}
where i = min\{k | ak $\geq$ $\alpha$\}, and j = max\{k | ak $\leq$ $\beta$\}.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
We show that the function is quasiconcave. We have f (p) $\geq$ $\gamma$ if and only if all
\end{flushleft}


\begin{flushleft}
intervals of width less than $\gamma$ have a probability less than 90\%,
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
pk $<$ 0.9
\end{flushleft}


\begin{flushleft}
k=i
\end{flushleft}





\begin{flushleft}
for all i, j that satisfy aj $-$ ai $<$ $\gamma$. This defines a convex set.
\end{flushleft}


\begin{flushleft}
The function is not convex, concave nor quasiconvex in general. Consider the example with n = 3, a1 = 0, a2 = 0.5 and a3 = 1. On the line p1 + p3 = 0.95, we
\end{flushleft}


\begin{flushleft}
have
\end{flushleft}


0


\begin{flushleft}
p1 + p3 = 0.95, p1 $\in$ [0.05, 0.1] $\cup$ [0.9, 0.95]
\end{flushleft}


\begin{flushleft}
0.5 p1 + p3 = 0.95, p1 $\in$ (0.1, 0.15] $\cup$ [0.85, 0.9)
\end{flushleft}


\begin{flushleft}
f (p) =
\end{flushleft}


1


\begin{flushleft}
p1 + p3 = 0.95, p1 $\in$ (0.15, 0.85)
\end{flushleft}


\begin{flushleft}
It is clear that f is not convex, concave nor quasiconvex on the line.
\end{flushleft}





\begin{flushleft}
3.25 Maximum probability distance between distributions. Let p, q $\in$ Rn represent two probability distributions on \{1, . . . , n\} (so p, q 0, 1T p = 1T q = 1). We define the maximum
\end{flushleft}


\begin{flushleft}
probability distance dmp (p, q) between p and q as the maximum difference in probability
\end{flushleft}


\begin{flushleft}
assigned by p and q, over all events:
\end{flushleft}


\begin{flushleft}
dmp (p, q) = max\{| prob(p, C) $-$ prob(q, C)| | C $\subseteq$ \{1, . . . , n\}\}.
\end{flushleft}


\begin{flushleft}
Here prob(p, C) is the probability of C, under the distribution p, i.e., prob(p, C) =
\end{flushleft}


\begin{flushleft}
p.
\end{flushleft}


\begin{flushleft}
i$\in$C i
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
Find a simple expression for dmp , involving p $-$ q 1 = i=1 |pi $-$ qi |, and show that dmp
\end{flushleft}


\begin{flushleft}
is a convex function on Rn × Rn . (Its domain is \{(p, q) | p, q 0, 1T p = 1T q = 1\}, but
\end{flushleft}


\begin{flushleft}
it has a natural extension to all of Rn × Rn .)
\end{flushleft}


\begin{flushleft}
Solution. Noting that
\end{flushleft}


\begin{flushleft}
˜ $-$ prob(q, C)),
\end{flushleft}


˜


\begin{flushleft}
prob(p, C) $-$ prob(q, C) = $-$(prob(p, C)
\end{flushleft}


\begin{flushleft}
˜ = \{1, . . . , n\} \ensuremath{\backslash} C, we can just as well express dmp as
\end{flushleft}


\begin{flushleft}
where C
\end{flushleft}


\begin{flushleft}
dmp (p, q) = max\{prob(p, C) $-$ prob(q, C) | C $\subseteq$ \{1, . . . , n\}\}.
\end{flushleft}


\begin{flushleft}
This shows that dmp is convex, since it is the maximum of 2n linear functions of (p, q).
\end{flushleft}


\begin{flushleft}
Let's now identify the (or a) subset C that maximizes
\end{flushleft}


\begin{flushleft}
prob(p, C) $-$ prob(q, C) =
\end{flushleft}





\begin{flushleft}
i$\in$C
\end{flushleft}





\begin{flushleft}
(pi $-$ qi ).
\end{flushleft}





\begin{flushleft}
The solution is
\end{flushleft}


\begin{flushleft}
C = \{i $\in$ \{1, . . . , n\} | pi $>$ qi \}.
\end{flushleft}





\begin{flushleft}
Let's show this. The indices for which pi = qi clearly don't matter, so we will ignore
\end{flushleft}


\begin{flushleft}
them, and assume without loss of generality that for each index, p$>$ qi or pi $<$ qi . Now
\end{flushleft}


\begin{flushleft}
consider any other subset C. If there is an element k in C but not C, then by adding
\end{flushleft}


\begin{flushleft}
k to C we increase prob(p, C) $-$ prob(q, C) by pk $-$ qk $>$ 0, so C could not have been
\end{flushleft}


\begin{flushleft}
optimal. Conversely, suppose that k $\in$ C \ensuremath{\backslash} C , so pk $-$ qk $<$ 0. If we remove k from C,
\end{flushleft}


\begin{flushleft}
we'd increase prob(p, C) $-$ prob(q, C) by qk $-$ pk $>$ 0, so C could not have been optimal.
\end{flushleft}


\begin{flushleft}
Thus, we have dmp (p, q) =
\end{flushleft}


\begin{flushleft}
(pi $-$ qi ). Now let's express this in terms of p $-$ q 1 .
\end{flushleft}


\begin{flushleft}
pi $>$qi
\end{flushleft}


\begin{flushleft}
Using
\end{flushleft}


\begin{flushleft}
(pi $-$ qi ) = 1T p $-$ 1T q = 0,
\end{flushleft}


\begin{flushleft}
(pi $-$ qi ) +
\end{flushleft}


\begin{flushleft}
pi $>$qi
\end{flushleft}





\begin{flushleft}
pi $\leq$qi
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
we have
\end{flushleft}





\begin{flushleft}
pi $>$qi
\end{flushleft}





\begin{flushleft}
(pi $-$ qi ) = $-$
\end{flushleft}





\begin{flushleft}
pi $\leq$qi
\end{flushleft}





\begin{flushleft}
so
\end{flushleft}


\begin{flushleft}
dmp (p, q)
\end{flushleft}





=





(1/2)


\begin{flushleft}
pi $>$qi
\end{flushleft}





\begin{flushleft}
(pi $-$ qi )
\end{flushleft}





\begin{flushleft}
(pi $-$ qi ) $-$ (1/2)
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=





(1/2)


\begin{flushleft}
i=1
\end{flushleft}





=





,





\begin{flushleft}
pi $\leq$qi
\end{flushleft}





\begin{flushleft}
(pi $-$ qi )
\end{flushleft}





\begin{flushleft}
|pi $-$ qi |
\end{flushleft}





\begin{flushleft}
(1/2) p $-$ q
\end{flushleft}





1.





\begin{flushleft}
This makes it very clear that dmp is convex.
\end{flushleft}


\begin{flushleft}
The best way to interpret this result is as an interpretation of the 1 -norm for probability
\end{flushleft}


\begin{flushleft}
distributions. It states that the 1 -distance between two probability distributions is twice
\end{flushleft}


\begin{flushleft}
the maximum difference in probability, over all events, of the distributions.
\end{flushleft}


\begin{flushleft}
3.26 More functions of eigenvalues. Let $\lambda$1 (X) $\geq$ $\lambda$2 (X) $\geq$ · · · $\geq$ $\lambda$n (X) denote the eigenvalues
\end{flushleft}


\begin{flushleft}
of a matrix X $\in$ Sn . We have already seen several functions of the eigenvalues that are
\end{flushleft}


\begin{flushleft}
convex or concave functions of X.
\end{flushleft}


\begin{flushleft}
$\bullet$ The maximum eigenvalue $\lambda$1 (X) is convex (example 3.10). The minimum eigenvalue
\end{flushleft}


\begin{flushleft}
$\lambda$n (X) is concave.
\end{flushleft}


\begin{flushleft}
$\bullet$ The sum of the eigenvalues (or trace), tr X = $\lambda$1 (X) + · · · + $\lambda$n (X), is linear.
\end{flushleft}


\begin{flushleft}
$\bullet$ The sum of the inverses of the eigenvalues (or trace of the inverse), tr(X $-$1 ) =
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
1/$\lambda$i (X), is convex on Sn
\end{flushleft}


\begin{flushleft}
++ (exercise 3.18).
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\bullet$ The geometric mean of the eigenvalues, (det X)1/n = ( i=1 $\lambda$i (X))1/n , and the
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
logarithm of the product of the eigenvalues, log det X = i=1 log $\lambda$i (X), are concave
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
on X $\in$ S++ (exercise 3.18 and page 74).
\end{flushleft}





\begin{flushleft}
In this problem we explore some more functions of eigenvalues, by exploiting variational
\end{flushleft}


\begin{flushleft}
characterizations.
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
(a) Sum of k largest eigenvalues. Show that
\end{flushleft}


\begin{flushleft}
$\lambda$ (X) is convex on Sn . Hint. [HJ85,
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
page 191] Use the variational characterization
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i (X) = sup\{tr(V T XV ) | V $\in$ Rn×k , V T V = I\}.
\end{flushleft}





\begin{flushleft}
Solution. The variational characterization shows that f is the pointwise supremum
\end{flushleft}


\begin{flushleft}
of a family of linear functions tr(V T XV ).
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
(b) Geometric mean of k smallest eigenvalues. Show that ( i=n$-$k+1 $\lambda$i (X))1/k is concave on Sn
\end{flushleft}


\begin{flushleft}
0, we have
\end{flushleft}


\begin{flushleft}
++ . Hint. [MO79, page 513] For X
\end{flushleft}


\begin{flushleft}
1/k
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\lambda$i (X)
\end{flushleft}





=





\begin{flushleft}
i=n$-$k+1
\end{flushleft}





1


\begin{flushleft}
inf\{tr(V T XV ) | V $\in$ Rn×k , det V T V = 1\}.
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
Solution. f is the pointwise infimum of a family of linear functions tr(V T XV ).
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(c) Log of product of k smallest eigenvalues. Show that
\end{flushleft}


\begin{flushleft}
log $\lambda$i (X) is concave
\end{flushleft}


\begin{flushleft}
i=n$-$k+1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
on S++ . Hint. [MO79, page 513] For X 0,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
(V T XV )ii
\end{flushleft}





\begin{flushleft}
$\lambda$i (X) = inf
\end{flushleft}


\begin{flushleft}
i=n$-$k+1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
V $\in$ Rn×k , V T V = I
\end{flushleft}





.





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
Solution. f is the pointwise infimum of a family of concave functions
\end{flushleft}


\begin{flushleft}
log(V T XV )ii .
\end{flushleft}





\begin{flushleft}
(V T XV )ii =
\end{flushleft}





\begin{flushleft}
log
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
3.27 Diagonal elements of Cholesky factor. Each X $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ has a unique Cholesky factorization
\end{flushleft}


\begin{flushleft}
X = LLT , where L is lower triangular, with Lii $>$ 0. Show that Lii is a concave function
\end{flushleft}


\begin{flushleft}
of X (with domain Sn
\end{flushleft}


++ ).


\begin{flushleft}
Hint. Lii can be expressed as Lii = (w $-$ z T Y $-$1 z)1/2 , where
\end{flushleft}


\begin{flushleft}
Y
\end{flushleft}


\begin{flushleft}
zT
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
is the leading i × i submatrix of X.
\end{flushleft}


\begin{flushleft}
Solution. The function f (z, Y ) = z T Y $-$1 z with dom f = \{(z, Y ) | Y
\end{flushleft}


\begin{flushleft}
jointly in z and Y . To see this note that
\end{flushleft}


\begin{flushleft}
(z, Y, t) $\in$ epi f
\end{flushleft}





$\Leftarrow$$\Rightarrow$





\begin{flushleft}
Y
\end{flushleft}





0,





\begin{flushleft}
Y
\end{flushleft}


\begin{flushleft}
zT
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
0\} is convex
\end{flushleft}





0,





\begin{flushleft}
so epi f is a convex set. Therefore, w $-$ z T Y $-$1 z is a concave function of X. Since the
\end{flushleft}


\begin{flushleft}
squareroot is an increasing concave function, it follows from the composition rules that
\end{flushleft}


\begin{flushleft}
lkk = (w $-$ z T Y $-$1 z)1/2 is a concave function of X.
\end{flushleft}





\begin{flushleft}
Operations that preserve convexity
\end{flushleft}


\begin{flushleft}
3.28 Expressing a convex function as the pointwise supremum of a family of affine functions.
\end{flushleft}


\begin{flushleft}
In this problem we extend the result proved on page 83 to the case where dom f = R n .
\end{flushleft}


\begin{flushleft}
Let f : Rn $\rightarrow$ R be a convex function. Define f˜ : Rn $\rightarrow$ R as the pointwise supremum of
\end{flushleft}


\begin{flushleft}
all affine functions that are global underestimators of f :
\end{flushleft}


\begin{flushleft}
f˜(x) = sup\{g(x) | g affine, g(z) $\leq$ f (z) for all z\}.
\end{flushleft}





\begin{flushleft}
(a) Show that f (x) = f˜(x) for x $\in$ int dom f .
\end{flushleft}


\begin{flushleft}
(b) Show that f = f˜ if f is closed (i.e., epi f is a closed set; see §A.3.3).
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}
(a) The point (x, f (x)) is in the boundary of epi f . (If it were in int epi f , then for
\end{flushleft}


\begin{flushleft}
small, positive we would have (x, f (x) $-$ ) $\in$ epi f , which is impossible.) From
\end{flushleft}


\begin{flushleft}
the results of §2.5.2, we know there is a supporting hyperplane to epi f at (x, f (x)),
\end{flushleft}


\begin{flushleft}
i.e., a $\in$ Rn , b $\in$ R such that
\end{flushleft}


\begin{flushleft}
aT z + bt $\geq$ aT x + bf (x) for all (z, t) $\in$ epi f.
\end{flushleft}





\begin{flushleft}
Since t can be arbitrarily large if (z, t) $\in$ epi f , we conclude that b $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Suppose b = 0. Then
\end{flushleft}


\begin{flushleft}
aT z $\geq$ aT x for all z $\in$ dom f
\end{flushleft}


\begin{flushleft}
which contradicts x $\in$ int dom f . Therefore b $>$ 0. Dividing the above inequality
\end{flushleft}


\begin{flushleft}
by b yields
\end{flushleft}


\begin{flushleft}
t $\geq$ f (x) + (a/b)T (x $-$ z) for all (z, t) $\in$ epi f.
\end{flushleft}


\begin{flushleft}
Therefore the affine function
\end{flushleft}


\begin{flushleft}
g(z) = f (x) + (a/b)T (x $-$ z)
\end{flushleft}





\begin{flushleft}
is an affine global underestimator of f , and hence by definition of f˜,
\end{flushleft}


\begin{flushleft}
f (x) $\geq$ f˜(x) $\geq$ g(x).
\end{flushleft}





\begin{flushleft}
However g(x) = f (x), so we must have f (x) = f˜(x).
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) A closed convex set is the intersection of all halfspaces that contain it (see chapter 2,
\end{flushleft}


\begin{flushleft}
example 2.20). We will apply this result to epi f . Define
\end{flushleft}


\begin{flushleft}
H = \{(a, b, c) $\in$ Rn+2 | (a, b) = 0,
\end{flushleft}





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
(x,t)$\in$epi f
\end{flushleft}





\begin{flushleft}
(aT x + bt) $\geq$ c\}.
\end{flushleft}





\begin{flushleft}
Loosely speaking, H is the set of all halfspaces that contain epi f . By the result in
\end{flushleft}


\begin{flushleft}
chapter 2,
\end{flushleft}


\begin{flushleft}
epi f =
\end{flushleft}


\begin{flushleft}
\{(x, t) | aT x + bt $\geq$ c\}.
\end{flushleft}


\begin{flushleft}
(3.28.A)
\end{flushleft}


\begin{flushleft}
(a,b,c)$\in$H
\end{flushleft}





\begin{flushleft}
It is clear that all elements of H satisfy b $\geq$ 0. If in fact b $>$ 0, then the affine
\end{flushleft}


\begin{flushleft}
function
\end{flushleft}


\begin{flushleft}
h(x) = $-$(a/b)T x + c/b,
\end{flushleft}


\begin{flushleft}
minorizes f , since
\end{flushleft}





\begin{flushleft}
t $\geq$ f (x) $\geq$ $-$(a/b)T x + c/t = h(x)
\end{flushleft}





\begin{flushleft}
for all (x, t) $\in$ epi f . Conversely, if h(x) = $-$aT x + c minorizes f , then (a, 1, c) $\in$ H.
\end{flushleft}


\begin{flushleft}
We need to prove that
\end{flushleft}


\begin{flushleft}
epi f =
\end{flushleft}


\begin{flushleft}
(a,b,c)$\in$H, b$>$0
\end{flushleft}





\begin{flushleft}
\{(x, t) | aT x + bt $\geq$ c\}.
\end{flushleft}





\begin{flushleft}
(In words, epi f is the intersection of all {`}non-vertical' halfspaces that contain epi f .)
\end{flushleft}


\begin{flushleft}
Note that H may contain elements with b = 0, so this does not immediately follow
\end{flushleft}


\begin{flushleft}
from (3.28.A).
\end{flushleft}


\begin{flushleft}
We will show that
\end{flushleft}





\begin{flushleft}
(a,b,c)$\in$H, b$>$0
\end{flushleft}





\begin{flushleft}
\{(x, t) | aT x + bt $\geq$ c\} =
\end{flushleft}





\begin{flushleft}
(a,b,c)$\in$H
\end{flushleft}





\begin{flushleft}
\{(x, t) | aT x + bt $\geq$ c\}.
\end{flushleft}





\begin{flushleft}
(3.28.B)
\end{flushleft}





\begin{flushleft}
It is obvious that the set on the left includes the set on the right. To show that
\end{flushleft}


\begin{flushleft}
they are identical, assume (¯
\end{flushleft}


\begin{flushleft}
x, t¯) lies in the set on the left, i.e.,
\end{flushleft}


\begin{flushleft}
aT x
\end{flushleft}


\begin{flushleft}
¯ + bt¯ $\geq$ c
\end{flushleft}


\begin{flushleft}
for all halfspaces aT x + bt $\geq$ c that are nonvertical (i.e., b $>$ 0) and contain epi f .
\end{flushleft}


\begin{flushleft}
Assume that (¯
\end{flushleft}


\begin{flushleft}
x, t¯) is not in the set on the right, i.e., there exist (˜
\end{flushleft}


\begin{flushleft}
a, ˜b, c˜) $\in$ H
\end{flushleft}


\begin{flushleft}
(necessarily with ˜b = 0), such that
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
˜T x
\end{flushleft}


\begin{flushleft}
¯ $<$ c˜.
\end{flushleft}


\begin{flushleft}
H contains at least one element (a0 , b0 , c0 ) with b0 $>$ 0. (Otherwise epi f would be
\end{flushleft}


\begin{flushleft}
an intersection of vertical halfspaces.) Consider the halfspace defined by (˜
\end{flushleft}


\begin{flushleft}
a, 0, c˜) +
\end{flushleft}


\begin{flushleft}
(a0 , b0 , c0 ) for small positive . This halfspace is nonvertical and it contains epi f :
\end{flushleft}


(˜


\begin{flushleft}
a + a 0 )T x + b 0 t $\geq$ a
\end{flushleft}


\begin{flushleft}
˜T x + (aT0 x + b0 t) $\geq$ c˜ + c0 ,
\end{flushleft}


\begin{flushleft}
for all (x, t) $\in$ epi f , because the halfspaces a
\end{flushleft}


\begin{flushleft}
˜ T x $\geq$ c˜ and aT0 x+b0 t $\geq$ c0 both contain
\end{flushleft}


\begin{flushleft}
epi f . However,
\end{flushleft}


(˜


\begin{flushleft}
a + a 0 )T x
\end{flushleft}


\begin{flushleft}
¯ + b0 t¯ = a
\end{flushleft}


\begin{flushleft}
˜T x
\end{flushleft}


\begin{flushleft}
¯ + (aT0 x
\end{flushleft}


\begin{flushleft}
¯ + b0 t¯) $<$ c˜ + c0
\end{flushleft}


\begin{flushleft}
for small , so the halfspace does not contain (¯
\end{flushleft}


\begin{flushleft}
x, t¯). This contradicts our assumption
\end{flushleft}


\begin{flushleft}
that (¯
\end{flushleft}


\begin{flushleft}
x, t¯) is in the intersection of all nonvertical halfspaces containing epi f . We
\end{flushleft}


\begin{flushleft}
conclude that the equality (3.28.B) holds.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
3.29 Representation of piecewise-linear convex functions. A function f : R n $\rightarrow$ R, with
\end{flushleft}


\begin{flushleft}
dom f = Rn , is called piecewise-linear if there exists a partition of Rn as
\end{flushleft}


\begin{flushleft}
R n = X 1 $\cup$ X2 $\cup$ · · · $\cup$ X L ,
\end{flushleft}


\begin{flushleft}
where int Xi = $\emptyset$ and int Xi $\cap$ int Xj = $\emptyset$ for i = j, and a family of affine functions
\end{flushleft}


\begin{flushleft}
aT1 x + b1 , . . . , aTL x + bL such that f (x) = aTi x + bi for x $\in$ Xi .
\end{flushleft}


\begin{flushleft}
Show that this means that f (x) = max\{aT1 x + b1 , . . . , aTL x + bL \}.
\end{flushleft}


\begin{flushleft}
Solution. By Jensen's inequality, we have for all x, y $\in$ dom f , and t $\in$ [0, 1],
\end{flushleft}


\begin{flushleft}
f (y + t(x $-$ y)) $\leq$ f (y) + t(f (x) $-$ f (y)),
\end{flushleft}


\begin{flushleft}
and hence
\end{flushleft}





\begin{flushleft}
f (y + t(x $-$ y)) $-$ f (y)
\end{flushleft}


.


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
Now suppose x $\in$ Xi . Choose any y $\in$ int Xj , for some j, and take t sufficiently small so
\end{flushleft}


\begin{flushleft}
that y + t(x $-$ y) $\in$ Xj . The above inequality reduces to
\end{flushleft}


\begin{flushleft}
f (x) $\geq$ f (y) +
\end{flushleft}





\begin{flushleft}
aTi x + bi $\geq$ aTj y + bj +
\end{flushleft}





\begin{flushleft}
(aTj (y + t(x $-$ y)) + bj $-$ aTj y $-$ bj )
\end{flushleft}


\begin{flushleft}
= aTj x + bj .
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
This is true for any j, so aTi x + bi $\geq$ maxj=1,...,L (aTj x + bj ). We conclude that
\end{flushleft}


\begin{flushleft}
aTi x + bi = max (aTj x + bj ).
\end{flushleft}


\begin{flushleft}
j=1,...,L
\end{flushleft}





\begin{flushleft}
3.30 Convex hull or envelope of a function. The convex hull or convex envelope of a function
\end{flushleft}


\begin{flushleft}
f : Rn $\rightarrow$ R is defined as
\end{flushleft}


\begin{flushleft}
g(x) = inf\{t | (x, t) $\in$ conv epi f \}.
\end{flushleft}


\begin{flushleft}
Geometrically, the epigraph of g is the convex hull of the epigraph of f .
\end{flushleft}


\begin{flushleft}
Show that g is the largest convex underestimator of f . In other words, show that if h is
\end{flushleft}


\begin{flushleft}
convex and satisfies h(x) $\leq$ f (x) for all x, then h(x) $\leq$ g(x) for all x.
\end{flushleft}


\begin{flushleft}
Solution. It is clear that g is convex, since by construction its epigraph is a convex set.
\end{flushleft}


\begin{flushleft}
Let h be a convex lower bound on f . Since h is convex, epi h is a convex set. Since h is a
\end{flushleft}


\begin{flushleft}
lower bound on f , epi f $\subseteq$ epi h. By definition the convex hull of a set is the intersection
\end{flushleft}


\begin{flushleft}
of all the convex sets that contain the set. It follows that conv epi f = epi g $\subseteq$ epi h,
\end{flushleft}


\begin{flushleft}
i.e., g(x) $\geq$ h(x) for all x.
\end{flushleft}





\begin{flushleft}
3.31 [Roc70, page 35] Largest homogeneous underestimator. Let f be a convex function. Define
\end{flushleft}


\begin{flushleft}
the function g as
\end{flushleft}


\begin{flushleft}
f ($\alpha$x)
\end{flushleft}


.


\begin{flushleft}
g(x) = inf
\end{flushleft}


\begin{flushleft}
$\alpha$$>$0
\end{flushleft}


\begin{flushleft}
$\alpha$
\end{flushleft}


\begin{flushleft}
(a) Show that g is homogeneous (g(tx) = tg(x) for all t $\geq$ 0).
\end{flushleft}





\begin{flushleft}
(b) Show that g is the largest homogeneous underestimator of f : If h is homogeneous
\end{flushleft}


\begin{flushleft}
and h(x) $\leq$ f (x) for all x, then we have h(x) $\leq$ g(x) for all x.
\end{flushleft}


\begin{flushleft}
(c) Show that g is convex.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) If t $>$ 0,
\end{flushleft}


\begin{flushleft}
f ($\alpha$tx)
\end{flushleft}


\begin{flushleft}
f ($\alpha$tx)
\end{flushleft}


\begin{flushleft}
= t inf
\end{flushleft}


\begin{flushleft}
= tg(x).
\end{flushleft}


\begin{flushleft}
$\alpha$$>$0
\end{flushleft}


\begin{flushleft}
$\alpha$
\end{flushleft}


\begin{flushleft}
t$\alpha$
\end{flushleft}


\begin{flushleft}
For t = 0, we have g(tx) = g(0) = 0.
\end{flushleft}


\begin{flushleft}
g(tx) = inf
\end{flushleft}





\begin{flushleft}
$\alpha$$>$0
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) If h is a homogeneous underestimator, then
\end{flushleft}


\begin{flushleft}
h(x) =
\end{flushleft}





\begin{flushleft}
h($\alpha$x)
\end{flushleft}


\begin{flushleft}
f ($\alpha$x)
\end{flushleft}


$\leq$


\begin{flushleft}
$\alpha$
\end{flushleft}


\begin{flushleft}
$\alpha$
\end{flushleft}





\begin{flushleft}
for all $\alpha$ $>$ 0. Taking the infimum over $\alpha$ gives h(x) $\leq$ g(x).
\end{flushleft}





\begin{flushleft}
(c) We can express g as
\end{flushleft}





\begin{flushleft}
g(x) = inf tf (x/t) = inf h(x, t)
\end{flushleft}


\begin{flushleft}
t$>$0
\end{flushleft}





\begin{flushleft}
t$>$0
\end{flushleft}





\begin{flushleft}
where h is the perspective function of f . We know h is convex, jointly in x and t,
\end{flushleft}


\begin{flushleft}
so g is convex.
\end{flushleft}


\begin{flushleft}
3.32 Products and ratios of convex functions. In general the product or ratio of two convex
\end{flushleft}


\begin{flushleft}
functions is not convex. However, there are some results that apply to functions on R.
\end{flushleft}


\begin{flushleft}
Prove the following.
\end{flushleft}


\begin{flushleft}
(a) If f and g are convex, both nondecreasing (or nonincreasing), and positive functions
\end{flushleft}


\begin{flushleft}
on an interval, then f g is convex.
\end{flushleft}


\begin{flushleft}
(b) If f , g are concave, positive, with one nondecreasing and the other nonincreasing,
\end{flushleft}


\begin{flushleft}
then f g is concave.
\end{flushleft}


\begin{flushleft}
(c) If f is convex, nondecreasing, and positive, and g is concave, nonincreasing, and
\end{flushleft}


\begin{flushleft}
positive, then f /g is convex.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We prove the result by verifying Jensen's inequality. f and g are positive and convex,
\end{flushleft}


\begin{flushleft}
hence for 0 $\leq$ $\theta$ $\leq$ 1,
\end{flushleft}


\begin{flushleft}
f ($\theta$x + (1 $-$ $\theta$)y) g($\theta$x + (1 $-$ $\theta$)y)
\end{flushleft}





$\leq$


=





\begin{flushleft}
($\theta$f (x) + (1 $-$ $\theta$)f (y)) ($\theta$g(x) + (1 $-$ $\theta$)g(y))
\end{flushleft}


\begin{flushleft}
$\theta$f (x)g(x) + (1 $-$ $\theta$)f (y)g(y)
\end{flushleft}


\begin{flushleft}
+ $\theta$(1 $-$ $\theta$)(f (y) $-$ f (x))(g(x) $-$ g(y)).
\end{flushleft}





\begin{flushleft}
The third term is less than or equal to zero if f and g are both increasing or both
\end{flushleft}


\begin{flushleft}
decreasing. Therefore
\end{flushleft}


\begin{flushleft}
f ($\theta$x + (1 $-$ $\theta$)y) g($\theta$x + (1 $-$ $\theta$)y) $\leq$ $\theta$f (x)g(x) + (1 $-$ $\theta$)f (y)g(y).
\end{flushleft}


\begin{flushleft}
(b) Reverse the inequalities in the solution of part (a).
\end{flushleft}


\begin{flushleft}
(c) It suffices to note that 1/g is convex, positive and increasing, so the result follows
\end{flushleft}


\begin{flushleft}
from part (a).
\end{flushleft}


\begin{flushleft}
3.33 Direct proof of perspective theorem. Give a direct proof that the perspective function g,
\end{flushleft}


\begin{flushleft}
as defined in §3.2.6, of a convex function f is convex: Show that dom g is a convex set,
\end{flushleft}


\begin{flushleft}
and that for (x, t), (y, s) $\in$ dom g, and 0 $\leq$ $\theta$ $\leq$ 1, we have
\end{flushleft}


\begin{flushleft}
g($\theta$x + (1 $-$ $\theta$)y, $\theta$t + (1 $-$ $\theta$)s) $\leq$ $\theta$g(x, t) + (1 $-$ $\theta$)g(y, s).
\end{flushleft}


\begin{flushleft}
Solution. The domain dom g = \{(x, t) | x/t $\in$ dom f, t $>$ 0\} is the inverse image of
\end{flushleft}


\begin{flushleft}
dom f under the perspective function P : Rn+1 $\rightarrow$ Rn , P (x, t) = x/t for t $>$ 0, so it is
\end{flushleft}


\begin{flushleft}
convex (see §2.3.3).
\end{flushleft}


\begin{flushleft}
Jensen's inequality can be proved directly as follows. Suppose s, t $>$ 0, x/t $\in$ dom f ,
\end{flushleft}


\begin{flushleft}
y/s $\in$ dom f , and 0 $\leq$ $\theta$ $\leq$ 1. Then
\end{flushleft}


\begin{flushleft}
g($\theta$x + (1 $-$ $\theta$)y, $\theta$t + (1 $-$ $\theta$)s)
\end{flushleft}


\begin{flushleft}
= ($\theta$t + (1 $-$ $\theta$)s)f (($\theta$x + (1 $-$ $\theta$)y)/($\theta$t + (1 $-$ $\theta$)s))
\end{flushleft}


\begin{flushleft}
= ($\theta$t + (1 $-$ $\theta$)s)f (($\theta$t(x/t) + (1 $-$ $\theta$)s(y/s))/($\theta$t + (1 $-$ $\theta$)s))
\end{flushleft}


\begin{flushleft}
$\leq$ $\theta$tf (x/t) + (1 $-$ $\theta$)sf (y/s).
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
3.34 The Minkowski function. The Minkowski function of a convex set C is defined as
\end{flushleft}


\begin{flushleft}
MC (x) = inf\{t $>$ 0 | t$-$1 x $\in$ C\}.
\end{flushleft}


\begin{flushleft}
(a)
\end{flushleft}


\begin{flushleft}
(b)
\end{flushleft}


\begin{flushleft}
(c)
\end{flushleft}


\begin{flushleft}
(d)
\end{flushleft}


\begin{flushleft}
(e)
\end{flushleft}





\begin{flushleft}
Draw a picture giving a geometric interpretation of how to find MC (x).
\end{flushleft}


\begin{flushleft}
Show that MC is homogeneous, i.e., MC ($\alpha$x) = $\alpha$MC (x) for $\alpha$ $\geq$ 0.
\end{flushleft}


\begin{flushleft}
What is dom MC ?
\end{flushleft}


\begin{flushleft}
Show that MC is a convex function.
\end{flushleft}


\begin{flushleft}
Suppose C is also closed, symmetric (if x $\in$ C then $-$x $\in$ C), and has nonempty
\end{flushleft}


\begin{flushleft}
interior. Show that MC is a norm. What is the corresponding unit ball?
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Consider the ray, excluding 0, generated by x, i.e., sx for s $>$ 0. The intersection
\end{flushleft}


\begin{flushleft}
of this ray and C is either empty (meaning, the ray doesn't intersect C), a finite
\end{flushleft}


\begin{flushleft}
interval, or another ray (meaning, the ray enters C and stays in C).
\end{flushleft}


\begin{flushleft}
In the first case, the set \{t $>$ 0 | t$-$1 x $\in$ C\} is empty, so the infimum is $\infty$. This
\end{flushleft}


\begin{flushleft}
means MC (x) = $\infty$. This case is illustrated in the figure below, on the left.
\end{flushleft}


\begin{flushleft}
In the third case, the set \{s $>$ 0 | sx $\in$ C\} has the form [a, $\infty$) or (a, $\infty$), so the set
\end{flushleft}


\begin{flushleft}
\{t $>$ 0 | t$-$1 x $\in$ C\} has the form (0, 1/a] or (0, 1/a). In this case we have MC (x) = 0.
\end{flushleft}


\begin{flushleft}
That is illustrated in the figure below to the right.
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
C
\end{flushleft}





\begin{flushleft}
C
\end{flushleft}


\begin{flushleft}
PSfrag replacements
\end{flushleft}





0


0





\begin{flushleft}
In the second case, the set \{s $>$ 0 | sx $\in$ C\} is a bounded , interval with endpoints
\end{flushleft}


\begin{flushleft}
a $\leq$ b, so we have MC (x) = 1/b. That is shown below. In this example, the optimal
\end{flushleft}


\begin{flushleft}
scale factor is around s $\approx$ 3/4, so MC (x) $\approx$ 4/3.
\end{flushleft}





\begin{flushleft}
C
\end{flushleft}


\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
s x
\end{flushleft}





0





\begin{flushleft}
In any case, if x = 0 $\in$ C then MC (0) = 0.
\end{flushleft}


\begin{flushleft}
(b) If $\alpha$ $>$ 0, then
\end{flushleft}


\begin{flushleft}
MC ($\alpha$x)
\end{flushleft}





=


=


=





\begin{flushleft}
inf\{t $>$ 0 | t$-$1 $\alpha$x $\in$ C\}
\end{flushleft}





\begin{flushleft}
$\alpha$ inf\{t/$\alpha$ $>$ 0 | t$-$1 $\alpha$x $\in$ C\}
\end{flushleft}


\begin{flushleft}
$\alpha$MC (x).
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
If $\alpha$ = 0, then
\end{flushleft}


\begin{flushleft}
MC ($\alpha$x) = MC (0) =
\end{flushleft}





0


$\infty$





\begin{flushleft}
0$\in$C
\end{flushleft}


\begin{flushleft}
0 $\in$ C.
\end{flushleft}





\begin{flushleft}
(c) dom MC = \{x | x/t $\in$ C for some t $>$ 0\}. This is also known as the conic hull of C,
\end{flushleft}


\begin{flushleft}
except that 0 $\in$ dom MC only if 0 $\in$ C.
\end{flushleft}





\begin{flushleft}
(d) We have already seen that dom MC is a convex set. Suppose x, y $\in$ dom MC , and
\end{flushleft}


\begin{flushleft}
let $\theta$ $\in$ [0, 1]. Consider any tx , ty $>$ 0 for which x/tx $\in$ C, y/ty $\in$ C. (There exists
\end{flushleft}


\begin{flushleft}
at least one such pair, because x, y $\in$ dom MC .) It follows from convexity of C that
\end{flushleft}


\begin{flushleft}
$\theta$x + (1 $-$ $\theta$)y
\end{flushleft}


\begin{flushleft}
$\theta$tx (x/tx ) + (1 $-$ $\theta$)ty (y/ty )
\end{flushleft}


\begin{flushleft}
$\in$C
\end{flushleft}


=


\begin{flushleft}
$\theta$tx + (1 $-$ $\theta$)ty )
\end{flushleft}


\begin{flushleft}
$\theta$tx + (1 $-$ $\theta$)ty
\end{flushleft}


\begin{flushleft}
and therefore
\end{flushleft}


\begin{flushleft}
MC ($\theta$x + (1 $-$ $\theta$)y) $\leq$ $\theta$tx + (1 $-$ $\theta$)ty .
\end{flushleft}





\begin{flushleft}
This is true for any tx , ty $>$ 0 that satisfy x/tx $\in$ C, y/ty $\in$ C. Therefore
\end{flushleft}


\begin{flushleft}
MC ($\theta$x + (1 $-$ $\theta$)y)
\end{flushleft}





$\leq$


=





\begin{flushleft}
$\theta$ inf\{tx $>$ 0 | x/tx $\in$ C\} + (1 $-$ $\theta$) inf\{ty $>$ 0 | y/ty $\in$ C\}
\end{flushleft}


\begin{flushleft}
$\theta$MC (x) + (1 $-$ $\theta$)MC (y).
\end{flushleft}





\begin{flushleft}
Here is an alternative snappy, modern style proof:
\end{flushleft}


\begin{flushleft}
$\bullet$ The indicator function of C, i.e., IC , is convex.
\end{flushleft}


\begin{flushleft}
$\bullet$ The perspective function, tIC (x/t) is convex in (x, t). But this is the same as
\end{flushleft}


\begin{flushleft}
IC (x/t), so IC (x/t) is convex in (x, t).
\end{flushleft}


\begin{flushleft}
$\bullet$ The function t + IC (x/t) is convex in (x, t).
\end{flushleft}


\begin{flushleft}
$\bullet$ Now let's minimize over t, to obtain inf t (t + IC (x/t)) = MC (x), which is convex
\end{flushleft}


\begin{flushleft}
by the minimization rule.
\end{flushleft}


\begin{flushleft}
(e) It is the norm with unit ball C.
\end{flushleft}


\begin{flushleft}
(a) Since by assumption, 0 $\in$ int C, MC (x) $>$ 0 for x = 0. By definition MC (0) = 0.
\end{flushleft}


\begin{flushleft}
(b) Homogeneity: for $\lambda$ $>$ 0,
\end{flushleft}


\begin{flushleft}
MC ($\lambda$x)
\end{flushleft}





=


=


=





\begin{flushleft}
inf\{t $>$ 0 | (t$\lambda$)$-$1 x $\in$ C\}
\end{flushleft}


\begin{flushleft}
$\lambda$ inf\{u $>$ 0 | u$-$1 x $\in$ C\}
\end{flushleft}


\begin{flushleft}
$\lambda$MC (x).
\end{flushleft}





\begin{flushleft}
By symmetry of C, we also have MC ($-$x) = $-$MC (x).
\end{flushleft}


\begin{flushleft}
(c) Triangle inequality. By convexity (part d), and homogeneity,
\end{flushleft}


\begin{flushleft}
MC (x + y) = 2MC ((1/2)x + (1/2)y) $\leq$ MC (x) + MC (y).
\end{flushleft}


\begin{flushleft}
3.35 Support function calculus. Recall that the support function of a set C $\subseteq$ R n is defined as
\end{flushleft}


\begin{flushleft}
SC (y) = sup\{y T x | x $\in$ C\}. On page 81 we showed that SC is a convex function.
\end{flushleft}


\begin{flushleft}
(a) Show that SB = Sconv B .
\end{flushleft}


\begin{flushleft}
(b) Show that SA+B = SA + SB .
\end{flushleft}


\begin{flushleft}
(c) Show that SA$\cup$B = max\{SA , SB \}.
\end{flushleft}





\begin{flushleft}
(d) Let B be closed and convex. Show that A $\subseteq$ B if and only if SA (y) $\leq$ SB (y) for all
\end{flushleft}


\begin{flushleft}
y.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
(a) Let A = conv B. Since B $\subseteq$ A, we obviously have SB (y) $\leq$ SA (y). Suppose we have
\end{flushleft}


\begin{flushleft}
strict inequality for some y, i.e.,
\end{flushleft}


\begin{flushleft}
yT u $<$ yT v
\end{flushleft}


\begin{flushleft}
for all u $\in$ B and some v $\in$ A. This leads to a contradiction, because by definition v
\end{flushleft}


\begin{flushleft}
is the convex combination of a set of points ui $\in$ B, i.e., v = i $\theta$i ui , with $\theta$i $\geq$ 0,
\end{flushleft}


\begin{flushleft}
$\theta$ = 1. Since
\end{flushleft}


\begin{flushleft}
i i
\end{flushleft}


\begin{flushleft}
y T ui $<$ y T v
\end{flushleft}


\begin{flushleft}
for all i, this would imply
\end{flushleft}


\begin{flushleft}
yT v =
\end{flushleft}





\begin{flushleft}
$\theta$ i y T ui $<$
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\theta$i y T v = y T v.
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
We conclude that we must have equality SB (y) = SA (y).
\end{flushleft}


\begin{flushleft}
(b) Follows from
\end{flushleft}


\begin{flushleft}
SA+B (y)
\end{flushleft}





=


=


=





\begin{flushleft}
sup\{y T (u + v) | u $\in$ A, v $\in$ B\}
\end{flushleft}





\begin{flushleft}
sup\{y T u | u $\in$ A\} + sup\{y T v | u $\in$ B\}
\end{flushleft}


\begin{flushleft}
SA (y) + SB (y).
\end{flushleft}





\begin{flushleft}
(c) Follows from
\end{flushleft}


\begin{flushleft}
SA$\cup$B (y)
\end{flushleft}





=


=


=





\begin{flushleft}
sup\{y T u | u $\in$ A $\cup$ B\}
\end{flushleft}





\begin{flushleft}
max\{sup\{y T u | u $\in$ A\}, sup\{y T v | u $\in$ B\}
\end{flushleft}


\begin{flushleft}
max\{SA (y), SB (y)\}.
\end{flushleft}





\begin{flushleft}
(d) Obviously, if A $\subseteq$ B, then SA (y) $\leq$ SB (y) for all y. We need to show that if A $\subseteq$ B,
\end{flushleft}


\begin{flushleft}
then SA (y) $>$ SB (y) for some y.
\end{flushleft}


\begin{flushleft}
Suppose A $\subseteq$ B. Consider a point x
\end{flushleft}


\begin{flushleft}
¯ $\in$ A, x
\end{flushleft}


\begin{flushleft}
¯ $\in$ B. Since B is closed and convex, x
\end{flushleft}


¯


\begin{flushleft}
can be strictly separated from B by a hyperplane, i.e., there is a y = 0 such that
\end{flushleft}


\begin{flushleft}
yT x
\end{flushleft}


\begin{flushleft}
¯ $>$ yT x
\end{flushleft}


\begin{flushleft}
for all x $\in$ B. It follows that SB (y) $<$ y T x
\end{flushleft}


\begin{flushleft}
¯ $\leq$ SA (y).
\end{flushleft}





\begin{flushleft}
Conjugate functions
\end{flushleft}


\begin{flushleft}
3.36 Derive the conjugates of the following functions.
\end{flushleft}


\begin{flushleft}
(a) Max function. f (x) = maxi=1,...,n xi on Rn .
\end{flushleft}


\begin{flushleft}
Solution. We will show that
\end{flushleft}


\begin{flushleft}
f ∗ (y) =
\end{flushleft}





\begin{flushleft}
if y 0, 1T y = 1
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





0


$\infty$





\begin{flushleft}
We first verify the domain of f ∗ . First suppose y has a negative component, say
\end{flushleft}


\begin{flushleft}
yk $<$ 0. If we choose a vector x with xk = $-$t, xi = 0 for i = k, and let t go to
\end{flushleft}


\begin{flushleft}
infinity, we see that
\end{flushleft}


\begin{flushleft}
xT y $-$ max xi = $-$tyk $\rightarrow$ $\infty$,
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
so y is not in dom f ∗ . Next, assume y
\end{flushleft}


\begin{flushleft}
t go to infinity, to show that
\end{flushleft}





\begin{flushleft}
0 but 1T y $>$ 1. We choose x = t1 and let
\end{flushleft}





\begin{flushleft}
xT y $-$ max xi = t1T y $-$ t
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
is unbounded above. Similarly, when y
\end{flushleft}


\begin{flushleft}
0 and 1T y $<$ 1, we choose x = $-$t1 and
\end{flushleft}


\begin{flushleft}
let t go to infinity.
\end{flushleft}


\begin{flushleft}
The remaining case for y is y 0 and 1T y = 1. In this case we have
\end{flushleft}


\begin{flushleft}
xT y $\leq$ max xi
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
for all x, and therefore x y$-$maxi xi $\leq$ 0 for all x, with equality for x = 0. Therefore
\end{flushleft}


\begin{flushleft}
f ∗ (y) = 0.
\end{flushleft}


\begin{flushleft}
(b) Sum of largest elements. f (x) =
\end{flushleft}


\begin{flushleft}
Solution. The conjugate is
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





0


$\infty$





\begin{flushleft}
f ∗ (y) =
\end{flushleft}





\begin{flushleft}
x[i] on Rn .
\end{flushleft}





\begin{flushleft}
0 y 1,
\end{flushleft}


\begin{flushleft}
otherwise,
\end{flushleft}





\begin{flushleft}
1T y = r
\end{flushleft}





\begin{flushleft}
We first verify the domain of f ∗ . Suppose y has a negative component, say yk $<$ 0.
\end{flushleft}


\begin{flushleft}
If we choose a vector x with xk = $-$t, xi = 0 for i = k, and let t go to infinity, we
\end{flushleft}


\begin{flushleft}
see that
\end{flushleft}


\begin{flushleft}
xT y $-$ f (x) = $-$tyk $\rightarrow$ $\infty$,
\end{flushleft}





\begin{flushleft}
so y is not in dom f ∗ .
\end{flushleft}


\begin{flushleft}
Next, suppose y has a component greater than 1, say yk $>$ 1. If we choose a vector
\end{flushleft}


\begin{flushleft}
x with xk = t, xi = 0 for i = k, and let t go to infinity, we see that
\end{flushleft}


\begin{flushleft}
xT y $-$ f (x) = tyk $-$ t $\rightarrow$ $\infty$,
\end{flushleft}


\begin{flushleft}
so y is not in dom f ∗ .
\end{flushleft}


\begin{flushleft}
Finally, assume that 1T x = r. We choose x = t1 and find that
\end{flushleft}


\begin{flushleft}
xT y $-$ f (x) = t1T y $-$ tr
\end{flushleft}


\begin{flushleft}
is unbounded above, as t $\rightarrow$ $\infty$ or t $\rightarrow$ $-$$\infty$.
\end{flushleft}


\begin{flushleft}
If y satisfies all the conditions we have
\end{flushleft}


\begin{flushleft}
xT y $\leq$ f (x)
\end{flushleft}


\begin{flushleft}
for all x, with equality for x = 0. Therefore f ∗ (y) = 0.
\end{flushleft}





\begin{flushleft}
(c) Piecewise-linear function on R. f (x) = maxi=1,...,m (ai x + bi ) on R. You can
\end{flushleft}


\begin{flushleft}
assume that the ai are sorted in increasing order, i.e., a1 $\leq$ · · · $\leq$ am , and that none
\end{flushleft}


\begin{flushleft}
of the functions ai x + bi is redundant, i.e., for each k there is at least one x with
\end{flushleft}


\begin{flushleft}
f (x) = ak x + bk .
\end{flushleft}


\begin{flushleft}
Solution. Under the assumption, the graph of f is a piecewise-linear, with breakpoints (bi $-$ bi+1 )/(ai+1 $-$ ai ), i = 1, . . . , m $-$ 1. We can write f ∗ as
\end{flushleft}


\begin{flushleft}
f ∗ (y) = sup xy $-$ max (ai x + bi )
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
i=1,...,m
\end{flushleft}





\begin{flushleft}
We see that dom f ∗ = [a1 , am ], since for y outside that range, the expression inside
\end{flushleft}


\begin{flushleft}
the supremum is unbounded above. For ai $\leq$ y $\leq$ ai+1 , the supremum in the
\end{flushleft}


\begin{flushleft}
definition of f ∗ is reached at the breakpoint between the segments i and i + 1, i.e.,
\end{flushleft}


\begin{flushleft}
at the point (bi+1 $-$ bi )/(ai+1 $-$ ai ), so we obtain
\end{flushleft}


\begin{flushleft}
f ∗ (y) = $-$bi $-$ (bi+1 $-$ bi )
\end{flushleft}





\begin{flushleft}
y $-$ ai
\end{flushleft}


\begin{flushleft}
ai+1 $-$ ai
\end{flushleft}





\begin{flushleft}
where i is defined by ai $\leq$ y $\leq$ ai+1 . Hence the graph of f ∗ is also a piecewise-linear
\end{flushleft}


\begin{flushleft}
curve connecting the points (ai , $-$bi ) for i = 1, . . . , m. Geometrically, the epigraph
\end{flushleft}


\begin{flushleft}
of f ∗ is the epigraphical hull of the points (ai , $-$bi ).
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
(d) Power function. f (x) = xp on R++ , where p $>$ 1. Repeat for p $<$ 0.
\end{flushleft}


\begin{flushleft}
Solution. We'll use standard notation: we define q by the equation 1/p + 1/q = 1,
\end{flushleft}


\begin{flushleft}
i.e., q = p/(p $-$ 1).
\end{flushleft}


\begin{flushleft}
We start with the case p $>$ 1. Then xp is strictly convex on R+ . For y $<$ 0 the
\end{flushleft}


\begin{flushleft}
function yx $-$ xp achieves its maximum for x $>$ 0 at x = 0, so f ∗ (y) = 0. For y $>$ 0
\end{flushleft}


\begin{flushleft}
the function achieves its maximum at x = (y/p)1/(p$-$1) , where it has value
\end{flushleft}


\begin{flushleft}
y(y/p)1/(p$-$1) $-$ (y/p)p/(p$-$1) = (p $-$ 1)(y/p)q .
\end{flushleft}


\begin{flushleft}
Therefore we have
\end{flushleft}


\begin{flushleft}
f ∗ (y) =
\end{flushleft}





\begin{flushleft}
y$\leq$0
\end{flushleft}


\begin{flushleft}
y $>$ 0.
\end{flushleft}





0


\begin{flushleft}
(p $-$ 1)(y/p)q
\end{flushleft}





\begin{flushleft}
For p $<$ 0 similar arguments show that dom f ∗ = $-$R++ and f ∗ (y) =
\end{flushleft}





\begin{flushleft}
$-$p
\end{flushleft}


\begin{flushleft}
($-$y/p)q .
\end{flushleft}


\begin{flushleft}
q
\end{flushleft}





\begin{flushleft}
(e) Geometric mean. f (x) = $-$( xi )1/n on Rn
\end{flushleft}


++ .


\begin{flushleft}
Solution. The conjugate function is
\end{flushleft}


\begin{flushleft}
f ∗ (y) =
\end{flushleft}





0


$\infty$





\begin{flushleft}
if y 0,
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
1/n
\end{flushleft}





\begin{flushleft}
($-$yi )
\end{flushleft}





\begin{flushleft}
$\geq$ 1/n
\end{flushleft}





\begin{flushleft}
We first verify the domain of f ∗ . Assume y has a positive component, say yk $>$ 0.
\end{flushleft}


\begin{flushleft}
Then we can choose xk = t and xi = 1, i = k, to show that
\end{flushleft}


\begin{flushleft}
xT y $-$ f (x) = tyk +
\end{flushleft}





\begin{flushleft}
i=k
\end{flushleft}





\begin{flushleft}
yi $-$ t1/n
\end{flushleft}





\begin{flushleft}
is unbounded above as a function of t $>$ 0. Hence the condition y
\end{flushleft}


\begin{flushleft}
0 is indeed
\end{flushleft}


\begin{flushleft}
required.
\end{flushleft}


\begin{flushleft}
Next assume that y
\end{flushleft}


\begin{flushleft}
0, but ( i ($-$yi ))1/n $<$ 1/n. We choose xi = $-$t/yi , and
\end{flushleft}


\begin{flushleft}
obtain
\end{flushleft}


\begin{flushleft}
1/n
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
x y $-$ f (x) = $-$tn $-$ t
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





1


($-$ )


\begin{flushleft}
yi
\end{flushleft}





$\rightarrow$$\infty$





\begin{flushleft}
as t $\rightarrow$ $\infty$. This demonstrates that the second condition for the domain of f ∗ is also
\end{flushleft}


\begin{flushleft}
needed.
\end{flushleft}


\begin{flushleft}
1/n
\end{flushleft}


\begin{flushleft}
($-$yi )
\end{flushleft}


\begin{flushleft}
$\geq$ 1/n, and x
\end{flushleft}


\begin{flushleft}
0. The arithmeticNow assume that y
\end{flushleft}


\begin{flushleft}
0 and
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
geometric mean inequality states that
\end{flushleft}


\begin{flushleft}
xT y
\end{flushleft}


$\geq$


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
1/n
\end{flushleft}





\begin{flushleft}
1/n
\end{flushleft}





1


$\geq$


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
($-$yi xi )
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
xi
\end{flushleft}





,





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i.e., xT y $\geq$ f (x) with equality for xi = $-$1/yi . Hence, f ∗ (y) = 0.
\end{flushleft}





\begin{flushleft}
(f) Negative generalized logarithm for second-order cone. f (x, t) = $-$ log(t 2 $-$ xT x) on
\end{flushleft}


\begin{flushleft}
\{(x, t) $\in$ Rn × R | x 2 $<$ t\}.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
f ∗ (y, u) = $-$2 + log 4 $-$ log(u2 $-$ y T y),
\end{flushleft}


\begin{flushleft}
We first verify the domain. Suppose y
\end{flushleft}


\begin{flushleft}
s y 2 $\geq$ $-$su, with s $\geq$ 0. Then
\end{flushleft}





2





\begin{flushleft}
dom f ∗ = \{(y, u) | y
\end{flushleft}





2





\begin{flushleft}
$<$ $-$u\}.
\end{flushleft}





\begin{flushleft}
$\geq$ $-$u. Choose x = sy, t = s( x
\end{flushleft}





\begin{flushleft}
y T x + tu $>$ sy T y $-$ su2 = s(u2 $-$ y T y) $\geq$ 0,
\end{flushleft}





2





+ 1) $>$





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
so y x + tu goes to infinity, at a linear rate, while the function $-$ log(t2 $-$ xT x) goes
\end{flushleft}


\begin{flushleft}
to $-$$\infty$ as $-$ log s. Therefore
\end{flushleft}


\begin{flushleft}
y T x + tu + log(t2 $-$ xT x)
\end{flushleft}


\begin{flushleft}
is unbounded above.
\end{flushleft}


\begin{flushleft}
Next, assume that y
\end{flushleft}





2





\begin{flushleft}
$<$ u. Setting the derivative of
\end{flushleft}


\begin{flushleft}
y T x + ut + log(t2 $-$ xT x)
\end{flushleft}





\begin{flushleft}
with respect to x and t equal to zero, and solving for t and x we see that the
\end{flushleft}


\begin{flushleft}
maximizer is
\end{flushleft}


\begin{flushleft}
2u
\end{flushleft}


\begin{flushleft}
2y
\end{flushleft}


,


\begin{flushleft}
t=$-$ 2
\end{flushleft}


.


\begin{flushleft}
x= 2
\end{flushleft}


\begin{flushleft}
u $-$ yT y
\end{flushleft}


\begin{flushleft}
u $-$ yT y
\end{flushleft}


\begin{flushleft}
This gives
\end{flushleft}


\begin{flushleft}
f ∗ (y, u)
\end{flushleft}





=


=





\begin{flushleft}
ut + y T x + log(t2 $-$ xT x)
\end{flushleft}





\begin{flushleft}
$-$2 + log 4 $-$ log(y 2 $-$ ut u).
\end{flushleft}





\begin{flushleft}
3.37 Show that the conjugate of f (X) = tr(X $-$1 ) with dom f = Sn
\end{flushleft}


\begin{flushleft}
++ is given by
\end{flushleft}


\begin{flushleft}
f ∗ (Y ) = $-$2 tr($-$Y )1/2 ,
\end{flushleft}





\begin{flushleft}
dom f ∗ = $-$Sn
\end{flushleft}


+.





\begin{flushleft}
Hint. The gradient of f is $\nabla$f (X) = $-$X $-$2 .
\end{flushleft}


\begin{flushleft}
Solution. We first verify the domain of f ∗ . Suppose Y has eigenvalue decomposition
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
Y = Q$\Lambda$QT =
\end{flushleft}





\begin{flushleft}
$\lambda$i qi qiT
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=2
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
with $\lambda$1 $>$ 0. Let X = Q diag(t, 1, . . . , 1)Q = tq1 q1T +
\end{flushleft}





\begin{flushleft}
qi qiT . We have
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
tr XY $-$ tr X $-$1 = t$\lambda$1 +
\end{flushleft}





\begin{flushleft}
i=2
\end{flushleft}





\begin{flushleft}
$\lambda$i $-$ 1/t $-$ (n $-$ 1),
\end{flushleft}





\begin{flushleft}
which grows unboundedly as t $\rightarrow$ $\infty$. Therefore Y $\in$ dom f ∗ .
\end{flushleft}


\begin{flushleft}
Next, assume Y
\end{flushleft}


\begin{flushleft}
0. If Y ≺ 0, we can find the maximum of
\end{flushleft}


\begin{flushleft}
tr XY $-$ tr X $-$1
\end{flushleft}





\begin{flushleft}
by setting the gradient equal to zero. We obtain Y = $-$X $-$2 , i.e., X = ($-$Y )$-$1/2 , and
\end{flushleft}


\begin{flushleft}
f ∗ (Y ) = $-$2 tr($-$Y )1/2 .
\end{flushleft}


\begin{flushleft}
Finally we verify that this expression remains valid when Y
\end{flushleft}


\begin{flushleft}
0, but Y is singular.
\end{flushleft}


\begin{flushleft}
This follows from the fact that conjugate functions are always closed, i.e., have closed
\end{flushleft}


\begin{flushleft}
epigraphs.
\end{flushleft}


\begin{flushleft}
3.38 Young's inequality. Let f : R $\rightarrow$ R be an increasing function, with f (0) = 0, and let g be
\end{flushleft}


\begin{flushleft}
its inverse. Define F and G as
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
f (a) da,
\end{flushleft}





\begin{flushleft}
F (x) =
\end{flushleft}


0





\begin{flushleft}
g(a) da.
\end{flushleft}





\begin{flushleft}
G(y) =
\end{flushleft}


0





\begin{flushleft}
Show that F and G are conjugates. Give a simple graphical interpretation of Young's
\end{flushleft}


\begin{flushleft}
inequality,
\end{flushleft}


\begin{flushleft}
xy $\leq$ F (x) + G(y).
\end{flushleft}


\begin{flushleft}
Solution. The inequality xy $\leq$ F (x) + G(y) has a simple geometric meaning, illustrated
\end{flushleft}


\begin{flushleft}
below.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
f (x)
\end{flushleft}





\begin{flushleft}
y
\end{flushleft}


\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
G(y)
\end{flushleft}





\begin{flushleft}
F (x)
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
F (x) is the shaded area under the graph of f , from 0 to x. G(y) is the area above the
\end{flushleft}


\begin{flushleft}
graph of f , from 0 to y. For fixed x and y, F (x) + G(y) is the total area below the
\end{flushleft}


\begin{flushleft}
graph, up to x, and above the graph, up to y. This is at least equal to xy, the area of the
\end{flushleft}


\begin{flushleft}
rectangle defined by x and y, hence
\end{flushleft}


\begin{flushleft}
F (x) + G(y) $\geq$ xy
\end{flushleft}


\begin{flushleft}
for all x, y.
\end{flushleft}


\begin{flushleft}
It is also clear that F (x) + G(y) = xy if and only if y = f (x). In other words
\end{flushleft}


\begin{flushleft}
G(y) = sup(xy $-$ F (x)),
\end{flushleft}





\begin{flushleft}
F (x) = sup(xy $-$ G(y)),
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
i.e., the functions are conjugates.
\end{flushleft}


\begin{flushleft}
3.39 Properties of conjugate functions.
\end{flushleft}


\begin{flushleft}
(a) Conjugate of convex plus affine function. Define g(x) = f (x) + cT x + d, where f is
\end{flushleft}


\begin{flushleft}
convex. Express g ∗ in terms of f ∗ (and c, d).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
g ∗ (y)
\end{flushleft}





\begin{flushleft}
sup(y T x $-$ f (x) $-$ cT x $-$ d)
\end{flushleft}





=





\begin{flushleft}
sup((y $-$ c)T x $-$ f (x)) $-$ d
\end{flushleft}


\begin{flushleft}
f ∗ (y $-$ c) $-$ d.
\end{flushleft}





=


=





\begin{flushleft}
(b) Conjugate of perspective. Express the conjugate of the perspective of a convex
\end{flushleft}


\begin{flushleft}
function f in terms of f ∗ .
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
g ∗ (y, s)
\end{flushleft}





=





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
x/t$\in$dom f,t$>$0
\end{flushleft}





=





\begin{flushleft}
sup
\end{flushleft}





\begin{flushleft}
(y T x + st $-$ tf (x/t))
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}





\begin{flushleft}
t$>$0 x/t$\in$dom f
\end{flushleft}





=





\begin{flushleft}
sup t(s +
\end{flushleft}


\begin{flushleft}
t$>$0
\end{flushleft}





=





\begin{flushleft}
(t(y T (x/t) + s $-$ f (x/t)))
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
x/t$\in$dom f
\end{flushleft}





\begin{flushleft}
(y T (x/t) $-$ f (x/t)))
\end{flushleft}





\begin{flushleft}
sup t(s + f ∗ (y))
\end{flushleft}


\begin{flushleft}
t$>$0
\end{flushleft}





=





0


$\infty$





\begin{flushleft}
s + f ∗ (y) $\leq$ 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(c) Conjugate and minimization. Let f (x, z) be convex in (x, z) and define g(x) =
\end{flushleft}


\begin{flushleft}
inf z f (x, z). Express the conjugate g ∗ in terms of f ∗ .
\end{flushleft}


\begin{flushleft}
As an application, express the conjugate of g(x) = inf z \{h(z) | Az + b = x\}, where h
\end{flushleft}


\begin{flushleft}
is convex, in terms of h∗ , A, and b.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
g ∗ (y)
\end{flushleft}





\begin{flushleft}
sup(xT y $-$ inf f (x, z))
\end{flushleft}





=





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





=





\begin{flushleft}
sup(x y $-$ f (x, z))
\end{flushleft}





=





\begin{flushleft}
f ∗ (y, 0).
\end{flushleft}





\begin{flushleft}
x,z
\end{flushleft}





\begin{flushleft}
To answer the second part of the problem, we apply the previous result to
\end{flushleft}


\begin{flushleft}
h(z)
\end{flushleft}


$\infty$





\begin{flushleft}
f (x, z) =
\end{flushleft}





\begin{flushleft}
Az + b = x
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
We have
\end{flushleft}


\begin{flushleft}
f ∗ (y, v)
\end{flushleft}





\begin{flushleft}
inf(y T x $-$ v T z $-$ f (x, z))
\end{flushleft}





=


=





\begin{flushleft}
Therefore
\end{flushleft}





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
Az+b=x
\end{flushleft}





\begin{flushleft}
(y T x $-$ v T z $-$ h(z))
\end{flushleft}





=





\begin{flushleft}
inf (y T (Az + b) $-$ v T z $-$ h(z))
\end{flushleft}





=





\begin{flushleft}
bT y + inf (y T Az $-$ v T z $-$ h(z))
\end{flushleft}





=





\begin{flushleft}
bT y + h∗ (AT y $-$ v).
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
g ∗ (y) = f ∗ (y, 0) = bT y + h∗ (AT y).
\end{flushleft}





\begin{flushleft}
(d) Conjugate of conjugate. Show that the conjugate of the conjugate of a closed convex
\end{flushleft}


\begin{flushleft}
function is itself: f = f ∗∗ if f is closed and convex. (A function is closed if its
\end{flushleft}


\begin{flushleft}
epigraph is closed; see §A.3.3.) Hint. Show that f ∗∗ is the pointwise supremum of
\end{flushleft}


\begin{flushleft}
all affine global underestimators of f . Then apply the result of exercise 3.28.
\end{flushleft}


\begin{flushleft}
Solution. By definition of f ∗ ,
\end{flushleft}


\begin{flushleft}
f ∗ (y) = sup(y T x $-$ f (x)).
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





∗





\begin{flushleft}
If y $\in$ dom f , then the affine function h(x) = y T x$-$f ∗ (y), minorizes f . Conversely,
\end{flushleft}


\begin{flushleft}
if h(x) = aT x + b minorizes f , then a $\in$ dom f ∗ and f ∗ (a) $\leq$ $-$b. The set of all
\end{flushleft}


\begin{flushleft}
affine functions that minorize f is therefore exactly equal to the set of all functions
\end{flushleft}


\begin{flushleft}
h(x) = y T x + c where
\end{flushleft}


\begin{flushleft}
y $\in$ dom f ∗ ,
\end{flushleft}


\begin{flushleft}
c $\leq$ $-$f ∗ (y).
\end{flushleft}


\begin{flushleft}
Therefore, by the result of exercise 3.28,
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
y$\in$dom f ∗
\end{flushleft}





\begin{flushleft}
(y T x $-$ f ∗ (y)) = f ∗∗ (y).
\end{flushleft}





\begin{flushleft}
3.40 Gradient and Hessian of conjugate function. Suppose f : Rn $\rightarrow$ R is convex and twice
\end{flushleft}


\begin{flushleft}
continuously differentiable. Suppose y¯ and x
\end{flushleft}


\begin{flushleft}
¯ are related by y¯ = $\nabla$f (¯
\end{flushleft}


\begin{flushleft}
x), and that $\nabla$ 2 f (¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


0.


\begin{flushleft}
(a) Show that $\nabla$f ∗ (¯
\end{flushleft}


\begin{flushleft}
y) = x
\end{flushleft}


¯.





\begin{flushleft}
(b) Show that $\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y ) = $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)$-$1 .
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
Solution. We use the implicit function theorem: Suppose F : Rn × Rm $\rightarrow$ R satisfies
\end{flushleft}


\begin{flushleft}
$\bullet$ F (¯
\end{flushleft}


\begin{flushleft}
u, v¯) = 0
\end{flushleft}





\begin{flushleft}
$\bullet$ F is continuously differentiable and Dv F (u, v) is nonsingular in a neighborhood of
\end{flushleft}


(¯


\begin{flushleft}
u, v¯).
\end{flushleft}


\begin{flushleft}
Then there exists a continuously differentiable function $\phi$ : Rn $\rightarrow$ Rm , that satisfies
\end{flushleft}


\begin{flushleft}
v¯ = $\phi$(¯
\end{flushleft}


\begin{flushleft}
u) and
\end{flushleft}


\begin{flushleft}
F (u, $\phi$(u)) = 0
\end{flushleft}


\begin{flushleft}
in a neighborhood of u
\end{flushleft}


¯.


\begin{flushleft}
Applying this to u = y, v = x, and F (u, v) = $\nabla$f (x) $-$ y, we see that there exists a
\end{flushleft}


\begin{flushleft}
continuously differentiable function g such that
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
¯ = g(¯
\end{flushleft}


\begin{flushleft}
y ),
\end{flushleft}


\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
$\nabla$f (g(y)) = y
\end{flushleft}





\begin{flushleft}
in a neighborhood around y¯. Differentiating both sides with respect to y gives
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (g(y))Dg(y) = I,
\end{flushleft}


\begin{flushleft}
i.e., Dg(y) = $\nabla$2 f (g(y))$-$1 , in a neighborhood of y¯.
\end{flushleft}


\begin{flushleft}
Now suppose y is near y¯. The maximum in the definition of f ∗ (y),
\end{flushleft}


\begin{flushleft}
f ∗ (y) = sup(˜
\end{flushleft}


\begin{flushleft}
y T x $-$ f (x)),
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
is attained at x = g(y), and the maximizer is unique, by the fact that $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
therefore have
\end{flushleft}


\begin{flushleft}
f ∗ (y) = y T g(y) $-$ f (g(y)).
\end{flushleft}





\begin{flushleft}
0. We
\end{flushleft}





\begin{flushleft}
Differentiating with respect to y gives
\end{flushleft}


\begin{flushleft}
$\nabla$f ∗ (y)
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
In particular,
\end{flushleft}





=


=


=





\begin{flushleft}
g(y) + Dg(y)T y $-$ Dg(y)T $\nabla$f (g(y))
\end{flushleft}


\begin{flushleft}
g(y) + Dg(y)T y $-$ Dg(y)T y
\end{flushleft}


\begin{flushleft}
g(y)
\end{flushleft}





\begin{flushleft}
$\nabla$2 f ∗ (y) = Dg(y) = $\nabla$2 f (g(y))$-$1 .
\end{flushleft}


\begin{flushleft}
$\nabla$f ∗ (¯
\end{flushleft}


\begin{flushleft}
y) = x
\end{flushleft}


¯,





\begin{flushleft}
$\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y ) = $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)$-$1 .
\end{flushleft}





\begin{flushleft}
3.41 Domain of conjugate function. Suppose f : Rn $\rightarrow$ R is a twice differentiable convex
\end{flushleft}


\begin{flushleft}
function and x $\in$ dom f . Show that for small enough u we have
\end{flushleft}


\begin{flushleft}
y = $\nabla$f (x) + $\nabla$2 f (x)u $\in$ dom f ∗ ,
\end{flushleft}


\begin{flushleft}
i.e., y T x $-$ f (x) is bounded above. It follows that dim(dom f ∗ ) $\geq$ rank $\nabla$2 f (x).
\end{flushleft}


\begin{flushleft}
Hint. Consider $\nabla$f (x + tv), where t is small, and v is any vector in Rn .
\end{flushleft}


\begin{flushleft}
Solution. Clearly $\nabla$f (x) $\in$ dom f ∗ , since $\nabla$f (x) maximizes $\nabla$f (x)T z $-$ f (z) over z. Let
\end{flushleft}


\begin{flushleft}
v $\in$ Rn . For t small enough, we have x + tv $\in$ dom f , and therefore w(t) = $\nabla$f (x + tv) $\in$
\end{flushleft}


\begin{flushleft}
dom f ∗ , since x + tv maximizes w(t)T z $-$ f (z) over z. Thus, w(t) = $\nabla$f (x + tv) defines a
\end{flushleft}


\begin{flushleft}
curve (or just a point), passing through $\nabla$f (x), that lies in dom f ∗ . The tangent to the
\end{flushleft}


\begin{flushleft}
curve at $\nabla$f (x) is given by
\end{flushleft}


\begin{flushleft}
w (0) =
\end{flushleft}





\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
$\nabla$f (x + tv)
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
= $\nabla$2 f (x)v.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Now in general, the tangent to a curve that lies in a convex set must lie in the linear part
\end{flushleft}


\begin{flushleft}
of the affine hull of the set, since it is a limit of (scaled) differences of points in the set.
\end{flushleft}


\begin{flushleft}
(Differences of two points in a convex set lie in the linear part of its affine hull.) It follows
\end{flushleft}


\begin{flushleft}
that for s small enough, we have $\nabla$f (x) + s$\nabla$2 f (x)v $\in$ dom f ∗ .
\end{flushleft}


\begin{flushleft}
Examples:
\end{flushleft}


\begin{flushleft}
$\bullet$ f = aT x + b linear: dom f ∗ = \{a\}.
\end{flushleft}


\begin{flushleft}
$\bullet$ functions with dom f ∗ = Rn
\end{flushleft}


\begin{flushleft}
$\bullet$ f = log
\end{flushleft}


\begin{flushleft}
exp(x): dom f ∗ = \{y 0 | 1T y = 1\} and
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) = $-$(1/1T z)2 zz T + (1/1T z) diag(z))
\end{flushleft}


\begin{flushleft}
where 1T z = 1.
\end{flushleft}


\begin{flushleft}
$\bullet$ f = xT P x + q T x + r: dom f ∗ = q + R(P )
\end{flushleft}





\begin{flushleft}
Quasiconvex functions
\end{flushleft}


\begin{flushleft}
3.42 Approximation width. Let f0 , . . . , fn : R $\rightarrow$ R be given continuous functions. We consider
\end{flushleft}


\begin{flushleft}
the problem of approximating f0 as a linear combination of f1 , . . . , fn . For x $\in$ Rn , we
\end{flushleft}


\begin{flushleft}
say that f = x1 f1 + · · · + xn fn approximates f0 with tolerance $>$ 0 over the interval
\end{flushleft}


\begin{flushleft}
[0, T ] if |f (t) $-$ f0 (t)| $\leq$ for 0 $\leq$ t $\leq$ T . Now we choose a fixed tolerance $>$ 0 and define
\end{flushleft}


\begin{flushleft}
the approximation width as the largest T such that f approximates f 0 over the interval
\end{flushleft}


\begin{flushleft}
[0, T ]:
\end{flushleft}


\begin{flushleft}
W (x) = sup\{T | |x1 f1 (t) + · · · + xn fn (t) $-$ f0 (t)| $\leq$
\end{flushleft}





\begin{flushleft}
for 0 $\leq$ t $\leq$ T \}.
\end{flushleft}





\begin{flushleft}
Show that W is quasiconcave.
\end{flushleft}


\begin{flushleft}
Solution. To show that W is quasiconcave we show that the sets \{x | W (x) $\geq$ $\alpha$\} are
\end{flushleft}


\begin{flushleft}
convex for all $\alpha$. We have W (x) $\geq$ $\alpha$ if and only if
\end{flushleft}


\begin{flushleft}
$-$ $\leq$ x1 f1 (t) + · · · + xn fn (t) $-$ f0 (t) $\leq$
\end{flushleft}


\begin{flushleft}
for all t $\in$ [0, $\alpha$). Therefore the set \{x | W (x) $\geq$ $\alpha$\} is an intersection of infinitely many
\end{flushleft}


\begin{flushleft}
halfspaces (two for each t), hence a convex set.
\end{flushleft}


\begin{flushleft}
3.43 First-order condition for quasiconvexity. Prove the first-order condition for quasiconvexity
\end{flushleft}


\begin{flushleft}
given in §3.4.3: A differentiable function f : Rn $\rightarrow$ R, with dom f convex, is quasiconvex
\end{flushleft}


\begin{flushleft}
if and only if for all x, y $\in$ dom f ,
\end{flushleft}


\begin{flushleft}
f (y) $\leq$ f (x) =$\Rightarrow$ $\nabla$f (x)T (y $-$ x) $\leq$ 0.
\end{flushleft}


\begin{flushleft}
Hint. It suffices to prove the result for a function on R; the general result follows by
\end{flushleft}


\begin{flushleft}
restriction to an arbitrary line.
\end{flushleft}


\begin{flushleft}
Solution. First suppose f is a differentiable function on R and satisfies
\end{flushleft}


\begin{flushleft}
f (y) $\leq$ f (x) =$\Rightarrow$ f (x)(y $-$ x) $\leq$ 0.
\end{flushleft}





\begin{flushleft}
(3.43.A)
\end{flushleft}





\begin{flushleft}
Suppose f (x1 ) $\geq$ f (x2 ) where x1 = x2 . We assume x2 $>$ x1 (the other case can be handled
\end{flushleft}


\begin{flushleft}
similarly), and show that f (z) $\leq$ f (x1 ) for z $\in$ [x1 , x2 ]. Suppose this is false, i.e., there
\end{flushleft}


\begin{flushleft}
exists a z $\in$ [x1 , x2 ] with f (z) $>$ f (x1 ). Since f is differentiable, we can choose a z that
\end{flushleft}


\begin{flushleft}
also satisfies f (z) $<$ 0. By (3.43.A), however, f (x1 ) $<$ f (z) implies f (z)(x1 $-$ z) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
which contradicts f (z) $<$ 0.
\end{flushleft}


\begin{flushleft}
To prove sufficiency, assume f is quasiconvex. Suppose f (x) $\geq$ f (y). By the definition of
\end{flushleft}


\begin{flushleft}
quasiconvexity f (x + t(y $-$ x)) $\leq$ f (x) for 0 $<$ t $\leq$ 1. Dividing both sides by t, and taking
\end{flushleft}


\begin{flushleft}
the limit for t $\rightarrow$ 0, we obtain
\end{flushleft}


\begin{flushleft}
lim
\end{flushleft}





\begin{flushleft}
t$\rightarrow$0
\end{flushleft}





\begin{flushleft}
which proves (3.43.A).
\end{flushleft}





\begin{flushleft}
f (x + t(y $-$ x)) $-$ f (x)
\end{flushleft}


\begin{flushleft}
= f (x)(y $-$ x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
3.44 Second-order conditions for quasiconvexity. In this problem we derive alternate representations of the second-order conditions for quasiconvexity given in §3.4.3. Prove the
\end{flushleft}


\begin{flushleft}
following.
\end{flushleft}


\begin{flushleft}
(a) A point x $\in$ dom f satisfies (3.21) if and only if there exists a $\sigma$ such that
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) + $\sigma$$\nabla$f (x)$\nabla$f (x)T
\end{flushleft}





0.





(3.26)





\begin{flushleft}
It satisfies (3.22) for all y = 0 if and only if there exists a $\sigma$ such
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) + $\sigma$$\nabla$f (x)$\nabla$f (x)T
\end{flushleft}





0.





(3.27)





\begin{flushleft}
Hint. We can assume without loss of generality that $\nabla$2 f (x) is diagonal.
\end{flushleft}





\begin{flushleft}
(b) A point x $\in$ dom f satisfies (3.21) if and only if either $\nabla$f (x) = 0 and $\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
or $\nabla$f (x) = 0 and the matrix
\end{flushleft}


\begin{flushleft}
H(x) =
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
$\nabla$f (x)T
\end{flushleft}





0,





\begin{flushleft}
$\nabla$f (x)
\end{flushleft}


0





\begin{flushleft}
has exactly one negative eigenvalue. It satisfies (3.22) for all y = 0 if and only if
\end{flushleft}


\begin{flushleft}
H(x) has exactly one nonpositive eigenvalue.
\end{flushleft}


\begin{flushleft}
Hint. You can use the result of part (a). The following result, which follows from
\end{flushleft}


\begin{flushleft}
the eigenvalue interlacing theorem in linear algebra, may also be useful: If B $\in$ Sn
\end{flushleft}


\begin{flushleft}
and a $\in$ Rn , then
\end{flushleft}


\begin{flushleft}
B a
\end{flushleft}


\begin{flushleft}
$\geq$ $\lambda$n (B).
\end{flushleft}


\begin{flushleft}
$\lambda$n
\end{flushleft}


\begin{flushleft}
aT 0
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We prove the equivalence of (3.21) and (3.26). If $\nabla$f (x) = 0, both conditions
\end{flushleft}


\begin{flushleft}
reduce to $\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
0, and they are obviously equivalent. We prove the result for
\end{flushleft}


\begin{flushleft}
$\nabla$f (x) = 0.
\end{flushleft}


\begin{flushleft}
To simplify the proof, we adopt the following notation. Let a $\in$ Rn , a = 0, and
\end{flushleft}


\begin{flushleft}
B $\in$ Sn . We show that
\end{flushleft}


\begin{flushleft}
aT x = 0 =$\Rightarrow$ xT Bx $\geq$ 0
\end{flushleft}


\begin{flushleft}
(3.44.A)
\end{flushleft}


\begin{flushleft}
if and only if there exists a $\sigma$ such that B + $\sigma$aaT
\end{flushleft}


0.


\begin{flushleft}
It is obvious that the condition is sufficient: if B + $\sigma$aaT
\end{flushleft}





\begin{flushleft}
0, then
\end{flushleft}





\begin{flushleft}
aT x = 0 =$\Rightarrow$ xT Bx = xT (B + $\sigma$aaT )x $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Conversely, suppose (3.44.A) holds for all y. Without loss of generality we can
\end{flushleft}


\begin{flushleft}
assume that B is diagonal, B = diag(b), with the elements of b sorted in decreasing
\end{flushleft}


\begin{flushleft}
order (b1 $\geq$ b2 $\geq$ · · · $\geq$ bn ). We know that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
aT x = 0 =$\Rightarrow$
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
bi x2i $\geq$ 0.
\end{flushleft}





\begin{flushleft}
If bn $\geq$ 0, there is nothing to prove: diag(b) + $\sigma$aaT
\end{flushleft}


\begin{flushleft}
0 for all $\sigma$ $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Suppose bn $<$ 0. Then we must have an = 0. (Otherwise, x = en would satisfy
\end{flushleft}


\begin{flushleft}
aT x = 0 and xT diag(b)x = bn $<$ 0, a contradiction.) Moreover, we must have
\end{flushleft}


\begin{flushleft}
bn$-$1 $\geq$ 0. Otherwise, the vector x with
\end{flushleft}


\begin{flushleft}
x1 = · · · = xn$-$2 = 0,
\end{flushleft}





\begin{flushleft}
xn$-$1 = 1,
\end{flushleft}





\begin{flushleft}
xn = $-$an$-$1 /an ,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
would satisfy aT x = 0 and xT diag(b)x = bn$-$1 + bn (an$-$1 /an )2 $<$ 0, which is a
\end{flushleft}


\begin{flushleft}
contradiction. In summary,
\end{flushleft}


\begin{flushleft}
an = 0,
\end{flushleft}





\begin{flushleft}
b1 $\geq$ · · · $\geq$ bn$-$1 $\geq$ 0.
\end{flushleft}





\begin{flushleft}
bn $<$ 0,
\end{flushleft}





\begin{flushleft}
(3.44.B)
\end{flushleft}





\begin{flushleft}
We can derive conditions on $\sigma$ guaranteeing that
\end{flushleft}


\begin{flushleft}
C = diag(b) + $\sigma$aaT
\end{flushleft}





0.





\begin{flushleft}
Define a
\end{flushleft}


\begin{flushleft}
¯ = (a1 , . . . , an$-$1 ), ¯b = (b1 , . . . , bn$-$1 ). We have Cnn = bn + $\sigma$a2n $>$ 0 if
\end{flushleft}


\begin{flushleft}
$\sigma$ $>$ $-$bn /a2n . The Schur complement of Cnn is
\end{flushleft}


\begin{flushleft}
diag(¯b) + $\sigma$¯
\end{flushleft}


\begin{flushleft}
aa
\end{flushleft}


\begin{flushleft}
¯T $-$
\end{flushleft}





\begin{flushleft}
a2n
\end{flushleft}


\begin{flushleft}
a2 $\sigma$ 2 + bn $\sigma$ $-$ a2n T
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
¯a
\end{flushleft}


\begin{flushleft}
¯T = diag(¯b) + n
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
¯a
\end{flushleft}


¯


2


\begin{flushleft}
bn + $\sigma$an
\end{flushleft}


\begin{flushleft}
bn + $\sigma$a2n
\end{flushleft}





\begin{flushleft}
and is positive semidefinite if if a2n $\sigma$ 2 + bn $\sigma$ $-$ a2n $\geq$ 0, i.e.,
\end{flushleft}


\begin{flushleft}
$\sigma$$\geq$
\end{flushleft}





\begin{flushleft}
b2n
\end{flushleft}


+ 1.


\begin{flushleft}
4a4n
\end{flushleft}





\begin{flushleft}
$-$bn
\end{flushleft}


+


\begin{flushleft}
2a2n
\end{flushleft}





\begin{flushleft}
Next, we prove the equivalence of (3.22) and (3.27). We need to show that
\end{flushleft}


\begin{flushleft}
aT x = 0 =$\Rightarrow$ xT Bx $>$ 0
\end{flushleft}





\begin{flushleft}
(3.44.C)
\end{flushleft}





\begin{flushleft}
if and only if there exists a $\sigma$ such that B + $\sigma$aaT
\end{flushleft}


0.


\begin{flushleft}
Again, it is obvious that the condition is sufficient: if B + $\sigma$aaT
\end{flushleft}





\begin{flushleft}
0, then
\end{flushleft}





\begin{flushleft}
aT x = 0 =$\Rightarrow$ xT Bx = xT (B + $\sigma$aaT )x $>$ 0.
\end{flushleft}


\begin{flushleft}
for all nonzero x.
\end{flushleft}


\begin{flushleft}
Conversely, suppose (3.44.C) holds for all x = 0. We use the same notation as above
\end{flushleft}


\begin{flushleft}
and assume B is diagonal. If bn $>$ 0 there is nothing to prove. If bn $\leq$ 0, we must
\end{flushleft}


\begin{flushleft}
have an = 0 and bn$-$1 $>$ 0. Indeed, if bn$-$1 $\leq$ 0, choosing
\end{flushleft}


\begin{flushleft}
x1 = · · · = xn$-$2 = 0,
\end{flushleft}





\begin{flushleft}
xn$-$1 = 1,
\end{flushleft}





\begin{flushleft}
xn = $-$an$-$1 /an
\end{flushleft}





\begin{flushleft}
would provide a vector with aT x = 0 and xT Bx $\leq$ 0. Therefore,
\end{flushleft}


\begin{flushleft}
an = 0,
\end{flushleft}





\begin{flushleft}
bn $\leq$ 0,
\end{flushleft}





\begin{flushleft}
b1 $\geq$ · · · $\geq$ bn$-$1 $>$ 0.
\end{flushleft}





\begin{flushleft}
(3.44.D)
\end{flushleft}





\begin{flushleft}
We can now proceed as in the proof above and construct a $\sigma$ satisfying B +$\sigma$aa T
\end{flushleft}





0.





2





\begin{flushleft}
(b) We first consider (3.21). If $\nabla$f (x) = 0, both conditions reduce to $\nabla$ f (x)
\end{flushleft}


\begin{flushleft}
0, so
\end{flushleft}


\begin{flushleft}
they are obviously equivalent. We prove the result for $\nabla$f (x) = 0. We use the same
\end{flushleft}


\begin{flushleft}
notation as in part (a), and consider the matrix
\end{flushleft}


\begin{flushleft}
C=
\end{flushleft}





\begin{flushleft}
B
\end{flushleft}


\begin{flushleft}
aT
\end{flushleft}





\begin{flushleft}
a
\end{flushleft}


0





\begin{flushleft}
$\in$ Sn+1
\end{flushleft}





\begin{flushleft}
with a = 0. We need to show that C has exactly one negative eigenvalue if and
\end{flushleft}


\begin{flushleft}
only if (3.44.A) holds, or equivalently, if and only if there exists a $\sigma$ such that
\end{flushleft}


\begin{flushleft}
B + $\sigma$aaT
\end{flushleft}


0.


\begin{flushleft}
We first note that C has at least one negative eigenvalue: the vector v = (a, t) with
\end{flushleft}


\begin{flushleft}
t $<$ aT Ba/(2 a 22 ) satisfies
\end{flushleft}


\begin{flushleft}
v T Cv = aT Ba + 2taT a $<$ 0.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
Assume that C has exactly one negative eigenvalue. Suppose (3.44.A) does not
\end{flushleft}


\begin{flushleft}
hold, i.e., there exists an x satisfying aT x = 0 and xT Bx $<$ 0. The vector u = (x, 0)
\end{flushleft}


\begin{flushleft}
satisfies
\end{flushleft}


\begin{flushleft}
uT Cu = uT Bu $<$ 0.
\end{flushleft}


\begin{flushleft}
We also note that u is orthogonal to the vector v defined above. So we have two
\end{flushleft}


\begin{flushleft}
orthogonal vectors u and v with uT Cu $<$ 0 and v T Cv $<$ 0, which contradicts our
\end{flushleft}


\begin{flushleft}
assumption that C has only one negative eigenvalue.
\end{flushleft}


\begin{flushleft}
Conversely, suppose (3.44.A) holds, or, equivalently, B + $\sigma$aaT
\end{flushleft}


\begin{flushleft}
0 for some $\sigma$.
\end{flushleft}


\begin{flushleft}
Define
\end{flushleft}


$\surd$


\begin{flushleft}
B a
\end{flushleft}


0


\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
$\sigma$
\end{flushleft}


\begin{flushleft}
B + $\sigma$aaT a
\end{flushleft}


\begin{flushleft}
$\surd$I
\end{flushleft}


\begin{flushleft}
C($\sigma$) =
\end{flushleft}


=


.


\begin{flushleft}
T
\end{flushleft}


0


1


\begin{flushleft}
$\sigma$ 1
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


0


\begin{flushleft}
aT
\end{flushleft}


0


\begin{flushleft}
Since B+$\sigma$aaT
\end{flushleft}


\begin{flushleft}
0, it follows from the hint that $\lambda$n (C($\sigma$)) $\geq$ 0, i.e., C($\sigma$) has exactly
\end{flushleft}


\begin{flushleft}
one negative eigenvalue. Since the inertia of a symmetric matrix is preserved under
\end{flushleft}


\begin{flushleft}
a congruence, C has exactly one negative eigenvalue.
\end{flushleft}


\begin{flushleft}
The equivalence of (3.21) and (3.26) follows similarly. Note that if $\nabla$f (x) = 0, both
\end{flushleft}


\begin{flushleft}
conditions reduce to $\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
0. If $\nabla$f (x) = 0, H(x) has at least one negative
\end{flushleft}


\begin{flushleft}
eigenvalue, and we need to show that the other eigenvalues are positive.
\end{flushleft}


\begin{flushleft}
3.45 Use the first and second-order conditions for quasiconvexity given in §3.4.3 to verify
\end{flushleft}


\begin{flushleft}
quasiconvexity of the function f (x) = $-$x1 x2 , with dom f = R2++ .
\end{flushleft}


\begin{flushleft}
Solution. The first and second derivatives of f are
\end{flushleft}


\begin{flushleft}
$\nabla$f (x) =
\end{flushleft}





\begin{flushleft}
$-$x2
\end{flushleft}


\begin{flushleft}
$-$x1
\end{flushleft}





,





\begin{flushleft}
$\nabla$2 f (x) =
\end{flushleft}





0


$-$1





$-$1


0





.





\begin{flushleft}
We start with the first-order condition
\end{flushleft}


\begin{flushleft}
f (x) $\leq$ f (y) =$\Rightarrow$ $\nabla$f (x)T (y $-$ x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
which in this case reduces to
\end{flushleft}


\begin{flushleft}
$-$y1 y2 $\leq$ $-$x1 x2 =$\Rightarrow$ $-$x2 (y1 $-$ x1 ) $-$ x1 (y2 $-$ x2 ) $\leq$ 0
\end{flushleft}


\begin{flushleft}
for x, y
\end{flushleft}





\begin{flushleft}
0. Simplifying each side we get
\end{flushleft}


\begin{flushleft}
y1 y2 $\geq$ x1 x2 =$\Rightarrow$ 2x1 x2 $\leq$ x1 y2 + x2 y1 ,
\end{flushleft}





\begin{flushleft}
and dividing by x1 x2 (which is positive) we get the equivalent statement
\end{flushleft}


\begin{flushleft}
(y1 /x1 )(y2 /x2 ) $\geq$ 1 =$\Rightarrow$ 1 $\leq$ ((y2 /x2 ) + (y1 /x1 )) /2,
\end{flushleft}


\begin{flushleft}
which is true (it is the arithmetic-geometric mean inequality).
\end{flushleft}


\begin{flushleft}
The second-order condition is
\end{flushleft}


\begin{flushleft}
y T $\nabla$f (x) = 0, y = 0 =$\Rightarrow$ y T $\nabla$2 f (x)y $>$ 0,
\end{flushleft}


\begin{flushleft}
which reduces to
\end{flushleft}


\begin{flushleft}
for x
\end{flushleft}





\begin{flushleft}
0, i.e.,
\end{flushleft}





\begin{flushleft}
which is correct if x
\end{flushleft}





\begin{flushleft}
$-$y1 x2 $-$ y2 x1 = 0, y = 0 =$\Rightarrow$ $-$2y1 y2 $>$ 0
\end{flushleft}





0.





\begin{flushleft}
y2 = $-$y1 x2 /x1 =$\Rightarrow$ $-$2y1 y2 $>$ 0,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
3.46 Quasilinear functions with domain Rn . A function on R that is quasilinear (i.e., quasiconvex and quasiconcave) is monotone, i.e., either nondecreasing or nonincreasing. In
\end{flushleft}


\begin{flushleft}
this problem we consider a generalization of this result to functions on Rn .
\end{flushleft}


\begin{flushleft}
Suppose the function f : Rn $\rightarrow$ R is quasilinear and continuous with dom f = Rn . Show
\end{flushleft}


\begin{flushleft}
that it can be expressed as f (x) = g(aT x), where g : R $\rightarrow$ R is monotone and a $\in$ Rn .
\end{flushleft}


\begin{flushleft}
In other words, a quasilinear function with domain Rn must be a monotone function of
\end{flushleft}


\begin{flushleft}
a linear function. (The converse is also true.)
\end{flushleft}


\begin{flushleft}
Solution. The sublevel set \{x | f (x) $\leq$ $\alpha$\} are closed and convex (note that f is continuous), and their complements \{x | f (x) $>$ $\alpha$\} are also convex. Therefore the sublevel sets
\end{flushleft}


\begin{flushleft}
are closed halfspaces, and can be expressed as
\end{flushleft}


\begin{flushleft}
\{x | f (x) $\leq$ $\alpha$\} = \{x | a($\alpha$)T x $\leq$ b($\alpha$)\}
\end{flushleft}


\begin{flushleft}
with a($\alpha$) 2 = 1.
\end{flushleft}


\begin{flushleft}
The sublevel sets are nested, i.e., they have the same normal vector a($\alpha$) = a for all $\alpha$,
\end{flushleft}


\begin{flushleft}
and b($\alpha$1 ) $\geq$ b($\alpha$2 ) if $\alpha$1 $>$ $\alpha$2 . In other words,
\end{flushleft}


\begin{flushleft}
\{x | f (x) $\leq$ $\alpha$\} = \{x | aT x $\leq$ b($\alpha$)\}
\end{flushleft}





\begin{flushleft}
where b is nondecreasing. If b is in fact increasing, we can define g = b$-$1 and say that
\end{flushleft}


\begin{flushleft}
\{x | f (x) $\leq$ $\alpha$\} = \{x | g(aT x) $\leq$ $\alpha$\}
\end{flushleft}


\begin{flushleft}
and by continuity of f , f (x) = g(aT x). If b is merely nondecreasing, we define
\end{flushleft}


\begin{flushleft}
g(t) = sup\{$\alpha$ | b($\alpha$) $\leq$ t\}.
\end{flushleft}





\begin{flushleft}
Log-concave and log-convex functions
\end{flushleft}


\begin{flushleft}
3.47 Suppose f : Rn $\rightarrow$ R is differentiable, dom f is convex, and f (x) $>$ 0 for all x $\in$ dom f .
\end{flushleft}


\begin{flushleft}
Show that f is log-concave if and only if for all x, y $\in$ dom f ,
\end{flushleft}


\begin{flushleft}
f (y)
\end{flushleft}


\begin{flushleft}
$\leq$ exp
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}





\begin{flushleft}
$\nabla$f (x)T (y $-$ x)
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}





.





\begin{flushleft}
Solution. This is the basic inequality
\end{flushleft}


\begin{flushleft}
h(y) $\geq$ h(x) + $\nabla$h(x)T (y $-$ x)
\end{flushleft}


\begin{flushleft}
applied to the convex function h(x) = $-$ log f (x), combined with $\nabla$h(x) = (1/f (x))$\nabla$f (x).
\end{flushleft}


\begin{flushleft}
3.48 Show that if f : Rn $\rightarrow$ R is log-concave and a $\geq$ 0, then the function g = f $-$ a is
\end{flushleft}


\begin{flushleft}
log-concave, where dom g = \{x $\in$ dom f | f (x) $>$ a\}.
\end{flushleft}


\begin{flushleft}
Solution. We have for x, y $\in$ dom f with f (x) $>$ a, f (y) $>$ a, and 0 $\leq$ $\theta$ $\leq$ 1,
\end{flushleft}


\begin{flushleft}
f ($\theta$x + (1 $-$ $\theta$)y) $-$ a
\end{flushleft}





$\geq$





$\geq$





\begin{flushleft}
f (x)$\theta$ f (y)1$-$$\theta$) $-$ a
\end{flushleft}





\begin{flushleft}
(f (x) $-$ a)$\theta$ (f (y) $-$ a)1$-$$\theta$ .
\end{flushleft}





\begin{flushleft}
The last inequality follows from H¨
\end{flushleft}


\begin{flushleft}
older's inequality
\end{flushleft}


\begin{flushleft}
1/$\theta$
\end{flushleft}





\begin{flushleft}
u1 v1 + u2 v2 $\leq$ (u1
\end{flushleft}





\begin{flushleft}
1/$\theta$
\end{flushleft}





\begin{flushleft}
1/(1$-$$\theta$)
\end{flushleft}





\begin{flushleft}
+ u2 )$\theta$ (v1
\end{flushleft}





\begin{flushleft}
1/(1$-$$\theta$) 1$-$$\theta$
\end{flushleft}





\begin{flushleft}
+ v2
\end{flushleft}





)





,





\begin{flushleft}
applied to
\end{flushleft}


\begin{flushleft}
u1 = (f (x) $-$ a)$\theta$ ,
\end{flushleft}





\begin{flushleft}
v1 = (f (y) $-$ a)1$-$$\theta$ ,
\end{flushleft}





\begin{flushleft}
u2 = a $\theta$ ,
\end{flushleft}





\begin{flushleft}
which yields
\end{flushleft}


\begin{flushleft}
f (x)$\theta$ f (y)1$-$$\theta$ $\geq$ (f (x) $-$ a)$\theta$ (f (y) $-$ a)1$-$$\theta$ + a.
\end{flushleft}





\begin{flushleft}
v2 = a1$-$$\theta$ ,
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
3.49 Show that the following functions are log-concave.
\end{flushleft}


\begin{flushleft}
(a) Logistic function: f (x) = ex /(1 + ex ) with dom f = R.
\end{flushleft}


\begin{flushleft}
Solution. We have
\end{flushleft}


\begin{flushleft}
log(ex /(1 + ex )) = x $-$ log(1 + ex ).
\end{flushleft}


\begin{flushleft}
The first term is linear, hence concave. Since the function log(1 + ex ) is convex (it
\end{flushleft}


\begin{flushleft}
is the log-sum-exp function, evaluated at x1 = 0, x2 = x), the second term above is
\end{flushleft}


\begin{flushleft}
concave. Thus, ex /(1 + ex ) is log-concave.
\end{flushleft}


\begin{flushleft}
(b) Harmonic mean:
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





1


,


\begin{flushleft}
1/x1 + · · · + 1/xn
\end{flushleft}





\begin{flushleft}
dom f = Rn
\end{flushleft}


++ .





\begin{flushleft}
Solution. The first and second derivatives of
\end{flushleft}


\begin{flushleft}
h(x) = log f (x) = $-$ log(1/x1 + · · · + 1/xn )
\end{flushleft}


\begin{flushleft}
are
\end{flushleft}


\begin{flushleft}
$\partial$h(x)
\end{flushleft}


\begin{flushleft}
$\partial$xi
\end{flushleft}





=





\begin{flushleft}
$\partial$ 2 h(x)
\end{flushleft}


\begin{flushleft}
$\partial$x2i
\end{flushleft}





=





\begin{flushleft}
$\partial$ 2 h(x)
\end{flushleft}


\begin{flushleft}
$\partial$xi $\partial$xj
\end{flushleft}





=





\begin{flushleft}
1/x2i
\end{flushleft}


\begin{flushleft}
1/x1 + · · · + 1/xn
\end{flushleft}





\begin{flushleft}
1/x4i
\end{flushleft}


\begin{flushleft}
$-$2/x3i
\end{flushleft}


+


\begin{flushleft}
1/x1 + · · · + 1/xn
\end{flushleft}


\begin{flushleft}
(1/x1 + · · · + 1/xn )2
\end{flushleft}


\begin{flushleft}
1/(x2i x2j )
\end{flushleft}


\begin{flushleft}
(1/x1 + · · · + 1/xn )2
\end{flushleft}





\begin{flushleft}
(i = j).
\end{flushleft}





\begin{flushleft}
We show that y T $\nabla$2 h(x)y ≺ 0 for all y = 0, i.e.,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
yi /x2i )2 $<$ 2(
\end{flushleft}





(


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
yi2 /x3i )
\end{flushleft}





\begin{flushleft}
1/xi )(
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
This follows from the Cauchy-Schwarz inequality (aT b)2 $\leq$ a
\end{flushleft}


1


\begin{flushleft}
ai = $\surd$ ,
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}


\begin{flushleft}
(c) Product over sum:
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
bi =
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


,


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}





2


2





\begin{flushleft}
b 22 , applied to
\end{flushleft}





\begin{flushleft}
yi
\end{flushleft}


$\surd$ .


\begin{flushleft}
xi x i
\end{flushleft}





\begin{flushleft}
dom f = Rn
\end{flushleft}


++ .





\begin{flushleft}
Solution. We must show that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log xi $-$ log
\end{flushleft}





\begin{flushleft}
xi
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
is concave on x 0. Let's consider a line described by x + tv, where and x, v $\in$ Rn
\end{flushleft}


\begin{flushleft}
and x 0: define
\end{flushleft}


\begin{flushleft}
f˜(t) =
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
log(xi + tvi ) $-$ log
\end{flushleft}





\begin{flushleft}
(xi + tvi ).
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
The first derivative is
\end{flushleft}


\begin{flushleft}
f˜ (t) =
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
1T v
\end{flushleft}


\begin{flushleft}
vi
\end{flushleft}


\begin{flushleft}
$-$ T
\end{flushleft}


,


\begin{flushleft}
xi + tvi
\end{flushleft}


\begin{flushleft}
1 x + t1T v
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
and the second derivative is
\end{flushleft}


\begin{flushleft}
f˜ (t) = $-$
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
(1T v)2
\end{flushleft}


\begin{flushleft}
vi2
\end{flushleft}


\begin{flushleft}
+ T
\end{flushleft}


.


2


\begin{flushleft}
(xi + tvi )
\end{flushleft}


\begin{flushleft}
(1 x + t1T v)2
\end{flushleft}





\begin{flushleft}
Therefore to establish concavity of f , we need to show that
\end{flushleft}


\begin{flushleft}
f˜ (0) = $-$
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
(1T v)2
\end{flushleft}


\begin{flushleft}
vi2
\end{flushleft}


\begin{flushleft}
+ T 2 $\leq$0
\end{flushleft}


2


\begin{flushleft}
(1 x)
\end{flushleft}


\begin{flushleft}
xi
\end{flushleft}





\begin{flushleft}
holds for all v, and all x 0.
\end{flushleft}


\begin{flushleft}
The inequality holds if 1T v = 0. If 1T v = 0, we note that the inequality is homogeneous of degree two in v, so we can assume without loss of generality that
\end{flushleft}


\begin{flushleft}
1T v = 1T x. This reduces the problem to verifying that
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
vi2
\end{flushleft}


$\geq$1


\begin{flushleft}
x2i
\end{flushleft}





\begin{flushleft}
holds whenever x 0 and 1T v = 1T x.
\end{flushleft}


\begin{flushleft}
To establish this, let's fix x, and minimize the convex, quadratic form over 1T v =
\end{flushleft}


\begin{flushleft}
1T x. The optimality conditions give
\end{flushleft}


\begin{flushleft}
vi
\end{flushleft}


\begin{flushleft}
= $\lambda$,
\end{flushleft}


\begin{flushleft}
x2i
\end{flushleft}


\begin{flushleft}
so we have vi = $\lambda$x2i . From 1T v = 1T x we can obtain $\lambda$, which gives
\end{flushleft}


\begin{flushleft}
xk 2
\end{flushleft}


\begin{flushleft}
x .
\end{flushleft}


\begin{flushleft}
2 i
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
k k
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
vi =
\end{flushleft}


\begin{flushleft}
Therefore the minimum value of
\end{flushleft}


(


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
vi2 /x2i over 1T v = 1T x is
\end{flushleft}





\begin{flushleft}
xk
\end{flushleft}


\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
k k
\end{flushleft}





\begin{flushleft}
vi 2
\end{flushleft}


) =


\begin{flushleft}
xi
\end{flushleft}





2





\begin{flushleft}
1T x
\end{flushleft}


\begin{flushleft}
x 2
\end{flushleft}





\begin{flushleft}
x2i =
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





2





$\geq$ 1,





\begin{flushleft}
because x 2 $\leq$ x 1 . This proves the inequality.
\end{flushleft}


\begin{flushleft}
(d) Determinant over trace:
\end{flushleft}


\begin{flushleft}
f (X) =
\end{flushleft}





\begin{flushleft}
det X
\end{flushleft}


,


\begin{flushleft}
tr X
\end{flushleft}





\begin{flushleft}
dom f = Sn
\end{flushleft}


++ .





\begin{flushleft}
Solution. We prove that
\end{flushleft}


\begin{flushleft}
h(X) = log f (X) = log det X $-$ log tr X
\end{flushleft}


\begin{flushleft}
is concave. Consider the restriction on a line X = Z + tV with Z
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
eigenvalue decomposition Z $-$1/2 V Z $-$1/2 = Q$\Lambda$QT = i=1 $\lambda$i qi qiT :
\end{flushleft}


\begin{flushleft}
h(Z + tV )
\end{flushleft}





=


=





\begin{flushleft}
log det(Z + tV ) $-$ log tr(Z + tV )
\end{flushleft}





\begin{flushleft}
log det Z $-$ log det(I + tZ $-$1/2 V Z $-$1/2 ) $-$ log tr Z(I + tZ $-$1/2 V Z 1/2 )
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=





=





\begin{flushleft}
0, and use the
\end{flushleft}





\begin{flushleft}
log det Z $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log det Z +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log(1 + t$\lambda$i ) $-$ log
\end{flushleft}


\begin{flushleft}
log(qiT Zqi ) $-$
\end{flushleft}





\begin{flushleft}
((qiT Zqi )(1 + t$\lambda$i )),
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$-$ log
\end{flushleft}





\begin{flushleft}
(qiT Zqi )(1 + t$\lambda$i ))
\end{flushleft}


\begin{flushleft}
log((qiT Zqi )(1 + t$\lambda$i ))
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
which is a constant, plus the function
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log yi $-$ log
\end{flushleft}





\begin{flushleft}
yi
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(which is concave; see (c)), evaluated at yi = (qiT Zqi )(1 + t$\lambda$i ).
\end{flushleft}


\begin{flushleft}
3.50 Coefficients of a polynomial as a function of the roots. Show that the coefficients of a
\end{flushleft}


\begin{flushleft}
polynomial with real negative roots are log-concave functions of the roots. In other words,
\end{flushleft}


\begin{flushleft}
the functions ai : Rn $\rightarrow$ R, defined by the identity
\end{flushleft}


\begin{flushleft}
sn + a1 ($\lambda$)sn$-$1 + · · · + an$-$1 ($\lambda$)s + an ($\lambda$) = (s $-$ $\lambda$1 )(s $-$ $\lambda$2 ) · · · (s $-$ $\lambda$n ),
\end{flushleft}


\begin{flushleft}
are log-concave on $-$Rn
\end{flushleft}


++ .


\begin{flushleft}
Hint. The function
\end{flushleft}


\begin{flushleft}
Sk (x) =
\end{flushleft}


\begin{flushleft}
1$\leq$i1 $<$i2 $<$···$<$ik $\leq$n
\end{flushleft}





\begin{flushleft}
xi 1 xi 2 · · · x i k ,
\end{flushleft}





\begin{flushleft}
with dom Sk $\in$ Rn
\end{flushleft}


\begin{flushleft}
+ and 1 $\leq$ k $\leq$ n, is called the kth elementary symmetric function on
\end{flushleft}


\begin{flushleft}
1/k
\end{flushleft}


\begin{flushleft}
Rn . It can be shown that Sk is concave (see [ML57]).
\end{flushleft}


\begin{flushleft}
Solution. The coefficients are given by ak ($\lambda$) = Sk ($-$$\lambda$). The result follows from the
\end{flushleft}


\begin{flushleft}
hint, because the logarithm of a nonnegative concave function is log-concave.
\end{flushleft}


\begin{flushleft}
3.51 [BL00, page 41] Let p be a polynomial on R, with all its roots real. Show that it is
\end{flushleft}


\begin{flushleft}
log-concave on any interval on which it is positive.
\end{flushleft}


\begin{flushleft}
Solution. We assume the polynomial has the form
\end{flushleft}


\begin{flushleft}
p(x) = $\alpha$(x $-$ s1 )(x $-$ s2 ) . . . (x $-$ sn ),
\end{flushleft}


\begin{flushleft}
with s1 $\leq$ s2 $\leq$ · · · $\leq$ sn , and $\alpha$ $>$ 0. (The case $\alpha$ $<$ 0 can be handled similarly).
\end{flushleft}


\begin{flushleft}
Suppose p is positive on the interval (sk , sk+1 ), which means n $-$ k (the number of roots
\end{flushleft}


\begin{flushleft}
to the right of the interval) must be even. We can write log p as
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log p(x) = log $\alpha$ +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(x $-$ sk )
\end{flushleft}





\begin{flushleft}
+ log((x $-$ sk+1 )(x $-$ sk+2 ))
\end{flushleft}


\begin{flushleft}
+ log((x $-$ sk+3 )(x $-$ sk+4 ))
\end{flushleft}


\begin{flushleft}
+ · · · + log((x $-$ sn$-$1 )(x $-$ sn )).
\end{flushleft}


\begin{flushleft}
The first terms are obviously concave. We need to show that
\end{flushleft}


\begin{flushleft}
f (x) = log((x $-$ a)(x $-$ b)) = log(x2 $-$ (a + b)x + ab)
\end{flushleft}


\begin{flushleft}
is concave if x $<$ a $\leq$ b. We have
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
x2
\end{flushleft}





\begin{flushleft}
2x $-$ (a + b)
\end{flushleft}


,


\begin{flushleft}
$-$ (a + b) + ab
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
2(x $-$ a)(x $-$ b) $-$ (2x $-$ (a + b))2
\end{flushleft}


.


\begin{flushleft}
(x2 $-$ (a + b)x + ab)2
\end{flushleft}





\begin{flushleft}
It is easily shown that the second derivative is less than or equal to zero:
\end{flushleft}


\begin{flushleft}
2(x $-$ a)(x $-$ b) $-$ ((x $-$ a) + (x $-$ b))2
\end{flushleft}


$\leq$





=


$\leq$





\begin{flushleft}
2(x $-$ a)(x $-$ b) $-$ (x $-$ a)2 $-$ (x $-$ b)2 $-$ 2(x $-$ a)(x $-$ b)
\end{flushleft}





\begin{flushleft}
$-$(x $-$ a)2 $-$ (x $-$ b)2
\end{flushleft}


0.





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
3.52 [MO79, §3.E.2] Log-convexity of moment functions. Suppose f : R $\rightarrow$ R is nonnegative
\end{flushleft}


\begin{flushleft}
with R+ $\subseteq$ dom f . For x $\geq$ 0 define
\end{flushleft}


$\infty$





\begin{flushleft}
$\phi$(x) =
\end{flushleft}





\begin{flushleft}
ux f (u) du.
\end{flushleft}





0





\begin{flushleft}
Show that $\phi$ is a log-convex function. (If x is a positive integer, and f is a probability
\end{flushleft}


\begin{flushleft}
density function, then $\phi$(x) is the xth moment of the distribution.)
\end{flushleft}


\begin{flushleft}
Use this to show that the Gamma function,
\end{flushleft}


$\infty$





\begin{flushleft}
$\Gamma$(x) =
\end{flushleft}





\begin{flushleft}
ux$-$1 e$-$u du,
\end{flushleft}





0





\begin{flushleft}
is log-convex for x $\geq$ 1.
\end{flushleft}


\begin{flushleft}
Solution. g(x, u) = ux f (u) is log-convex (as well as log-concave) in x for all u $>$ 0. It
\end{flushleft}


\begin{flushleft}
follows directly from the property on page 106 that
\end{flushleft}


$\infty$





\begin{flushleft}
$\phi$(x) =
\end{flushleft}





$\infty$





\begin{flushleft}
g(x, u) du =
\end{flushleft}





\begin{flushleft}
ux f (u) du
\end{flushleft}





0





0





\begin{flushleft}
is log-convex.
\end{flushleft}


\begin{flushleft}
3.53 Suppose x and y are independent random vectors in Rn , with log-concave probability
\end{flushleft}


\begin{flushleft}
density functions f and g, respectively. Show that the probability density function of the
\end{flushleft}


\begin{flushleft}
sum z = x + y is log-concave.
\end{flushleft}


\begin{flushleft}
Solution. The probability density function of x + y is f ∗ g.
\end{flushleft}


\begin{flushleft}
3.54 Log-concavity of Gaussian cumulative distribution function. The cumulative distribution
\end{flushleft}


\begin{flushleft}
function of a Gaussian random variable,
\end{flushleft}


1


\begin{flushleft}
f (x) = $\surd$
\end{flushleft}


\begin{flushleft}
2$\pi$
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
e$-$t
\end{flushleft}





2





/2





\begin{flushleft}
dt,
\end{flushleft}





$-$$\infty$





\begin{flushleft}
is log-concave. This follows from the general result that the convolution of two log-concave
\end{flushleft}


\begin{flushleft}
functions is log-concave. In this problem we guide you through a simple self-contained
\end{flushleft}


\begin{flushleft}
proof that f is log-concave. Recall that f is log-concave if and only if f (x)f (x) $\leq$ f (x)2
\end{flushleft}


\begin{flushleft}
for all x.
\end{flushleft}


\begin{flushleft}
(a) Verify that f (x)f (x) $\leq$ f (x)2 for x $\geq$ 0. That leaves us the hard part, which is to
\end{flushleft}


\begin{flushleft}
show the inequality for x $<$ 0.
\end{flushleft}


\begin{flushleft}
(b) Verify that for any t and x we have t2 /2 $\geq$ $-$x2 /2 + xt.
\end{flushleft}


\begin{flushleft}
(c) Using part (b) show that e$-$t
\end{flushleft}





2





/2





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
e$-$t
\end{flushleft}





\begin{flushleft}
$\leq$ ex
\end{flushleft}





2





/2





$-$$\infty$





2





\begin{flushleft}
/2$-$xt
\end{flushleft}





\begin{flushleft}
dt $\leq$ ex
\end{flushleft}





\begin{flushleft}
. Conclude that
\end{flushleft}





2





\begin{flushleft}
x
\end{flushleft}


/2





\begin{flushleft}
e$-$xt dt.
\end{flushleft}


$-$$\infty$





\begin{flushleft}
(d) Use part (c) to verify that f (x)f (x) $\leq$ f (x)2 for x $\leq$ 0.
\end{flushleft}





\begin{flushleft}
Solution. The derivatives of f are
\end{flushleft}


$\surd$


2


\begin{flushleft}
f (x) = e$-$x /2 / 2$\pi$,
\end{flushleft}





\begin{flushleft}
f (x) = $-$xe$-$x
\end{flushleft}





2





/2





$\surd$


\begin{flushleft}
/ 2$\pi$.
\end{flushleft}





\begin{flushleft}
(a) f (x) $\leq$ 0 for x $\geq$ 0.
\end{flushleft}


\begin{flushleft}
(b) Since t2 /2 is convex we have
\end{flushleft}


\begin{flushleft}
t2 /2 $\geq$ x2 /2 + x(t $-$ x) = xt $-$ x2 /2.
\end{flushleft}


\begin{flushleft}
This is the general inequality
\end{flushleft}


\begin{flushleft}
g(t) $\geq$ g(x) + g (x)(t $-$ x),
\end{flushleft}





\begin{flushleft}
which holds for any differentiable convex function, applied to g(t) = t2 /2.
\end{flushleft}





\newpage
3





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
(c) Take exponentials and integrate.
\end{flushleft}


\begin{flushleft}
(d) This basic inequality reduces to
\end{flushleft}


\begin{flushleft}
$-$xe$-$x
\end{flushleft}





2





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
e$-$t
\end{flushleft}





/2





2





/2





$-$$\infty$





\begin{flushleft}
dt $\leq$ e$-$x
\end{flushleft}





2





\begin{flushleft}
i.e.,
\end{flushleft}


2





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
e$-$t
\end{flushleft}





2





/2





$-$$\infty$





\begin{flushleft}
e$-$x /2
\end{flushleft}


.


\begin{flushleft}
$-$x
\end{flushleft}





\begin{flushleft}
dt $\leq$
\end{flushleft}





\begin{flushleft}
This follows from part (c) because
\end{flushleft}





2





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
e$-$xt dt =
\end{flushleft}


$-$$\infty$





\begin{flushleft}
e$-$x
\end{flushleft}


.


\begin{flushleft}
$-$x
\end{flushleft}





\begin{flushleft}
3.55 Log-concavity of the cumulative distribution function of a log-concave probability density.
\end{flushleft}


\begin{flushleft}
In this problem we extend the result of exercise 3.54. Let g(t) = exp($-$h(t)) be a differentiable log-concave probability density function, and let
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





$-$$\infty$





\begin{flushleft}
be its cumulative distribution.
\end{flushleft}


\begin{flushleft}
f (x)f (x) $\leq$ (f (x))2 for all x.
\end{flushleft}





\begin{flushleft}
e$-$h(t) dt
\end{flushleft}





\begin{flushleft}
g(t) dt =
\end{flushleft}


$-$$\infty$





\begin{flushleft}
We will show that f is log-concave, i.e., it satisfies
\end{flushleft}





\begin{flushleft}
(a) Express the derivatives of f in terms of the function h. Verify that f (x)f (x) $\leq$
\end{flushleft}


\begin{flushleft}
(f (x))2 if h (x) $\geq$ 0.
\end{flushleft}


\begin{flushleft}
(b) Assume that h (x) $<$ 0. Use the inequality
\end{flushleft}


\begin{flushleft}
h(t) $\geq$ h(x) + h (x)(t $-$ x)
\end{flushleft}


\begin{flushleft}
(which follows from convexity of h), to show that
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


$-$$\infty$





\begin{flushleft}
e$-$h(t) dt $\leq$
\end{flushleft}





\begin{flushleft}
e$-$h(x)
\end{flushleft}


.


\begin{flushleft}
$-$h (x)
\end{flushleft}





\begin{flushleft}
Use this inequality to verify that f (x)f (x) $\leq$ (f (x))2 if h (x) $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) f (x) =
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


$-$$\infty$





\begin{flushleft}
e$-$h(t) dt, f (x) = e$-$h(x) , f (x) = $-$h (x)e$-$h(x) . Log-concavity means
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
$-$h (x)e$-$h(x)
\end{flushleft}





$-$$\infty$





\begin{flushleft}
e$-$h(t) dt $\leq$ e$-$2h(x) ,
\end{flushleft}





\begin{flushleft}
which is obviously true if $-$h (x) $\leq$ 0.
\end{flushleft}


\begin{flushleft}
(b) Take exponentials and integrate both sides of $-$h(t) $\leq$ $-$h(x) $-$ h (x)(t $-$ x):
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
e$-$h(t) dt
\end{flushleft}


$-$$\infty$





$\leq$





\begin{flushleft}
exh
\end{flushleft}





=





\begin{flushleft}
exh
\end{flushleft}





\begin{flushleft}
e$-$h(t) dt
\end{flushleft}


$-$$\infty$





\begin{flushleft}
$-$h(x)
\end{flushleft}





\begin{flushleft}
e
\end{flushleft}


\begin{flushleft}
$-$h (x)
\end{flushleft}





$\leq$





\begin{flushleft}
e$-$h(x) .
\end{flushleft}





\begin{flushleft}
(x)
\end{flushleft}





\begin{flushleft}
dt
\end{flushleft}





$-$$\infty$





\begin{flushleft}
(x)$-$h(x) $-$xh (x)
\end{flushleft}





=


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
($-$h (x))
\end{flushleft}





\begin{flushleft}
e$-$th
\end{flushleft}





\begin{flushleft}
(x)$-$h(x)
\end{flushleft}





\begin{flushleft}
e
\end{flushleft}





\begin{flushleft}
/($-$h (x))
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
3.56 More log-concave densities. Show that the following densities are log-concave.
\end{flushleft}


\begin{flushleft}
(a) [MO79, page 493] The gamma density, defined by
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
$\alpha$$\lambda$ $\lambda$$-$1 $-$$\alpha$x
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
e
\end{flushleft}


,


\begin{flushleft}
$\Gamma$($\lambda$)
\end{flushleft}





\begin{flushleft}
with dom f = R+ . The parameters $\lambda$ and $\alpha$ satisfy $\lambda$ $\geq$ 1, $\alpha$ $>$ 0.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
log f (x) = log(($\alpha$$\lambda$ /$\Gamma$($\lambda$)) + ($\lambda$ $-$ 1) log x $-$ $\alpha$x.
\end{flushleft}


\begin{flushleft}
(b) [MO79, page 306] The Dirichlet density
\end{flushleft}


\begin{flushleft}
$\Gamma$(1T $\lambda$)
\end{flushleft}


\begin{flushleft}
x$\lambda$1 $-$1 · · · x$\lambda$nn $-$1
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
$\Gamma$($\lambda$1 ) · · · $\Gamma$($\lambda$n+1 ) 1
\end{flushleft}





\begin{flushleft}
$\lambda$n+1 $-$1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





1$-$





\begin{flushleft}
xi
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
with dom f = \{x $\in$ Rn
\end{flushleft}


\begin{flushleft}
++ | 1 x $<$ 1\}. The parameter $\lambda$ satisfies $\lambda$
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





1.





\begin{flushleft}
log f (x)
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=





\begin{flushleft}
log($\Gamma$($\lambda$)/($\Gamma$($\lambda$1 ) · · · $\Gamma$($\lambda$n+1 ))) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
($\lambda$i $-$ 1) log xi + ($\lambda$n+1 $-$ 1) log(1 $-$ 1T x).
\end{flushleft}





\begin{flushleft}
Convexity with respect to a generalized inequality
\end{flushleft}


\begin{flushleft}
3.57 Show that the function f (X) = X $-$1 is matrix convex on Sn
\end{flushleft}


++ .


\begin{flushleft}
Solution. We must show that for arbitrary v $\in$ Rn , the function
\end{flushleft}


\begin{flushleft}
g(X) = v T X $-$1 v.
\end{flushleft}


\begin{flushleft}
is convex in X on Sn
\end{flushleft}


\begin{flushleft}
++ . This follows from example 3.4.
\end{flushleft}


\begin{flushleft}
3.58 Schur complement. Suppose X $\in$ Sn partitioned as
\end{flushleft}


\begin{flushleft}
X=
\end{flushleft}





\begin{flushleft}
A
\end{flushleft}


\begin{flushleft}
BT
\end{flushleft}





\begin{flushleft}
B
\end{flushleft}


\begin{flushleft}
C
\end{flushleft}





,





\begin{flushleft}
where A $\in$ Sk . The Schur complement of X (with respect to A) is S = C $-$ B T A$-$1 B
\end{flushleft}


\begin{flushleft}
(see §A.5.5). Show that the Schur complement, viewed as function from Sn into Sn$-$k , is
\end{flushleft}


\begin{flushleft}
matrix concave on Sn
\end{flushleft}


++ .


\begin{flushleft}
Solution. Let v $\in$ Rn$-$k . We must show that the function
\end{flushleft}


\begin{flushleft}
v T (C $-$ B T A$-$1 B)v
\end{flushleft}


\begin{flushleft}
is concave in X on Sn
\end{flushleft}


\begin{flushleft}
++ . This follows from example 3.4.
\end{flushleft}


\begin{flushleft}
3.59 Second-order conditions for K-convexity. Let K $\subseteq$ Rm be a proper convex cone, with
\end{flushleft}


\begin{flushleft}
associated generalized inequality K . Show that a twice differentiable function f : Rn $\rightarrow$
\end{flushleft}


\begin{flushleft}
Rm , with convex domain, is K-convex if and only if for all x $\in$ dom f and all y $\in$ Rn ,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
yi yj
\end{flushleft}


\begin{flushleft}
$\partial$xi $\partial$xj
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





0,





\begin{flushleft}
i.e., the second derivative is a K-nonnegative bilinear form. (Here $\partial$ 2 f /$\partial$xi $\partial$xj $\in$ Rm ,
\end{flushleft}


\begin{flushleft}
with components $\partial$ 2 fk /$\partial$xi $\partial$xj , for k = 1, . . . , m; see §A.4.1.)
\end{flushleft}





\newpage
3


\begin{flushleft}
Solution. f is K-convex if and only if v T f is convex for all v
\end{flushleft}


\begin{flushleft}
v T f (x) is
\end{flushleft}





\begin{flushleft}
Convex functions
\end{flushleft}





\begin{flushleft}
K∗
\end{flushleft}





\begin{flushleft}
0. The Hessian of
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\nabla$2 (v T f (x)) =
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
vi $\nabla$2 fk (x).
\end{flushleft}





\begin{flushleft}
This is positive semidefinite if and only if for all y
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
y T $\nabla$2 (v T f (x))y =
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i,j=1 k=1
\end{flushleft}





\begin{flushleft}
which is equivalent to
\end{flushleft}





\begin{flushleft}
vk $\nabla$2 fk (x)yi yj =
\end{flushleft}





\begin{flushleft}
vk (
\end{flushleft}


\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
$\nabla$2 fk (x)yi yj ) $\geq$ 0,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
$\nabla$2 fk (x)yi yj
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





0





\begin{flushleft}
by definition of dual cone.
\end{flushleft}


\begin{flushleft}
3.60 Sublevel sets and epigraph of K-convex functions. Let K $\subseteq$ Rm be a proper convex cone
\end{flushleft}


\begin{flushleft}
with associated generalized inequality K , and let f : Rn $\rightarrow$ Rm . For $\alpha$ $\in$ Rm , the
\end{flushleft}


\begin{flushleft}
$\alpha$-sublevel set of f (with respect to K ) is defined as
\end{flushleft}


\begin{flushleft}
C$\alpha$ = \{x $\in$ Rn | f (x)
\end{flushleft}


\begin{flushleft}
The epigraph of f , with respect to
\end{flushleft}





\begin{flushleft}
K,
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\alpha$\}.
\end{flushleft}





\begin{flushleft}
is defined as the set
\end{flushleft}





\begin{flushleft}
epiK f = \{(x, t) $\in$ Rn+m | f (x)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
t\}.
\end{flushleft}





\begin{flushleft}
Show the following:
\end{flushleft}


\begin{flushleft}
(a) If f is K-convex, then its sublevel sets C$\alpha$ are convex for all $\alpha$.
\end{flushleft}


\begin{flushleft}
(b) f is K-convex if and only if epiK f is a convex set.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) For any x, y $\in$ C$\alpha$ , and 0 $\leq$ $\theta$ $\leq$ 1,
\end{flushleft}


\begin{flushleft}
f ($\theta$x + (1 $-$ $\theta$)y)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\theta$f (x) + (1 $-$ $\theta$)f (y)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\alpha$.
\end{flushleft}





\begin{flushleft}
(b) For any (x, u), (y, v) $\in$ epi f , and 0 $\leq$ $\theta$ $\leq$ 1,
\end{flushleft}


\begin{flushleft}
f ($\theta$x + (1 $-$ $\theta$)y)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\theta$f (x) + (1 $-$ $\theta$)f (y)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\theta$u + (1 $-$ $\theta$)v.
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 4
\end{flushleft}





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Basic terminology and optimality conditions
\end{flushleft}


\begin{flushleft}
4.1 Consider the optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x1 , x2 )
\end{flushleft}


\begin{flushleft}
2x1 + x2 $\geq$ 1
\end{flushleft}


\begin{flushleft}
x1 + 3x2 $\geq$ 1
\end{flushleft}


\begin{flushleft}
x1 $\geq$ 0, x2 $\geq$ 0.
\end{flushleft}





\begin{flushleft}
Make a sketch of the feasible set. For each of the following objective functions, give the
\end{flushleft}


\begin{flushleft}
optimal set and the optimal value.
\end{flushleft}


\begin{flushleft}
(a)
\end{flushleft}


\begin{flushleft}
(b)
\end{flushleft}


\begin{flushleft}
(c)
\end{flushleft}


\begin{flushleft}
(d)
\end{flushleft}


\begin{flushleft}
(e)
\end{flushleft}





\begin{flushleft}
f0 (x1 , x2 ) = x1 + x2 .
\end{flushleft}


\begin{flushleft}
f0 (x1 , x2 ) = $-$x1 $-$ x2 .
\end{flushleft}


\begin{flushleft}
f0 (x1 , x2 ) = x1 .
\end{flushleft}


\begin{flushleft}
f0 (x1 , x2 ) = max\{x1 , x2 \}.
\end{flushleft}


\begin{flushleft}
f0 (x1 , x2 ) = x21 + 9x22 .
\end{flushleft}





\begin{flushleft}
Solution. The feasible set is the convex hull of (0, $\infty$), (0, 1), (2/5, 1/5), (1, 0), ($\infty$, 0).
\end{flushleft}


\begin{flushleft}
(a)
\end{flushleft}


\begin{flushleft}
(b)
\end{flushleft}


\begin{flushleft}
(c)
\end{flushleft}


\begin{flushleft}
(d)
\end{flushleft}


\begin{flushleft}
(e)
\end{flushleft}





\begin{flushleft}
x = (2/5, 1/5).
\end{flushleft}


\begin{flushleft}
Unbounded below.
\end{flushleft}


\begin{flushleft}
Xopt = \{(0, x2 ) | x2 $\geq$ 1\}.
\end{flushleft}


\begin{flushleft}
x = (1/3, 1/3).
\end{flushleft}


\begin{flushleft}
x = (1/2, 1/6). This is optimal because it satisfies 2x1 +x2 = 7/6 $>$ 1, x1 +3x2 = 1,
\end{flushleft}


\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
$\nabla$f0 (x ) = (1, 3)
\end{flushleft}


\begin{flushleft}
is perpendicular to the line x1 + 3x2 = 1.
\end{flushleft}





\begin{flushleft}
4.2 Consider the optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
f0 (x) = $-$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(bi $-$ aTi x)
\end{flushleft}





\begin{flushleft}
with domain dom f0 = \{x | Ax ≺ b\}, where A $\in$ Rm×n (with rows aTi ). We assume that
\end{flushleft}


\begin{flushleft}
dom f0 is nonempty.
\end{flushleft}


\begin{flushleft}
Prove the following facts (which include the results quoted without proof on page 141).
\end{flushleft}


\begin{flushleft}
(a) dom f0 is unbounded if and only if there exists a v = 0 with Av 0.
\end{flushleft}


\begin{flushleft}
(b) f0 is unbounded below if and only if there exists a v with Av
\end{flushleft}


\begin{flushleft}
0, Av = 0. Hint.
\end{flushleft}


\begin{flushleft}
There exists a v such that Av
\end{flushleft}


\begin{flushleft}
0, Av = 0 if and only if there exists no z
\end{flushleft}


0


\begin{flushleft}
such that AT z = 0. This follows from the theorem of alternatives in example 2.21,
\end{flushleft}


\begin{flushleft}
page 50.
\end{flushleft}


\begin{flushleft}
(c) If f0 is bounded below then its minimum is attained, i.e., there exists an x that
\end{flushleft}


\begin{flushleft}
satisfies the optimality condition (4.23).
\end{flushleft}


\begin{flushleft}
(d) The optimal set is affine: Xopt = \{x + v | Av = 0\}, where x is any optimal point.
\end{flushleft}


\begin{flushleft}
Solution. We assume x0 $\in$ dom f .
\end{flushleft}


\begin{flushleft}
(a) If such a v exists, then dom f0 is clearly unbounded, since x0 + tv $\in$ dom f0 for all
\end{flushleft}


\begin{flushleft}
t $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Conversely, suppose xk is a sequence of points in dom f0 with xk 2 $\rightarrow$ $\infty$. Define
\end{flushleft}


\begin{flushleft}
v k = xk / xk 2 . The sequence has a convergent subsequence because v k 2 = 1 for
\end{flushleft}


\begin{flushleft}
all k. Let v be its limit. We have v 2 = 1 and, since aTi v k $<$ bi / xk 2 for all k,
\end{flushleft}


\begin{flushleft}
aTi v $\leq$ 0. Therefore Av 0 and v = 0.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(b) If there exists such a v, then f0 is clearly unbounded below. Let j be an index with
\end{flushleft}


\begin{flushleft}
aTj v $<$ 0. For t $\geq$ 0,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
f0 (x0 + tv)
\end{flushleft}





=





$\leq$





$-$


$-$





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=j
\end{flushleft}





\begin{flushleft}
log(bi $-$ aTi x0 $-$ taTi v)
\end{flushleft}


\begin{flushleft}
log(bi $-$ aTi x0 ) $-$ log(bj $-$ aTj x0 $-$ taTj v),
\end{flushleft}





\begin{flushleft}
and the righthand side decreases without bound as t increases.
\end{flushleft}


\begin{flushleft}
Conversely, suppose f is unbounded below. Let xk be a sequence with b $-$ Axk
\end{flushleft}


\begin{flushleft}
and f0 (xk ) $\rightarrow$ $-$$\infty$. By convexity,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
f0 (xk ) $\geq$ f0 (x0 ) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





1


\begin{flushleft}
aTi (xk $-$ x0 ) = f0 (x0 ) + m $-$
\end{flushleft}


\begin{flushleft}
bi $-$ aTi x0
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





0,





\begin{flushleft}
bi $-$ aTi xk
\end{flushleft}


\begin{flushleft}
bi $-$ aTi x0
\end{flushleft}





\begin{flushleft}
aTi xk )
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





$\rightarrow$ $\infty$.


\begin{flushleft}
so if f0 (x ) $\rightarrow$ $-$$\infty$, we must have maxi (bi $-$
\end{flushleft}


\begin{flushleft}
Suppose there exists a z with z 0, AT z = 0. Then
\end{flushleft}


\begin{flushleft}
z T b = z T (b $-$ Axk ) $\geq$ zi max(bi $-$ aTi xk ) $\rightarrow$ $\infty$.
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
We have reached a contradiction, and conclude that there is no such z. Using the
\end{flushleft}


\begin{flushleft}
theorem of alternatives, there must be a v with Av 0, Av = 0.
\end{flushleft}


\begin{flushleft}
(c) We can assume that rank A = n.
\end{flushleft}


\begin{flushleft}
If dom f0 is bounded, then the result follows from the fact that the sublevel sets of
\end{flushleft}


\begin{flushleft}
f0 are closed.
\end{flushleft}


\begin{flushleft}
If dom f0 is unbounded, let v be a direction in which it is unbounded, i.e., v = 0,
\end{flushleft}


\begin{flushleft}
Av 0. Since rank A = 0, we must have Av = 0, but this implies f0 is unbounded.
\end{flushleft}


\begin{flushleft}
We conclude that if rank A = n, then f0 is bounded below if and only if its domain
\end{flushleft}


\begin{flushleft}
is bounded, and therefore its minimum is attained.
\end{flushleft}


\begin{flushleft}
(d) Again, we can limit ourselves to the case in which rank A = n. We have to show
\end{flushleft}


\begin{flushleft}
that f0 has at most one optimal point. The Hessian of f0 at x is
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) = AT diag(d)A,
\end{flushleft}





\begin{flushleft}
di =
\end{flushleft}





1


,


\begin{flushleft}
(bi $-$ aTi x)2
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
which is positive definite if rank A = n, i.e., f0 is strictly convex. Therefore the
\end{flushleft}


\begin{flushleft}
optimal point, if it exists, is unique.
\end{flushleft}


\begin{flushleft}
4.3 Prove that x = (1, 1/2, $-$1) is optimal for the optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
P =
\end{flushleft}





13


12


$-$2





12


17


6





$-$2


6


12





\begin{flushleft}
(1/2)xT P x + q T x + r
\end{flushleft}


\begin{flushleft}
$-$1 $\leq$ xi $\leq$ 1, i = 1, 2, 3,
\end{flushleft}


,





\begin{flushleft}
q=
\end{flushleft}





$-$22.0


$-$14.5


13.0





,





\begin{flushleft}
r = 1.
\end{flushleft}





\begin{flushleft}
Solution. We verify that x satisfies the optimality condition (4.21). The gradient of
\end{flushleft}


\begin{flushleft}
the objective function at x is
\end{flushleft}


\begin{flushleft}
$\nabla$f0 (x ) = ($-$1, 0, 2).
\end{flushleft}


\begin{flushleft}
Therefore the optimality condition is that
\end{flushleft}


\begin{flushleft}
$\nabla$f0 (x )T (y $-$ x) = $-$1(y1 $-$ 1) + 2(y2 + 1) $\geq$ 0
\end{flushleft}


\begin{flushleft}
for all y satisfying $-$1 $\leq$ yi $\leq$ 1, which is clearly true.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
4.4 [P. Parrilo] Symmetries and convex optimization. Suppose G = \{Q1 , . . . , Qk \} $\subseteq$ Rn×n is a
\end{flushleft}


\begin{flushleft}
group, i.e., closed under products and inverse. We say that the function f : Rn $\rightarrow$ R is Ginvariant, or symmetric with respect to G, if f (Qi x) = f (x) holds for all x and i = 1, . . . , k.
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
We define x = (1/k) i=1 Qi x, which is the average of x over its G-orbit. We define the
\end{flushleft}


\begin{flushleft}
fixed subspace of G as
\end{flushleft}


\begin{flushleft}
F = \{x | Qi x = x, i = 1, . . . , k\}.
\end{flushleft}


\begin{flushleft}
(a) Show that for any x $\in$ Rn , we have x $\in$ F.
\end{flushleft}





\begin{flushleft}
(b) Show that if f : Rn $\rightarrow$ R is convex and G-invariant, then f (x) $\leq$ f (x).
\end{flushleft}


\begin{flushleft}
(c) We say the optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
is G-invariant if the objective f0 is G-invariant, and the feasible set is G-invariant,
\end{flushleft}


\begin{flushleft}
which means
\end{flushleft}


\begin{flushleft}
f1 (x) $\leq$ 0, . . . , fm (x) $\leq$ 0 =$\Rightarrow$ f1 (Qi x) $\leq$ 0, . . . , fm (Qi x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
for i = 1, . . . , k. Show that if the problem is convex and G-invariant, and there exists
\end{flushleft}


\begin{flushleft}
an optimal point, then there exists an optimal point in F. In other words, we can
\end{flushleft}


\begin{flushleft}
adjoin the equality constraints x $\in$ F to the problem, without loss of generality.
\end{flushleft}





\begin{flushleft}
(d) As an example, suppose f is convex and symmetric, i.e., f (P x) = f (x) for every
\end{flushleft}


\begin{flushleft}
permutation P . Show that if f has a minimizer, then it has a minimizer of the form
\end{flushleft}


\begin{flushleft}
$\alpha$1. (This means to minimize f over x $\in$ Rn , we can just as well minimize f (t1)
\end{flushleft}


\begin{flushleft}
over t $\in$ R.)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Qj x = (1/k)
\end{flushleft}


\begin{flushleft}
Qj Qi = Q l .
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Qj Qi x $\in$ F, because for each Ql $\in$ G there exists a Qi $\in$ G s.t.
\end{flushleft}





\begin{flushleft}
(b) Using convexity and invariance of f ,
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
f (x) $\leq$ (1/k)
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
f (Qi x) = (1/k)
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
f (x) = f (x).
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(c) Suppose x is an optimal solution. Then x is feasible, with
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
f0 (x )
\end{flushleft}





=





\begin{flushleft}
Qi x)
\end{flushleft}





\begin{flushleft}
f0 ((1/k)
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





$\leq$





\begin{flushleft}
(1/k)
\end{flushleft}





=





\begin{flushleft}
f0 (x ).
\end{flushleft}





\begin{flushleft}
f0 (Qi x)
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Therefore x is also optimal.
\end{flushleft}


\begin{flushleft}
(d) Suppose x is a minimizer of f . Let x = (1/n!) P P x , where the sum is over all
\end{flushleft}


\begin{flushleft}
permutations. Since x is invariant under any permutation, we conclude that x = $\alpha$1
\end{flushleft}


\begin{flushleft}
for some $\alpha$ $\in$ R. By Jensen's inequality we have
\end{flushleft}


\begin{flushleft}
f (x) $\leq$ (1/n!)
\end{flushleft}


\begin{flushleft}
which shows that x is also a minimizer.
\end{flushleft}





\begin{flushleft}
f (P x ) = f (x ),
\end{flushleft}


\begin{flushleft}
P
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
4.5 Equivalent convex problems. Show that the following three convex problems are equivalent. Carefully explain how the solution of each problem is obtained from the solution of
\end{flushleft}


\begin{flushleft}
the other problems. The problem data are the matrix A $\in$ Rm×n (with rows aTi ), the
\end{flushleft}


\begin{flushleft}
vector b $\in$ Rm , and the constant M $>$ 0.
\end{flushleft}


\begin{flushleft}
(a) The robust least-squares problem
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$\phi$(aTi x $-$ bi ),
\end{flushleft}





\begin{flushleft}
with variable x $\in$ Rn , where $\phi$ : R $\rightarrow$ R is defined as
\end{flushleft}


\begin{flushleft}
u2
\end{flushleft}


\begin{flushleft}
M (2|u| $-$ M )
\end{flushleft}





\begin{flushleft}
$\phi$(u) =
\end{flushleft}





\begin{flushleft}
|u| $\leq$ M
\end{flushleft}


\begin{flushleft}
|u| $>$ M.
\end{flushleft}





\begin{flushleft}
(This function is known as the Huber penalty function; see §6.1.2.)
\end{flushleft}





\begin{flushleft}
(b) The least-squares problem with variable weights
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(aTi x
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





0,





\begin{flushleft}
$-$ bi )2 /(wi + 1) + M 2 1T w
\end{flushleft}





\begin{flushleft}
with variables x $\in$ Rn and w $\in$ Rm , and domain D = \{(x, w) $\in$ Rn ×Rm | w $-$1\}.
\end{flushleft}


\begin{flushleft}
Hint. Optimize over w assuming x is fixed, to establish a relation with the problem
\end{flushleft}


\begin{flushleft}
in part (a).
\end{flushleft}


\begin{flushleft}
(This problem can be interpreted as a weighted least-squares problem in which we
\end{flushleft}


\begin{flushleft}
are allowed to adjust the weight of the ith residual. The weight is one if wi = 0, and
\end{flushleft}


\begin{flushleft}
decreases if we increase wi . The second term in the objective penalizes large values
\end{flushleft}


\begin{flushleft}
of w, i.e., large adjustments of the weights.)
\end{flushleft}


\begin{flushleft}
(c) The quadratic program
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(u2i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
+ 2M vi )
\end{flushleft}


\begin{flushleft}
$-$u $-$ v Ax $-$ b u + v
\end{flushleft}


\begin{flushleft}
0 u M1
\end{flushleft}


\begin{flushleft}
v 0.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Problems (a) and (b). For fixed u, the solution of the minimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
u2 /(w + 1) + M 2 w
\end{flushleft}


\begin{flushleft}
w 0
\end{flushleft}





\begin{flushleft}
is given by
\end{flushleft}


\begin{flushleft}
w=
\end{flushleft}





\begin{flushleft}
|u|/M $-$ 1
\end{flushleft}


0





\begin{flushleft}
|u| $\geq$ M
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
(w = 0|u|/M $-$1 is the unconstrained minimizer of the objective function. If |u|/M $-$
\end{flushleft}


\begin{flushleft}
1 $\geq$ 0 it is the optimum. Otherwise w = 0 is the optimum.) The optimal value is
\end{flushleft}


\begin{flushleft}
inf u2 /(w + 1) + M 2 w =
\end{flushleft}





\begin{flushleft}
w 0
\end{flushleft}





\begin{flushleft}
M (2|u| $-$ M )
\end{flushleft}


\begin{flushleft}
u2
\end{flushleft}





\begin{flushleft}
|u| $\geq$ M
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
It follows that the optimal value of x in both problems is the same. The optimal w
\end{flushleft}


\begin{flushleft}
in the second problem is given by
\end{flushleft}


\begin{flushleft}
wi =
\end{flushleft}





\begin{flushleft}
|aTi x $-$ bi |/M $-$ 1
\end{flushleft}


0





\begin{flushleft}
|aTi x $-$ bi | $\geq$ M
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) Problems (a) and (c). Suppose we fix x in problem (c).
\end{flushleft}


\begin{flushleft}
First we note that at the optimum we must have ui + vi = |aTi x $-$ bi |. Otherwise,
\end{flushleft}


\begin{flushleft}
i.e., if ui , vi satisfy ui + vi $>$ |aTi x + bi with 0 $\leq$ ui $\leq$ M and vi $\geq$ 0, then, since
\end{flushleft}


\begin{flushleft}
ui and vi are not both zero, we can decrease ui and/or vi without violating the
\end{flushleft}


\begin{flushleft}
constraints. This also decreases the objective.
\end{flushleft}


\begin{flushleft}
At the optimum we therefore have
\end{flushleft}


\begin{flushleft}
vi = |aTi x $-$ bi | $-$ ui .
\end{flushleft}


\begin{flushleft}
Eliminating v yields the equivalent problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





0





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(u2i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
$\leq$ ui $\leq$
\end{flushleft}





\begin{flushleft}
$-$ 2M ui + 2M |aTi x $-$ bi |)
\end{flushleft}


\begin{flushleft}
min\{M, |aTi x $-$ bi |\}
\end{flushleft}





\begin{flushleft}
If |aTi x $-$ bi | $\leq$ M , the optimal choice for ui is ui = |aTi x $-$ bi |. In this case the ith
\end{flushleft}


\begin{flushleft}
term in the objective function reduces to |aTi x $-$ bi |. If |aTi x $-$ bi | $>$ M , we choose
\end{flushleft}


\begin{flushleft}
ui = M , and the ith term in the objective function reduces to 2M |aTi x $-$ bi | $-$ M 2 .
\end{flushleft}


\begin{flushleft}
We conclude that, for fixed x, the optimal value of the problem in (c) is given by
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\phi$(aTi x $-$ bi ).
\end{flushleft}





\begin{flushleft}
4.6 Handling convex equality constraints. A convex optimization problem can have only linear
\end{flushleft}


\begin{flushleft}
equality constraint functions. In some special cases, however, it is possible to handle
\end{flushleft}


\begin{flushleft}
convex equality constraint functions, i.e., constraints of the form g(x) = 0, where g is
\end{flushleft}


\begin{flushleft}
convex. We explore this idea in this problem.
\end{flushleft}


\begin{flushleft}
Consider the optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
h(x) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





(4.65)





\begin{flushleft}
where fi and h are convex functions with domain Rn . Unless h is affine, this is not a
\end{flushleft}


\begin{flushleft}
convex optimization problem. Consider the related problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
h(x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





(4.66)





\begin{flushleft}
where the convex equality constraint has been relaxed to a convex inequality. This problem is, of course, convex.
\end{flushleft}


\begin{flushleft}
Now suppose we can guarantee that at any optimal solution x of the convex problem (4.66), we have h(x ) = 0, i.e., the inequality h(x) $\leq$ 0 is always active at the solution.
\end{flushleft}


\begin{flushleft}
Then we can solve the (nonconvex) problem (4.65) by solving the convex problem (4.66).
\end{flushleft}


\begin{flushleft}
Show that this is the case if there is an index r such that
\end{flushleft}


\begin{flushleft}
$\bullet$ f0 is monotonically increasing in xr
\end{flushleft}


\begin{flushleft}
$\bullet$ f1 , . . . , fm are nonincreasing in xr
\end{flushleft}





\begin{flushleft}
$\bullet$ h is monotonically decreasing in xr .
\end{flushleft}





\begin{flushleft}
We will see specific examples in exercises 4.31 and 4.58.
\end{flushleft}


\begin{flushleft}
Solution. Suppose x is optimal for the relaxed problem, and h(x ) $<$ 0. By the last
\end{flushleft}


\begin{flushleft}
property, we can decrease xr while staying in the boundary of g. By decreasing xr we
\end{flushleft}


\begin{flushleft}
decrease the objective, preserve the inequalities fi (x) $\leq$ 0, and increase the function h.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
4.7 Convex-concave fractional problems. Consider a problem of the form
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)/(cT x + d)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
Ax = b
\end{flushleft}





\begin{flushleft}
where f0 , f1 , . . . , fm are convex, and the domain of the objective function is defined as
\end{flushleft}


\begin{flushleft}
\{x $\in$ dom f0 | cT x + d $>$ 0\}.
\end{flushleft}


\begin{flushleft}
(a) Show that this is a quasiconvex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. The domain of the objective is convex, because f0 is convex. The sublevel
\end{flushleft}


\begin{flushleft}
sets are convex because f0 (x)/(cT x + d) $\leq$ $\alpha$ if and only if cT x + d $>$ 0 and f0 (x) $\leq$
\end{flushleft}


\begin{flushleft}
$\alpha$(cT x + d).
\end{flushleft}


\begin{flushleft}
(b) Show that the problem is equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
g0 (y, t)
\end{flushleft}


\begin{flushleft}
gi (y, t) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
Ay = bt
\end{flushleft}


\begin{flushleft}
cT y + dt = 1,
\end{flushleft}





\begin{flushleft}
where gi is the perspective of fi (see §3.2.6). The variables are y $\in$ Rn and t $\in$ R.
\end{flushleft}


\begin{flushleft}
Show that this problem is convex.
\end{flushleft}


\begin{flushleft}
Solution. Suppose x is feasible in the original problem. Define t = 1/(cT x + d)
\end{flushleft}


\begin{flushleft}
(a positive number), y = x/(cT x + d). Then t $>$ 0 and it is easily verified that
\end{flushleft}


\begin{flushleft}
t, y are feasible in the transformed problem, with the objective value g0 (y, t) =
\end{flushleft}


\begin{flushleft}
f0 (x)/(cT x + d).
\end{flushleft}


\begin{flushleft}
Conversely, suppose y, t are feasible for the transformed problem. We must have
\end{flushleft}


\begin{flushleft}
t $>$ 0, by definition of the domain of the perspective function. Define x = y/t. We
\end{flushleft}


\begin{flushleft}
have x $\in$ dom fi for i = 0, . . . , m (again, by definition of perspective). x is feasible
\end{flushleft}


\begin{flushleft}
in the original problem, because
\end{flushleft}


\begin{flushleft}
fi (x) = gi (y, t)/t $\leq$ 0,
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
Ax = A(y/t) = b.
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
From the last equality, c x + d = (c y + dt)/t = 1/t, and hence,
\end{flushleft}


\begin{flushleft}
t = 1/(cT x + d),
\end{flushleft}





\begin{flushleft}
f0 (x)/(cT x + d) = tf0 (x) = g0 (y, t).
\end{flushleft}





\begin{flushleft}
Therefore x is feasible in the original problem, with the objective value g0 (y, t).
\end{flushleft}


\begin{flushleft}
In conclusion, from any feasible point of one problem we can derive a feasible point
\end{flushleft}


\begin{flushleft}
of the other problem, with the same objective value.
\end{flushleft}


\begin{flushleft}
(c) Following a similar argument, derive a convex formulation for the convex-concave
\end{flushleft}


\begin{flushleft}
fractional problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)/h(x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
Ax = b
\end{flushleft}





\begin{flushleft}
where f0 , f1 , . . . , fm are convex, h is concave, the domain of the objective function
\end{flushleft}


\begin{flushleft}
is defined as \{x $\in$ dom f0 $\cap$ dom h | h(x) $>$ 0\} and f0 (x) $\geq$ 0 everywhere.
\end{flushleft}


\begin{flushleft}
As an example, apply your technique to the (unconstrained) problem with
\end{flushleft}


\begin{flushleft}
f0 (x) = (tr F (x))/m,
\end{flushleft}





\begin{flushleft}
h(x) = (det(F (x))1/m ,
\end{flushleft}





\begin{flushleft}
with dom(f0 /h) = \{x | F (x) 0\}, where F (x) = F0 + x1 F1 + · · · + xn Fn for given
\end{flushleft}


\begin{flushleft}
Fi $\in$ Sm . In this problem, we minimize the ratio of the arithmetic mean over the
\end{flushleft}


\begin{flushleft}
geometric mean of the eigenvalues of an affine matrix function F (x).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) We first verify that the problem is quasiconvex. The domain of the objective function is convex, and its sublevel sets are convex because for $\alpha$ $\geq$ 0,
\end{flushleft}


\begin{flushleft}
f0 (x)/h(x) $\leq$ $\alpha$ if and only if f0 (x) $-$ $\alpha$h(x) $\leq$ 0, which is a convex inequality.
\end{flushleft}


\begin{flushleft}
For $\alpha$ $<$ 0, the sublevel sets are empty.
\end{flushleft}


\begin{flushleft}
(b) The convex formulation is
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
g0 (y, t)
\end{flushleft}


\begin{flushleft}
gi (y, t) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
Ay = bt
\end{flushleft}


\begin{flushleft}
˜ t) $\leq$ $-$1
\end{flushleft}


\begin{flushleft}
h(y,
\end{flushleft}





\begin{flushleft}
˜ is the perspective of $-$h.
\end{flushleft}


\begin{flushleft}
where gi is the perspective of fi and h
\end{flushleft}


\begin{flushleft}
To verify the equivalence, assume first that x is feasible in the original problem.
\end{flushleft}


\begin{flushleft}
Define t = 1/h(x) and y = x/h(x). Then t $>$ 0 and
\end{flushleft}


\begin{flushleft}
gi (y, t) = tfi (y/t) = tfi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
Ay = Ax/h(x) = bt.
\end{flushleft}





\begin{flushleft}
˜ t) = th(y/t) = h(x)/h(x) = 1 and
\end{flushleft}


\begin{flushleft}
Moreover, h(y,
\end{flushleft}


\begin{flushleft}
g0 (y, t) = tf0 (y/t) = f0 (x)/h(x).
\end{flushleft}


\begin{flushleft}
We see that for every feasible point in the original problem we can find a feasible
\end{flushleft}


\begin{flushleft}
point in the transformed problem, with the same objective value.
\end{flushleft}


\begin{flushleft}
Conversely, assume y, t are feasible in the transformed problem. By definition
\end{flushleft}


\begin{flushleft}
of perspective, t $>$ 0. Define x = y/t. We have
\end{flushleft}


\begin{flushleft}
fi (x) = fi (y/t) = gi (y, t)/t $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
Ax = A(y/t) = b.
\end{flushleft}





\begin{flushleft}
From the last inequality, we have
\end{flushleft}


\begin{flushleft}
˜ t) = $-$th(y/t) = $-$th(x) $\leq$ $-$1.
\end{flushleft}


\begin{flushleft}
h(y,
\end{flushleft}


\begin{flushleft}
This implies that h(x) $>$ 0 and th(x) $\geq$ 1. And finally, the objective is
\end{flushleft}


\begin{flushleft}
f0 (x)/h(x) = g0 (y, t)/(th(x)) $\leq$ g0 (y, t).
\end{flushleft}


\begin{flushleft}
We conclude that with every feasible point in the transformed problem there is
\end{flushleft}


\begin{flushleft}
a corresponding feasible point in the original problem with the same or lower
\end{flushleft}


\begin{flushleft}
objective value.
\end{flushleft}


\begin{flushleft}
Putting the two parts together, we can conclude that the two problems have
\end{flushleft}


\begin{flushleft}
the same optimal value, and that optimal solutions for one problem are optimal
\end{flushleft}


\begin{flushleft}
for the other (if both are solvable).
\end{flushleft}


\begin{flushleft}
(c)
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(1/m) tr(tF0 + y1 F1 + · · · + yn Fn )
\end{flushleft}


\begin{flushleft}
det(tF0 + y1 F1 + · · · + yn Fn )1/m $\geq$ 1
\end{flushleft}





\begin{flushleft}
with domain
\end{flushleft}


\begin{flushleft}
\{(y, t) | t $>$ 0, tF0 + y1 F1 + · · · + yn Fn
\end{flushleft}





0\}.





\begin{flushleft}
Linear optimization problems
\end{flushleft}


\begin{flushleft}
4.8 Some simple LPs. Give an explicit solution of each of the following LPs.
\end{flushleft}


\begin{flushleft}
(a) Minimizing a linear function over an affine set.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}





\begin{flushleft}
Solution. We distinguish three possibilities.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
$\bullet$ The problem is infeasible (b $\in$ R(A)). The optimal value is $\infty$.
\end{flushleft}


\begin{flushleft}
$\bullet$ The problem is feasible, and c is orthogonal to the nullspace of A. We can
\end{flushleft}


\begin{flushleft}
decompose c as
\end{flushleft}


\begin{flushleft}
c = AT $\lambda$ + cˆ,
\end{flushleft}


\begin{flushleft}
Aˆ
\end{flushleft}


\begin{flushleft}
c = 0.
\end{flushleft}


(ˆ


\begin{flushleft}
c is the component in the nullspace of A; AT $\lambda$ is orthogonal to the nullspace.)
\end{flushleft}


\begin{flushleft}
If cˆ = 0, then on the feasible set the objective function reduces to a constant:
\end{flushleft}


\begin{flushleft}
cT x = $\lambda$T Ax + cˆT x = $\lambda$T b.
\end{flushleft}


\begin{flushleft}
The optimal value is $\lambda$T b. All feasible solutions are optimal.
\end{flushleft}


\begin{flushleft}
$\bullet$ The problem is feasible, and c is not in the range of AT (ˆ
\end{flushleft}


\begin{flushleft}
c = 0). The problem
\end{flushleft}


\begin{flushleft}
is unbounded (p = $-$$\infty$). To verify this, note that x = x0 $-$ tˆ
\end{flushleft}


\begin{flushleft}
c is feasible for
\end{flushleft}


\begin{flushleft}
all t; as t goes to infinity, the objective value decreases unboundedly.
\end{flushleft}


\begin{flushleft}
In summary,
\end{flushleft}


\begin{flushleft}
p =
\end{flushleft}





+$\infty$


\begin{flushleft}
$\lambda$T b
\end{flushleft}


$-$$\infty$





\begin{flushleft}
b $\in$ R(A)
\end{flushleft}


\begin{flushleft}
c = AT $\lambda$ for some $\lambda$
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
(b) Minimizing a linear function over a halfspace.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
aT x $\leq$ b,
\end{flushleft}





\begin{flushleft}
where a = 0.
\end{flushleft}


\begin{flushleft}
Solution. This problem is always feasible. The vector c can be decomposed into a
\end{flushleft}


\begin{flushleft}
component parallel to a and a component orthogonal to a:
\end{flushleft}


\begin{flushleft}
c = a$\lambda$ + cˆ,
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
with a cˆ = 0.
\end{flushleft}


\begin{flushleft}
$\bullet$ If $\lambda$ $>$ 0, the problem is unbounded below. Choose x = $-$ta, and let t go to
\end{flushleft}


\begin{flushleft}
infinity:
\end{flushleft}


\begin{flushleft}
cT x = $-$tcT a = $-$t$\lambda$aT a $\rightarrow$ $-$$\infty$
\end{flushleft}


\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
aT x $-$ b = $-$taT a $-$ b $\leq$ 0
\end{flushleft}





\begin{flushleft}
for large t, so x is feasible for large t. Intuitively, by going very far in the
\end{flushleft}


\begin{flushleft}
direction $-$a, we find feasible points with arbitrarily negative objective values.
\end{flushleft}


\begin{flushleft}
$\bullet$ If cˆ = 0, the problem is unbounded below. Choose x = ba $-$ tˆ
\end{flushleft}


\begin{flushleft}
c and let t go to
\end{flushleft}


\begin{flushleft}
infinity.
\end{flushleft}


\begin{flushleft}
$\bullet$ If c = a$\lambda$ for some $\lambda$ $\leq$ 0, the optimal value is cT ab = $\lambda$b.
\end{flushleft}





\begin{flushleft}
In summary, the optimal value is
\end{flushleft}


\begin{flushleft}
p =
\end{flushleft}





\begin{flushleft}
$\lambda$b
\end{flushleft}


$-$$\infty$





\begin{flushleft}
c = a$\lambda$ for some $\lambda$ $\leq$ 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
(c) Minimizing a linear function over a rectangle.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
l x
\end{flushleft}





\begin{flushleft}
u,
\end{flushleft}





\begin{flushleft}
where l and u satisfy l u.
\end{flushleft}


\begin{flushleft}
Solution. The objective and the constraints are separable: The objective is a sum of
\end{flushleft}


\begin{flushleft}
terms ci xi , each dependent on one variable only; each constraint depends on only one
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
variable. We can therefore solve the problem by minimizing over each component of x
\end{flushleft}


\begin{flushleft}
independently. The optimal xi minimizes ci xi subject to the constraint li $\leq$ xi $\leq$ ui .
\end{flushleft}


\begin{flushleft}
If ci $>$ 0, then xi = li ; if ci $<$ 0, then xi = ui ; if ci = 0, then any xi in the interval
\end{flushleft}


\begin{flushleft}
[li , ui ] is optimal. Therefore, the optimal value of the problem is
\end{flushleft}


\begin{flushleft}
p = l T c+ + u T c$-$ ,
\end{flushleft}


$-$


\begin{flushleft}
where c+
\end{flushleft}


\begin{flushleft}
i = max\{ci , 0\} and ci = max\{$-$ci , 0\}.
\end{flushleft}


\begin{flushleft}
(d) Minimizing a linear function over the probability simplex.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
1T x = 1,
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





0.





\begin{flushleft}
What happens if the equality constraint is replaced by an inequality 1T x $\leq$ 1?
\end{flushleft}


\begin{flushleft}
We can interpret this LP as a simple portfolio optimization problem. The vector
\end{flushleft}


\begin{flushleft}
x represents the allocation of our total budget over different assets, with x i the
\end{flushleft}


\begin{flushleft}
fraction invested in asset i. The return of each investment is fixed and given by $-$ci ,
\end{flushleft}


\begin{flushleft}
so our total return (which we want to maximize) is $-$cT x. If we replace the budget
\end{flushleft}


\begin{flushleft}
constraint 1T x = 1 with an inequality 1T x $\leq$ 1, we have the option of not investing
\end{flushleft}


\begin{flushleft}
a portion of the total budget.
\end{flushleft}


\begin{flushleft}
Solution. Suppose the components of c are sorted in increasing order with
\end{flushleft}


\begin{flushleft}
c1 = c2 = · · · = ck $<$ ck+1 $\leq$ · · · $\leq$ cn .
\end{flushleft}


\begin{flushleft}
We have
\end{flushleft}





\begin{flushleft}
cT x $\geq$ c1 (1T x) = cmin
\end{flushleft}


\begin{flushleft}
for all feasible x, with equality if and only if
\end{flushleft}


\begin{flushleft}
x1 + · · · + xk = 1,
\end{flushleft}





\begin{flushleft}
x1 $\geq$ 0, . . . , xk $\geq$ 0,
\end{flushleft}





\begin{flushleft}
xk+1 = · · · = xn = 0.
\end{flushleft}





\begin{flushleft}
We conclude that the optimal value is p = c1 = cmin . In the investment interpretation this choice is quite obvious. If the returns are fixed and known, we invest our
\end{flushleft}


\begin{flushleft}
total budget in the investment with the highest return.
\end{flushleft}


\begin{flushleft}
If we replace the equality with an inequality, the optimal value is equal to
\end{flushleft}


\begin{flushleft}
p = min\{0, cmin \}.
\end{flushleft}


\begin{flushleft}
(If cmin $\leq$ 0, we make the same choice for x as above. Otherwise, we choose x = 0.)
\end{flushleft}


\begin{flushleft}
(e) Minimizing a linear function over a unit box with a total budget constraint.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
1T x = $\alpha$,
\end{flushleft}





0





\begin{flushleft}
x
\end{flushleft}





1,





\begin{flushleft}
where $\alpha$ is an integer between 0 and n. What happens if $\alpha$ is not an integer (but
\end{flushleft}


\begin{flushleft}
satisfies 0 $\leq$ $\alpha$ $\leq$ n)? What if we change the equality to an inequality 1T x $\leq$ $\alpha$?
\end{flushleft}


\begin{flushleft}
Solution. We first consider the case of integer $\alpha$. Suppose
\end{flushleft}


\begin{flushleft}
c1 $\leq$ · · · $\leq$ ci$-$1 $<$ ci = · · · = c$\alpha$ = · · · = ck $<$ ck+1 $\leq$ · · · $\leq$ cn .
\end{flushleft}


\begin{flushleft}
The optimal value is
\end{flushleft}


\begin{flushleft}
c1 + c 2 + · · · + c $\alpha$
\end{flushleft}


\begin{flushleft}
i.e., the sum of the smallest $\alpha$ elements of c. x is optimal if and only if
\end{flushleft}


\begin{flushleft}
x1 = · · · = xi$-$1 = 1,
\end{flushleft}





\begin{flushleft}
xi + · · · + xk = $\alpha$ $-$ i + 1,
\end{flushleft}





\begin{flushleft}
xk+1 = · · · = xn = 0.
\end{flushleft}





\begin{flushleft}
If $\alpha$ is not an integer, the optimal value is
\end{flushleft}


\begin{flushleft}
p = c1 + c2 + · · · + c
\end{flushleft}





\begin{flushleft}
$\alpha$
\end{flushleft}





\begin{flushleft}
+ c1+
\end{flushleft}





\begin{flushleft}
$\alpha$
\end{flushleft}





\begin{flushleft}
($\alpha$ $-$ $\alpha$ ).
\end{flushleft}





\begin{flushleft}
In the case of an inequality constraint 1T x $\leq$ $\alpha$, with $\alpha$ an integer between 0 and n,
\end{flushleft}


\begin{flushleft}
the optimal value is the sum of the $\alpha$ smallest nonpositive coefficients of c.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(f) Minimizing a linear function over a unit box with a weighted budget constraint.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
dT x = $\alpha$,
\end{flushleft}





0





\begin{flushleft}
x
\end{flushleft}





1,





\begin{flushleft}
with d 0, and 0 $\leq$ $\alpha$ $\leq$ 1T d.
\end{flushleft}


\begin{flushleft}
Solution. We make a change of variables yi = di xi , and consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(c /di )yi
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}





\begin{flushleft}
1 x = $\alpha$,
\end{flushleft}





0





\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
d.
\end{flushleft}





\begin{flushleft}
Suppose the ratios ci /di have been sorted in increasing order:
\end{flushleft}


\begin{flushleft}
c2
\end{flushleft}


\begin{flushleft}
cn
\end{flushleft}


\begin{flushleft}
c1
\end{flushleft}


$\leq$


$\leq$ ··· $\leq$


.


\begin{flushleft}
d1
\end{flushleft}


\begin{flushleft}
d2
\end{flushleft}


\begin{flushleft}
dn
\end{flushleft}


\begin{flushleft}
To minimize the objective, we choose
\end{flushleft}


\begin{flushleft}
y1 = d 1 ,
\end{flushleft}





\begin{flushleft}
y2 = d 2 ,
\end{flushleft}





...,





\begin{flushleft}
yk+1 = $\alpha$ $-$ (d1 + · · · + dk ),
\end{flushleft}





\begin{flushleft}
yk = d k ,
\end{flushleft}





\begin{flushleft}
yk+2 = · · · = yn = 0,
\end{flushleft}





\begin{flushleft}
where k = max\{i $\in$ \{1, . . . , n\} | d1 + · · · + di $\leq$ $\alpha$\} (and k = 0 if d1 $>$ $\alpha$). In terms
\end{flushleft}


\begin{flushleft}
of the original variables,
\end{flushleft}


\begin{flushleft}
x1 = · · · = xk = 1,
\end{flushleft}





\begin{flushleft}
xk+1 = ($\alpha$ $-$ (d1 + · · · + dk ))/dk+1 ,
\end{flushleft}





\begin{flushleft}
xk+2 = · · · = xn = 0.
\end{flushleft}





\begin{flushleft}
4.9 Square LP. Consider the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
with A square and nonsingular. Show that the optimal value is given by
\end{flushleft}


\begin{flushleft}
p =
\end{flushleft}





\begin{flushleft}
cT A$-$1 b
\end{flushleft}


$-$$\infty$





\begin{flushleft}
A$-$T c 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
Solution. Make a change of variables y = Ax. The problem is equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT A$-$1 y
\end{flushleft}


\begin{flushleft}
y b.
\end{flushleft}





\begin{flushleft}
If A$-$T c
\end{flushleft}


\begin{flushleft}
0, the optimal solution is y = b, with p = cT A$-$1 b. Otherwise, the LP is
\end{flushleft}


\begin{flushleft}
unbounded below.
\end{flushleft}


\begin{flushleft}
4.10 Converting general LP to standard form. Work out the details on page 147 of §4.3.
\end{flushleft}


\begin{flushleft}
Explain in detail the relation between the feasible sets, the optimal solutions, and the
\end{flushleft}


\begin{flushleft}
optimal values of the standard form LP and the original LP.
\end{flushleft}


\begin{flushleft}
Solution. Suppose x is feasible in (4.27). Define
\end{flushleft}


\begin{flushleft}
x+
\end{flushleft}


\begin{flushleft}
i = min\{0, xi \},
\end{flushleft}





\begin{flushleft}
x$-$
\end{flushleft}


\begin{flushleft}
i = min\{0, $-$xi \},
\end{flushleft}





\begin{flushleft}
s = h $-$ Gx.
\end{flushleft}





\begin{flushleft}
It is easily verified that x+ , x$-$ , s are feasible in the standard form LP, with objective
\end{flushleft}


\begin{flushleft}
value
\end{flushleft}


\begin{flushleft}
cT x+ $-$ cT x$-$ + d = cT x $-$ d.
\end{flushleft}





\begin{flushleft}
Hence, for each feasible point in (4.27) we can find a feasible point in the standard form
\end{flushleft}


\begin{flushleft}
LP with the same objective value. In particular, this implies that the optimal value of
\end{flushleft}


\begin{flushleft}
the standard form LP is less than or equal to the optimal value of (4.27).
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Conversely, suppose x+ , x$-$ , s are feasible in the standard form LP. Define x = x+ $-$ x$-$ .
\end{flushleft}


\begin{flushleft}
It is clear that x is feasible for (4.27), with objective value cT x + d = cT x+ $-$ cT x$-$ + d.
\end{flushleft}


\begin{flushleft}
Hence, for each feasible point in the standard form LP we can find a feasible point in (4.27)
\end{flushleft}


\begin{flushleft}
with the same objective value. This implies that the optimal value of the standard form
\end{flushleft}


\begin{flushleft}
LP is greater than or equal to the optimal value of (4.27).
\end{flushleft}


\begin{flushleft}
We conclude that the optimal values are equal.
\end{flushleft}


\begin{flushleft}
4.11 Problems involving 1 - and $\infty$ -norms. Formulate the following problems as LPs. Explain
\end{flushleft}


\begin{flushleft}
in detail the relation between the optimal solution of each problem and the solution of its
\end{flushleft}


\begin{flushleft}
equivalent LP.
\end{flushleft}


\begin{flushleft}
(a)
\end{flushleft}


\begin{flushleft}
(b)
\end{flushleft}


\begin{flushleft}
(c)
\end{flushleft}


\begin{flushleft}
(d)
\end{flushleft}


\begin{flushleft}
(e)
\end{flushleft}





\begin{flushleft}
Minimize
\end{flushleft}


\begin{flushleft}
Minimize
\end{flushleft}


\begin{flushleft}
Minimize
\end{flushleft}


\begin{flushleft}
Minimize
\end{flushleft}


\begin{flushleft}
Minimize
\end{flushleft}





\begin{flushleft}
Ax $-$ b $\infty$ ( $\infty$ -norm approximation).
\end{flushleft}


\begin{flushleft}
Ax $-$ b 1 ( 1 -norm approximation).
\end{flushleft}


\begin{flushleft}
Ax $-$ b 1 subject to x $\infty$ $\leq$ 1.
\end{flushleft}


\begin{flushleft}
x 1 subject to Ax $-$ b $\infty$ $\leq$ 1.
\end{flushleft}


\begin{flushleft}
Ax $-$ b 1 + x $\infty$ .
\end{flushleft}





\begin{flushleft}
In each problem, A $\in$ Rm×n and b $\in$ Rm are given. (See §6.1 for more problems involving
\end{flushleft}


\begin{flushleft}
approximation and constrained approximation.)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Equivalent to the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
Ax $-$ b t1
\end{flushleft}


\begin{flushleft}
Ax $-$ b $\geq$ $-$t1.
\end{flushleft}





\begin{flushleft}
in the variables x, t. To see the equivalence, assume x is fixed in this problem, and
\end{flushleft}


\begin{flushleft}
we optimize only over t. The constraints say that
\end{flushleft}


\begin{flushleft}
$-$t $\leq$ aTk x $-$ bk $\leq$ t
\end{flushleft}





\begin{flushleft}
for each k, i.e., t $\geq$ |aTk x $-$ bk |, i.e.,
\end{flushleft}





\begin{flushleft}
t $\geq$ max |aTk x $-$ bk | = Ax $-$ b
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





$\infty$.





\begin{flushleft}
Clearly, if x is fixed, the optimal value of the LP is p (x) = Ax $-$ b $\infty$ . Therefore
\end{flushleft}


\begin{flushleft}
optimizing over t and x simultaneously is equivalent to the original problem.
\end{flushleft}


\begin{flushleft}
(b) Equivalent to the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
1T s
\end{flushleft}


\begin{flushleft}
subject to Ax $-$ b s
\end{flushleft}


\begin{flushleft}
Ax $-$ b $\geq$ $-$s.
\end{flushleft}


\begin{flushleft}
Assume x is fixed in this problem, and we optimize only over s. The constraints say
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


\begin{flushleft}
$-$sk $\leq$ aTk x $-$ bk $\leq$ sk
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
for each k, i.e., sk $\geq$ |ak x $-$ bk |. The objective function of the LP is separable, so
\end{flushleft}


\begin{flushleft}
we achieve the optimum over s by choosing
\end{flushleft}


\begin{flushleft}
sk = |aTk x $-$ bk |,
\end{flushleft}


\begin{flushleft}
and obtain the optimal value p (x) = Ax $-$ b 1 . Therefore optimizing over t and s
\end{flushleft}


\begin{flushleft}
simultaneously is equivalent to the original problem.
\end{flushleft}


\begin{flushleft}
(c) Equivalent to the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
with variables x $\in$ Rn and y $\in$ Rm .
\end{flushleft}





\begin{flushleft}
1T y
\end{flushleft}


\begin{flushleft}
$-$y Ax $-$ b
\end{flushleft}


\begin{flushleft}
$-$1 $\leq$ x $\leq$ 1,
\end{flushleft}





\begin{flushleft}
y
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(d) Equivalent to the LP
\end{flushleft}


\begin{flushleft}
1T y
\end{flushleft}


\begin{flushleft}
$-$y $\leq$ x $\leq$ y
\end{flushleft}


\begin{flushleft}
$-$1 $\leq$ Ax $-$ b $\leq$ 1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
with variables x and y.
\end{flushleft}


\begin{flushleft}
Another good solution is to write x as the difference of two nonnegative vectors
\end{flushleft}


\begin{flushleft}
x = x+ $-$ x$-$ , and to express the problem as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1 T x+ + 1 T x$-$
\end{flushleft}


\begin{flushleft}
$-$1 Ax+ $-$ Ax$-$ $-$ b
\end{flushleft}


\begin{flushleft}
x+ 0, x$-$ 0,
\end{flushleft}





1





\begin{flushleft}
with variables x+ $\in$ Rn and x$-$ $\in$ Rn .
\end{flushleft}





\begin{flushleft}
(e) Equivalent to
\end{flushleft}





\begin{flushleft}
1T y + t
\end{flushleft}


\begin{flushleft}
$-$y Ax $-$ b y
\end{flushleft}


\begin{flushleft}
$-$t1 x t1,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
with variables x, y, and t.
\end{flushleft}





\begin{flushleft}
4.12 Network flow problem. Consider a network of n nodes, with directed links connecting each
\end{flushleft}


\begin{flushleft}
pair of nodes. The variables in the problem are the flows on each link: xij will denote the
\end{flushleft}


\begin{flushleft}
flow from node i to node j. The cost of the flow along the link from node i to node j is
\end{flushleft}


\begin{flushleft}
given by cij xij , where cij are given constants. The total cost across the network is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
C=
\end{flushleft}





\begin{flushleft}
cij xij .
\end{flushleft}


\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
Each link flow xij is also subject to a given lower bound lij (usually assumed to be
\end{flushleft}


\begin{flushleft}
nonnegative) and an upper bound uij .
\end{flushleft}


\begin{flushleft}
The external supply at node i is given by bi , where bi $>$ 0 means an external flow enters
\end{flushleft}


\begin{flushleft}
the network at node i, and bi $<$ 0 means that at node i, an amount |bi | flows out of the
\end{flushleft}


\begin{flushleft}
network. We assume that 1T b = 0, i.e., the total external supply equals total external
\end{flushleft}


\begin{flushleft}
demand. At each node we have conservation of flow: the total flow into node i along links
\end{flushleft}


\begin{flushleft}
and the external supply, minus the total flow out along the links, equals zero.
\end{flushleft}


\begin{flushleft}
The problem is to minimize the total cost of flow through the network, subject to the
\end{flushleft}


\begin{flushleft}
constraints described above. Formulate this problem as an LP.
\end{flushleft}


\begin{flushleft}
Solution. This can be formulated as the LP
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
C = i,j=1 cij xij
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
bi + j=1 xij $-$
\end{flushleft}


\begin{flushleft}
lij $\leq$ xij $\leq$ uij .
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
xji = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n
\end{flushleft}





\begin{flushleft}
4.13 Robust LP with interval coefficients. Consider the problem, with variable x $\in$ R n ,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
b for all A $\in$ A,
\end{flushleft}





\begin{flushleft}
where A $\subseteq$ Rm×n is the set
\end{flushleft}


\begin{flushleft}
A = \{A $\in$ Rm×n | A¯ij $-$ Vij $\leq$ Aij $\leq$ A¯ij + Vij , i = 1, . . . , m, j = 1, . . . , n\}.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(The matrices A¯ and V are given.) This problem can be interpreted as an LP where each
\end{flushleft}


\begin{flushleft}
coefficient of A is only known to lie in an interval, and we require that x must satisfy the
\end{flushleft}


\begin{flushleft}
constraints for all possible values of the coefficients.
\end{flushleft}


\begin{flushleft}
Express this problem as an LP. The LP you construct should be efficient, i.e., it should
\end{flushleft}


\begin{flushleft}
not have dimensions that grow exponentially with n or m.
\end{flushleft}


\begin{flushleft}
Solution. The problem is equivalent to
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
¯ + V |x|
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
where |x| = (|x1 |, |x2 |, . . . , |xn |). This in turn is equivalent to the LP
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
¯ +Vy b
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}


\begin{flushleft}
$-$y x y
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
with variables x $\in$ Rn , y $\in$ Rn .
\end{flushleft}





\begin{flushleft}
4.14 Approximating a matrix in infinity norm. The
\end{flushleft}


\begin{flushleft}
Rm×n , denoted A $\infty$ , is given by
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





$\infty$





\begin{flushleft}
Ax $\infty$
\end{flushleft}


\begin{flushleft}
= max
\end{flushleft}


\begin{flushleft}
i=1,...,m
\end{flushleft}


\begin{flushleft}
x $\infty$
\end{flushleft}





\begin{flushleft}
= sup
\end{flushleft}


\begin{flushleft}
x=0
\end{flushleft}





\begin{flushleft}
induced norm of a matrix A $\in$
\end{flushleft}





\begin{flushleft}
$\infty$ -norm
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
|aij |.
\end{flushleft}





\begin{flushleft}
This norm is sometimes called the max-row-sum norm, for obvious reasons (see §A.1.5).
\end{flushleft}


\begin{flushleft}
Consider the problem of approximating a matrix, in the max-row-sum norm, by a linear
\end{flushleft}


\begin{flushleft}
combination of other matrices. That is, we are given k + 1 matrices A0 , . . . , Ak $\in$ Rm×n ,
\end{flushleft}


\begin{flushleft}
and need to find x $\in$ Rk that minimizes
\end{flushleft}


\begin{flushleft}
A 0 + x 1 A1 + · · · + x k Ak
\end{flushleft}





$\infty$.





\begin{flushleft}
Express this problem as a linear program. Explain the significance of any extra variables
\end{flushleft}


\begin{flushleft}
in your LP. Carefully explain how your LP formulation solves this problem, e.g., what is
\end{flushleft}


\begin{flushleft}
the relation between the feasible set for your LP and this problem?
\end{flushleft}


\begin{flushleft}
Solution. The problem can be formulated as an LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
$-$S
\end{flushleft}


\begin{flushleft}
S1
\end{flushleft}





\begin{flushleft}
A 0 + x 1 A 1 + · · · + x k ak
\end{flushleft}


\begin{flushleft}
t1,
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
with variables S $\in$ Rm×n , t $\in$ R and x $\in$ Rk . The inequality
\end{flushleft}


\begin{flushleft}
inequality between matrices, i.e., with respect to the cone
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
S
\end{flushleft}





\begin{flushleft}
denotes componentwise
\end{flushleft}





\begin{flushleft}
K = \{X $\in$ Rm×n | Xij $\geq$ 0, i = 1, . . . , m, j = 1 . . . , n\}.
\end{flushleft}


\begin{flushleft}
To see the equivalence, suppose x and S are feasible in the LP. The last constraint means
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
t$\geq$
\end{flushleft}





\begin{flushleft}
sij ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
so the optimal choice of t is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
Sij .
\end{flushleft}





\begin{flushleft}
t = max
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
This shows that the LP is equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
maxi ( j=1 Sij )
\end{flushleft}


\begin{flushleft}
$-$S K A0 + x1 A1 + · · · + xk ak
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
S.
\end{flushleft}





\begin{flushleft}
Suppose x is given in this problem, and we optimize over S. The constraints in the LP
\end{flushleft}


\begin{flushleft}
state that
\end{flushleft}


\begin{flushleft}
$-$Sij $\leq$ A(x)ij $\leq$ Sij ,
\end{flushleft}





\begin{flushleft}
(where A(x) = A0 + x1 A1 + · · · + xk Ak ), and since the objective is monotone increasing
\end{flushleft}


\begin{flushleft}
in Sij , the optimal choice for Sij is
\end{flushleft}


\begin{flushleft}
Sij = |A(x)ij |.
\end{flushleft}


\begin{flushleft}
The problem is now reduced to the original problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
maxi=1,...,m
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
|A(x)ij |.
\end{flushleft}





\begin{flushleft}
4.15 Relaxation of Boolean LP. In a Boolean linear program, the variable x is constrained to
\end{flushleft}


\begin{flushleft}
have components equal to zero or one:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax b
\end{flushleft}


\begin{flushleft}
xi $\in$ \{0, 1\},
\end{flushleft}





(4.67)


\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
In general, such problems are very difficult to solve, even though the feasible set is finite
\end{flushleft}


\begin{flushleft}
(containing at most 2n points).
\end{flushleft}


\begin{flushleft}
In a general method called relaxation, the constraint that xi be zero or one is replaced
\end{flushleft}


\begin{flushleft}
with the linear inequalities 0 $\leq$ xi $\leq$ 1:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax b
\end{flushleft}


\begin{flushleft}
0 $\leq$ xi $\leq$ 1,
\end{flushleft}





(4.68)


\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
We refer to this problem as the LP relaxation of the Boolean LP (4.67). The LP relaxation
\end{flushleft}


\begin{flushleft}
is far easier to solve than the original Boolean LP.
\end{flushleft}


\begin{flushleft}
(a) Show that the optimal value of the LP relaxation (4.68) is a lower bound on the
\end{flushleft}


\begin{flushleft}
optimal value of the Boolean LP (4.67). What can you say about the Boolean LP
\end{flushleft}


\begin{flushleft}
if the LP relaxation is infeasible?
\end{flushleft}


\begin{flushleft}
(b) It sometimes happens that the LP relaxation has a solution with xi $\in$ \{0, 1\}. What
\end{flushleft}


\begin{flushleft}
can you say in this case?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The feasible set of the relaxation includes the feasible set of the Boolean LP. It
\end{flushleft}


\begin{flushleft}
follows that the Boolean LP is infeasible if the relaxation is infeasible, and that
\end{flushleft}


\begin{flushleft}
the optimal value of the relaxation is less than or equal to the optimal value of the
\end{flushleft}


\begin{flushleft}
Boolean LP.
\end{flushleft}


\begin{flushleft}
(b) The optimal solution of the relaxation is also optimal for the Boolean LP.
\end{flushleft}


\begin{flushleft}
4.16 Minimum fuel optimal control. We consider a linear dynamical system with state x(t) $\in$
\end{flushleft}


\begin{flushleft}
Rn , t = 0, . . . , N , and actuator or input signal u(t) $\in$ R, for t = 0, . . . , N $-$ 1. The
\end{flushleft}


\begin{flushleft}
dynamics of the system is given by the linear recurrence
\end{flushleft}


\begin{flushleft}
x(t + 1) = Ax(t) + bu(t),
\end{flushleft}





\begin{flushleft}
t = 0, . . . , N $-$ 1,
\end{flushleft}





\begin{flushleft}
where A $\in$ Rn×n and b $\in$ Rn are given. We assume that the initial state is zero, i.e.,
\end{flushleft}


\begin{flushleft}
x(0) = 0.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
The minimum fuel optimal control problem is to choose the inputs u(0), . . . , u(N $-$ 1) so
\end{flushleft}


\begin{flushleft}
as to minimize the total fuel consumed, which is given by
\end{flushleft}


\begin{flushleft}
F =
\end{flushleft}





\begin{flushleft}
N $-$1
\end{flushleft}





\begin{flushleft}
f (u(t)),
\end{flushleft}





\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
subject to the constraint that x(N ) = xdes , where N is the (given) time horizon, and
\end{flushleft}


\begin{flushleft}
xdes $\in$ Rn is the (given) desired final or target state. The function f : R $\rightarrow$ R is the fuel
\end{flushleft}


\begin{flushleft}
use map for the actuator, and gives the amount of fuel used as a function of the actuator
\end{flushleft}


\begin{flushleft}
signal amplitude. In this problem we use
\end{flushleft}


\begin{flushleft}
f (a) =
\end{flushleft}





\begin{flushleft}
|a|
\end{flushleft}


\begin{flushleft}
2|a| $-$ 1
\end{flushleft}





\begin{flushleft}
|a| $\leq$ 1
\end{flushleft}


\begin{flushleft}
|a| $>$ 1.
\end{flushleft}





\begin{flushleft}
This means that fuel use is proportional to the absolute value of the actuator signal, for
\end{flushleft}


\begin{flushleft}
actuator signals between $-$1 and 1; for larger actuator signals the marginal fuel efficiency
\end{flushleft}


\begin{flushleft}
is half.
\end{flushleft}


\begin{flushleft}
Formulate the minimum fuel optimal control problem as an LP.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
1T t
\end{flushleft}


\begin{flushleft}
subject to Hu = xdes
\end{flushleft}


\begin{flushleft}
$-$y u y
\end{flushleft}


\begin{flushleft}
t y
\end{flushleft}


\begin{flushleft}
t 2y $-$ 1
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
H=
\end{flushleft}





\begin{flushleft}
AN $-$1 b
\end{flushleft}





\begin{flushleft}
AN $-$2 b
\end{flushleft}





···





\begin{flushleft}
Ab
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}





.





\begin{flushleft}
4.17 Optimal activity levels. We consider the selection of n nonnegative activity levels, denoted
\end{flushleft}


\begin{flushleft}
x1 , . . . , xn . These activities consume m resources, which are limited. Activity j consumes
\end{flushleft}


\begin{flushleft}
Aij xj of resource i, where Aij are given. The total resource consumption is additive, so
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
the total of resource i consumed is ci =
\end{flushleft}


\begin{flushleft}
A x . (Ordinarily we have Aij $\geq$ 0, i.e.,
\end{flushleft}


\begin{flushleft}
j=1 ij j
\end{flushleft}


\begin{flushleft}
activity j consumes resource i. But we allow the possibility that Aij $<$ 0, which means
\end{flushleft}


\begin{flushleft}
that activity j actually generates resource i as a by-product.) Each resource consumption
\end{flushleft}


\begin{flushleft}
is limited: we must have ci $\leq$ cmax
\end{flushleft}


\begin{flushleft}
, where cmax
\end{flushleft}


\begin{flushleft}
are given. Each activity generates revenue,
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
which is a piecewise-linear concave function of the activity level:
\end{flushleft}


\begin{flushleft}
rj (xj ) =
\end{flushleft}





\begin{flushleft}
p j xj
\end{flushleft}


\begin{flushleft}
(xj $-$ qj )
\end{flushleft}


\begin{flushleft}
pj qj + pdisc
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
0 $\leq$ xj $\leq$ qj
\end{flushleft}


\begin{flushleft}
xj $\geq$ q j .
\end{flushleft}





\begin{flushleft}
is the
\end{flushleft}


\begin{flushleft}
Here pj $>$ 0 is the basic price, qj $>$ 0 is the quantity discount level, and pdisc
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}


\begin{flushleft}
$<$ pj .) The
\end{flushleft}


\begin{flushleft}
quantity discount price, for (the product of) activity j. (We have 0 $<$ pdisc
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
total revenue is the sum of the revenues associated with each activity, i.e.,
\end{flushleft}


\begin{flushleft}
r (xj ).
\end{flushleft}


\begin{flushleft}
j=1 j
\end{flushleft}


\begin{flushleft}
The goal is to choose activity levels that maximize the total revenue while respecting the
\end{flushleft}


\begin{flushleft}
resource limits. Show how to formulate this problem as an LP.
\end{flushleft}


\begin{flushleft}
Solution. The basic problem can be expressed as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
rj (xj )
\end{flushleft}


\begin{flushleft}
x 0
\end{flushleft}


\begin{flushleft}
Ax cmax .
\end{flushleft}





\begin{flushleft}
This is a convex optimization problem since the objective is concave and the constraints
\end{flushleft}


\begin{flushleft}
are a set of linear inequalities. To transform it to an equivalent LP, we first express the
\end{flushleft}


\begin{flushleft}
revenue functions as
\end{flushleft}


\begin{flushleft}
rj (xj ) = min\{pj xj , pj qj + pdisc
\end{flushleft}


\begin{flushleft}
(xj $-$ qj )\},
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
which holds since rj is concave. It follows that rj (xj ) $\geq$ uj if and only if
\end{flushleft}


\begin{flushleft}
pj qj + pdisc
\end{flushleft}


\begin{flushleft}
(xj $-$ qj ) $\geq$ uj .
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
p j xj $\geq$ u j ,
\end{flushleft}


\begin{flushleft}
We can form an LP as
\end{flushleft}


\begin{flushleft}
1T u
\end{flushleft}


\begin{flushleft}
x 0
\end{flushleft}


\begin{flushleft}
Ax cmax
\end{flushleft}


\begin{flushleft}
p j xj $\geq$ u j ,
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
pj qj + pdisc
\end{flushleft}


\begin{flushleft}
(xj $-$ qj ) $\geq$ uj ,
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
j = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
with variables x and u.
\end{flushleft}


\begin{flushleft}
To show that this LP is equivalent to the original problem, let us fix x. The last set of
\end{flushleft}


\begin{flushleft}
constraints in the LP ensure that ui $\leq$ ri (x), so we conclude that for every feasible x, u
\end{flushleft}


\begin{flushleft}
in the LP, the LP objective is less than or equal to the total revenue. On the other hand,
\end{flushleft}


\begin{flushleft}
we can always take ui = ri (x), in which case the two objectives are equal.
\end{flushleft}


\begin{flushleft}
4.18 Separating hyperplanes and spheres. Suppose you are given two sets of points in R n ,
\end{flushleft}


\begin{flushleft}
\{v 1 , v 2 , . . . , v K \} and \{w 1 , w2 , . . . , wL \}. Formulate the following two problems as LP feasibility problems.
\end{flushleft}


\begin{flushleft}
(a) Determine a hyperplane that separates the two sets, i.e., find a $\in$ Rn and b $\in$ R
\end{flushleft}


\begin{flushleft}
with a = 0 such that
\end{flushleft}


\begin{flushleft}
aT v i $\leq$ b,
\end{flushleft}





\begin{flushleft}
aT wi $\geq$ b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , K,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , L.
\end{flushleft}





\begin{flushleft}
Note that we require a = 0, so you have to make sure that your formulation excludes
\end{flushleft}


\begin{flushleft}
the trivial solution a = 0, b = 0. You can assume that
\end{flushleft}


\begin{flushleft}
v1
\end{flushleft}


1





\begin{flushleft}
rank
\end{flushleft}





\begin{flushleft}
v2
\end{flushleft}


1





···


···





\begin{flushleft}
vK
\end{flushleft}


1





\begin{flushleft}
w1
\end{flushleft}


1





\begin{flushleft}
w2
\end{flushleft}


1





···


···





\begin{flushleft}
wL
\end{flushleft}


1





\begin{flushleft}
=n+1
\end{flushleft}





\begin{flushleft}
(i.e., the affine hull of the K + L points has dimension n).
\end{flushleft}


\begin{flushleft}
(b) Determine a sphere separating the two sets of points, i.e., find xc $\in$ Rn and R $\geq$ 0
\end{flushleft}


\begin{flushleft}
such that
\end{flushleft}


\begin{flushleft}
v i $-$ xc
\end{flushleft}





2





\begin{flushleft}
$\leq$ R,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , K,
\end{flushleft}





\begin{flushleft}
w i $-$ xc
\end{flushleft}





2





\begin{flushleft}
$\geq$ R,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , L.
\end{flushleft}





\begin{flushleft}
(Here xc is the center of the sphere; R is its radius.)
\end{flushleft}


\begin{flushleft}
(See chapter 8 for more on separating hyperplanes, separating spheres, and related topics.)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The conditions
\end{flushleft}


\begin{flushleft}
aT v i $\leq$ b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , K,
\end{flushleft}





\begin{flushleft}
aT wi $\geq$ b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , L
\end{flushleft}





\begin{flushleft}
form a set of K + L linear inequalities in the variables a, b, which we can write in
\end{flushleft}


\begin{flushleft}
matrix form as
\end{flushleft}


\begin{flushleft}
Bx 0
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
B=
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
$-$(v 1 )T
\end{flushleft}


..


.


\begin{flushleft}
$-$(v K )T
\end{flushleft}


\begin{flushleft}
$-$(w1 )T
\end{flushleft}


..


.


\begin{flushleft}
$-$(wL )T
\end{flushleft}





1


..


.


1


$-$1


..


.


$-$1





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
(K+L)×(n+1)
\end{flushleft}


,


\begin{flushleft}
$\in$R
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
x=
\end{flushleft}





\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}





.





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We are interested in nonzero solutions of Bx 0.
\end{flushleft}


\begin{flushleft}
The rank assumption implies that rank B = n + 1. Therefore, its nullspace contains
\end{flushleft}


\begin{flushleft}
only the zero vector, i.e., x = 0 implies Bx = 0. We can force x to be nonzero by
\end{flushleft}


\begin{flushleft}
adding a constraint 1T Bx = 1. (On the right hand side we could choose any other
\end{flushleft}


\begin{flushleft}
positive constraint instead of 1.) This forces at least one component of Bx to be
\end{flushleft}


\begin{flushleft}
positive. In other words we can find nonzero solution to Bx 0 by solving the LP
\end{flushleft}


\begin{flushleft}
feasibility problem
\end{flushleft}


\begin{flushleft}
Bx 0,
\end{flushleft}


\begin{flushleft}
1T Bx = 1.
\end{flushleft}


\begin{flushleft}
(b) We begin by writing the inequalities as
\end{flushleft}


\begin{flushleft}
vi
\end{flushleft}


\begin{flushleft}
wi
\end{flushleft}





2


\begin{flushleft}
i T
\end{flushleft}


\begin{flushleft}
2 $-$ 2(v ) xc +
\end{flushleft}


\begin{flushleft}
i T
\end{flushleft}


2


\begin{flushleft}
2 $-$ 2(w ) xc +
\end{flushleft}





\begin{flushleft}
xc 22 $\leq$ R2 , i = 1, . . . , K,
\end{flushleft}


\begin{flushleft}
xc 22 $\geq$ R2 , i = 1, . . . , L.
\end{flushleft}





\begin{flushleft}
These inequalities are not linear in xc and R. However, if we use as variables xc and
\end{flushleft}


\begin{flushleft}
$\gamma$ = R2 $-$ xc 22 , then they reduce to
\end{flushleft}


\begin{flushleft}
vi
\end{flushleft}





2


2





\begin{flushleft}
$-$ 2(v i )T xc $\leq$ $\gamma$,
\end{flushleft}





\begin{flushleft}
wi
\end{flushleft}





\begin{flushleft}
i = 1, . . . , K,
\end{flushleft}





2


2





\begin{flushleft}
$-$ 2(w i )T xc $\geq$ $\gamma$,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , L,
\end{flushleft}





\begin{flushleft}
which is a set of linear inequalities in xc $\in$ Rn and $\gamma$ $\in$ R. We can solve this
\end{flushleft}


\begin{flushleft}
feasibility problem for xc and $\gamma$, and compute R as
\end{flushleft}


\begin{flushleft}
R=
\end{flushleft}


2





\begin{flushleft}
We can be certain that $\gamma$ + xc
\end{flushleft}


\begin{flushleft}
$\gamma$ + xc
\end{flushleft}





2


2





\begin{flushleft}
$\geq$ vi
\end{flushleft}





2


2





\begin{flushleft}
$\gamma$ + xc
\end{flushleft}





2


2.





\begin{flushleft}
$\geq$ 0: If xc and $\gamma$ are feasible, then
\end{flushleft}





\begin{flushleft}
$-$ 2(v i )T xc + xc
\end{flushleft}





2


2





\begin{flushleft}
= v i $-$ xc
\end{flushleft}





2


2





$\geq$ 0.





\begin{flushleft}
4.19 Consider the problem
\end{flushleft}


\begin{flushleft}
Ax $-$ b 1 /(cT x + d)
\end{flushleft}


\begin{flushleft}
x $\infty$ $\leq$ 1,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
where A $\in$ Rm×n , b $\in$ Rm , c $\in$ Rn , and d $\in$ R. We assume that d $>$ c 1 , which implies
\end{flushleft}


\begin{flushleft}
that cT x + d $>$ 0 for all feasible x.
\end{flushleft}


\begin{flushleft}
(a) Show that this is a quasiconvex optimization problem.
\end{flushleft}


\begin{flushleft}
(b) Show that it is equivalent to the convex optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
Ay $-$ bt 1
\end{flushleft}


\begin{flushleft}
y $\infty$$\leq$t
\end{flushleft}


\begin{flushleft}
cT y + dt = 1,
\end{flushleft}





\begin{flushleft}
with variables y $\in$ Rn , t $\in$ R.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) f0 (x) $\leq$ $\alpha$ if and only if
\end{flushleft}


\begin{flushleft}
Ax $-$ b
\end{flushleft}





1





\begin{flushleft}
$-$ $\alpha$(cT x + d) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
which is a convex constraint.
\end{flushleft}


\begin{flushleft}
(b) Suppose x
\end{flushleft}





$\infty$





\begin{flushleft}
$\leq$ 1. We have cT x + d $>$ 0, because d $>$ c 1 . Define
\end{flushleft}


\begin{flushleft}
y = x/(cT x + d),
\end{flushleft}





\begin{flushleft}
t = 1/(cT x + d).
\end{flushleft}





\begin{flushleft}
Then y and t are feasible in the convex problem with objective value
\end{flushleft}


\begin{flushleft}
Ay $-$ bt
\end{flushleft}





1





\begin{flushleft}
= Ax $-$ b 1 /(cT x + d).
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
Conversely, suppose y, t are feasible for the convex problem. We must have t $>$ 0,
\end{flushleft}


\begin{flushleft}
since t = 0 would imply y = 0, which contradicts cT y + dt = 1. Define
\end{flushleft}


\begin{flushleft}
x = y/t.
\end{flushleft}


\begin{flushleft}
Then x
\end{flushleft}





$\infty$





\begin{flushleft}
$\leq$ 1, and cT x + d = 1/t, and hence
\end{flushleft}


\begin{flushleft}
Ax $-$ b 1 /(cT x + d) = Ay $-$ bt 1 .
\end{flushleft}





\begin{flushleft}
4.20 Power assignment in a wireless communication system. We consider n transmitters with
\end{flushleft}


\begin{flushleft}
powers p1 , . . . , pn $\geq$ 0, transmitting to n receivers. These powers are the optimization
\end{flushleft}


\begin{flushleft}
variables in the problem. We let G $\in$ Rn×n denote the matrix of path gains from the
\end{flushleft}


\begin{flushleft}
transmitters to the receivers; Gij $\geq$ 0 is the path gain from transmitter j to receiver i.
\end{flushleft}


\begin{flushleft}
The signal power at receiver i is then Si = Gii pi , and the interference power at receiver i
\end{flushleft}


\begin{flushleft}
is Ii = k=i Gik pk . The signal to interference plus noise ratio, denoted SINR, at receiver
\end{flushleft}


\begin{flushleft}
i, is given by Si /(Ii + $\sigma$i ), where $\sigma$i $>$ 0 is the (self-) noise power in receiver i. The
\end{flushleft}


\begin{flushleft}
objective in the problem is to maximize the minimum SINR ratio, over all receivers, i.e.,
\end{flushleft}


\begin{flushleft}
to maximize
\end{flushleft}


\begin{flushleft}
Si
\end{flushleft}


\begin{flushleft}
min
\end{flushleft}


.


\begin{flushleft}
i=1,...,n Ii + $\sigma$i
\end{flushleft}


\begin{flushleft}
There are a number of constraints on the powers that must be satisfied, in addition to the
\end{flushleft}


\begin{flushleft}
obvious one pi $\geq$ 0. The first is a maximum allowable power for each transmitter, i.e.,
\end{flushleft}


\begin{flushleft}
pi $\leq$ Pimax , where Pimax $>$ 0 is given. In addition, the transmitters are partitioned into
\end{flushleft}


\begin{flushleft}
groups, with each group sharing the same power supply, so there is a total power constraint
\end{flushleft}


\begin{flushleft}
for each group of transmitter powers. More precisely, we have subsets K1 , . . . , Km of
\end{flushleft}


\begin{flushleft}
\{1, . . . , n\} with K1 $\cup$ · · · $\cup$ Km = \{1, . . . , n\}, and Kj $\cap$ Kl = 0 if j = l. For each group Kl ,
\end{flushleft}


\begin{flushleft}
the total associated transmitter power cannot exceed Plgp $>$ 0:
\end{flushleft}





\begin{flushleft}
k$\in$Kl
\end{flushleft}





\begin{flushleft}
pk $\leq$ Plgp ,
\end{flushleft}





\begin{flushleft}
l = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Finally, we have a limit Pkrc $>$ 0 on the total received power at each receiver:
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
Gik pk $\leq$ Pirc ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
(This constraint reflects the fact that the receivers will saturate if the total received power
\end{flushleft}


\begin{flushleft}
is too large.)
\end{flushleft}


\begin{flushleft}
Formulate the SINR maximization problem as a generalized linear-fractional program.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
maxi ( k=i Gik pk + $\sigma$i )/(Gii pi )
\end{flushleft}


\begin{flushleft}
0 $\leq$ pi $\leq$ Pimax
\end{flushleft}


\begin{flushleft}
p $\leq$ Plgp
\end{flushleft}


\begin{flushleft}
k$\in$Kl k
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
Gik pk $\leq$ Pirc
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
Quadratic optimization problems
\end{flushleft}


\begin{flushleft}
4.21 Some simple QCQPs. Give an explicit solution of each of the following QCQPs.
\end{flushleft}


\begin{flushleft}
(a) Minimizing a linear function over an ellipsoid centered at the origin.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
xT Ax $\leq$ 1,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
where A $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ and c = 0. What is the solution if the problem is not convex
\end{flushleft}


\begin{flushleft}
(A $\in$ Sn
\end{flushleft}


+ )?


\begin{flushleft}
Solution. If A 0, the solution is
\end{flushleft}


\begin{flushleft}
x = $-$$\surd$
\end{flushleft}





1


\begin{flushleft}
cT A$-$1 c
\end{flushleft}





\begin{flushleft}
A$-$1 c,
\end{flushleft}





\begin{flushleft}
p = $-$ A$-$1/2 c
\end{flushleft}





2





$\surd$


\begin{flushleft}
= $-$ cT A$-$1 c.
\end{flushleft}





\begin{flushleft}
This can be shown as follows. We make a change of variables y = A1/2 x, and write
\end{flushleft}


\begin{flushleft}
c˜ = A$-$1/2 c. With this new variable the optimization problem becomes
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
c˜T y
\end{flushleft}


\begin{flushleft}
y T y $\leq$ 1,
\end{flushleft}





\begin{flushleft}
i.e., we minimize a linear function over the unit ball. The answer is y = $-$˜
\end{flushleft}


\begin{flushleft}
c/ c˜ 2 .
\end{flushleft}


\begin{flushleft}
In the general case, we can make a change of variables based on the eigenvalue
\end{flushleft}


\begin{flushleft}
decomposition
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\lambda$i qi qiT .
\end{flushleft}





\begin{flushleft}
A = Q diag($\lambda$)QT =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
We define y = Qx, b = Qc, and express the problem as
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
bi y i
\end{flushleft}


\begin{flushleft}
$\lambda$i yi2 $\leq$ 1.
\end{flushleft}





\begin{flushleft}
If $\lambda$i $>$ 0 for all i, the problem reduces to the case we already discussed. Otherwise,
\end{flushleft}


\begin{flushleft}
we can distinguish several cases.
\end{flushleft}


\begin{flushleft}
$\bullet$ $\lambda$n $<$ 0. The problem is unbounded below. By letting yn $\rightarrow$ $\pm$$\infty$, we can make
\end{flushleft}


\begin{flushleft}
any point feasible.
\end{flushleft}


\begin{flushleft}
$\bullet$ $\lambda$n = 0. If for some i, bi = 0 and $\lambda$i = 0, the problem is unbounded below.
\end{flushleft}


\begin{flushleft}
$\bullet$ $\lambda$n = 0, and bi = 0 for all i with $\lambda$i = 0. In this case we can reduce the problem
\end{flushleft}


\begin{flushleft}
to a smaller one with all $\lambda$i $>$ 0.
\end{flushleft}


\begin{flushleft}
(b) Minimizing a linear function over an ellipsoid.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
(x $-$ xc )T A(x $-$ xc ) $\leq$ 1,
\end{flushleft}





\begin{flushleft}
where A $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ and c = 0.
\end{flushleft}


\begin{flushleft}
Solution. We make a change of variables
\end{flushleft}


\begin{flushleft}
y = A1/2 (x $-$ xc ),
\end{flushleft}





\begin{flushleft}
x = A$-$1/2 y + xc ,
\end{flushleft}





\begin{flushleft}
and consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT A$-$1/2 y + cT xc
\end{flushleft}


\begin{flushleft}
y T y $\leq$ 1.
\end{flushleft}





\begin{flushleft}
The solution is
\end{flushleft}


\begin{flushleft}
y = $-$(1/ A$-$1/2 c 2 )A$-$1/2 c,
\end{flushleft}





\begin{flushleft}
x = xc $-$ (1/ A$-$1/2 c 2 )A$-$1 c.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(c) Minimizing a quadratic form over an ellipsoid centered at the origin.
\end{flushleft}


\begin{flushleft}
xT Bx
\end{flushleft}


\begin{flushleft}
xT Ax $\leq$ 1,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
where A $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ and B $\in$ S+ . Also consider the nonconvex extension with B $\in$ S+ .
\end{flushleft}


\begin{flushleft}
(See §B.1.)
\end{flushleft}


\begin{flushleft}
Solution. If B
\end{flushleft}


\begin{flushleft}
0, then the optimal value is obviously zero (since xT Bx $\geq$ 0 for
\end{flushleft}


\begin{flushleft}
all x, with equality if x = 0).
\end{flushleft}


\begin{flushleft}
In the general case, we use the following fact from linear algebra. The smallest
\end{flushleft}


\begin{flushleft}
eigenvalue of B $\in$ Sn , can be characterized as
\end{flushleft}





\begin{flushleft}
$\lambda$min (B) = inf xT Bx.
\end{flushleft}


\begin{flushleft}
xT x=1
\end{flushleft}





\begin{flushleft}
To solve the optimization problem
\end{flushleft}


\begin{flushleft}
xT Bx
\end{flushleft}


\begin{flushleft}
xT Ax $\leq$ 1,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
with A 0, we make a change of variables y = A1/2 x. This is possible since A
\end{flushleft}


\begin{flushleft}
so A1/2 is defined and nonsingular. In the new variables the problem becomes
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





0,





\begin{flushleft}
y T A$-$1/2 BA$-$1/2 y
\end{flushleft}


\begin{flushleft}
y T y $\leq$ 1.
\end{flushleft}





\begin{flushleft}
If the constraint y T y $\leq$ 1 is active at the optimum (y T y = 1), then the optimal
\end{flushleft}


\begin{flushleft}
value is
\end{flushleft}


\begin{flushleft}
$\lambda$min (A$-$1/2 BA$-$1/2 ),
\end{flushleft}


\begin{flushleft}
by the result mentioned above. If y T y $<$ 1 at the optimum, then it must be at a
\end{flushleft}


\begin{flushleft}
point where the gradient of the objective function vanishes, i.e., By = 0. In that
\end{flushleft}


\begin{flushleft}
case the optimal value is zero.
\end{flushleft}


\begin{flushleft}
To summarize, the optimal value is
\end{flushleft}


\begin{flushleft}
p =
\end{flushleft}





\begin{flushleft}
$\lambda$min (A$-$1/2 BA$-$1/2 )
\end{flushleft}


0





\begin{flushleft}
$\lambda$min (A$-$1/2 BA$-$1/2 ) $\leq$ 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
In the first case any (normalized) eigenvector of A$-$1/2 BA$-$1/2 corresponding to the
\end{flushleft}


\begin{flushleft}
smallest eigenvalue is an optimal y. In the second case y = 0 is optimal.
\end{flushleft}


\begin{flushleft}
4.22 Consider the QCQP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(1/2)xT P x + q T x + r
\end{flushleft}


\begin{flushleft}
xT x $\leq$ 1,
\end{flushleft}





$-$1


\begin{flushleft}
¯ and $\lambda$
\end{flushleft}


\begin{flushleft}
¯ is the largest
\end{flushleft}


\begin{flushleft}
with P $\in$ Sn
\end{flushleft}


\begin{flushleft}
q where $\lambda$ = max\{0, $\lambda$\}
\end{flushleft}


\begin{flushleft}
++ . Show that x = $-$(P + $\lambda$I)
\end{flushleft}


\begin{flushleft}
solution of the nonlinear equation
\end{flushleft}





\begin{flushleft}
q T (P + $\lambda$I)$-$2 q = 1.
\end{flushleft}


\begin{flushleft}
Solution. x is optimal if and only if
\end{flushleft}


\begin{flushleft}
xT x $<$ 1,
\end{flushleft}


\begin{flushleft}
or
\end{flushleft}





\begin{flushleft}
xT x = 1,
\end{flushleft}





\begin{flushleft}
Px + q = 0
\end{flushleft}


\begin{flushleft}
P x + q = $-$$\lambda$x
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
for some $\lambda$ $\geq$ 0. (Geometrically, either x is in the interior of the ball and the gradient
\end{flushleft}


\begin{flushleft}
vanishes, or x is on the boundary, and the negative gradient is parallel to the outward
\end{flushleft}


\begin{flushleft}
pointing normal.)
\end{flushleft}


\begin{flushleft}
The algorithm goes as follows. First solve P x = $-$q. If the solution has norm less than
\end{flushleft}


\begin{flushleft}
or equal to one ( P $-$1 q 2 $\leq$ 1), it is optimal. Otherwise, from the optimality conditions,
\end{flushleft}


\begin{flushleft}
x must satisfy x 2 = 1 and (P + $\lambda$)x = $-$q for some $\lambda$ $\geq$ 0. Define
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
f ($\lambda$) = (P + $\lambda$)
\end{flushleft}





$-$1





\begin{flushleft}
q
\end{flushleft}





2


2





=


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
qi2
\end{flushleft}


,


\begin{flushleft}
($\lambda$ + $\lambda$i )2
\end{flushleft}





\begin{flushleft}
where $\lambda$i $>$ 0 are the eigenvalues of P . (Note that P + $\lambda$I
\end{flushleft}


\begin{flushleft}
0 for all $\lambda$ $\geq$ 0 because
\end{flushleft}


\begin{flushleft}
P
\end{flushleft}


\begin{flushleft}
0.) We have f (0) = P $-$1 q 22 $>$ 1. Also f monotonically decreases to zero as $\lambda$ $\rightarrow$ $\infty$.
\end{flushleft}


\begin{flushleft}
¯ Solve
\end{flushleft}


\begin{flushleft}
Therefore the nonlinear equation f ($\lambda$) = 1 has exactly one nonnegative solution $\lambda$.
\end{flushleft}


\begin{flushleft}
¯ The optimal solution is x = $-$(P + $\lambda$I)
\end{flushleft}


\begin{flushleft}
¯ $-$1 q.
\end{flushleft}


\begin{flushleft}
for $\lambda$.
\end{flushleft}


4.23





\begin{flushleft}
4 -norm
\end{flushleft}





\begin{flushleft}
approximation via QCQP. Formulate the
\end{flushleft}


\begin{flushleft}
Ax $-$ b
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(aTi x
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





=(





4





\begin{flushleft}
4 -norm
\end{flushleft}





\begin{flushleft}
approximation problem
\end{flushleft}





\begin{flushleft}
$-$ bi )4 )1/4
\end{flushleft}





\begin{flushleft}
as a QCQP. The matrix A $\in$ Rm×n (with rows aTi ) and the vector b $\in$ Rm are given.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
z2
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
subject to ai x $-$ bi = yi , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
yi2 $\leq$ zi , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
4.24 Complex
\end{flushleft}





1 -,





2-





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
$\infty$ -norm
\end{flushleft}





\begin{flushleft}
approximation. Consider the problem
\end{flushleft}


\begin{flushleft}
Ax $-$ b
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
p,
\end{flushleft}





\begin{flushleft}
where A $\in$ Cm×n , b $\in$ Cm , and the variable is x $\in$ Cn . The complex
\end{flushleft}


\begin{flushleft}
by
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
is defined
\end{flushleft}





\begin{flushleft}
1/p
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
p -norm
\end{flushleft}





\begin{flushleft}
|yi |p
\end{flushleft}





\begin{flushleft}
for p $\geq$ 1, and y $\infty$ = maxi=1,...,m |yi |. For p = 1, 2, and $\infty$, express the complex
\end{flushleft}


\begin{flushleft}
approximation problem as a QCQP or SOCP with real variables and data.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}
p -norm
\end{flushleft}





\begin{flushleft}
(a) Minimizing Ax $-$ b 2 is equivalent to minimizing its square. So, let us expand
\end{flushleft}


\begin{flushleft}
Ax $-$ b 22 around the real and complex parts of Ax $-$ b:
\end{flushleft}


\begin{flushleft}
Ax $-$ b
\end{flushleft}





2


2





2


2





2


2





+





\begin{flushleft}
(Ax $-$ b)
\end{flushleft}





\begin{flushleft}
A x$-$ A x$-$ b
\end{flushleft}





=





\begin{flushleft}
If we define z T = [ xT
\end{flushleft}


\begin{flushleft}
Ax $-$ b
\end{flushleft}





\begin{flushleft}
(Ax $-$ b)
\end{flushleft}





=





2


2





+





2


2





\begin{flushleft}
A x + A x $-$ b 22 .
\end{flushleft}





\begin{flushleft}
xT ] as requested, then this becomes
\end{flushleft}


=


=





\begin{flushleft}
[ A $-$ A]z $-$ b
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
$-$ A
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





2





\begin{flushleft}
z$-$
\end{flushleft}





\begin{flushleft}
+ [ A
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
A]z $-$ b
\end{flushleft}





2





.


2





\begin{flushleft}
The values of F and g can be extracted from the above expression.
\end{flushleft}





2





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(b) First, let's write out the optimization problem term-by-term:
\end{flushleft}


\begin{flushleft}
Ax $-$ b
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





$\infty$





\begin{flushleft}
is equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
|aTi x $-$ b| $<$ t
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
where aT1 , . . . , aTm are the rows of A. We have introduced a new optimization variable
\end{flushleft}


\begin{flushleft}
t.
\end{flushleft}


\begin{flushleft}
Each term |aTi x $-$ b| must now be written in terms of real variables (we'll use the
\end{flushleft}


\begin{flushleft}
same z as before):
\end{flushleft}


\begin{flushleft}
|aTi x $-$ b|2
\end{flushleft}





=





\begin{flushleft}
( aTi x $-$ aTi x $-$ b)2 + ( aTi x + aTi x $-$ b)2
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}





=





\begin{flushleft}
$-$ aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}





2





\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
z$-$
\end{flushleft}





.


2





\begin{flushleft}
So now we have reduced the problem to the real minimization,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
aTi $-$ aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
z$-$
\end{flushleft}





\begin{flushleft}
$<$t
\end{flushleft}


2





\begin{flushleft}
This is a minimization over a second-order cone. It can be converted into a QCQP
\end{flushleft}


\begin{flushleft}
by squaring both sides of the constraint and defining $\lambda$ = t2 :
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$\lambda$
\end{flushleft}


\begin{flushleft}
aTi $-$ aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(c) The
\end{flushleft}





\begin{flushleft}
1 -norm
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
z$-$
\end{flushleft}





2





\begin{flushleft}
$<$$\lambda$
\end{flushleft}


2





\begin{flushleft}
minimization problem is to minimize Ax $-$ b 1 , i.e.,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
|aTi x $-$ b|
\end{flushleft}





\begin{flushleft}
Let us introduce new variables t1 , . . . , tm , and rewrite the minimization as follows:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
|ai x $-$ b|
\end{flushleft}





\begin{flushleft}
$<$ ti ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
The conversion to second-order constraints is similar to part (b):
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}





\begin{flushleft}
$-$ aTi
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}





\begin{flushleft}
z$-$
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
$<$ ti ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





2





\begin{flushleft}
4.25 Linear separation of two sets of ellipsoids. Suppose we are given K + L ellipsoids
\end{flushleft}


\begin{flushleft}
Ei = \{Pi u + qi | u
\end{flushleft}





2





$\leq$ 1\},





\begin{flushleft}
i = 1, . . . , K + L,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
where Pi $\in$ S . We are interested in finding a hyperplane that strictly separates E1 , . . . ,
\end{flushleft}


\begin{flushleft}
EK from EK+1 , . . . , EK+L , i.e., we want to compute a $\in$ Rn , b $\in$ R such that
\end{flushleft}


\begin{flushleft}
aT x + b $>$ 0 for x $\in$ E1 $\cup$ · · · $\cup$ EK ,
\end{flushleft}





\begin{flushleft}
aT x + b $<$ 0 for x $\in$ EK+1 $\cup$ · · · $\cup$ EK+L ,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
or prove that no such hyperplane exists. Express this problem as an SOCP feasibility
\end{flushleft}


\begin{flushleft}
problem.
\end{flushleft}


\begin{flushleft}
Solution. We first note that the problem is homogeneous in a and b, so we can replace
\end{flushleft}


\begin{flushleft}
the strict inequalities aT x + b $>$ 0 and aT x + b $<$ 0 with aT x + b $\geq$ 1 and aT x + b $\leq$ $-$1,
\end{flushleft}


\begin{flushleft}
respectively.
\end{flushleft}


\begin{flushleft}
The variables a and b must satisfy
\end{flushleft}


\begin{flushleft}
inf (aT Pi u + aT qi ) $\geq$ 1,
\end{flushleft}





\begin{flushleft}
1, . . . , L
\end{flushleft}





\begin{flushleft}
u 2 $\leq$1
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
sup (aT Pi u + aT qi ) $\leq$ $-$1,
\end{flushleft}





\begin{flushleft}
i = K + 1, . . . , K + L.
\end{flushleft}





\begin{flushleft}
u 2 $\leq$1
\end{flushleft}





\begin{flushleft}
The lefthand sides can be expressed as
\end{flushleft}


\begin{flushleft}
inf (aT Pi u+aT qi ) = $-$ PiT a 2 +aT qi +b,
\end{flushleft}





\begin{flushleft}
sup (aT Pi u+aT qi ) = PiT a 2 +aT qi +b.
\end{flushleft}





\begin{flushleft}
u 2 $\leq$1
\end{flushleft}





\begin{flushleft}
u 2 $\leq$1
\end{flushleft}





\begin{flushleft}
We therefore obtain a set of second-order cone constraints in a, b:
\end{flushleft}


\begin{flushleft}
$-$ PiT a 2 + aT qi + b $\geq$ 1,
\end{flushleft}


\begin{flushleft}
PiT a 2 + aT qi + b $\leq$ $-$1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , L
\end{flushleft}


\begin{flushleft}
i = K + 1, . . . , K + L.
\end{flushleft}





\begin{flushleft}
4.26 Hyperbolic constraints as SOC constraints. Verify that x $\in$ Rn , y, z $\in$ R satisfy
\end{flushleft}


\begin{flushleft}
xT x $\leq$ yz,
\end{flushleft}





\begin{flushleft}
y $\geq$ 0,
\end{flushleft}





\begin{flushleft}
z$\geq$0
\end{flushleft}





\begin{flushleft}
if and only if
\end{flushleft}


\begin{flushleft}
2x
\end{flushleft}


\begin{flushleft}
y$-$z
\end{flushleft}





2





\begin{flushleft}
$\leq$ y + z,
\end{flushleft}





\begin{flushleft}
y $\geq$ 0,
\end{flushleft}





\begin{flushleft}
z $\geq$ 0.
\end{flushleft}





\begin{flushleft}
Use this observation to cast the following problems as SOCPs.
\end{flushleft}


\begin{flushleft}
(a) Maximizing harmonic mean.
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
with domain \{x | Ax
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
1/(aTi x $-$ bi )
\end{flushleft}





$-$1





,





\begin{flushleft}
b\}, where aTi is the ith row of A.
\end{flushleft}





\begin{flushleft}
(b) Maximizing geometric mean.
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
with domain \{x | Ax
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(aTi x
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$ bi )
\end{flushleft}





\begin{flushleft}
1/m
\end{flushleft}





,





\begin{flushleft}
b\}, where aTi is the ith row of A.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The problem is equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T t
\end{flushleft}


\begin{flushleft}
ti (aTi x + bi ) $\geq$ 1,
\end{flushleft}


\begin{flushleft}
t 0.
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
Writing the hyperbolic constraints as SOC constraints yields an SOCP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T t
\end{flushleft}


2


\begin{flushleft}
$\leq$ aTi x + bi + ti ,
\end{flushleft}


\begin{flushleft}
aTi x + bi $-$ ti
\end{flushleft}


2


\begin{flushleft}
ti $\geq$ 0, aTi x + bi $\geq$ 0, i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(b) We can assume without loss of generality that m = 2K for some positive integer K.
\end{flushleft}


\begin{flushleft}
(If not, define ai = 0 and bi = $-$1 for i = m + 1, . . . , 2K , where 2K is the smallest
\end{flushleft}


\begin{flushleft}
power of two greater than m.)
\end{flushleft}


\begin{flushleft}
Let us first take m = 4 (K = 2) as an example. The problem is equivalent to
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
y 1 y2 y3 y4
\end{flushleft}


\begin{flushleft}
y = Ax $-$ b
\end{flushleft}


\begin{flushleft}
y 0,
\end{flushleft}





\begin{flushleft}
which we can write as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
and also as
\end{flushleft}





\begin{flushleft}
t 1 t2
\end{flushleft}


\begin{flushleft}
y = Ax $-$ b
\end{flushleft}


\begin{flushleft}
y1 y2 $\geq$ t21
\end{flushleft}


\begin{flushleft}
y3 y4 $\geq$ t22
\end{flushleft}


\begin{flushleft}
y 0, t1 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t2 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
y = Ax $-$ b
\end{flushleft}


\begin{flushleft}
y1 y2 $\geq$ t21
\end{flushleft}


\begin{flushleft}
y3 y4 $\geq$ t22
\end{flushleft}


\begin{flushleft}
t1 t2 $\geq$ t 2
\end{flushleft}


\begin{flushleft}
y 0, t1 , t2 , t $\geq$ 0.
\end{flushleft}





\begin{flushleft}
Expressing the three hyperbolic constraints
\end{flushleft}


\begin{flushleft}
y1 y2 $\geq$ t21 ,
\end{flushleft}





\begin{flushleft}
y3 y4 $\geq$ t22 ,
\end{flushleft}





\begin{flushleft}
t1 t2 $\geq$ t 2
\end{flushleft}





\begin{flushleft}
as SOC constraints yields an SOCP:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$-$t
\end{flushleft}





\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
2t1
\end{flushleft}


\begin{flushleft}
y1 $-$ y 2
\end{flushleft}





2





\begin{flushleft}
2t2
\end{flushleft}


\begin{flushleft}
y3 $-$ y 4
\end{flushleft}





2





\begin{flushleft}
2t
\end{flushleft}


\begin{flushleft}
t1 $-$ t 2
\end{flushleft}





2





\begin{flushleft}
$\leq$ y1 + y2 ,
\end{flushleft}





\begin{flushleft}
y1 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
y2 $\geq$ 0
\end{flushleft}





\begin{flushleft}
$\leq$ y3 + y4 ,
\end{flushleft}





\begin{flushleft}
y3 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
y4 $\geq$ 0
\end{flushleft}





\begin{flushleft}
$\leq$ t1 + t2 ,
\end{flushleft}





\begin{flushleft}
t1 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
t2 $\geq$ 0
\end{flushleft}





\begin{flushleft}
y = Ax $-$ b.
\end{flushleft}


\begin{flushleft}
We can express the problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
y00
\end{flushleft}


\begin{flushleft}
yK$-$1,j$-$1 = aTj x $-$ bj , j = 1, . . . , m
\end{flushleft}


2


\begin{flushleft}
$\leq$ yi+1,2k yi+1,2k +1 , i = 0, . . . , K $-$ 2,
\end{flushleft}


\begin{flushleft}
yik
\end{flushleft}


\begin{flushleft}
Ax b,
\end{flushleft}





\begin{flushleft}
k = 0, . . . 2i $-$ 1
\end{flushleft}





\begin{flushleft}
where we have introduced auxiliary variables yij for i = 0, . . . , K$-$1, j = 0, . . . , 2i $-$1.
\end{flushleft}


\begin{flushleft}
Expressing the hyperbolic constraints as SOC constraints yields an SOCP.
\end{flushleft}


\begin{flushleft}
The equivalence can be proved by recursively expanding the objective function:
\end{flushleft}


\begin{flushleft}
y00
\end{flushleft}





$\leq$


$\leq$





\begin{flushleft}
y10 y11
\end{flushleft}


\begin{flushleft}
(y20 y21 ) (y22 y23 )
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


$\leq$


$\leq$


=





\begin{flushleft}
(y30 y31 )(y32 y33 )(y34 y35 )(y36 y37 )
\end{flushleft}


···


\begin{flushleft}
yK$-$1,0 yK$-$1,1 · · · yK$-$1,2K $-$1
\end{flushleft}


\begin{flushleft}
(aT1 x $-$ b1 ) · · · (aTm x $-$ bm ).
\end{flushleft}





\begin{flushleft}
4.27 Matrix fractional minimization via SOCP. Express the following problem as an SOCP:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(Ax + b)T (I + B diag(x)B T )$-$1 (Ax + b)
\end{flushleft}


\begin{flushleft}
x 0,
\end{flushleft}





\begin{flushleft}
with A $\in$ Rm×n , b $\in$ Rm , B $\in$ Rm×n . The variable is x $\in$ Rn .
\end{flushleft}


\begin{flushleft}
Hint. First show that the problem is equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
v T v + wT diag(x)$-$1 w
\end{flushleft}


\begin{flushleft}
v + Bw = Ax + b
\end{flushleft}


\begin{flushleft}
x 0,
\end{flushleft}





\begin{flushleft}
with variables v $\in$ Rm , w, x $\in$ Rn . (If xi = 0 we interpret wi2 /xi as zero if wi = 0 and as
\end{flushleft}


\begin{flushleft}
$\infty$ otherwise.) Then use the results of exercise 4.26.
\end{flushleft}


\begin{flushleft}
Solution. To show the equivalence with the problem in the hint, we assume x
\end{flushleft}


\begin{flushleft}
0 is
\end{flushleft}


\begin{flushleft}
fixed, and optimize over v and w. This is a quadratic problem with equality constraints.
\end{flushleft}


\begin{flushleft}
The optimality conditions are
\end{flushleft}


\begin{flushleft}
v = $\nu$,
\end{flushleft}





\begin{flushleft}
w = diag(x)B T $\nu$
\end{flushleft}





\begin{flushleft}
for some $\nu$. Substituting in the equality constraint, we see that $\nu$ must satisfy
\end{flushleft}


\begin{flushleft}
(I + B diag(x)B T )$\nu$ = Ax + b,
\end{flushleft}


\begin{flushleft}
and, since the matrix on the left is invertible for x
\end{flushleft}


\begin{flushleft}
v = $\nu$ = (I +B diag(x)B T )$-$1 (Ax+b),
\end{flushleft}





0,





\begin{flushleft}
w = diag(x)B T (I +B diag(x)B T )$-$1 (Ax+b).
\end{flushleft}





\begin{flushleft}
Substituting in the objective of the problem in the hint, we obtain
\end{flushleft}


\begin{flushleft}
v T v + wT diag(x)$-$1 w = (Ax + b)T (I + B diag(x)B T )$-$1 (Ax + b).
\end{flushleft}


\begin{flushleft}
This shows that the problem is equivalent to the problem in the hint.
\end{flushleft}


\begin{flushleft}
As in exercise 4.26, we now introduce hyperbolic constraints and formulate the problem
\end{flushleft}


\begin{flushleft}
in the hint as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
t + 1T s
\end{flushleft}


\begin{flushleft}
subject to v T v $\leq$ t
\end{flushleft}


\begin{flushleft}
wi2 $\leq$ si xi , i = 1, . . . , n
\end{flushleft}


\begin{flushleft}
x 0
\end{flushleft}


\begin{flushleft}
with variables t $\in$ R, s, x, w $\in$ Rn , v $\in$ Rm . Converting the hyperbolic constraints into
\end{flushleft}


\begin{flushleft}
SOC constraints results in an SOCP.
\end{flushleft}


\begin{flushleft}
4.28 Robust quadratic programming. In §4.4.2 we discussed robust linear programming as an
\end{flushleft}


\begin{flushleft}
application of second-order cone programming. In this problem we consider a similar
\end{flushleft}


\begin{flushleft}
robust variation of the (convex) quadratic program
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(1/2)xT P x + q T x + r
\end{flushleft}


\begin{flushleft}
Ax b.
\end{flushleft}





\begin{flushleft}
For simplicity we assume that only the matrix P is subject to errors, and the other
\end{flushleft}


\begin{flushleft}
parameters (q, r, A, b) are exactly known. The robust quadratic program is defined as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
supP $\in$E ((1/2)xT P x + q T x + r)
\end{flushleft}


\begin{flushleft}
Ax b
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
where E is the set of possible matrices P .
\end{flushleft}


\begin{flushleft}
For each of the following sets E, express the robust QP as a convex problem. Be as specific
\end{flushleft}


\begin{flushleft}
as you can. If the problem can be expressed in a standard form (e.g., QP, QCQP, SOCP,
\end{flushleft}


\begin{flushleft}
SDP), say so.
\end{flushleft}


\begin{flushleft}
(a) A finite set of matrices: E = \{P1 , . . . , PK \}, where Pi $\in$ Sn
\end{flushleft}


\begin{flushleft}
+ , i = 1, . . . , K.
\end{flushleft}


\begin{flushleft}
(b) A set specified by a nominal value P0 $\in$ Sn
\end{flushleft}


\begin{flushleft}
plus
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
bound
\end{flushleft}


\begin{flushleft}
on the eigenvalues of the
\end{flushleft}


+


\begin{flushleft}
deviation P $-$ P0 :
\end{flushleft}


\begin{flushleft}
E = \{P $\in$ Sn | $-$$\gamma$I P $-$ P0 $\gamma$I\}
\end{flushleft}


\begin{flushleft}
where $\gamma$ $\in$ R and P0 $\in$ Sn
\end{flushleft}


+,


\begin{flushleft}
(c) An ellipsoid of matrices:
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
E=
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
P i ui
\end{flushleft}





\begin{flushleft}
P0 +
\end{flushleft}





2





\begin{flushleft}
i=1
\end{flushleft}





$\leq$1





.





\begin{flushleft}
You can assume Pi $\in$ Sn
\end{flushleft}


\begin{flushleft}
+ , i = 0, . . . , K.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The objective function is a maximum of convex function, hence convex.
\end{flushleft}


\begin{flushleft}
We can write the problem as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
(1/2)xT Pi x + q T x + r $\leq$ t, i = 1, . . . , K
\end{flushleft}


\begin{flushleft}
Ax b,
\end{flushleft}





\begin{flushleft}
which is a QCQP in the variable x and t.
\end{flushleft}


\begin{flushleft}
(b) For given x, the supremum of xT ∆P x over $-$$\gamma$I
\end{flushleft}


\begin{flushleft}
∆P
\end{flushleft}





\begin{flushleft}
$\gamma$I is given by
\end{flushleft}





\begin{flushleft}
xT ∆P x = $\gamma$xT x.
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
$-$$\gamma$I
\end{flushleft}





\begin{flushleft}
∆P
\end{flushleft}





\begin{flushleft}
$\gamma$I
\end{flushleft}





\begin{flushleft}
Therefore we can express the robust QP as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(1/2)xT (P0 + $\gamma$I)x + q T x + r
\end{flushleft}


\begin{flushleft}
Ax b
\end{flushleft}





\begin{flushleft}
which is a QP.
\end{flushleft}


\begin{flushleft}
(c) For given x, the quadratic objective function is
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





(1/2)





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
x P0 x + sup
\end{flushleft}





\begin{flushleft}
ui (xT Pi x)
\end{flushleft}





\begin{flushleft}
+ qT x + r
\end{flushleft}





\begin{flushleft}
u 2 $\leq$1
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


1/2





\begin{flushleft}
K
\end{flushleft}





=





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
(1/2)x P0 x + (1/2)
\end{flushleft}





\begin{flushleft}
(x Pi x)
\end{flushleft}





2





\begin{flushleft}
+ q T x + r.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
This is a convex function of x: each of the functions xT Pi x is convex since Pi 0.
\end{flushleft}


\begin{flushleft}
The second term is a composition h(g1 (x), . . . , gK (x)) of h(y) = y 2 with gi (x) =
\end{flushleft}


\begin{flushleft}
xT Pi x. The functions gi are convex and nonnegative. The function h is convex and,
\end{flushleft}


\begin{flushleft}
for y $\in$ RK
\end{flushleft}


\begin{flushleft}
+ , nondecreasing in each of its arguments. Therefore the composition is
\end{flushleft}


\begin{flushleft}
convex.
\end{flushleft}


\begin{flushleft}
The resulting problem can be expressed as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(1/2)xT P0 x + y
\end{flushleft}


\begin{flushleft}
(1/2)xT Pi x $\leq$ yi ,
\end{flushleft}


\begin{flushleft}
Ax b
\end{flushleft}





2





\begin{flushleft}
+ qT x + r
\end{flushleft}


\begin{flushleft}
i = 1, . . . , K
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
which can be further reduced to an SOCP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
u+t
\end{flushleft}


1/2





\begin{flushleft}
P0 x
\end{flushleft}


\begin{flushleft}
2u $-$ 1/4
\end{flushleft}





\begin{flushleft}
subject to
\end{flushleft}





2





\begin{flushleft}
$\leq$ 2u + 1/4
\end{flushleft}





1/2





\begin{flushleft}
Pi x
\end{flushleft}


\begin{flushleft}
2yi $-$ 1/4
\end{flushleft}





2





\begin{flushleft}
$\leq$ 2yi + 1/4,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , K
\end{flushleft}





\begin{flushleft}
y 2$\leq$t
\end{flushleft}


\begin{flushleft}
Ax b.
\end{flushleft}


\begin{flushleft}
The variables are x, u, t, and y $\in$ RK .
\end{flushleft}


\begin{flushleft}
Note that if we square both sides of the first inequality, we obtain
\end{flushleft}


\begin{flushleft}
xT P0 x + (2u $-$ 1/4)2 $\leq$ (2u + 1/4)2 ,
\end{flushleft}


\begin{flushleft}
i.e., xT P0 x $\leq$ 2u. Similarly, the other constraints are equivalent to (1/2)xT Pi x $\leq$ yi .
\end{flushleft}





\begin{flushleft}
4.29 Maximizing probability of satisfying a linear inequality. Let c be a random variable in R n ,
\end{flushleft}


\begin{flushleft}
normally distributed with mean c¯ and covariance matrix R. Consider the problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
prob(cT x $\geq$ $\alpha$)
\end{flushleft}


\begin{flushleft}
F x g, Ax = b.
\end{flushleft}





\begin{flushleft}
Find the conditions under which this is equivalent to a convex or quasiconvex optimization
\end{flushleft}


\begin{flushleft}
problem. When these conditions hold, formulate the problem as a QP, QCQP, or SOCP
\end{flushleft}


\begin{flushleft}
(if the problem is convex), or explain how you can solve it by solving a sequence of QP,
\end{flushleft}


\begin{flushleft}
QCQP, or SOCP feasibility problems (if the problem is quasiconvex).
\end{flushleft}


\begin{flushleft}
Solution. The problem can be expressed as a convex or quasiconvex problem if $\alpha$ $<$ c¯T x
\end{flushleft}


\begin{flushleft}
for all feasible x.
\end{flushleft}


\begin{flushleft}
Before working out the details, we first consider the special case with c¯ = 0. In this case
\end{flushleft}


\begin{flushleft}
cT x is a random variable, normally distributed with E(cT x) = 0 and E(cT x)2 = xT Rx.
\end{flushleft}


\begin{flushleft}
If $\alpha$ $<$ 0, maximizing prob(cT x $\geq$ $\alpha$) means minimizing the variance, i.e., minimizing
\end{flushleft}


\begin{flushleft}
xT Rx, subject to the constraints on x, which is a convex problem (in fact a QP). On the
\end{flushleft}


\begin{flushleft}
other hand, if $\alpha$ $>$ 0, we maximize prob(cT x $\geq$ $\alpha$) by maximizing the variance xT Rx,
\end{flushleft}


\begin{flushleft}
which is very difficult.
\end{flushleft}


\begin{flushleft}
We now turn to the general case with c¯ = 0. Define u = cT x, a scalar random variable,
\end{flushleft}


\begin{flushleft}
normally distributed with E u = c¯T x and E(u $-$ E u)2 = xT Rx. The random variable
\end{flushleft}


\begin{flushleft}
u $-$ c¯T x
\end{flushleft}


$\surd$


\begin{flushleft}
xT Rx
\end{flushleft}


\begin{flushleft}
has a normal distribution with mean zero, and unit variance, so
\end{flushleft}


\begin{flushleft}
prob(u $\geq$ $\alpha$) = prob
\end{flushleft}


\begin{flushleft}
where $\Phi$(z) =
\end{flushleft}





$\surd$1


\begin{flushleft}
2$\pi$
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
e$-$t
\end{flushleft}





2





/2





\begin{flushleft}
u $-$ c¯T x
\end{flushleft}


\begin{flushleft}
$\alpha$ $-$ c¯T x
\end{flushleft}


$\surd$


$\geq$ $\surd$


\begin{flushleft}
xT Rx
\end{flushleft}


\begin{flushleft}
xT Rx
\end{flushleft}





\begin{flushleft}
=1$-$$\Phi$
\end{flushleft}





\begin{flushleft}
$\alpha$ $-$ c¯T x
\end{flushleft}


$\surd$


\begin{flushleft}
xT Rx
\end{flushleft}





,





\begin{flushleft}
dt, a monotonically increasing function.
\end{flushleft}


$\surd$


\begin{flushleft}
To maximize 1 $-$ $\Phi$, we can minimize ($\alpha$ $-$ c¯T x)/ xT Rx, i.e., solve the problem
\end{flushleft}


$\surd$


\begin{flushleft}
maximize (¯
\end{flushleft}


\begin{flushleft}
cT x $-$ $\alpha$)/ xT Rx
\end{flushleft}


\begin{flushleft}
subject to F x g
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}


$-$$\infty$





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
Equivalently, if c¯T x $>$ $\alpha$ for all feasible x, we can also minimize the reciprocal of the
\end{flushleft}


\begin{flushleft}
objective function:
\end{flushleft}


$\surd$


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
xT Rx/(¯
\end{flushleft}


\begin{flushleft}
cT x $-$ $\alpha$)
\end{flushleft}


\begin{flushleft}
subject to F x g
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}


\begin{flushleft}
If c¯T x $>$ $\alpha$ for all feasible x, this is a quasiconvex optimization problem, which we can
\end{flushleft}


\begin{flushleft}
solve by bisection. Each bisection step requires the solution of of an SOCP feasibility
\end{flushleft}


\begin{flushleft}
problem
\end{flushleft}


$\surd$


\begin{flushleft}
xT Rx $\leq$ t(¯
\end{flushleft}


\begin{flushleft}
cT x $-$ $\alpha$),
\end{flushleft}


\begin{flushleft}
F x g,
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}


\begin{flushleft}
The problem can also be expressed as a convex problem, by making a change of variables
\end{flushleft}


\begin{flushleft}
y=
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


,


\begin{flushleft}
c¯T x $-$ $\alpha$
\end{flushleft}





\begin{flushleft}
t=
\end{flushleft}





1


.


\begin{flushleft}
c¯T x $-$ $\alpha$
\end{flushleft}





\begin{flushleft}
This yields the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
y T Ry
\end{flushleft}


\begin{flushleft}
F y gt
\end{flushleft}


\begin{flushleft}
Ay = bt
\end{flushleft}


\begin{flushleft}
cT0 y $-$ $\alpha$t = 1
\end{flushleft}


\begin{flushleft}
t $\geq$ 0.
\end{flushleft}





\begin{flushleft}
If we square the objective this is a quadratic program.
\end{flushleft}





\begin{flushleft}
Geometric programming
\end{flushleft}


\begin{flushleft}
4.30 A heated fluid at temperature T (degrees above ambient temperature) flows in a pipe
\end{flushleft}


\begin{flushleft}
with fixed length and circular cross section with radius r. A layer of insulation, with
\end{flushleft}


\begin{flushleft}
thickness w
\end{flushleft}


\begin{flushleft}
r, surrounds the pipe to reduce heat loss through the pipe walls. The
\end{flushleft}


\begin{flushleft}
design variables in this problem are T , r, and w.
\end{flushleft}


\begin{flushleft}
The heat loss is (approximately) proportional to T r/w, so over a fixed lifetime, the energy
\end{flushleft}


\begin{flushleft}
cost due to heat loss is given by $\alpha$1 T r/w. The cost of the pipe, which has a fixed wall
\end{flushleft}


\begin{flushleft}
thickness, is approximately proportional to the total material, i.e., it is given by $\alpha$2 r. The
\end{flushleft}


\begin{flushleft}
cost of the insulation is also approximately proportional to the total insulation material,
\end{flushleft}


\begin{flushleft}
i.e., $\alpha$3 rw (using w
\end{flushleft}


\begin{flushleft}
r). The total cost is the sum of these three costs.
\end{flushleft}


\begin{flushleft}
The heat flow down the pipe is entirely due to the flow of the fluid, which has a fixed
\end{flushleft}


\begin{flushleft}
velocity, i.e., it is given by $\alpha$4 T r2 . The constants $\alpha$i are all positive, as are the variables
\end{flushleft}


\begin{flushleft}
T , r, and w.
\end{flushleft}


\begin{flushleft}
Now the problem: maximize the total heat flow down the pipe, subject to an upper limit
\end{flushleft}


\begin{flushleft}
Cmax on total cost, and the constraints
\end{flushleft}


\begin{flushleft}
Tmin $\leq$ T $\leq$ Tmax ,
\end{flushleft}





\begin{flushleft}
rmin $\leq$ r $\leq$ rmax ,
\end{flushleft}





\begin{flushleft}
wmin $\leq$ w $\leq$ wmax ,
\end{flushleft}





\begin{flushleft}
Express this problem as a geometric program.
\end{flushleft}


\begin{flushleft}
Solution. The problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\alpha$4 T r 2
\end{flushleft}


\begin{flushleft}
$\alpha$1 T w$-$1 + $\alpha$2 r + $\alpha$3 rw $\leq$ Cmax
\end{flushleft}


\begin{flushleft}
Tmin $\leq$ T $\leq$ Tmax
\end{flushleft}


\begin{flushleft}
rmin $\leq$ r $\leq$ rmax
\end{flushleft}


\begin{flushleft}
wmin $\leq$ w $\leq$ wmax
\end{flushleft}


\begin{flushleft}
w $\leq$ 0.1r.
\end{flushleft}





\begin{flushleft}
w $\leq$ 0.1r.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
This is equivalent to the GP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(1/$\alpha$4 )T $-$1 r$-$2
\end{flushleft}


\begin{flushleft}
($\alpha$1 /Cmax )T w$-$1 + ($\alpha$2 /Cmax )r + ($\alpha$3 /Cmax )rw $\leq$ 1
\end{flushleft}


\begin{flushleft}
(1/Tmax )T $\leq$ 1, Tmin T $-$1 $\leq$ 1
\end{flushleft}


\begin{flushleft}
(1/rmax )r $\leq$ 1, rmin r$-$1 $\leq$ 1
\end{flushleft}


\begin{flushleft}
(1/wmax )w $\leq$ 1, wmin w$-$1 $\leq$ 1
\end{flushleft}


\begin{flushleft}
10wr $-$1 $\leq$ 1.
\end{flushleft}





\begin{flushleft}
4.31 Recursive formulation of optimal beam design problem. Show that the GP (4.46) is equivalent to the GP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
wh
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
wi /wmax $\leq$
\end{flushleft}





\begin{flushleft}
1, wmin /wi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
hi /hmax $\leq$ 1, hmin /hi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
hi /(wi Smax ) $\leq$ 1 i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
6iF/($\sigma$max wi h2i ) $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
(2i $-$ 1)di /vi + vi+1 /vi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
(i $-$ 1/3)di /yi + vi+1 /yi + yi+1 /yi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
y1 /ymax $\leq$ 1
\end{flushleft}


\begin{flushleft}
Ewi h3i di /(6F ) = 1, i = 1, . . . , N.
\end{flushleft}





\begin{flushleft}
The variables are wi , hi , vi , di , yi for i = 1, . . . , N .
\end{flushleft}


\begin{flushleft}
Solution. The problem is then
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
wh
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
wmin $\leq$ wi $\leq$
\end{flushleft}





\begin{flushleft}
wmax , i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
hmin $\leq$ hi $\leq$ hmax , i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
Smin $\leq$ hi /wi $\leq$ Smax i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
6iF/(wi h2i ) $\leq$ $\sigma$max , i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
vi = (2i $-$ 1)di + vi+1 , i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
yi = (i $-$ 1/3)di + vi+1 + yi+1 , i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
y1 $\leq$ ymax
\end{flushleft}


\begin{flushleft}
di = 6F/(Ewi h3i ), i = 1, . . . , N,
\end{flushleft}





\begin{flushleft}
(4.31.A)
\end{flushleft}





\begin{flushleft}
where to simplify notation we use variables di = 6F/(Ewi h3i ), and define yN +1 = dN +1 =
\end{flushleft}


\begin{flushleft}
0. The variables in the problem are wi , hi , vi , yi , di , for i = 1, . . . , N .
\end{flushleft}


\begin{flushleft}
This problem is not a GP, since the equalities that define vi and yi are not monomial
\end{flushleft}


\begin{flushleft}
inequalities. (The objective and other constraints, however, are fine.) Two approaches can
\end{flushleft}


\begin{flushleft}
be used to transform the problem (4.31.A) into an equivalent GP. One simple approach is
\end{flushleft}


\begin{flushleft}
to eliminate v1 , . . . , vN and y2 , . . . , yN , using the recursion (4.45). This recursion shows
\end{flushleft}


\begin{flushleft}
that yi and vi are all posynomials in the variables wi , hi , and in particular, the constraint
\end{flushleft}


\begin{flushleft}
y1 $\leq$ ymax is a posynomial inequality.
\end{flushleft}


\begin{flushleft}
We now describe another method, that would be better in practice if the number of
\end{flushleft}


\begin{flushleft}
segments is more than a small number, since it preserves the problem structure. To
\end{flushleft}


\begin{flushleft}
express this as a GP, we replace the equalities that define vi and yi by the inequalities
\end{flushleft}


\begin{flushleft}
vi $\geq$ (2i $-$ 1)di + vi+1 ,
\end{flushleft}





\begin{flushleft}
yi $\geq$ (i $-$ 1/3)di + vi+1 + yi+1 ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , N.
\end{flushleft}





\begin{flushleft}
(4.31.B)
\end{flushleft}





\begin{flushleft}
This can be done without loss of generality. To see this, suppose we substitute the
\end{flushleft}


\begin{flushleft}
inequalities (4.31.B) in (4.31.A), and suppose h, w, v, y, d are feasible. The variables v 1
\end{flushleft}


\begin{flushleft}
and y1 appear in the following four inequalities
\end{flushleft}


\begin{flushleft}
v1 $\geq$ d 1 ,
\end{flushleft}





\begin{flushleft}
y1 $\geq$ (2/3)d1 ,
\end{flushleft}





\begin{flushleft}
v2 $\geq$ 3d2 + v1 ,
\end{flushleft}





\begin{flushleft}
y2 $\geq$ (5/3)d2 + v1 + y1 .
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
It is clear that setting v1 = d1 and y1 = (2/3)d1 , without changing any of the other
\end{flushleft}


\begin{flushleft}
variables, yields a feasible point with the same objective value. Next, consider the four
\end{flushleft}


\begin{flushleft}
inequalities that involve v2 and y2 :
\end{flushleft}


\begin{flushleft}
v2 $\geq$ 3d2 + v1 ,
\end{flushleft}





\begin{flushleft}
y2 $\geq$ (5/3)d2 + v1 + y1 ,
\end{flushleft}





\begin{flushleft}
v3 $\geq$ 5d3 + v2 ,
\end{flushleft}





\begin{flushleft}
y3 $\geq$ (7/3)d3 + v2 + y2 .
\end{flushleft}





\begin{flushleft}
Again, it is clear that we can decrease v2 and y2 until the first two inequalities are tight,
\end{flushleft}


\begin{flushleft}
without changing the objective value. Continuing the argument, we conclude that the
\end{flushleft}


\begin{flushleft}
two problems are equivalent.
\end{flushleft}


\begin{flushleft}
It is now straightforward to express the problem as the GP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
wh
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
wi /wmax $\leq$
\end{flushleft}





\begin{flushleft}
1, wmin /wi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
hi /hmax $\leq$ 1, hmin /hi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
hi /(wi Smax ) $\leq$ 1 i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
6iF/($\sigma$max wi h2i ) $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
(2i $-$ 1)di /vi + vi+1 /vi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
(i $-$ 1/3)di /yi + vi+1 /yi + yi+1 /yi $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
y1 /ymax $\leq$ 1
\end{flushleft}


\begin{flushleft}
Ewi h3i /(6F di ) = 1, i = 1, . . . , N.
\end{flushleft}





\begin{flushleft}
4.32 Approximating a function as a monomial. Suppose the function f : Rn $\rightarrow$ R is differentiable at a point x0
\end{flushleft}


\begin{flushleft}
0, with f (x0 ) $>$ 0. How would you find a monomial function
\end{flushleft}


\begin{flushleft}
fˆ : Rn $\rightarrow$ R such that f (x0 ) = fˆ(x0 ) and for x near x0 , fˆ(x) is very near f (x)?
\end{flushleft}


\begin{flushleft}
Solution. We'll give two ways to solve this problem. They both end up with the same
\end{flushleft}


\begin{flushleft}
solution.
\end{flushleft}


\begin{flushleft}
Let the monomial approximant have the form
\end{flushleft}


\begin{flushleft}
fˆ(x) = cxa1 1 · · · xann ,
\end{flushleft}


\begin{flushleft}
where c $>$ 0.
\end{flushleft}


\begin{flushleft}
Method 1. First-order matching. To make fˆ(x) very near f (x) in the vicinity of x0 , we
\end{flushleft}


\begin{flushleft}
will make the function values agree, and also set the gradient of both functions equal at
\end{flushleft}


\begin{flushleft}
the point x0 :
\end{flushleft}


\begin{flushleft}
$\partial$f
\end{flushleft}


\begin{flushleft}
$\partial$ fˆ
\end{flushleft}


.


=


\begin{flushleft}
fˆ(x0 ) = f (x0 ),
\end{flushleft}


\begin{flushleft}
$\partial$xi x
\end{flushleft}


\begin{flushleft}
$\partial$xi x0
\end{flushleft}


0





\begin{flushleft}
We have
\end{flushleft}





\begin{flushleft}
$\partial$ fˆ
\end{flushleft}


ˆ


\begin{flushleft}
= cai xa1 1 · · · xiai $-$1 · · · xann = ai x$-$1
\end{flushleft}


\begin{flushleft}
i f (x),
\end{flushleft}


\begin{flushleft}
$\partial$xi
\end{flushleft}


\begin{flushleft}
which gives us an explicit expression for the exponents ai :
\end{flushleft}


\begin{flushleft}
ai =
\end{flushleft}





\begin{flushleft}
xi $\partial$f
\end{flushleft}


ˆ


\begin{flushleft}
f (x) $\partial$xi
\end{flushleft}





.


\begin{flushleft}
x0
\end{flushleft}





\begin{flushleft}
All that is left is to find the coefficient c of the monomial approximant. To do this we use
\end{flushleft}


\begin{flushleft}
the condition fˆ(x0 ) = f (x0 ):
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
c = a1
\end{flushleft}


.


\begin{flushleft}
x1 · · · xann x
\end{flushleft}


0





\begin{flushleft}
Method 2. Log transformation. As is done to transform a GP to convex form, we take the
\end{flushleft}


\begin{flushleft}
log of the function f and the variables, to get
\end{flushleft}


\begin{flushleft}
g(y) = log f (y),
\end{flushleft}





\begin{flushleft}
yi = log xi ,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
and similarly for the approximating monomial:
\end{flushleft}


\begin{flushleft}
gˆ(y) = log fˆ(y) = c˜ + aT y,
\end{flushleft}


\begin{flushleft}
where c˜ = log c. Note that the transformation takes the monomial into an affine function.
\end{flushleft}


\begin{flushleft}
After this transformation, the problem is this: find an affine function that fits g(y) very
\end{flushleft}


\begin{flushleft}
well near the point y0 = log x0 . That's easy --- the answer is to form the first-order Taylor
\end{flushleft}


\begin{flushleft}
approximation of g at y0 :
\end{flushleft}


\begin{flushleft}
g(y0 ) + $\nabla$g(y0 )T (y $-$ y0 ) = c˜ + aT y.
\end{flushleft}


\begin{flushleft}
This implies
\end{flushleft}


\begin{flushleft}
c˜ = g(y0 ) $-$ $\nabla$g(y0 )T y0 ,
\end{flushleft}





\begin{flushleft}
a = $\nabla$g(y0 ).
\end{flushleft}





\begin{flushleft}
If we work out what this means in terms of f , we end up with the same formulas for c
\end{flushleft}


\begin{flushleft}
and ai as in method 1 above.
\end{flushleft}


\begin{flushleft}
4.33 Express the following problems as convex optimization problems.
\end{flushleft}


\begin{flushleft}
(a) Minimize max\{p(x), q(x)\}, where p and q are posynomials.
\end{flushleft}


\begin{flushleft}
(b) Minimize exp(p(x)) + exp(q(x)), where p and q are posynomials.
\end{flushleft}


\begin{flushleft}
(c) Minimize p(x)/(r(x) $-$ q(x)), subject to r(x) $>$ q(x), where p, q are posynomials,
\end{flushleft}


\begin{flushleft}
and r is a monomial.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) This is equivalent to the GP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
p(x)/t $\leq$ 1,
\end{flushleft}





\begin{flushleft}
q(x)/t $\leq$ 1.
\end{flushleft}





\begin{flushleft}
Now make the logarithmic change of variables xi = eyi .
\end{flushleft}


\begin{flushleft}
(b) Equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
exp(t1 ) + exp(t2 )
\end{flushleft}


\begin{flushleft}
p(x) $\leq$ t1 , q(x) $\leq$ t2 .
\end{flushleft}





\begin{flushleft}
Now make the logarithmic change of variables xi = eyi (but not to t1 , t2 ).
\end{flushleft}


\begin{flushleft}
(c) Equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
which is a GP.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
p(x) $\leq$ t(r(x) $-$ q(x)),
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
(p(x)/t + q(x))/r(x) $\leq$ 1,
\end{flushleft}





\begin{flushleft}
4.34 Log-convexity of Perron-Frobenius eigenvalue. Let A $\in$ Rn×n be an elementwise positive
\end{flushleft}


\begin{flushleft}
matrix, i.e., Aij $>$ 0. (The results of this problem hold for irreducible nonnegative
\end{flushleft}


\begin{flushleft}
matrices as well.) Let $\lambda$pf (A) denotes its Perron-Frobenius eigenvalue, i.e., its eigenvalue
\end{flushleft}


\begin{flushleft}
of largest magnitude. (See the definition and the example on page 165.) Show that
\end{flushleft}


\begin{flushleft}
log $\lambda$pf (A) is a convex function of log Aij . This means, for example, that we have the
\end{flushleft}


\begin{flushleft}
inequality
\end{flushleft}


\begin{flushleft}
$\lambda$pf (C) $\leq$ ($\lambda$pf (A)$\lambda$pf (B))1/2 ,
\end{flushleft}


\begin{flushleft}
where Cij = (Aij Bij )1/2 , and A and B are elementwise positive matrices.
\end{flushleft}


\begin{flushleft}
Hint. Use the characterization of the Perron-Frobenius eigenvalue given in (4.47), or,
\end{flushleft}


\begin{flushleft}
alternatively, use the characterization
\end{flushleft}


\begin{flushleft}
log $\lambda$pf (A) = lim (1/k) log(1T Ak 1).
\end{flushleft}


\begin{flushleft}
k$\rightarrow$$\infty$
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
Solution. Define $\alpha$ij = log Aij . From the characterization in the text
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log $\lambda$pf (A)
\end{flushleft}





=





\begin{flushleft}
e$\alpha$ij vj /vi )
\end{flushleft}





\begin{flushleft}
inf max log(
\end{flushleft}





\begin{flushleft}
v 0 i=1,...,n
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=





\begin{flushleft}
inf max
\end{flushleft}





\begin{flushleft}
y i=1,...,n
\end{flushleft}





\begin{flushleft}
log(
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
e$\alpha$ij +yj ) $-$ yi
\end{flushleft}





\begin{flushleft}
where we made a change of variables vi = eyi . The functions
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
e$\alpha$ij +yj
\end{flushleft}





\begin{flushleft}
log
\end{flushleft}





\begin{flushleft}
$-$ yi
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
are convex, jointly in $\alpha$ and y, so
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
e$\alpha$ij +yj
\end{flushleft}





\begin{flushleft}
max log
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
$-$ yi
\end{flushleft}





\begin{flushleft}
is jointly convex in $\alpha$ and y. Minimizing over y therefore gives a convex function of $\alpha$.
\end{flushleft}


\begin{flushleft}
From the characterization in the hint
\end{flushleft}


\begin{flushleft}
(Ak )ij ).
\end{flushleft}





\begin{flushleft}
log $\lambda$pf (A) = lim (1/k) log(
\end{flushleft}


\begin{flushleft}
k$\rightarrow$$\infty$
\end{flushleft}





\begin{flushleft}
i,j
\end{flushleft}





\begin{flushleft}
Ak expanded as a sum of exponentials of linear functions of $\alpha$ij . So log $\lambda$pf (A) is the
\end{flushleft}


\begin{flushleft}
pointwise limit of a set of convex functions.
\end{flushleft}


\begin{flushleft}
4.35 Signomial and geometric programs. A signomial is a linear combination of monomials of
\end{flushleft}


\begin{flushleft}
some positive variables x1 , . . . , xn . Signomials are more general than posynomials, which
\end{flushleft}


\begin{flushleft}
are signomials with all positive coefficients. A signomial program is an optimization
\end{flushleft}


\begin{flushleft}
problem of the form
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
hi (x) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
where f0 , . . . , fm and h1 , . . . , hp are signomials. In general, signomial programs are very
\end{flushleft}


\begin{flushleft}
difficult to solve.
\end{flushleft}


\begin{flushleft}
Some signomial programs can be transformed to GPs, and therefore solved efficiently.
\end{flushleft}


\begin{flushleft}
Show how to do this for a signomial program of the following form:
\end{flushleft}


\begin{flushleft}
$\bullet$ The objective signomial f0 is a posynomial, i.e., its terms have only positive coefficients.
\end{flushleft}


\begin{flushleft}
$\bullet$ Each inequality constraint signomial f1 , . . . , fm has exactly one term with a negative
\end{flushleft}


\begin{flushleft}
coefficient: fi = pi $-$ qi where pi is posynomial, and qi is monomial.
\end{flushleft}





\begin{flushleft}
$\bullet$ Each equality constraint signomial h1 , . . . , hp has exactly one term with a positive
\end{flushleft}


\begin{flushleft}
coefficient and one term with a negative coefficient: hi = ri $-$ si where ri and si are
\end{flushleft}


\begin{flushleft}
monomials.
\end{flushleft}


\begin{flushleft}
Solution. For the inequality constraints, move the single negative term to the righthand
\end{flushleft}


\begin{flushleft}
side, then divide by it, to get a posynomial inequality: fi (x) $\leq$ 0 is equivalent to pi /qi $\leq$ 1.
\end{flushleft}


\begin{flushleft}
For the equality constraints, move the negative term to the righthand side, then divide
\end{flushleft}


\begin{flushleft}
by it, to get a monomial equality: hi (x) = 0 is equivalent to ri /si = 1.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
4.36 Explain how to reformulate a general GP as an equivalent GP in which every posynomial
\end{flushleft}


\begin{flushleft}
(in the objective and constraints) has at most two monomial terms. Hint. Express each
\end{flushleft}


\begin{flushleft}
sum (of monomials) as a sum of sums, each with two terms.
\end{flushleft}


\begin{flushleft}
Solution. Consider a posynomial inequality with t $>$ 2 terms,
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
gi (x) $\leq$ 1,
\end{flushleft}





\begin{flushleft}
where gi are monomials. We introduce new variables s1 , . . . , st$-$2 , and express the posynomial inequality as the set of posynomial inequalities
\end{flushleft}


\begin{flushleft}
g1 (x) + g2 (x)
\end{flushleft}


\begin{flushleft}
g3 (x) + s1
\end{flushleft}





\begin{flushleft}
gt (x) + st$-$2
\end{flushleft}





$\leq$


$\leq$


..


.


$\leq$





\begin{flushleft}
s1
\end{flushleft}


\begin{flushleft}
s2
\end{flushleft}





1.





\begin{flushleft}
By dividing by the righthand side, these become posynomial inequalities with two terms
\end{flushleft}


\begin{flushleft}
each. They are clearly equivalent to the original posynomial inequality. Clearly si is
\end{flushleft}


\begin{flushleft}
i+1
\end{flushleft}


\begin{flushleft}
an upper bound on
\end{flushleft}


\begin{flushleft}
g (x), so the last inequality, gt (x) + st$-$2 $\leq$ 1, implies the
\end{flushleft}


\begin{flushleft}
j=1 j
\end{flushleft}


\begin{flushleft}
i+1
\end{flushleft}





\begin{flushleft}
original posynomial inequality. Conversely, we can always take si = j=1 gj (x), so if the
\end{flushleft}


\begin{flushleft}
original posynomial is satisfied, there are s1 , . . . , st$-$2 that satisfy the two-term posynomial
\end{flushleft}


\begin{flushleft}
inequalities above.
\end{flushleft}





\begin{flushleft}
4.37 Generalized posynomials and geometric programming. Let x 1 , . . . , xn be positive variables,
\end{flushleft}


\begin{flushleft}
and suppose the functions fi : Rn $\rightarrow$ R, i = 1, . . . , k, are posynomials of x1 , . . . , xn . If
\end{flushleft}


\begin{flushleft}
$\phi$ : Rk $\rightarrow$ R is a polynomial with nonnegative coefficients, then the composition
\end{flushleft}


\begin{flushleft}
h(x) = $\phi$(f1 (x), . . . , fk (x))
\end{flushleft}





(4.69)





\begin{flushleft}
is a posynomial, since posynomials are closed under products, sums, and multiplication
\end{flushleft}


\begin{flushleft}
by nonnegative scalars. For example, suppose f1 and f2 are posynomials, and consider
\end{flushleft}


\begin{flushleft}
the polynomial $\phi$(z1 , z2 ) = 3z12 z2 + 2z1 + 3z23 (which has nonnegative coefficients). Then
\end{flushleft}


\begin{flushleft}
h = 3f12 f2 + 2f1 + f23 is a posynomial.
\end{flushleft}


\begin{flushleft}
In this problem we consider a generalization of this idea, in which $\phi$ is allowed to be
\end{flushleft}


\begin{flushleft}
a posynomial, i.e., can have fractional exponents. Specifically, assume that $\phi$ : Rk $\rightarrow$
\end{flushleft}


\begin{flushleft}
R is a posynomial, with all its exponents nonnegative. In this case we will call the
\end{flushleft}


\begin{flushleft}
function h defined in (4.69) a generalized posynomial. As an example, suppose f 1 and f2
\end{flushleft}


\begin{flushleft}
are posynomials, and consider the posynomial (with nonnegative exponents) $\phi$(z1 , z2 ) =
\end{flushleft}


\begin{flushleft}
2z10.3 z21.2 + z1 z20.5 + 2. Then the function
\end{flushleft}


\begin{flushleft}
h(x) = 2f1 (x)0.3 f2 (x)1.2 + f1 (x)f2 (x)0.5 + 2
\end{flushleft}


\begin{flushleft}
is a generalized posynomial. Note that it is not a posynomial, however (unless f 1 and f2
\end{flushleft}


\begin{flushleft}
are monomials or constants).
\end{flushleft}


\begin{flushleft}
A generalized geometric program (GGP) is an optimization problem of the form
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
h0 (x)
\end{flushleft}


\begin{flushleft}
hi (x) $\leq$ 1,
\end{flushleft}


\begin{flushleft}
gi (x) = 1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





(4.70)





\begin{flushleft}
where g1 , . . . , gp are monomials, and h0 , . . . , hm are generalized posynomials.
\end{flushleft}


\begin{flushleft}
Show how to express this generalized geometric program as an equivalent geometric program. Explain any new variables you introduce, and explain how your GP is equivalent
\end{flushleft}


\begin{flushleft}
to the GGP (4.70).
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
We first start by transforming to the epigraph form, by introducing a variable t and
\end{flushleft}


\begin{flushleft}
introducing a new inequality constraint h0 (x) $\leq$ t, which can be written as h0 (x)/t $\leq$ 1,
\end{flushleft}


\begin{flushleft}
which is a valid generalized posynomial inequality constraint. Now we'll show how to deal
\end{flushleft}


\begin{flushleft}
with the generalized posynomial inequality constraint
\end{flushleft}


\begin{flushleft}
$\phi$(f1 (x), . . . , fk (x)) $\leq$ 1,
\end{flushleft}





\begin{flushleft}
(4.37.A)
\end{flushleft}





\begin{flushleft}
where $\phi$ is a posynomial with nonnegative exponents, and f1 , . . . , fk are posynomials.
\end{flushleft}


\begin{flushleft}
We'll use the standard trick: introduce new variables t1 , . . . , tk , and replace the single
\end{flushleft}


\begin{flushleft}
generalized posynomial inequality constraint (4.37.A) with
\end{flushleft}


\begin{flushleft}
$\phi$(t1 , . . . , tk ) $\leq$ 1,
\end{flushleft}





\begin{flushleft}
f1 (x) $\leq$ t1 , . . . , fk (x) $\leq$ tk ,
\end{flushleft}





\begin{flushleft}
(4.37.B)
\end{flushleft}





\begin{flushleft}
which is easily transformed to a set of k + 1 ordinary posynomial inequalities (by dividing
\end{flushleft}


\begin{flushleft}
the last inequalities by ti ). We claim that this set of posynomial inequalities is equivalent
\end{flushleft}


\begin{flushleft}
to the original generalized posynomial inequality. To see this, suppose that x, t1 , . . . , xk
\end{flushleft}


\begin{flushleft}
satisfy (4.37.B). Now we use the fact that the function $\phi$ is monotone nondecreasing in
\end{flushleft}


\begin{flushleft}
each argument (since its exponents are all nonnegative), which implies that
\end{flushleft}


\begin{flushleft}
$\phi$(f1 (x), . . . , fk (x)) $\leq$ 1.
\end{flushleft}


\begin{flushleft}
Conversely, suppose that (4.37.A) holds. Then, defining ti = fi (x), i = 1, . . . , k, we find
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


\begin{flushleft}
$\phi$(t1 , . . . , tk ) $\leq$ 1,
\end{flushleft}


\begin{flushleft}
f1 (x) = t1 , . . . , fk (x) = tk
\end{flushleft}





\begin{flushleft}
holds, which implies (4.37.B).
\end{flushleft}


\begin{flushleft}
If we carry out this procedure for each generalized posynomial inequality, we obtain a GP.
\end{flushleft}


\begin{flushleft}
Since the inequalities are each equivalent, using the argument above, the two problems
\end{flushleft}


\begin{flushleft}
are equivalent.
\end{flushleft}





\begin{flushleft}
Semidefinite programming and conic form problems
\end{flushleft}


\begin{flushleft}
4.38 LMIs and SDPs with one variable. The generalized eigenvalues of a matrix pair (A, B),
\end{flushleft}


\begin{flushleft}
where A, B $\in$ Sn , are defined as the roots of the polynomial det($\lambda$B $-$ A) (see §A.5.3).
\end{flushleft}


\begin{flushleft}
Suppose B is nonsingular, and that A and B can be simultaneously diagonalized by a
\end{flushleft}


\begin{flushleft}
congruence, i.e., there exists a nonsingular R $\in$ Rn×n such that
\end{flushleft}


\begin{flushleft}
RT AR = diag(a),
\end{flushleft}





\begin{flushleft}
RT BR = diag(b),
\end{flushleft}





\begin{flushleft}
where a, b $\in$ Rn . (A sufficient condition for this to hold is that there exists t1 , t2 such
\end{flushleft}


\begin{flushleft}
that t1 A + t2 B 0.)
\end{flushleft}


\begin{flushleft}
(a) Show that the generalized eigenvalues of (A, B) are real, and given by $\lambda$i = ai /bi ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}


\begin{flushleft}
(b) Express the solution of the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
ct
\end{flushleft}


\begin{flushleft}
tB
\end{flushleft}





\begin{flushleft}
A,
\end{flushleft}





\begin{flushleft}
with variable t $\in$ R, in terms of a and b.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) If B is nonsingular, RT BR must be nonsingular, i.e., bi = 0 for all i. We have
\end{flushleft}


\begin{flushleft}
det($\lambda$B $-$ A) = (det R)2
\end{flushleft}





\begin{flushleft}
($\lambda$bi $-$ ai ) = 0
\end{flushleft}





\begin{flushleft}
so $\lambda$ is a generalized eigenvalue if and only if $\lambda$ = ai /bi for some i.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) We have tB
\end{flushleft}





\begin{flushleft}
A if and only if tb
\end{flushleft}





\begin{flushleft}
a, i.e.,
\end{flushleft}


\begin{flushleft}
t $\leq$ ai /bi
\end{flushleft}


\begin{flushleft}
t $\geq$ ai /bi
\end{flushleft}





\begin{flushleft}
bi $>$ 0
\end{flushleft}


\begin{flushleft}
bi $<$ 0.
\end{flushleft}





\begin{flushleft}
The feasible set is an interval defined by,
\end{flushleft}


\begin{flushleft}
max ai /bi $\leq$ t $\leq$ min ai /bi .
\end{flushleft}


\begin{flushleft}
bi $<$0
\end{flushleft}





\begin{flushleft}
bi $>$0
\end{flushleft}





\begin{flushleft}
If the interval is nonempty and bounded, the optimal solution is one of the endpoints,
\end{flushleft}


\begin{flushleft}
depending on the sign of c.
\end{flushleft}


\begin{flushleft}
4.39 SDPs and congruence transformations. Consider the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
x 1 F1 + x 2 F2 + · · · + x n Fn + G
\end{flushleft}





0,





\begin{flushleft}
with Fi , G $\in$ Sk , c $\in$ Rn .
\end{flushleft}


\begin{flushleft}
(a) Suppose R $\in$ Rk×k is nonsingular. Show that the SDP is equivalent to the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


˜


\begin{flushleft}
x1 F˜1 + x2 F˜2 + · · · + xn F˜n + G
\end{flushleft}





0,





\begin{flushleft}
˜ = RT GR.
\end{flushleft}


\begin{flushleft}
where F˜i = RT Fi R, G
\end{flushleft}


\begin{flushleft}
˜ are diagonal. Show that
\end{flushleft}


\begin{flushleft}
(b) Suppose there exists a nonsingular R such that F˜i and G
\end{flushleft}


\begin{flushleft}
the SDP is equivalent to an LP.
\end{flushleft}


\begin{flushleft}
˜ have the form
\end{flushleft}


\begin{flushleft}
(c) Suppose there exists a nonsingular R such that F˜i and G
\end{flushleft}


\begin{flushleft}
F˜i =
\end{flushleft}





\begin{flushleft}
$\alpha$i I
\end{flushleft}


\begin{flushleft}
aTi
\end{flushleft}





\begin{flushleft}
ai
\end{flushleft}


\begin{flushleft}
$\alpha$i
\end{flushleft}





,





\begin{flushleft}
i = 1, . . . , n,
\end{flushleft}





˜=


\begin{flushleft}
G
\end{flushleft}





\begin{flushleft}
$\beta$I
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}


\begin{flushleft}
$\beta$
\end{flushleft}





,





\begin{flushleft}
where $\alpha$i , $\beta$ $\in$ R, ai , b $\in$ Rk$-$1 . Show that the SDP is equivalent to an SOCP with
\end{flushleft}


\begin{flushleft}
a single second-order cone constraint.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Let A $\in$ Sn and R $\in$ Rn×n with R nonsingular. A 0 if and only if xT Ax $\geq$ 0 for
\end{flushleft}


\begin{flushleft}
all x. Hence, with x = Ry, y T RT ARy $\geq$ 0 for all y, i.e., y T RT AR 0.
\end{flushleft}





\begin{flushleft}
(b) A diagonal matrix is positive semidefinite if and only if its diagonal elements are
\end{flushleft}


\begin{flushleft}
nonnegative.
\end{flushleft}


\begin{flushleft}
(c) The LMI is equivalent to
\end{flushleft}


\begin{flushleft}
F˜ (x) =
\end{flushleft}





\begin{flushleft}
($\alpha$t x + $\beta$)I
\end{flushleft}


\begin{flushleft}
(Ax + b)T
\end{flushleft}





\begin{flushleft}
where A has columns ai , i.e., Ax + b
\end{flushleft}





2





\begin{flushleft}
Ax + b
\end{flushleft}


\begin{flushleft}
($\alpha$T x + $\beta$)I
\end{flushleft}





0.





\begin{flushleft}
$\leq$ $\alpha$T x + $\beta$.
\end{flushleft}





\begin{flushleft}
4.40 LPs, QPs, QCQPs, and SOCPs as SDPs. Express the following problems as SDPs.
\end{flushleft}


\begin{flushleft}
(a) The LP (4.27).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x + d
\end{flushleft}


\begin{flushleft}
diag(Gx $-$ h)
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}





0





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(b) The QP (4.34), the QCQP (4.35) and the SOCP (4.36). Hint. Suppose A $\in$ S r++ ,
\end{flushleft}


\begin{flushleft}
C $\in$ Ss , and B $\in$ Rr×s . Then
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}


\begin{flushleft}
BT
\end{flushleft}





\begin{flushleft}
B
\end{flushleft}


\begin{flushleft}
C
\end{flushleft}





\begin{flushleft}
0 $\Leftarrow$$\Rightarrow$ C $-$ B T A$-$1 B
\end{flushleft}





0.





\begin{flushleft}
For a more complete statement, which applies also to singular A, and a proof,
\end{flushleft}


\begin{flushleft}
see §A.5.5.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) QP. Express P = W W T with W $\in$ Rn×r .
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t + 2q T x + r
\end{flushleft}


\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
WTx
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
x W
\end{flushleft}


\begin{flushleft}
tI
\end{flushleft}


\begin{flushleft}
diag(Gx $-$ h) 0
\end{flushleft}


\begin{flushleft}
Ax = b,
\end{flushleft}





0





\begin{flushleft}
with variables x, t $\in$ R.
\end{flushleft}


\begin{flushleft}
(b) QCQP. Express Pi = Wi WiT with Wi $\in$ Rn×ri .
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t0 + 2q0T x + r0
\end{flushleft}


\begin{flushleft}
ti + 2qiT x + ri $\leq$ 0,
\end{flushleft}


\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
WiT x
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
x Wi
\end{flushleft}


\begin{flushleft}
ti I
\end{flushleft}


\begin{flushleft}
Ax = b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


0,





\begin{flushleft}
i = 0, 1, . . . , m
\end{flushleft}





\begin{flushleft}
with variables x, ti $\in$ R.
\end{flushleft}


\begin{flushleft}
(c) SOCP.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
(cTi x + di )I
\end{flushleft}


\begin{flushleft}
(Axi + bi )T
\end{flushleft}


\begin{flushleft}
F x = g.
\end{flushleft}





\begin{flushleft}
Ai x + bi
\end{flushleft}


\begin{flushleft}
(cTi x + di )I
\end{flushleft}





0,





\begin{flushleft}
i = 1, . . . , N
\end{flushleft}





\begin{flushleft}
By the result in the hint, the constraint is equivalent with Ai x+bi 2 $<$ cTi x+di
\end{flushleft}


\begin{flushleft}
when cTi x + di $>$ 0. We have to check the case cTi x + di = 0 separately. In this
\end{flushleft}


\begin{flushleft}
case, the LMI constraint means Ai x + bi = 0, so we can conclude that the LMI
\end{flushleft}


\begin{flushleft}
constraint and the SOC constraint are equivalent.
\end{flushleft}


\begin{flushleft}
(c) The matrix fractional optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
(Ax + b)T F (x)$-$1 (Ax + b)
\end{flushleft}





\begin{flushleft}
where A $\in$ Rm×n , b $\in$ Rm ,
\end{flushleft}


\begin{flushleft}
F (x) = F0 + x1 F1 + · · · + xn Fn ,
\end{flushleft}


\begin{flushleft}
with Fi $\in$ Sm , and we take the domain of the objective to be \{x | F (x)
\end{flushleft}


\begin{flushleft}
can assume the problem is feasible (there exists at least one x with F (x)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
F (x)
\end{flushleft}


\begin{flushleft}
Ax + b
\end{flushleft}


0


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
(Ax + b)T
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
with variables x, t $\in$ R. The LMI constraint is equivalent to
\end{flushleft}


\begin{flushleft}
(Ax + b)T F (x)$-$1 (Ax + b) $\leq$ t
\end{flushleft}





\begin{flushleft}
0\}. You
\end{flushleft}


0).





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
if F (x) 0.
\end{flushleft}


\begin{flushleft}
More generally, let
\end{flushleft}


\begin{flushleft}
f0 (x) = (Ax + b)T F (x)$-$1 (Ax + b),
\end{flushleft}





\begin{flushleft}
dom f0 (x) = \{x | F (x)
\end{flushleft}





0\}.





\begin{flushleft}
We have
\end{flushleft}


\begin{flushleft}
epi f0 =
\end{flushleft}





\begin{flushleft}
(x, t)
\end{flushleft}





\begin{flushleft}
F (x)
\end{flushleft}





0,





\begin{flushleft}
F (x)
\end{flushleft}


\begin{flushleft}
(Ax + b)T
\end{flushleft}





\begin{flushleft}
Ax + b
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





0





.





\begin{flushleft}
Then cl(epi f0 ) = epi g where g is defined by
\end{flushleft}


\begin{flushleft}
epi g
\end{flushleft}





=





\begin{flushleft}
g(x)
\end{flushleft}





=





\begin{flushleft}
F (x)
\end{flushleft}


\begin{flushleft}
(Ax + b)T
\end{flushleft}





\begin{flushleft}
(x, t)
\end{flushleft}


\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
F (x)
\end{flushleft}


\begin{flushleft}
(Ax + b)T
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
Ax + b
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
Ax + b
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





0


0





.





\begin{flushleft}
We conclude that both problems have the same optimal values. An optimal solution
\end{flushleft}


\begin{flushleft}
for the matrix fractional problem is optimal for the SDP. An optimal solution for
\end{flushleft}


\begin{flushleft}
the SDP, with F (x)
\end{flushleft}


\begin{flushleft}
0, is optimal for the matrix fractional problem. If F (x)
\end{flushleft}


\begin{flushleft}
is singular at the optimal solution of the SDP, then the optimum for the matrix
\end{flushleft}


\begin{flushleft}
fractional problem is not attained.
\end{flushleft}


\begin{flushleft}
4.41 LMI tests for copositive matrices and P0 -matrices. A matrix A $\in$ Sn is said to be copositive
\end{flushleft}


\begin{flushleft}
if xT Ax $\geq$ 0 for all x
\end{flushleft}


\begin{flushleft}
0 (see exercise 2.35). A matrix A $\in$ Rn×n is said to be a P0 matrix if maxi=1,...,n xi (Ax)i $\geq$ 0 for all x. Checking whether a matrix is copositive or
\end{flushleft}


\begin{flushleft}
a P0 -matrix is very difficult in general. However, there exist useful sufficient conditions
\end{flushleft}


\begin{flushleft}
that can be verified using semidefinite programming.
\end{flushleft}


\begin{flushleft}
(a) Show that A is copositive if it can be decomposed as a sum of a positive semidefinite
\end{flushleft}


\begin{flushleft}
and an elementwise nonnegative matrix:
\end{flushleft}


\begin{flushleft}
A = B + C,
\end{flushleft}





\begin{flushleft}
B
\end{flushleft}





0,





\begin{flushleft}
Cij $\geq$ 0,
\end{flushleft}





\begin{flushleft}
i, j = 1, . . . , n.
\end{flushleft}





(4.71)





\begin{flushleft}
Express the problem of finding B and C that satisfy (4.71) as an SDP feasibility
\end{flushleft}


\begin{flushleft}
problem.
\end{flushleft}


\begin{flushleft}
(b) Show that A is a P0 -matrix if there exists a positive diagonal matrix D such that
\end{flushleft}


\begin{flushleft}
DA + AT D
\end{flushleft}





0.





(4.72)





\begin{flushleft}
Express the problem of finding a D that satisfies (4.72) as an SDP feasibility problem.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Suppose A satisfies (4.71). Let x
\end{flushleft}





\begin{flushleft}
0. Then
\end{flushleft}





\begin{flushleft}
xT Ax = xT Bx + xT Cx $\geq$ 0,
\end{flushleft}


\begin{flushleft}
because B
\end{flushleft}





\begin{flushleft}
0 and Cij $\geq$ 0 for all i, j.
\end{flushleft}





\begin{flushleft}
(b) Suppose A satisfies (4.72). Then
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
xT (DA + AT D)x = 2
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
dk xk (Axk ) $\geq$ 0
\end{flushleft}





\begin{flushleft}
for all x. Since dk $>$ 0, we must have xk (Axk ) $\geq$ 0 for at least one k.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
4.42 Complex LMIs and SDPs. A complex LMI has the form
\end{flushleft}


\begin{flushleft}
x1 F 1 + · · · + x n F n + G
\end{flushleft}





0





\begin{flushleft}
where F1 , . . . , Fn , G are complex n × n Hermitian matrices, i.e., FiH = Fi , GH = G, and
\end{flushleft}


\begin{flushleft}
x $\in$ Rn is a real variable. A complex SDP is the problem of minimizing a (real) linear
\end{flushleft}


\begin{flushleft}
function of x subject to a complex LMI constraint.
\end{flushleft}


\begin{flushleft}
Complex LMIs and SDPs can be transformed to real LMIs and SDPs, using the fact that
\end{flushleft}


\begin{flushleft}
X
\end{flushleft}


\begin{flushleft}
X
\end{flushleft}





0 $\Leftarrow$$\Rightarrow$





\begin{flushleft}
X
\end{flushleft}





\begin{flushleft}
$-$ X
\end{flushleft}


\begin{flushleft}
X
\end{flushleft}





0,





\begin{flushleft}
where X $\in$ Rn×n is the real part of the complex Hermitian matrix X, and X $\in$ Rn×n
\end{flushleft}


\begin{flushleft}
is the imaginary part of X.
\end{flushleft}


\begin{flushleft}
Verify this result, and show how to pose a complex SDP as a real SDP.
\end{flushleft}


\begin{flushleft}
Solution. For a Hermitian matrix $\surd$X = ( X)T and X = $-$ X T . Now let z = u + iv,
\end{flushleft}


\begin{flushleft}
where u, v are real vectors, and i = $-$1. We have
\end{flushleft}


\begin{flushleft}
z H Xz
\end{flushleft}





\begin{flushleft}
(u $-$ iv)T ( X + i X)(u + iv)
\end{flushleft}





=





\begin{flushleft}
uT Xu + v T Xv $-$ uT Xv + v T Xu
\end{flushleft}





=





\begin{flushleft}
uT
\end{flushleft}





=





\begin{flushleft}
vT
\end{flushleft}





\begin{flushleft}
X
\end{flushleft}


\begin{flushleft}
X
\end{flushleft}





\begin{flushleft}
$-$ X
\end{flushleft}


\begin{flushleft}
X
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}





.





\begin{flushleft}
Therefore z H Xz $\geq$ 0 for all z if and only if the 2n × 2n real (symmetric) matrix above is
\end{flushleft}


\begin{flushleft}
positive semidefinite.
\end{flushleft}


\begin{flushleft}
Thus, we can convert a complex LMI into a real LMI with twice the size. The conversion
\end{flushleft}


\begin{flushleft}
is linear, a complex LMI becomes a real LMI, of twice the size.
\end{flushleft}


\begin{flushleft}
To pose
\end{flushleft}


\begin{flushleft}
4.43 Eigenvalue optimization via SDP. Suppose A : Rn $\rightarrow$ Sm is affine, i.e.,
\end{flushleft}


\begin{flushleft}
A(x) = A0 + x1 A1 + · · · + xn An
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
where Ai $\in$ S . Let $\lambda$1 (x) $\geq$ $\lambda$2 (x) $\geq$ · · · $\geq$ $\lambda$m (x) denote the eigenvalues of A(x). Show
\end{flushleft}


\begin{flushleft}
how to pose the following problems as SDPs.
\end{flushleft}


\begin{flushleft}
(a) Minimize the maximum eigenvalue $\lambda$1 (x).
\end{flushleft}


\begin{flushleft}
(b) Minimize the spread of the eigenvalues, $\lambda$1 (x) $-$ $\lambda$m (x).
\end{flushleft}


\begin{flushleft}
(c) Minimize the condition number of A(x), subject to A(x) 0. The condition number
\end{flushleft}


\begin{flushleft}
is defined as $\kappa$(A(x)) = $\lambda$1 (x)/$\lambda$m (x), with domain \{x | A(x) 0\}. You may assume
\end{flushleft}


\begin{flushleft}
that A(x) 0 for at least one x.
\end{flushleft}


\begin{flushleft}
Hint. You need to minimize $\lambda$/$\gamma$, subject to
\end{flushleft}


\begin{flushleft}
0 ≺ $\gamma$I
\end{flushleft}





\begin{flushleft}
A(x)
\end{flushleft}





\begin{flushleft}
$\lambda$I.
\end{flushleft}





\begin{flushleft}
Change variables to y = x/$\gamma$, t = $\lambda$/$\gamma$, s = 1/$\gamma$.
\end{flushleft}


\begin{flushleft}
(d) Minimize the sum of the absolute values of the eigenvalues, |$\lambda$1 (x)| + · · · + |$\lambda$m (x)|.
\end{flushleft}


\begin{flushleft}
Hint. Express A(x) as A(x) = A+ $-$ A$-$ , where A+ 0, A$-$ 0.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We use the property that $\lambda$1 (x) $\leq$ t if and only if A(x)
\end{flushleft}


\begin{flushleft}
maximum eigenvalue by solving the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
The variables are x $\in$ R and t $\in$ R.
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
A(x)
\end{flushleft}





\begin{flushleft}
tI.
\end{flushleft}





\begin{flushleft}
tI. We minimize the
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) $\lambda$1 (x) $\leq$ t1 if and only if A(x) t1 I and $\lambda$m (A(x)) $\geq$ t2 if and only if A(x)
\end{flushleft}


\begin{flushleft}
so we can minimize $\lambda$1 $-$ $\lambda$m by solving
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t 1 $-$ t2
\end{flushleft}


\begin{flushleft}
t2 I A(x)
\end{flushleft}





\begin{flushleft}
t2 I,
\end{flushleft}





\begin{flushleft}
t1 I.
\end{flushleft}





\begin{flushleft}
This is an SDP with variables t1 $\in$ R, t2 $\in$ R, and x $\in$ Rn .
\end{flushleft}





\begin{flushleft}
(c) We first note that the problem is equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\lambda$/$\gamma$
\end{flushleft}


\begin{flushleft}
$\gamma$I A(x)
\end{flushleft}





\begin{flushleft}
(4.43.A)
\end{flushleft}





\begin{flushleft}
$\lambda$I
\end{flushleft}





\begin{flushleft}
if we take as domain of the objective \{($\lambda$, $\gamma$) | $\gamma$ $>$ 0\}. This problem is quasiconvex,
\end{flushleft}


\begin{flushleft}
and can be solved by bisection: The optimal value is less than or equal to $\alpha$ if and
\end{flushleft}


\begin{flushleft}
only if the inequalities
\end{flushleft}


\begin{flushleft}
$\lambda$ $\leq$ $\gamma$$\alpha$,
\end{flushleft}





\begin{flushleft}
$\gamma$I
\end{flushleft}





\begin{flushleft}
A(x)
\end{flushleft}





\begin{flushleft}
$\lambda$I,
\end{flushleft}





\begin{flushleft}
$\gamma$$>$0
\end{flushleft}





\begin{flushleft}
(with variables $\gamma$, $\lambda$, x) are feasible.
\end{flushleft}


\begin{flushleft}
Following the hint we can also pose the problem as the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
I sA0 + y1 A1 + · · · + yn An
\end{flushleft}


\begin{flushleft}
s $\geq$ 0.
\end{flushleft}





\begin{flushleft}
tI
\end{flushleft}





\begin{flushleft}
(4.43.B)
\end{flushleft}





\begin{flushleft}
We now verify more carefully that the two problems are equivalent. Let p be the
\end{flushleft}


\begin{flushleft}
optimal value of (4.43.A), and psdp is the optimal value of the SDP (4.43.B).
\end{flushleft}


\begin{flushleft}
We first show that p $\geq$ psdp . Let $\lambda$/$\gamma$ be the objective value of (4.43.A), evaluated
\end{flushleft}


\begin{flushleft}
at a feasible point ($\gamma$, $\lambda$, x). Define s = 1/$\gamma$, y = x/$\gamma$, t = $\lambda$/$\gamma$. This yields a feasible
\end{flushleft}


\begin{flushleft}
point in (4.43.B), with objective value t = $\lambda$/$\gamma$. This proves that p $\geq$ psdp .
\end{flushleft}


\begin{flushleft}
Next, we show that p∗sdp $\geq$ p∗ . Suppose that s, y, t are feasible in (4.43.B). If s $>$ 0,
\end{flushleft}


\begin{flushleft}
then $\gamma$ = 1/s, x = y/s, $\lambda$ = t/s are feasible in (4.43.A) with objective value t. If
\end{flushleft}


\begin{flushleft}
s = 0, we have
\end{flushleft}


\begin{flushleft}
I y1 A1 + · · · + yn An tI.
\end{flushleft}





\begin{flushleft}
Choose x = $\tau$ y, with $\tau$ sufficiently large so that A($\tau$ y)
\end{flushleft}


\begin{flushleft}
$\lambda$1 ($\tau$ y) $\leq$ $\lambda$1 (0) + $\tau$ t,
\end{flushleft}





\begin{flushleft}
A0 + $\tau$ I
\end{flushleft}





\begin{flushleft}
0. We have
\end{flushleft}





\begin{flushleft}
$\lambda$m ($\tau$ y) $\geq$ $\lambda$m (0) + $\tau$
\end{flushleft}





\begin{flushleft}
so for $\tau$ sufficiently large,
\end{flushleft}


\begin{flushleft}
$\kappa$(x0 + $\tau$ y) $\leq$
\end{flushleft}





\begin{flushleft}
$\lambda$1 (0) + t$\tau$
\end{flushleft}


.


\begin{flushleft}
$\lambda$m (0) + $\tau$
\end{flushleft}





\begin{flushleft}
Letting $\tau$ go to infinity, we can construct feasible points in (4.43.A), with objective
\end{flushleft}


\begin{flushleft}
value arbitrarily close to t. We conclude that t $\geq$ p if (s, y, t) are feasible in (4.43.B).
\end{flushleft}


\begin{flushleft}
Minimizing over t yields psdp $\geq$ p .
\end{flushleft}





\begin{flushleft}
(d) This problem can be expressed as the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr A+ + tr A$-$
\end{flushleft}


\begin{flushleft}
A(x) = A+ $-$ A$-$
\end{flushleft}


\begin{flushleft}
A+ 0, A$-$ 0,
\end{flushleft}





\begin{flushleft}
(4.43.C)
\end{flushleft}





\begin{flushleft}
with variables x, A+ , A$-$ . We can show the equivalence as follows. First assume x
\end{flushleft}


\begin{flushleft}
is fixed in (4.43.C), and that A+ and A$-$ are the only variables. We will show that
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
the optimal A+ and A$-$ are easily constructed from the eigenvalue decomposition
\end{flushleft}


\begin{flushleft}
of A(x), and that at the optimum we have
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
tr A+ + tr A$-$ =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|$\lambda$i (A(x))|.
\end{flushleft}





\begin{flushleft}
Let A(x) = Q$\Lambda$QT be the eigenvalue decomposition of A(x).
\end{flushleft}


\begin{flushleft}
QT A+ Q, A˜$-$ = QT A$-$ Q, we can write problem (4.43.C) as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr A˜+ + tr A˜$-$
\end{flushleft}


\begin{flushleft}
$\Lambda$ = A˜+ $-$ A˜$-$
\end{flushleft}


\begin{flushleft}
A˜+ 0, A˜$-$
\end{flushleft}





\begin{flushleft}
Defining A˜+ =
\end{flushleft}





\begin{flushleft}
(4.43.D)
\end{flushleft}


0,





\begin{flushleft}
with variables A˜+ and A˜$-$ . Here we have used the fact that
\end{flushleft}


\begin{flushleft}
tr A+ = tr QQT A+ = tr QT A+ Q = tr A˜+ .
\end{flushleft}


\begin{flushleft}
When solving problem (4.43.D), we can assume without loss of generality that the
\end{flushleft}


\begin{flushleft}
matrices A˜+ and A˜$-$ are diagonal. (If they are not diagonal, we can set the offdiagonal elements equal to zero, without changing the objective value and without
\end{flushleft}


\begin{flushleft}
changing feasibility.) The optimal values for the diagonal elements are:
\end{flushleft}


\begin{flushleft}
A˜+
\end{flushleft}


\begin{flushleft}
ii = max\{$\lambda$i , 0\},
\end{flushleft}





\begin{flushleft}
A˜$-$
\end{flushleft}


\begin{flushleft}
ii = max\{$-$$\lambda$i , 0\},
\end{flushleft}





\begin{flushleft}
|$\lambda$i |. Going back to the problem (4.43.C), we have shown
\end{flushleft}


\begin{flushleft}
and the optimal value
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
that if we fix x, and optimize over A+ and A$-$ , the optimal value of the problem is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|$\lambda$i (A(x))|.
\end{flushleft}





\begin{flushleft}
Since the constraints are linear in x, we can allow x to be a variable. Minimizing
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
over x, A+ , and A$-$ jointly is equivalent to minimizing
\end{flushleft}


\begin{flushleft}
|$\lambda$i (A(x))|.
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
4.44 Optimization over polynomials. Pose the following problem as an SDP. Find the polynomial p : R $\rightarrow$ R,
\end{flushleft}


\begin{flushleft}
p(t) = x1 + x2 t + · · · + x2k+1 t2k ,
\end{flushleft}


\begin{flushleft}
that satisfies given bounds li $\leq$ p(ti ) $\leq$ ui , at m specified points ti , and, of all the
\end{flushleft}


\begin{flushleft}
polynomials that satisfy these bounds, has the greatest minimum value:
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
inf t p(t)
\end{flushleft}


\begin{flushleft}
li $\leq$ p(ti ) $\leq$ ui ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
2k+1
\end{flushleft}





\begin{flushleft}
The variables are x $\in$ R
\end{flushleft}


.


\begin{flushleft}
Hint. Use the LMI characterization of nonnegative polynomials derived in exercise 2.37,
\end{flushleft}


\begin{flushleft}
part (b).
\end{flushleft}


\begin{flushleft}
Solution. First reformulate the problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\gamma$
\end{flushleft}


\begin{flushleft}
p(t) $-$ $\gamma$ $\geq$ 0, t $\in$ R
\end{flushleft}


\begin{flushleft}
li $\leq$ p(ti ) $\leq$ ui , i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
(variables x, $\gamma$). Now use the LMI characterization to get an SDP:
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\gamma$
\end{flushleft}


\begin{flushleft}
x1 $-$ $\gamma$ = Y11
\end{flushleft}


\begin{flushleft}
xi = m+n=i+1 Ymn , i = 2, . . . , 2k + 1
\end{flushleft}


\begin{flushleft}
li $\leq$ i p(ti ) $\leq$ ui , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
Y
\end{flushleft}


0.





\begin{flushleft}
The variables are x $\in$ R2k+1 , $\gamma$ $\in$ R, Y $\in$ Sk+1 .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
4.45 [Nes00, Par00] Sum-of-squares representation via LMIs. Consider a polynomial p : R n $\rightarrow$
\end{flushleft}


\begin{flushleft}
R of degree 2k. The polynomial is said to be positive semidefinite (PSD) if p(x) $\geq$ 0
\end{flushleft}


\begin{flushleft}
for all x $\in$ Rn . Except for special cases (e.g., n = 1 or k = 1), it is extremely difficult
\end{flushleft}


\begin{flushleft}
to determine whether or not a given polynomial is PSD, let alone solve an optimization
\end{flushleft}


\begin{flushleft}
problem, with the coefficients of p as variables, with the constraint that p be PSD.
\end{flushleft}


\begin{flushleft}
A famous sufficient condition for a polynomial to be PSD is that it have the form
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
qi (x)2 ,
\end{flushleft}





\begin{flushleft}
p(x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
for some polynomials qi , with degree no more than k. A polynomial p that has this
\end{flushleft}


\begin{flushleft}
sum-of-squares form is called SOS.
\end{flushleft}


\begin{flushleft}
The condition that a polynomial p be SOS (viewed as a constraint on its coefficients)
\end{flushleft}


\begin{flushleft}
turns out to be equivalent to an LMI, and therefore a variety of optimization problems,
\end{flushleft}


\begin{flushleft}
with SOS constraints, can be posed as SDPs. You will explore these ideas in this problem.
\end{flushleft}


\begin{flushleft}
(a) Let f1 , . . . , fs be all monomials of degree k or less. (Here we mean monomial in
\end{flushleft}


\begin{flushleft}
mn
\end{flushleft}


1


\begin{flushleft}
the standard sense, i.e., xm
\end{flushleft}


\begin{flushleft}
1 · · · xn , where mi $\in$ Z+ , and not in the sense used in
\end{flushleft}


\begin{flushleft}
geometric programming.) Show that if p can be expressed as a positive semidefinite
\end{flushleft}


\begin{flushleft}
quadratic form p = f T V f , with V $\in$ Ss+ , then p is SOS. Conversely, show that if
\end{flushleft}


\begin{flushleft}
p is SOS, then it can be expressed as a positive semidefinite quadratic form in the
\end{flushleft}


\begin{flushleft}
monomials, i.e., p = f T V f , for some V $\in$ Ss+ .
\end{flushleft}





\begin{flushleft}
(b) Show that the condition p = f T V f is a set of linear equality constraints relating the
\end{flushleft}


\begin{flushleft}
coefficients of p and the matrix V . Combined with part (a) above, this shows that
\end{flushleft}


\begin{flushleft}
the condition that p be SOS is equivalent to a set of linear equalities relating V and
\end{flushleft}


\begin{flushleft}
the coefficients of p, and the matrix inequality V
\end{flushleft}


0.


\begin{flushleft}
(c) Work out the LMI conditions for SOS explicitly for the case where p is polynomial
\end{flushleft}


\begin{flushleft}
of degree four in two variables.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}
(a) Factor V as V = W W T , where W $\in$ Rs×r and let wi denote the ith column of W .
\end{flushleft}


\begin{flushleft}
We have
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
p = fT
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
wi wiT f =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(wiT f )2 ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i.e., p is SOS.
\end{flushleft}


\begin{flushleft}
Conversely, if p is SOS, it can be expressed as p =
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
V = i=1 wi wiT .
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
(wiT f )2 ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
so p = f T V F for
\end{flushleft}





\begin{flushleft}
(b) Expanding the quadratic form gives
\end{flushleft}


\begin{flushleft}
s
\end{flushleft}





\begin{flushleft}
p=
\end{flushleft}





\begin{flushleft}
Vij fi fj ,
\end{flushleft}


\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
and equating coefficients on both sides proves the result.
\end{flushleft}


\begin{flushleft}
(c) Solution for degree 2: The monomials of degree 2 or less are
\end{flushleft}


\begin{flushleft}
f1 = 1,
\end{flushleft}





\begin{flushleft}
f 2 = x1 ,
\end{flushleft}





\begin{flushleft}
f3 = x2 ,
\end{flushleft}





\begin{flushleft}
f5 = x21 ,
\end{flushleft}





\begin{flushleft}
f 6 = x 1 x2 ,
\end{flushleft}





\begin{flushleft}
f7 = x22
\end{flushleft}





\begin{flushleft}
and the general expression for p
\end{flushleft}


\begin{flushleft}
p(x)
\end{flushleft}





=





\begin{flushleft}
c1 + c2 x1 + c3 x2 + c4 x21 + c5 x1 x2 + c6 x22 + c7 x31 + c8 x21 x2
\end{flushleft}


\begin{flushleft}
+ c9 x1 x22 + c10 x32 + c11 x41 + c12 x31 x2 + c13 x21 x22 + c14 x1 x32 + c15 x42
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
The equality constraints are
\end{flushleft}


\begin{flushleft}
c1 = V11 ,
\end{flushleft}





\begin{flushleft}
c2 = 2V12 ,
\end{flushleft}





\begin{flushleft}
c3 = 2V13 ,
\end{flushleft}





\begin{flushleft}
c4 = V22 + 2V15 ,
\end{flushleft}





\begin{flushleft}
c5 = 2V23 + 2V16 ,
\end{flushleft}





\begin{flushleft}
c6 = V33 + 2V17 , c7 = 2V25 , c8 = 2V26 + 2V25 , c9 = 2V27 + 2V36 , c10 = 2V37 ,
\end{flushleft}


\begin{flushleft}
c11 = V55 , c12 = 2V56 , c13 = 2V57 , c14 = 2V67 , c15 = V77 .
\end{flushleft}


\begin{flushleft}
These, together with V $\in$ S7+ , are the (necessary and sufficient) LMI conditions for
\end{flushleft}


\begin{flushleft}
p to be SOS.
\end{flushleft}


\begin{flushleft}
4.46 Multidimensional moments. The moments of a random variable t on R2 are defined as
\end{flushleft}


\begin{flushleft}
$\mu$ij = E ti1 tj2 , where i, j are nonnegative integers. In this problem we derive necessary
\end{flushleft}


\begin{flushleft}
conditions for a set of numbers $\mu$ij , 0 $\leq$ i, j $\leq$ 2k, i + j $\leq$ 2k, to be the moments of a
\end{flushleft}


\begin{flushleft}
distribution on R2 .
\end{flushleft}


\begin{flushleft}
Let p : R2 $\rightarrow$ R be a polynomial of degree k with coefficients cij ,
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
k$-$i
\end{flushleft}





\begin{flushleft}
cij ti1 tj2 ,
\end{flushleft}





\begin{flushleft}
p(t) =
\end{flushleft}


\begin{flushleft}
i=0 j=0
\end{flushleft}





\begin{flushleft}
and let t be a random variable with moments $\mu$ij . Suppose c $\in$ R(k+1)(k+2)/2 contains
\end{flushleft}


\begin{flushleft}
the coefficients cij in some specific order, and $\mu$ $\in$ R(k+1)(2k+1) contains the moments $\mu$ij
\end{flushleft}


\begin{flushleft}
in the same order. Show that E p(t)2 can be expressed as a quadratic form in c:
\end{flushleft}


\begin{flushleft}
E p(t)2 = cT H($\mu$)c,
\end{flushleft}


\begin{flushleft}
where H : R(k+1)(2k+1) $\rightarrow$ S(k+1)(k+2)/2 is a linear function of $\mu$. From this, conclude
\end{flushleft}


\begin{flushleft}
that $\mu$ must satisfy the LMI H($\mu$) 0.
\end{flushleft}


\begin{flushleft}
Remark: For random variables on R, the matrix H can be taken as the Hankel matrix
\end{flushleft}


\begin{flushleft}
defined in (4.52). In this case, H($\mu$) 0 is a necessary and sufficient condition for $\mu$ to be
\end{flushleft}


\begin{flushleft}
the moments of a distribution, or the limit of a sequence of moments. On R2 , however,
\end{flushleft}


\begin{flushleft}
the LMI is only a necessary condition.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
y = (c00 , c10 , c01 , c20 , c11 , c02 , c30 , c21 , c12 , c03 , . . . , ck0 , ck$-$1,1 , . . . , c0k )
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
E p(t)
\end{flushleft}





2





=





2





\begin{flushleft}
k$-$i
\end{flushleft}





\begin{flushleft}
cij ti1 tj2
\end{flushleft}





\begin{flushleft}
E
\end{flushleft}


\begin{flushleft}
i=0 j=0
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





=





\begin{flushleft}
k$-$i
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
k$-$m
\end{flushleft}





\begin{flushleft}
cij cmn (ti+m
\end{flushleft}


\begin{flushleft}
tj+n
\end{flushleft}


)


1


2





\begin{flushleft}
E
\end{flushleft}


\begin{flushleft}
i=0 j=0 m=0 n=0
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
k$-$i
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
k$-$m
\end{flushleft}





\begin{flushleft}
cij cmn $\mu$i+m,j+n ,
\end{flushleft}





=


\begin{flushleft}
i=0 j=0 m=0 n=0
\end{flushleft}





\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
Hij,mn = $\mu$i+m,j+n .
\end{flushleft}


\begin{flushleft}
For example, with k = 2,
\end{flushleft}


\begin{flushleft}
E(c00 + c10 t1 + c01 t2 + c20 t21 + c11 t1 t2 + c02 t22 )2
\end{flushleft}





=





\begin{flushleft}
c00
\end{flushleft}





\begin{flushleft}
c10
\end{flushleft}





\begin{flushleft}
c01
\end{flushleft}





\begin{flushleft}
c20
\end{flushleft}





\begin{flushleft}
c11
\end{flushleft}





\begin{flushleft}
c02
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
$\mu$00
\end{flushleft}


\begin{flushleft}
$\mu$10
\end{flushleft}


\begin{flushleft}
$\mu$01
\end{flushleft}


\begin{flushleft}
$\mu$20
\end{flushleft}


\begin{flushleft}
$\mu$11
\end{flushleft}


\begin{flushleft}
$\mu$02
\end{flushleft}





\begin{flushleft}
$\mu$10
\end{flushleft}


\begin{flushleft}
$\mu$20
\end{flushleft}


\begin{flushleft}
$\mu$11
\end{flushleft}


\begin{flushleft}
$\mu$30
\end{flushleft}


\begin{flushleft}
$\mu$21
\end{flushleft}


\begin{flushleft}
$\mu$12
\end{flushleft}





\begin{flushleft}
$\mu$01
\end{flushleft}


\begin{flushleft}
$\mu$11
\end{flushleft}


\begin{flushleft}
$\mu$02
\end{flushleft}


\begin{flushleft}
$\mu$21
\end{flushleft}


\begin{flushleft}
$\mu$12
\end{flushleft}


\begin{flushleft}
$\mu$03
\end{flushleft}





\begin{flushleft}
$\mu$20
\end{flushleft}


\begin{flushleft}
$\mu$30
\end{flushleft}


\begin{flushleft}
$\mu$21
\end{flushleft}


\begin{flushleft}
$\mu$40
\end{flushleft}


\begin{flushleft}
$\mu$31
\end{flushleft}


\begin{flushleft}
$\mu$22
\end{flushleft}





\begin{flushleft}
$\mu$11
\end{flushleft}


\begin{flushleft}
$\mu$21
\end{flushleft}


\begin{flushleft}
$\mu$12
\end{flushleft}


\begin{flushleft}
$\mu$31
\end{flushleft}


\begin{flushleft}
$\mu$22
\end{flushleft}


\begin{flushleft}
$\mu$13
\end{flushleft}





\begin{flushleft}
$\mu$02
\end{flushleft}


\begin{flushleft}
$\mu$12
\end{flushleft}


\begin{flushleft}
$\mu$03
\end{flushleft}


\begin{flushleft}
$\mu$22
\end{flushleft}


\begin{flushleft}
$\mu$13
\end{flushleft}


\begin{flushleft}
$\mu$04
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
c00
\end{flushleft}


\begin{flushleft}
c10
\end{flushleft}


\begin{flushleft}
c01
\end{flushleft}


\begin{flushleft}
c20
\end{flushleft}


\begin{flushleft}
c11
\end{flushleft}


\begin{flushleft}
c02
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
.
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
4.47 Maximum determinant positive semidefinite matrix completion. We consider a matrix
\end{flushleft}


\begin{flushleft}
A $\in$ Sn , with some entries specified, and the others not specified. The positive semidefinite
\end{flushleft}


\begin{flushleft}
matrix completion problem is to determine values of the unspecified entries of the matrix
\end{flushleft}


\begin{flushleft}
so that A 0 (or to determine that such a completion does not exist).
\end{flushleft}


\begin{flushleft}
(a) Explain why we can assume without loss of generality that the diagonal entries of
\end{flushleft}


\begin{flushleft}
A are specified.
\end{flushleft}


\begin{flushleft}
(b) Show how to formulate the positive semidefinite completion problem as an SDP
\end{flushleft}


\begin{flushleft}
feasibility problem.
\end{flushleft}


\begin{flushleft}
(c) Assume that A has at least one completion that is positive definite, and the diagonal entries of A are specified (i.e., fixed). The positive definite completion with
\end{flushleft}


\begin{flushleft}
largest determinant is called the maximum determinant completion. Show that the
\end{flushleft}


\begin{flushleft}
maximum determinant completion is unique. Show that if A is the maximum determinant completion, then (A )$-$1 has zeros in all the entries of the original matrix
\end{flushleft}


\begin{flushleft}
that were not specified. Hint. The gradient of the function f (X) = log det X is
\end{flushleft}


\begin{flushleft}
$\nabla$f (X) = X $-$1 (see §A.4.1).
\end{flushleft}


\begin{flushleft}
(d) Suppose A is specified on its tridiagonal part, i.e., we are given A11 , . . . , Ann and
\end{flushleft}


\begin{flushleft}
A12 , . . . , An$-$1,n . Show that if there exists a positive definite completion of A, then
\end{flushleft}


\begin{flushleft}
there is a positive definite completion whose inverse is tridiagonal.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) If a diagonal entry, say Aii , were not specified, then we would take it to be infinitely
\end{flushleft}


\begin{flushleft}
large, i.e., we would take Aii $\rightarrow$ $\infty$. Then, the condition that A
\end{flushleft}


\begin{flushleft}
0 reduces to
\end{flushleft}


\begin{flushleft}
A˜
\end{flushleft}


\begin{flushleft}
0, where A˜ is the matrix A with ith row and column removed. Repeating
\end{flushleft}


\begin{flushleft}
this procedure for each unspecified diagonal entry of A, we see that we can just as
\end{flushleft}


\begin{flushleft}
well consider the submatrix of A corresponding to rows and columns with specified
\end{flushleft}


\begin{flushleft}
diagonal entries.
\end{flushleft}


\begin{flushleft}
(b) The problem is evidently an LMI, since A is clearly an affine function of its unspecified entries, and we require A 0.
\end{flushleft}


\begin{flushleft}
(c) We can just as well minimize f (A) = $-$ log det A, which is a strictly convex function
\end{flushleft}


\begin{flushleft}
of A (provided A
\end{flushleft}


\begin{flushleft}
0. Since the objective is strictly convex, there is at most one
\end{flushleft}


\begin{flushleft}
optimum point. The objective grows unboundedly as A approaches the boundary
\end{flushleft}


\begin{flushleft}
of the positive definite set, and the set of feasible entries for the matrix is bounded
\end{flushleft}


\begin{flushleft}
(since the diagonal entries are fixed, and for a matrix to be positive definite, no
\end{flushleft}


\begin{flushleft}
entry can exceed the maximum diagonal entry). Therefore, there is exactly one
\end{flushleft}


\begin{flushleft}
minimizer of $-$ log det A, and it occurs away from the boundary. The optimality
\end{flushleft}


\begin{flushleft}
condition is simple: it is that the gradient vanishes. Now suppose the i, j entry of
\end{flushleft}


\begin{flushleft}
A is unspecified (i.e., a variable). Then we have, at the optimal A ,
\end{flushleft}


\begin{flushleft}
$\partial$f
\end{flushleft}


\begin{flushleft}
= 2 tr(A )$-$1 Eij = 0.
\end{flushleft}


\begin{flushleft}
$\partial$Aij
\end{flushleft}


\begin{flushleft}
But this is nothing more than twice the i, j entry of (A )$-$1 . Thus, all entries of
\end{flushleft}


\begin{flushleft}
(A )$-$1 corresponding to unspecified entries in A must vanish.
\end{flushleft}


\begin{flushleft}
(d) The maximum determinant positive definite completion will be tridiagonal, by part (c).
\end{flushleft}


\begin{flushleft}
4.48 Generalized eigenvalue minimization. Recall (from example 3.37, or §A.5.3) that the
\end{flushleft}


\begin{flushleft}
largest generalized eigenvalue of a pair of matrices (A, B) $\in$ Sk × Sk++ is given by
\end{flushleft}


\begin{flushleft}
$\lambda$max (A, B) = sup
\end{flushleft}


\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
uT Au
\end{flushleft}


\begin{flushleft}
= max\{$\lambda$ | det($\lambda$B $-$ A) = 0\}.
\end{flushleft}


\begin{flushleft}
uT Bu
\end{flushleft}





\begin{flushleft}
As we have seen, this function is quasiconvex (if we take Sk × Sk++ as its domain).
\end{flushleft}


\begin{flushleft}
We consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$\lambda$max (A(x), B(x))
\end{flushleft}





(4.73)





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
where A, B : Rn $\rightarrow$ Sk are affine functions, defined as
\end{flushleft}


\begin{flushleft}
A(x) = A0 + x1 A1 + · · · + xn An ,
\end{flushleft}





\begin{flushleft}
B(x) = B0 + x1 B1 + · · · + xn Bn .
\end{flushleft}





\begin{flushleft}
with Ai , Bi $\in$ Sk .
\end{flushleft}


\begin{flushleft}
(a) Give a family of convex functions $\phi$t : Sk × Sk $\rightarrow$ R, that satisfy
\end{flushleft}


\begin{flushleft}
$\lambda$max (A, B) $\leq$ t $\Leftarrow$$\Rightarrow$ $\phi$t (A, B) $\leq$ 0
\end{flushleft}


\begin{flushleft}
for all (A, B) $\in$ Sk × Sk++ . Show that this allows us to solve (4.73) by solving a
\end{flushleft}


\begin{flushleft}
sequence of convex feasibility problems.
\end{flushleft}


\begin{flushleft}
(b) Give a family of matrix-convex functions $\Phi$t : Sk × Sk $\rightarrow$ Sk that satisfy
\end{flushleft}


\begin{flushleft}
$\lambda$max (A, B) $\leq$ t $\Leftarrow$$\Rightarrow$ $\Phi$t (A, B)
\end{flushleft}





0





\begin{flushleft}
for all (A, B) $\in$ Sk × Sk++ . Show that this allows us to solve (4.73) by solving a
\end{flushleft}


\begin{flushleft}
sequence of convex feasibility problems with LMI constraints.
\end{flushleft}


\begin{flushleft}
(c) Suppose B(x) = (aT x+b)I, with a = 0. Show that (4.73) is equivalent to the convex
\end{flushleft}


\begin{flushleft}
problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
$\lambda$max (sA0 + y1 A1 + · · · + yn An )
\end{flushleft}


\begin{flushleft}
subject to aT y + bs = 1
\end{flushleft}


\begin{flushleft}
s $\geq$ 0,
\end{flushleft}


\begin{flushleft}
with variables y $\in$ Rn , s $\in$ R.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Take $\phi$t (A, B) = $\lambda$max (A $-$ tB). f0 (A, B) $\leq$ t if and only if
\end{flushleft}


\begin{flushleft}
B $-$1/2 AB $-$1/2
\end{flushleft}





\begin{flushleft}
tI
\end{flushleft}





$\Leftarrow$$\Rightarrow$


$\Leftarrow$$\Rightarrow$





\begin{flushleft}
tB $-$ A 0
\end{flushleft}


\begin{flushleft}
$\lambda$max (A $-$ tB) $\leq$ 0.
\end{flushleft}





\begin{flushleft}
(b) Take $\Phi$t (A, B) = A $-$ tB.
\end{flushleft}





\begin{flushleft}
(c) We will refer to the generalized eigenvalue minimization problem as the GEVP, and
\end{flushleft}


\begin{flushleft}
to the eigenvalue optimization problem as the EVP.
\end{flushleft}


\begin{flushleft}
The GEVP is feasible because a = 0, so there exist x with aT x + b $>$ 0.
\end{flushleft}


\begin{flushleft}
Suppose x is feasible for the GEVP. Then
\end{flushleft}


\begin{flushleft}
y = (1/(aT x + b))x,
\end{flushleft}





\begin{flushleft}
s = 1/(aT x + b)
\end{flushleft}





\begin{flushleft}
is feasible for the EVP (aT y + bs = 1 and s $\geq$ 0). The objective value of (y, s) in
\end{flushleft}


\begin{flushleft}
the EVP is equal to the objective value of x in the GEVP:
\end{flushleft}


\begin{flushleft}
$\lambda$max
\end{flushleft}





1


\begin{flushleft}
(A0 + x1 A1 + · · · + xn An )
\end{flushleft}


\begin{flushleft}
aT x + b)
\end{flushleft}





\begin{flushleft}
= $\lambda$max (A(x), (aT x + b)I).
\end{flushleft}





\begin{flushleft}
Conversely, suppose y, s are feasible for the EVP. If s = 0, then x = y/s satisfies
\end{flushleft}


\begin{flushleft}
aT x + b = 1/s $>$ 0, so x is feasible for the GEVP. Moreover,
\end{flushleft}


\begin{flushleft}
$\lambda$max (A(x), (aT x + b)I) = $\lambda$max (
\end{flushleft}





1


\begin{flushleft}
A(x)) = $\lambda$max (sA0 + y1 A1 + · · · + yn An ),
\end{flushleft}


\begin{flushleft}
(aT x + b)
\end{flushleft}





\begin{flushleft}
i.e., the objective values are the same.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
If y, s are feasible for the EVP with s = 0, then for all x
\end{flushleft}


\begin{flushleft}
ˆ with aT x
\end{flushleft}


\begin{flushleft}
ˆ + b $>$ 0,
\end{flushleft}


\begin{flushleft}
aT (ˆ
\end{flushleft}


\begin{flushleft}
x + ty) + b = aT x
\end{flushleft}


\begin{flushleft}
ˆ + b + t $>$ 0, so x = x
\end{flushleft}


\begin{flushleft}
ˆ = ty is feasible in the GEVP for all t $\geq$ 0.
\end{flushleft}


\begin{flushleft}
The objective value of x is
\end{flushleft}


\begin{flushleft}
$\lambda$max (A(ˆ
\end{flushleft}


\begin{flushleft}
x + ty), (aT (x0 + ty) + b)I)
\end{flushleft}





=





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
uT (A(ˆ
\end{flushleft}


\begin{flushleft}
x) + t(y1 A1 + · · · + yn An ))u
\end{flushleft}


\begin{flushleft}
(aT x
\end{flushleft}


\begin{flushleft}
ˆ + b + t)uT u
\end{flushleft}


\begin{flushleft}
tuT (y1 A1 + · · · + yn An ))u
\end{flushleft}


\begin{flushleft}
tuT u
\end{flushleft}





$\rightarrow$





\begin{flushleft}
sup
\end{flushleft}





=





\begin{flushleft}
$\lambda$max (y1 A1 + · · · + yn An )
\end{flushleft}





\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
so there are feasible points in the GEVP with objective values arbitrarily close to
\end{flushleft}


\begin{flushleft}
the objective value of y, s in the EVP.
\end{flushleft}


\begin{flushleft}
We conclude that the optimal values of the EVP and the GEVP are equal.
\end{flushleft}


\begin{flushleft}
4.49 Generalized fractional programming. Let K $\in$ Rm be a proper cone. Show that the
\end{flushleft}


\begin{flushleft}
function f0 : Rn $\rightarrow$ Rm , defined by
\end{flushleft}


\begin{flushleft}
f0 (x) = inf\{t | Cx + d
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
dom f0 = \{x | F x + g
\end{flushleft}





\begin{flushleft}
t(F x + g)\},
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





0\},





\begin{flushleft}
with C, F $\in$ Rm×n , d, g $\in$ Rm , is quasiconvex.
\end{flushleft}


\begin{flushleft}
A quasiconvex optimization problem with objective function of this form is called a generalized fractional program. Express the generalized linear-fractional program of page 152
\end{flushleft}


\begin{flushleft}
and the generalized eigenvalue minimization problem (4.73) as generalized fractional programs.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) f0 (x) $\leq$ $\alpha$ if and only if Cx + d K $\alpha$(F x + g) and F x + g K 0.
\end{flushleft}


\begin{flushleft}
To see this, we first note that if Cx + d K $\alpha$(F x + g), and F x + g K 0, then
\end{flushleft}


\begin{flushleft}
obviously f0 (x) $\leq$ $\alpha$.
\end{flushleft}


\begin{flushleft}
Conversely, if f0 (x) $\leq$ $\alpha$ and F x + g K 0, then Cx + d K tˆ(F x + g) for at least
\end{flushleft}


\begin{flushleft}
one tˆ $\leq$ $\alpha$, and therefore (since F x + g K 0),
\end{flushleft}


\begin{flushleft}
Cx + d
\end{flushleft}


\begin{flushleft}
for all t $\geq$ tˆ. In particular, Cx + d
\end{flushleft}





\begin{flushleft}
(b) Choose K =
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
t(F x + g)
\end{flushleft}





\begin{flushleft}
$\alpha$(F x + g).
\end{flushleft}





\begin{flushleft}
Rr+ .
\end{flushleft}





\begin{flushleft}
Cx + d
\end{flushleft}





\begin{flushleft}
t(F x + g), F x + g
\end{flushleft}





\begin{flushleft}
0 $\Leftarrow$$\Rightarrow$ t $\geq$ max
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
cTi x + di
\end{flushleft}


.


\begin{flushleft}
fiT x + gi
\end{flushleft}





\begin{flushleft}
(c) Choose K $\in$ Sk+ .
\end{flushleft}


\begin{flushleft}
A(x)
\end{flushleft}





\begin{flushleft}
tB(x), B(x)
\end{flushleft}





\begin{flushleft}
0 $\Leftarrow$$\Rightarrow$ $\lambda$max (A(x), B(x)) $\leq$ t.
\end{flushleft}





\begin{flushleft}
Vector and multicriterion optimization
\end{flushleft}


\begin{flushleft}
4.50 Bi-criterion optimization. Figure 4.11 shows the optimal trade-off curve and the set of
\end{flushleft}


\begin{flushleft}
achievable values for the bi-criterion optimization problem
\end{flushleft}


\begin{flushleft}
minimize (w.r.t. R2+ )
\end{flushleft}





\begin{flushleft}
( Ax $-$ b 2 , x 22 ),
\end{flushleft}





\begin{flushleft}
for some A $\in$ R100×10 , b $\in$ R100 . Answer the following questions using information from
\end{flushleft}


\begin{flushleft}
the plot. We denote by xls the solution of the least-squares problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
Ax $-$ b 22 .
\end{flushleft}





\newpage
4


\begin{flushleft}
(a) What is xls
\end{flushleft}





\begin{flushleft}
Convex optimization problems
\end{flushleft}





2?





\begin{flushleft}
(b) What is Axls $-$ b 2 ?
\end{flushleft}


\begin{flushleft}
(c) What is b 2 ?
\end{flushleft}





\begin{flushleft}
(d) Give the optimal value of the problem
\end{flushleft}


\begin{flushleft}
Ax $-$ b 22
\end{flushleft}


\begin{flushleft}
x 22 = 1.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
(e) Give the optimal value of the problem
\end{flushleft}





\begin{flushleft}
Ax $-$ b 22
\end{flushleft}


\begin{flushleft}
x 22 $\leq$ 1.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
(f) Give the optimal value of the problem
\end{flushleft}





\begin{flushleft}
minimize Ax $-$ b
\end{flushleft}





2


2





\begin{flushleft}
+ x 22 .
\end{flushleft}





\begin{flushleft}
(g) What is the rank of A?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a)
\end{flushleft}





\begin{flushleft}
xls
\end{flushleft}





\begin{flushleft}
(b)
\end{flushleft}





\begin{flushleft}
Axls $-$ b 22 = 2.
\end{flushleft}


$\surd$


\begin{flushleft}
b 2 = 10.
\end{flushleft}





\begin{flushleft}
(c)
\end{flushleft}





2





= 3.





\begin{flushleft}
(d) About 5.
\end{flushleft}


\begin{flushleft}
(e) About 5.
\end{flushleft}


\begin{flushleft}
(f) About 3 + 4.
\end{flushleft}


\begin{flushleft}
(g) rank A = 10, since the LS solution is unique.
\end{flushleft}


\begin{flushleft}
4.51 Monotone transformation of objective in vector optimization. Consider the vector optimization problem (4.56). Suppose we form a new vector optimization problem by replacing
\end{flushleft}


\begin{flushleft}
the objective f0 with $\phi$ ◦ f0 , where $\phi$ : Rq $\rightarrow$ Rq satisfies
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
v, u = v =$\Rightarrow$ $\phi$(u)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\phi$(v), $\phi$(u) = $\phi$(v).
\end{flushleft}





\begin{flushleft}
Show that a point x is Pareto optimal (or optimal) for one problem if and only if it is
\end{flushleft}


\begin{flushleft}
Pareto optimal (optimal) for the other, so the two problems are equivalent. In particular,
\end{flushleft}


\begin{flushleft}
composing each objective in a multicriterion problem with an increasing function does
\end{flushleft}


\begin{flushleft}
not affect the Pareto optimal points.
\end{flushleft}


\begin{flushleft}
Solution. Follows from
\end{flushleft}


\begin{flushleft}
f0 (x)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
f0 (y) $\Leftarrow$$\Rightarrow$ $\phi$(f0 (x))
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
$\phi$(f0 (y))
\end{flushleft}





\begin{flushleft}
with equality only if f0 (x) = f0 (y).
\end{flushleft}


\begin{flushleft}
4.52 Pareto optimal points and the boundary of the set of achievable values. Consider a vector
\end{flushleft}


\begin{flushleft}
optimization problem with cone K. Let P denote the set of Pareto optimal values, and
\end{flushleft}


\begin{flushleft}
let O denote the set of achievable objective values. Show that P $\subseteq$ O $\cap$ bd O, i.e., every
\end{flushleft}


\begin{flushleft}
Pareto optimal value is an achievable objective value that lies in the boundary of the set
\end{flushleft}


\begin{flushleft}
of achievable objective values.
\end{flushleft}


\begin{flushleft}
Solution. P $\subseteq$ O, because that is part of the definition of Pareto optimal points. Suppose
\end{flushleft}


\begin{flushleft}
f0 (x) $\in$ P, f0 (x) $\in$ int O. Then f0 (x) + z $\in$ O for all sufficiently small z, including small
\end{flushleft}


\begin{flushleft}
values of z ≺K 0. This means that f0 (x) is not a Pareto optimal value.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
4.53 Suppose the vector optimization problem (4.56) is convex. Show that the set
\end{flushleft}


\begin{flushleft}
A = O + K = \{t $\in$ Rq | f0 (x)
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
t for some feasible x\},
\end{flushleft}





\begin{flushleft}
is convex. Also show that the minimal elements of A are the same as the minimal points
\end{flushleft}


\begin{flushleft}
of O.
\end{flushleft}


\begin{flushleft}
Solution. If f0 (x1 ) K t1 and f0 (x2 ) K t2 for feasible x1 , x2 , then for 0 $\leq$ $\theta$ $\leq$ 1,
\end{flushleft}


\begin{flushleft}
$\theta$x1 + (1 $-$ $\theta$)x2 is feasible, and
\end{flushleft}


\begin{flushleft}
f0 ($\theta$x1 + (1 $-$ $\theta$)x2 )
\end{flushleft}





\begin{flushleft}
$\theta$f0 (x1 ) + (1 $-$ $\theta$)f0 (y1 )
\end{flushleft}


\begin{flushleft}
$\theta$t1 + (1 $-$ $\theta$)t2 ,
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
i.e., $\theta$t1 + (1 $-$ $\theta$)t2 $\in$ A.
\end{flushleft}


\begin{flushleft}
Suppose u is minimal for A, i.e.,
\end{flushleft}


\begin{flushleft}
v $\in$ A, v
\end{flushleft}





\begin{flushleft}
u =$\Rightarrow$ v = u.
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
We can express u as u = u
\end{flushleft}


\begin{flushleft}
ˆ + z, where u
\end{flushleft}


\begin{flushleft}
ˆ $\in$ O and z K 0. We must have z = 0, otherwise
\end{flushleft}


\begin{flushleft}
the point v = u
\end{flushleft}


\begin{flushleft}
ˆ + z/2 $\in$ A, v K u and v = u. In other words, u $\in$ O. Furthermore, u is
\end{flushleft}


\begin{flushleft}
minimal in O, because
\end{flushleft}


\begin{flushleft}
v $\in$ O, v
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
u =$\Rightarrow$ v $\in$ A, v
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
u =$\Rightarrow$ v = u.
\end{flushleft}





\begin{flushleft}
Conversely, suppose u is minimal for O, i.e.,
\end{flushleft}


\begin{flushleft}
v $\in$ O, v
\end{flushleft}


\begin{flushleft}
Then for all v = vˆ + z $\in$ A, with vˆ $\in$ O, z
\end{flushleft}


\begin{flushleft}
vˆ + z
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
u, vˆ $\in$ O, z
\end{flushleft}





\begin{flushleft}
u =$\Rightarrow$ v = u.
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}


\begin{flushleft}
K
\end{flushleft}





0,





0





=$\Rightarrow$


=$\Rightarrow$





\begin{flushleft}
vˆ K u, vˆ $\in$ O
\end{flushleft}


\begin{flushleft}
vˆ = u, z = 0.
\end{flushleft}





\begin{flushleft}
4.54 Scalarization and optimal points. Suppose a (not necessarily convex) vector optimization
\end{flushleft}


\begin{flushleft}
problem has an optimal point x . Show that x is a solution of the associated scalarized
\end{flushleft}


\begin{flushleft}
problem for any choice of $\lambda$ K ∗ 0. Also show the converse: If a point x is a solution of
\end{flushleft}


\begin{flushleft}
the scalarized problem for any choice of $\lambda$ K ∗ 0, then it is an optimal point for the (not
\end{flushleft}


\begin{flushleft}
necessarily convex) vector optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. Follows from the dual characterization of minimum elements in §2.6.3: f 0 (x )
\end{flushleft}


\begin{flushleft}
is the minimum element of the achievable set O, if and only if for all $\lambda$ K ∗ 0, $\lambda$T f0 (x )
\end{flushleft}


\begin{flushleft}
is the unique minimizer of $\lambda$T z over O.
\end{flushleft}


\begin{flushleft}
4.55 Generalization of weighted-sum scalarization. In §4.7.4 we showed how to obtain Pareto
\end{flushleft}


\begin{flushleft}
optimal solutions of a vector optimization problem by replacing the vector objective f 0 :
\end{flushleft}


\begin{flushleft}
Rn $\rightarrow$ Rq with the scalar objective $\lambda$T f0 , where $\lambda$ K ∗ 0. Let $\psi$ : Rq $\rightarrow$ R be a
\end{flushleft}


\begin{flushleft}
K-increasing function, i.e., satisfying
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
v, u = v =$\Rightarrow$ $\psi$(u) $<$ $\psi$(v).
\end{flushleft}





\begin{flushleft}
Show that any solution of the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\psi$(f0 (x))
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
hi (x) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p
\end{flushleft}





\begin{flushleft}
is Pareto optimal for the vector optimization problem
\end{flushleft}


\begin{flushleft}
minimize (w.r.t. K)
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
hi (x) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
Note that $\psi$(u) = $\lambda$T u, where $\lambda$ K ∗ 0, is a special case.
\end{flushleft}


\begin{flushleft}
As a related example, show that in a multicriterion optimization problem (i.e., a vector
\end{flushleft}


\begin{flushleft}
optimization problem with f0 = F : Rn $\rightarrow$ Rq , and K = Rq+ ), a unique solution of the
\end{flushleft}


\begin{flushleft}
scalar optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
maxi=1,...,q Fi (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
hi (x) = 0, i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
is Pareto optimal.
\end{flushleft}


\begin{flushleft}
Solution. Suppose x is a solution of the scalar problem. Now, suppose
\end{flushleft}


\begin{flushleft}
u $\in$ O,
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
f0 (x ),
\end{flushleft}





\begin{flushleft}
u = f0 (x ).
\end{flushleft}





\begin{flushleft}
Because $\psi$ is increasing, $\psi$(u) $<$ $\psi$(f0 (x )). However, this contradicts the fact that x is
\end{flushleft}


\begin{flushleft}
minimizes $\psi$ ◦ f0 .
\end{flushleft}





\begin{flushleft}
Miscellaneous problems
\end{flushleft}


\begin{flushleft}
4.56 [P. Parrilo] We consider the problem of minimizing the convex function f0 : Rn $\rightarrow$ R
\end{flushleft}


\begin{flushleft}
q
\end{flushleft}


\begin{flushleft}
over the convex hull of the union of some convex sets, conv
\end{flushleft}


\begin{flushleft}
C . These sets are
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
described via convex inequalities,
\end{flushleft}


\begin{flushleft}
Ci = \{x | fij (x) $\leq$ 0, j = 1, . . . , ki \},
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
where fij : R $\rightarrow$ R are convex. Our goal is to formulate this problem as a convex
\end{flushleft}


\begin{flushleft}
optimization problem.
\end{flushleft}


\begin{flushleft}
The obvious approach is to introduce variables x1 , . . . , xq $\in$ Rn , with xi $\in$ Ci , $\theta$ $\in$ Rq
\end{flushleft}


\begin{flushleft}
with $\theta$
\end{flushleft}


\begin{flushleft}
0, 1T $\theta$ = 1, and a variable x $\in$ Rn , with x = $\theta$1 x1 + · · · + $\theta$q xq . This equality
\end{flushleft}


\begin{flushleft}
constraint is not affine in the variables, so this approach does not yield a convex problem.
\end{flushleft}


\begin{flushleft}
A more sophisticated formulation is given by
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
si fij (zi /si ) $\leq$ 0, i = 1, . . . , q,
\end{flushleft}


\begin{flushleft}
1T s = 1, s 0
\end{flushleft}


\begin{flushleft}
x = z1 + · · · + z q ,
\end{flushleft}





\begin{flushleft}
j = 1, . . . , ki
\end{flushleft}





\begin{flushleft}
with variables z1 , . . . , zq $\in$ Rn , x $\in$ Rn , and s1 , . . . , sq $\in$ R. (When si = 0, we take
\end{flushleft}


\begin{flushleft}
si fij (zi /si ) to be 0 if zi = 0 and $\infty$ if zi = 0.) Explain why this problem is convex, and
\end{flushleft}


\begin{flushleft}
equivalent to the original problem.
\end{flushleft}


\begin{flushleft}
Solution. Since fij are convex functions, so are the perspectives si fij (zi /si ). Thus the
\end{flushleft}


\begin{flushleft}
problem is convex.
\end{flushleft}


\begin{flushleft}
Now we show it is equivalent to the original problem. First, suppose that x is feasible for
\end{flushleft}


\begin{flushleft}
the original problem, and can be expressed as x = $\theta$1 x1 + · · · + $\theta$q xq , where xi $\in$ Ci , and
\end{flushleft}


\begin{flushleft}
$\theta$
\end{flushleft}


\begin{flushleft}
0, 1T $\theta$ = 1. Define zi = $\theta$i xi , and si = $\theta$i . We claim that z1 , . . . , zq , s1 , . . . , sq , x
\end{flushleft}


\begin{flushleft}
are feasible for the reformulated problem. Clearly we have x = z1 + · · · + zq , and s 0,
\end{flushleft}


\begin{flushleft}
1T s = 1. For si $>$ 0, we have zi /si = xi $\in$ Ci , so
\end{flushleft}


\begin{flushleft}
fij (zi /si ) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
j = 1, . . . , ki .
\end{flushleft}





\begin{flushleft}
Multiplying by si yields the inequalities in the reformulated problem. For si = 0, the
\end{flushleft}


\begin{flushleft}
inequalities hold since we take si fij (zi /si ) = 0.
\end{flushleft}


\begin{flushleft}
Conversely, let z1 , . . . , zq , s1 , . . . , sq , x be feasible for the reformulated problem. When
\end{flushleft}


\begin{flushleft}
si = 0, we must also have zi = 0, so we can ignore these, and assume without loss of
\end{flushleft}


\begin{flushleft}
generality that all si $>$ 0. Define xi = zi /si . Dividing the inequalities
\end{flushleft}


\begin{flushleft}
fij (zi /si ) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
j = 1, . . . , ki
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
by si yields
\end{flushleft}


\begin{flushleft}
which shows xi $\in$ Ci . From
\end{flushleft}





\begin{flushleft}
fij (xi ) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
j = 1, . . . , ki ,
\end{flushleft}





\begin{flushleft}
x = z 1 + · · · + z q = s 1 x1 + · · · + s q xq
\end{flushleft}


\begin{flushleft}
we see that x is a convex combination of x1 , . . . , xq , and therefore is feasible for the original
\end{flushleft}


\begin{flushleft}
problem.
\end{flushleft}


\begin{flushleft}
It follows that the two problems are equivalent.
\end{flushleft}


\begin{flushleft}
4.57 Capacity of a communication channel. We consider a communication channel, with input
\end{flushleft}


\begin{flushleft}
X(t) $\in$ \{1, . . . , n\}, and output Y (t) $\in$ \{1, . . . , m\}, for t = 1, 2, . . . (in seconds, say). The
\end{flushleft}


\begin{flushleft}
relation between the input and the output is given statistically:
\end{flushleft}


\begin{flushleft}
pij = prob(Y (t) = i|X(t) = j),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
j = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
The matrix P $\in$ Rm×n is called the channel transition matrix, and the channel is called
\end{flushleft}


\begin{flushleft}
a discrete memoryless channel.
\end{flushleft}


\begin{flushleft}
A famous result of Shannon states that information can be sent over the communication
\end{flushleft}


\begin{flushleft}
channel, with arbitrarily small probability of error, at any rate less than a number C,
\end{flushleft}


\begin{flushleft}
called the channel capacity, in bits per second. Shannon also showed that the capacity of
\end{flushleft}


\begin{flushleft}
a discrete memoryless channel can be found by solving an optimization problem. Assume
\end{flushleft}


\begin{flushleft}
that X has a probability distribution denoted x $\in$ Rn , i.e.,
\end{flushleft}


\begin{flushleft}
xj = prob(X = j),
\end{flushleft}





\begin{flushleft}
j = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
The mutual information between X and Y is given by
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
I(X; Y ) =
\end{flushleft}





\begin{flushleft}
xj pij log2
\end{flushleft}


\begin{flushleft}
i=1 j=1
\end{flushleft}





\begin{flushleft}
pij
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
x p
\end{flushleft}


\begin{flushleft}
k=1 k ik
\end{flushleft}





.





\begin{flushleft}
Then the channel capacity C is given by
\end{flushleft}


\begin{flushleft}
C = sup I(X; Y ),
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
where the supremum is over all possible probability distributions for the input X, i.e.,
\end{flushleft}


\begin{flushleft}
over x 0, 1T x = 1.
\end{flushleft}


\begin{flushleft}
Show how the channel capacity can be computed using convex optimization.
\end{flushleft}


\begin{flushleft}
Hint. Introduce the variable y = P x, which gives the probability distribution of the
\end{flushleft}


\begin{flushleft}
output Y , and show that the mutual information can be expressed as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
I(X; Y ) = cT x $-$
\end{flushleft}





\begin{flushleft}
yi log2 yi ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
where cj = i=1 pij log2 pij , j = 1, . . . , n.
\end{flushleft}


\begin{flushleft}
Solution. The capacity is the optimal value of the problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x) =
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





0,





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
1T x = 1,
\end{flushleft}





\begin{flushleft}
xj pij log
\end{flushleft}





\begin{flushleft}
pij
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
xk pik
\end{flushleft}





\begin{flushleft}
with variable x. It is possible to argue directly that the objective f0 (which is the mutual
\end{flushleft}


\begin{flushleft}
information between X and Y ) is concave in x. This can be done several ways, starting
\end{flushleft}


\begin{flushleft}
from the example 3.19.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
Another (related) approach is to follow the hint given, and introduce y = P x as another
\end{flushleft}


\begin{flushleft}
variable. We can express the mutual information in terms of x and y as
\end{flushleft}


\begin{flushleft}
I(X; Y )
\end{flushleft}





\begin{flushleft}
xj pij log
\end{flushleft}





=


\begin{flushleft}
i,j
\end{flushleft}





\begin{flushleft}
j
\end{flushleft}





=


\begin{flushleft}
where cj = $-$
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
pij log pij $-$
\end{flushleft}





\begin{flushleft}
xj
\end{flushleft}





=





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$-$cT x $-$
\end{flushleft}





\begin{flushleft}
pij
\end{flushleft}


\begin{flushleft}
x p
\end{flushleft}


\begin{flushleft}
k k ik
\end{flushleft}


\begin{flushleft}
yi log yi
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
yi log yi ,
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
pij log pij . Therefore the channel capacity problem can be expressed as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
I(X; Y ) = $-$cT x $-$
\end{flushleft}


\begin{flushleft}
x 0, 1T x = 1
\end{flushleft}


\begin{flushleft}
y = P x,
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
yi log yi
\end{flushleft}





\begin{flushleft}
with variables x and y. The objective is a constant plus the entropy of y, hence concave,
\end{flushleft}


\begin{flushleft}
so this is a convex optimization problem.
\end{flushleft}


\begin{flushleft}
4.58 Optimal consumption. In this problem we consider the optimal way to consume (or spend)
\end{flushleft}


\begin{flushleft}
an initial amount of money (or other asset) k0 over time. The variables are c1 , . . . , cT ,
\end{flushleft}


\begin{flushleft}
where ct $\geq$ 0 denotes the consumption in period t. The utility derived from a consumption
\end{flushleft}


\begin{flushleft}
level c is given by u(c), where u : R $\rightarrow$ R is an increasing concave function. The present
\end{flushleft}


\begin{flushleft}
value of the utility derived from the consumption is given by
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
$\beta$ t u(ct ),
\end{flushleft}





\begin{flushleft}
U=
\end{flushleft}


\begin{flushleft}
t=1
\end{flushleft}





\begin{flushleft}
where 0 $<$ $\beta$ $<$ 1 is a discount factor.
\end{flushleft}


\begin{flushleft}
Let kt denote the amount of money available for investment in period t. We assume
\end{flushleft}


\begin{flushleft}
that it earns an investment return given by f (kt ), where f : R $\rightarrow$ R is an increasing,
\end{flushleft}


\begin{flushleft}
concave investment return function, which satisfies f (0) = 0. For example if the funds
\end{flushleft}


\begin{flushleft}
earn simple interest at rate R percent per period, we have f (a) = (R/100)a. The amount
\end{flushleft}


\begin{flushleft}
to be consumed, i.e., ct , is withdrawn at the end of the period, so we have the recursion
\end{flushleft}


\begin{flushleft}
kt+1 = kt + f (kt ) $-$ ct ,
\end{flushleft}





\begin{flushleft}
t = 0, . . . , T.
\end{flushleft}





\begin{flushleft}
The initial sum k0 $>$ 0 is given. We require kt $\geq$ 0, t = 1, . . . , T +1 (but more sophisticated
\end{flushleft}


\begin{flushleft}
models, which allow kt $<$ 0, can be considered).
\end{flushleft}


\begin{flushleft}
Show how to formulate the problem of maximizing U as a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Explain how the problem you formulate is equivalent to this one, and exactly how the
\end{flushleft}


\begin{flushleft}
two are related.
\end{flushleft}


\begin{flushleft}
Hint. Show that we can replace the recursion for kt given above with the inequalities
\end{flushleft}


\begin{flushleft}
kt+1 $\leq$ kt + f (kt ) $-$ ct ,
\end{flushleft}





\begin{flushleft}
t = 0, . . . , T.
\end{flushleft}





\begin{flushleft}
(Interpretation: the inequalities give you the option of throwing money away in each
\end{flushleft}


\begin{flushleft}
period.) For a more general version of this trick, see exercise 4.6.
\end{flushleft}


\begin{flushleft}
Solution. We start with the problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
U = t=1 $\beta$ t u(ct )
\end{flushleft}


\begin{flushleft}
kt+1 = kt + f (kt ) $-$ ct , t = 0, . . . , T
\end{flushleft}


\begin{flushleft}
kt $\geq$ 0, t = 1, . . . , T + 1,
\end{flushleft}





\begin{flushleft}
with variables c1 , . . . , cT and k1 , . . . , kT +1 . The objective is concave, since it is a positive
\end{flushleft}


\begin{flushleft}
weighted sum of concave functions. But the budget recursion constraints are not convex,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
since they are equality constraints involving the (possibly) nonlinear function f . The hint
\end{flushleft}


\begin{flushleft}
explains what to do: we look instead at the modified problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
U = t=1 $\beta$ t u(ct )
\end{flushleft}


\begin{flushleft}
kt+1 $\leq$ kt + f (kt ) $-$ ct , t = 0, . . . , T
\end{flushleft}


\begin{flushleft}
kt $\geq$ 0, t = 1, . . . , T + 1.
\end{flushleft}





\begin{flushleft}
This problem is convex, since the budget inequalities can be written as
\end{flushleft}


\begin{flushleft}
kt+1 $-$ kt $-$ f (kt ) + ct $\leq$ 0,
\end{flushleft}


\begin{flushleft}
where the lefthand side is a convex function of the variables c and k.
\end{flushleft}


\begin{flushleft}
We will now show that when we solve the modified problem with the inequality constraints,
\end{flushleft}


\begin{flushleft}
for any optimal solution we actually get equality for each of the budget constraints. This
\end{flushleft}


\begin{flushleft}
means that the solution of the modified problem is actually optimal for the original problem as well. To see this, we note that by changing the equality constraints into inequalities,
\end{flushleft}


\begin{flushleft}
we are relaxing the constraints (i.e., making them looser), and therefore, if anything, we
\end{flushleft}


\begin{flushleft}
improve the objective compared to the original problem.
\end{flushleft}


\begin{flushleft}
Let c and k be optimal for the modified problem. Suppose that at some period s, we
\end{flushleft}


\begin{flushleft}
have
\end{flushleft}


\begin{flushleft}
ks+1 $<$ ks + f (ks ) $-$ cs .
\end{flushleft}


\begin{flushleft}
This looks pretty suspicious, since it means that in period t, we are actually throwing
\end{flushleft}


\begin{flushleft}
away money (i.e., we are not investing or consuming all of our available funds). Now
\end{flushleft}


\begin{flushleft}
consider a new consumption stream c˜ defined as
\end{flushleft}


\begin{flushleft}
c˜t =
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
ct
\end{flushleft}


\begin{flushleft}
ct +
\end{flushleft}





\begin{flushleft}
t=s
\end{flushleft}


\begin{flushleft}
t=s
\end{flushleft}





\begin{flushleft}
is a small positive number such that
\end{flushleft}


\begin{flushleft}
ks+1 $\leq$ ks + f (ks ) $-$ cs
\end{flushleft}





\begin{flushleft}
holds. In words, c˜ is the same consumption stream as c , except in the period when
\end{flushleft}


\begin{flushleft}
we throw away some money (in c ) we just consume a little more. Clearly we have
\end{flushleft}


\begin{flushleft}
U (˜
\end{flushleft}


\begin{flushleft}
c) $\geq$ U (c ), since the two streams consume the same amount for every period except
\end{flushleft}


\begin{flushleft}
one, in which we consume more with c˜. (Here we use the fact that U is increasing.)
\end{flushleft}


\begin{flushleft}
˜ be the asset stream that results from the consumption stream c˜. Then all the
\end{flushleft}


\begin{flushleft}
Let k
\end{flushleft}


\begin{flushleft}
˜ and yet c has a lower
\end{flushleft}


\begin{flushleft}
constraints of the original problem are satisfied for c˜ and k,
\end{flushleft}


\begin{flushleft}
objective value than c˜. That contradicts optimality of c . We conclude that for c , we
\end{flushleft}


\begin{flushleft}
have
\end{flushleft}


\begin{flushleft}
kt+1 = kt + f (kt ) $-$ ct .
\end{flushleft}


\begin{flushleft}
4.59 Robust optimization. In some optimization problems there is uncertainty or variation
\end{flushleft}


\begin{flushleft}
in the objective and constraint functions, due to parameters or factors that are either
\end{flushleft}


\begin{flushleft}
beyond our control or unknown. We can model this situation by making the objective
\end{flushleft}


\begin{flushleft}
and constraint functions f0 , . . . , fm functions of the optimization variable x $\in$ Rn and
\end{flushleft}


\begin{flushleft}
a parameter vector u $\in$ Rk that is unknown, or varies. In the stochastic optimization
\end{flushleft}


\begin{flushleft}
approach, the parameter vector u is modeled as a random variable with a known distribution, and we work with the expected values Eu fi (x, u). In the worst-case analysis
\end{flushleft}


\begin{flushleft}
approach, we are given a set U that u is known to lie in, and we work with the maximum
\end{flushleft}


\begin{flushleft}
or worst-case values supu$\in$U fi (x, u). To simplify the discussion, we assume there are no
\end{flushleft}


\begin{flushleft}
equality constraints.
\end{flushleft}


\begin{flushleft}
(a) Stochastic optimization. We consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
E f0 (x, u)
\end{flushleft}


\begin{flushleft}
E fi (x, u) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
where the expectation is with respect to u. Show that if fi are convex in x for each
\end{flushleft}


\begin{flushleft}
u, then this stochastic optimization problem is convex.
\end{flushleft}


\begin{flushleft}
(b) Worst-case optimization. We consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
supu$\in$U f0 (x, u)
\end{flushleft}


\begin{flushleft}
supu$\in$U fi (x, u) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Show that if fi are convex in x for each u, then this worst-case optimization problem
\end{flushleft}


\begin{flushleft}
is convex.
\end{flushleft}


\begin{flushleft}
(c) Finite set of possible parameter values. The observations made in parts (a) and (b)
\end{flushleft}


\begin{flushleft}
are most useful when we have analytical or easily evaluated expressions for the
\end{flushleft}


\begin{flushleft}
expected values E fi (x, u) or the worst-case values supu$\in$U fi (x, u).
\end{flushleft}


\begin{flushleft}
Suppose we are given the set of possible values of the parameter is finite, i.e., we
\end{flushleft}


\begin{flushleft}
have u $\in$ \{u1 , . . . , uN \}. For the stochastic case, we are also given the probabilities
\end{flushleft}


\begin{flushleft}
of each value: prob(u = ui ) = pi , where p $\in$ RN , p 0, 1T p = 1. In the worst-case
\end{flushleft}


\begin{flushleft}
formulation, we simply take U $\in$ \{u1 , . . . , uN \}.
\end{flushleft}


\begin{flushleft}
Show how to set up the worst-case and stochastic optimization problems explicitly
\end{flushleft}


\begin{flushleft}
(i.e., give explicit expressions for supu$\in$U fi and Eu fi ).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Follows from the fact that the inequality
\end{flushleft}


\begin{flushleft}
fi ($\theta$x + (1 $-$ $\theta$)y, u) $\leq$ $\theta$f (x, u) + (1 $-$ $\theta$)f (y, u)
\end{flushleft}


\begin{flushleft}
is preserved when we take expectations on both sides.
\end{flushleft}


\begin{flushleft}
(b) If fi (x, u) is convex in x for fixed u, then supu fi (x, u) is convex in x.
\end{flushleft}


\begin{flushleft}
(c) Stochastic formulation:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
pk f0 (x, uk )
\end{flushleft}


\begin{flushleft}
pk fi (x, uk ) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
maxk f0 (x, uk )
\end{flushleft}


\begin{flushleft}
maxk fi (x, uk ) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
Worst-case formulation:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
4.60 Log-optimal investment strategy. We consider a portfolio problem with n assets held over
\end{flushleft}


\begin{flushleft}
N periods. At the beginning of each period, we re-invest our total wealth, redistributing
\end{flushleft}


\begin{flushleft}
it over the n assets using a fixed, constant, allocation strategy x $\in$ Rn , where x
\end{flushleft}


0,


\begin{flushleft}
1T x = 1. In other words, if W (t $-$ 1) is our wealth at the beginning of period t, then
\end{flushleft}


\begin{flushleft}
during period t we invest xi W (t $-$ 1) in asset i. We denote by $\lambda$(t) the total return during
\end{flushleft}


\begin{flushleft}
period t, i.e., $\lambda$(t) = W (t)/W (t $-$ 1). At the end of the N periods our wealth has been
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
multiplied by the factor t=1 $\lambda$(t). We call
\end{flushleft}


1


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
log $\lambda$(t)
\end{flushleft}


\begin{flushleft}
t=1
\end{flushleft}





\begin{flushleft}
the growth rate of the investment over the N periods. We are interested in determining
\end{flushleft}


\begin{flushleft}
an allocation strategy x that maximizes growth of our total wealth for large N .
\end{flushleft}


\begin{flushleft}
We use a discrete stochastic model to account for the uncertainty in the returns. We
\end{flushleft}


\begin{flushleft}
assume that during each period there are m possible scenarios, with probabilities $\pi$ j ,
\end{flushleft}


\begin{flushleft}
j = 1, . . . , m. In scenario j, the return for asset i over one period is given by pij .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Therefore, the return $\lambda$(t) of our portfolio during period t is a random variable, with
\end{flushleft}


\begin{flushleft}
m possible values pT1 x, . . . , pTm x, and distribution
\end{flushleft}


\begin{flushleft}
$\pi$j = prob($\lambda$(t) = pTj x),
\end{flushleft}





\begin{flushleft}
j = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
We assume the same scenarios for each period, with (identical) independent distributions.
\end{flushleft}


\begin{flushleft}
Using the law of large numbers, we have
\end{flushleft}


\begin{flushleft}
lim
\end{flushleft}





\begin{flushleft}
N $\rightarrow$$\infty$
\end{flushleft}





1


\begin{flushleft}
log
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
W (N )
\end{flushleft}


\begin{flushleft}
W (0)
\end{flushleft}





\begin{flushleft}
= lim
\end{flushleft}





\begin{flushleft}
N $\rightarrow$$\infty$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





1


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
$\pi$j log(pTj x).
\end{flushleft}





\begin{flushleft}
log $\lambda$(t) = E log $\lambda$(t) =
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
In other words, with investment strategy x, the long term growth rate is given by
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\pi$j log(pTj x).
\end{flushleft}





\begin{flushleft}
Rlt =
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
The investment strategy x that maximizes this quantity is called the log-optimal investment strategy, and can be found by solving the optimization problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
$\pi$j log(pTj x)
\end{flushleft}


\begin{flushleft}
0, 1T x = 1,
\end{flushleft}





\begin{flushleft}
with variable x $\in$ Rn .
\end{flushleft}


\begin{flushleft}
Show that this is a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. Actually, there's not much to do in this problem. The constraints, x
\end{flushleft}


0,


\begin{flushleft}
1T x = 1, are clearly convex, so we just need to show that the objective is concave (since
\end{flushleft}


\begin{flushleft}
it is to be maximized). We can do that in just a few steps: First, note that log is concave,
\end{flushleft}


\begin{flushleft}
so log(pTj x) is concave in x (on the domain, which is the open halfspace \{x | pTj x 0\}).
\end{flushleft}


\begin{flushleft}
Since $\pi$j $\geq$ 0, we conclude that the sum of concave functions
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\pi$j log(pTj x)
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
is concave.
\end{flushleft}


\begin{flushleft}
4.61 Optimization with logistic model. A random variable X $\in$ \{0, 1\} satisfies
\end{flushleft}


\begin{flushleft}
prob(X = 1) = p =
\end{flushleft}





\begin{flushleft}
exp(aT x + b)
\end{flushleft}


,


\begin{flushleft}
1 + exp(aT x + b)
\end{flushleft}





\begin{flushleft}
where x $\in$ Rn is a vector of variables that affect the probability, and a and b are known
\end{flushleft}


\begin{flushleft}
parameters. We can think of X = 1 as the event that a consumer buys a product, and
\end{flushleft}


\begin{flushleft}
x as a vector of variables that affect the probability, e.g., advertising effort, retail price,
\end{flushleft}


\begin{flushleft}
discounted price, packaging expense, and other factors. The variable x, which we are to
\end{flushleft}


\begin{flushleft}
optimize over, is subject to a set of linear constraints, F x g.
\end{flushleft}


\begin{flushleft}
Formulate the following problems as convex optimization problems.
\end{flushleft}


\begin{flushleft}
(a) Maximizing buying probability. The goal is to choose x to maximize p.
\end{flushleft}


\begin{flushleft}
(b) Maximizing expected profit. Let cT x+d be the profit derived from selling the product,
\end{flushleft}


\begin{flushleft}
which we assume is positive for all feasible x. The goal is to maximize the expected
\end{flushleft}


\begin{flushleft}
profit, which is p(cT x + d).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(a) The function eu /(1 + eu ) is monotonically increasing in u, so we can maximize
\end{flushleft}


\begin{flushleft}
exp(aT x + b)/(1 + exp(aT x + b)) by maximizing aT x + b, which leads to the LP
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
aT x + b
\end{flushleft}


\begin{flushleft}
F x g.
\end{flushleft}





\begin{flushleft}
(b) Here we have to maximize p(cT x + d), or equivalently, its logarithm:
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
aT x + b $-$ log 1 + exp(aT x + b) + log(cT x + d)
\end{flushleft}


\begin{flushleft}
F x g.
\end{flushleft}





\begin{flushleft}
This is a convex problem, since the objective is a concave function of x. (Recall that
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
f (x) = log i=1 exp(aTi x + bi ) is convex.)
\end{flushleft}


\begin{flushleft}
4.62 Optimal power and bandwidth allocation in a Gaussian broadcast channel. We consider a
\end{flushleft}


\begin{flushleft}
communication system in which a central node transmits messages to n receivers. ({`}Gaussian' refers to the type of noise that corrupts the transmissions.) Each receiver channel
\end{flushleft}


\begin{flushleft}
is characterized by its (transmit) power level Pi $\geq$ 0 and its bandwidth Wi $\geq$ 0. The
\end{flushleft}


\begin{flushleft}
power and bandwidth of a receiver channel determine its bit rate Ri (the rate at which
\end{flushleft}


\begin{flushleft}
information can be sent) via
\end{flushleft}


\begin{flushleft}
Ri = $\alpha$i Wi log(1 + $\beta$i Pi /Wi ),
\end{flushleft}


\begin{flushleft}
where $\alpha$i and $\beta$i are known positive constants. For Wi = 0, we take Ri = 0 (which is
\end{flushleft}


\begin{flushleft}
what you get if you take the limit as Wi $\rightarrow$ 0).
\end{flushleft}


\begin{flushleft}
The powers must satisfy a total power constraint, which has the form
\end{flushleft}


\begin{flushleft}
P1 + · · · + Pn = Ptot ,
\end{flushleft}


\begin{flushleft}
where Ptot $>$ 0 is a given total power available to allocate among the channels. Similarly,
\end{flushleft}


\begin{flushleft}
the bandwidths must satisfy
\end{flushleft}


\begin{flushleft}
W1 + · · · + Wn = Wtot ,
\end{flushleft}


\begin{flushleft}
where Wtot $>$ 0 is the (given) total available bandwidth. The optimization variables in
\end{flushleft}


\begin{flushleft}
this problem are the powers and bandwidths, i.e., P1 , . . . , Pn , W1 , . . . , Wn .
\end{flushleft}


\begin{flushleft}
The objective is to maximize the total utility,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
ui (Ri ),
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where ui : R $\rightarrow$ R is the utility function associated with the ith receiver. (You can
\end{flushleft}


\begin{flushleft}
think of ui (Ri ) as the revenue obtained for providing a bit rate Ri to receiver i, so the
\end{flushleft}


\begin{flushleft}
objective is to maximize the total revenue.) You can assume that the utility functions u i
\end{flushleft}


\begin{flushleft}
are nondecreasing and concave.
\end{flushleft}


\begin{flushleft}
Pose this problem as a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. If we substitute the expression for Ri in the objective, we obtain
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
u ($\alpha$i Wi log(1 + $\beta$i Pi /Wi ))
\end{flushleft}


\begin{flushleft}
1T P = Ptot , 1T W = Wtot
\end{flushleft}


\begin{flushleft}
P
\end{flushleft}


\begin{flushleft}
0, W
\end{flushleft}


0





\begin{flushleft}
with variables P, W $\in$ Rn . We show that Ri is a concave function of (Pi , Wi ). It will
\end{flushleft}


\begin{flushleft}
follow that u(Ri ) is concave since it is a nondecreasing concave function of a concave
\end{flushleft}


\begin{flushleft}
function. The total utility U is then concave since it is the sum of concave functions.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
To show that Ri is concave in (Pi , Wi ) we can derive the Hessian, which is
\end{flushleft}


\begin{flushleft}
$\nabla$2 R i =
\end{flushleft}





\begin{flushleft}
$-$$\alpha$i $\beta$i2
\end{flushleft}


\begin{flushleft}
Wi (1 + $\beta$i Pi /Wi )2
\end{flushleft}





1


\begin{flushleft}
$-$Pi
\end{flushleft}





1


\begin{flushleft}
$-$Pi
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





.





\begin{flushleft}
Since $\alpha$i , $\beta$i , Wi , and Pi are positive, $\nabla$2 Ri is negative semidefinite.
\end{flushleft}


\begin{flushleft}
An alternative proof follows fromt the fact that t log(1 + x/t) is concave in (x, t) for t $>$ 0,
\end{flushleft}


\begin{flushleft}
since it is the perspective of log(1 + x), and log(1 + x) is concave.
\end{flushleft}


\begin{flushleft}
Another approach is to relax the bit-rate equality constraint, and write the problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
U = i=1 u(Ri )
\end{flushleft}


\begin{flushleft}
Ri $\leq$ $\alpha$i Wi log(1 + $\beta$i Pi /Wi )
\end{flushleft}


\begin{flushleft}
1T P = Ptot , 1T W = Wtot ,
\end{flushleft}





\begin{flushleft}
with variables Pi , Wi , and Ri . The bit-rate inequality is convex, since the lefthand side
\end{flushleft}


\begin{flushleft}
is a convex function of the variables (actually, linear), and the righthand side is a concave
\end{flushleft}


\begin{flushleft}
function of the variables. Since the objective is concave, this is a convex optimization
\end{flushleft}


\begin{flushleft}
problem. We need to show now is that when we solve this convex optimization problem,
\end{flushleft}


\begin{flushleft}
we end up with equality in the bit-rate inequality constraints. But this is easy: for each
\end{flushleft}


\begin{flushleft}
variable Ri , the objective is monotonically increasing in Ri , so we want each Ri are large
\end{flushleft}


\begin{flushleft}
as possible. Examining the constraints, we see that this occurs when
\end{flushleft}


\begin{flushleft}
Ri = $\alpha$i Wi log(1 + $\beta$i Pi /Wi ).
\end{flushleft}





\begin{flushleft}
4.63 Optimally balancing manufacturing cost and yield. The vector x $\in$ R n denotes the nominal parameters in a manufacturing process. The yield of the process, i.e., the fraction of
\end{flushleft}


\begin{flushleft}
manufactured goods that is acceptable, is given by Y (x). We assume that Y is log-concave
\end{flushleft}


\begin{flushleft}
(which is often the case; see example 3.43). The cost per unit to manufacture the product
\end{flushleft}


\begin{flushleft}
is given by cT x, where c $\in$ Rn . The cost per acceptable unit is cT x/Y (x). We want to
\end{flushleft}


\begin{flushleft}
minimize cT x/Y (x), subject to some convex constraints on x such as a linear inequalities
\end{flushleft}


\begin{flushleft}
Ax b. (You can assume that over the feasible set we have cT x $>$ 0 and Y (x) $>$ 0.)
\end{flushleft}


\begin{flushleft}
This problem is not a convex or quasiconvex optimization problem, but it can be solved
\end{flushleft}


\begin{flushleft}
using convex optimization and a one-dimensional search. The basic ideas are given below;
\end{flushleft}


\begin{flushleft}
you must supply all details and justification.
\end{flushleft}


\begin{flushleft}
(a) Show that the function f : R $\rightarrow$ R given by
\end{flushleft}


\begin{flushleft}
f (a) = sup\{Y (x) | Ax
\end{flushleft}





\begin{flushleft}
b, cT x = a\},
\end{flushleft}





\begin{flushleft}
which gives the maximum yield versus cost, is log-concave. This means that by
\end{flushleft}


\begin{flushleft}
solving a convex optimization problem (in x) we can evaluate the function f .
\end{flushleft}


\begin{flushleft}
(b) Suppose that we evaluate the function f for enough values of a to give a good approximation over the range of interest. Explain how to use these data to (approximately)
\end{flushleft}


\begin{flushleft}
solve the problem of minimizing cost per good product.
\end{flushleft}


\begin{flushleft}
Solution. We first verify that the objective is not convex or quasiconvex. For cT x/Y (x)
\end{flushleft}


\begin{flushleft}
to be quasiconvex, we need the constraint
\end{flushleft}


\begin{flushleft}
cT x/Y (x) $\leq$ t $\Leftarrow$$\Rightarrow$ log(cT x) $-$ log Y (x) $\leq$ log t
\end{flushleft}


\begin{flushleft}
to be convex. By assumption, $-$ log Y (x) is convex, but in general we can't assume that
\end{flushleft}


\begin{flushleft}
the sum with log(cT x) is convex.
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
(a) The function f (a) is log-concave because
\end{flushleft}


\begin{flushleft}
log f (a) = sup F (x, a)
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
log Y (x)
\end{flushleft}


$-$$\infty$





\begin{flushleft}
F (x, a) =
\end{flushleft}





\begin{flushleft}
Ax b, cT x = a
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
F has domain \{(x, a) | Ax b, cT x = a\}, which is a convex set. On its domain it is
\end{flushleft}


\begin{flushleft}
equal to log Y (x), a concave function. Therefore F is concave, and maximizing over
\end{flushleft}


\begin{flushleft}
a gives another concave function.
\end{flushleft}


\begin{flushleft}
(b) We would like to solve the problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log(Y (x)/cT x)
\end{flushleft}


\begin{flushleft}
Ax b.
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log Y (x) $-$ log a
\end{flushleft}


\begin{flushleft}
Ax b
\end{flushleft}


\begin{flushleft}
cT x = a,
\end{flushleft}





\begin{flushleft}
or, equivalently,
\end{flushleft}





\begin{flushleft}
with variables x and a. By first optimizing over x and then over a, we can write the
\end{flushleft}


\begin{flushleft}
problem as
\end{flushleft}


\begin{flushleft}
maximize log f (a) $-$ log a,
\end{flushleft}


\begin{flushleft}
with variable a. The objective function is the sum of a concave and a convex function.
\end{flushleft}


\begin{flushleft}
By evaluating log f (a) $-$ log a for a large set of values of a, we can approximately
\end{flushleft}


\begin{flushleft}
solve the problem.
\end{flushleft}


\begin{flushleft}
Another useful observation is as follows. If we evaluate the objective function at
\end{flushleft}


\begin{flushleft}
some a = a
\end{flushleft}


\begin{flushleft}
ˆ. This yields not only the value, but also a concave lower bound
\end{flushleft}


\begin{flushleft}
log f (a) $-$ log a
\end{flushleft}





$\geq$


=





\begin{flushleft}
log f (a) $-$ log a
\end{flushleft}


\begin{flushleft}
ˆ $-$ (a $-$ a
\end{flushleft}


ˆ)/ˆ


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
log f (a) $-$ a/ˆ
\end{flushleft}


\begin{flushleft}
a $-$ log a
\end{flushleft}


ˆ + 1.





\begin{flushleft}
By repeatedly maximizing the lower bound and linearizing, we can find a local
\end{flushleft}


\begin{flushleft}
maximum of f (a)/a.
\end{flushleft}


\begin{flushleft}
4.64 Optimization with recourse. In an optimization problem with recourse, also called twostage optimization, the cost function and constraints depend not only on our choice of
\end{flushleft}


\begin{flushleft}
variables, but also on a discrete random variable s $\in$ \{1, . . . , S\}, which is interpreted as
\end{flushleft}


\begin{flushleft}
specifying which of S scenarios occurred. The scenario random variable s has known
\end{flushleft}


\begin{flushleft}
probability distribution $\pi$, with $\pi$i = prob(s = i), i = 1, . . . , S.
\end{flushleft}


\begin{flushleft}
In two-stage optimization, we are to choose the values of two variables, x $\in$ R n and
\end{flushleft}


\begin{flushleft}
z $\in$ Rq . The variable x must be chosen before the particular scenario s is known; the
\end{flushleft}


\begin{flushleft}
variable z, however, is chosen after the value of the scenario random variable is known.
\end{flushleft}


\begin{flushleft}
In other words, z is a function of the scenario random variable s. To describe our choice
\end{flushleft}


\begin{flushleft}
z, we list the values we would choose under the different scenarios, i.e., we list the vectors
\end{flushleft}


\begin{flushleft}
z1 , . . . , z S $\in$ R q .
\end{flushleft}


\begin{flushleft}
Here z3 is our choice of z when s = 3 occurs, and so on. The set of values
\end{flushleft}


\begin{flushleft}
x $\in$ Rn ,
\end{flushleft}





\begin{flushleft}
z1 , . . . , z S $\in$ Rq
\end{flushleft}





\begin{flushleft}
is called the policy, since it tells us what choice to make for x (independent of which
\end{flushleft}


\begin{flushleft}
scenario occurs), and also, what choice to make for z in each possible scenario.
\end{flushleft}


\begin{flushleft}
The variable z is called the recourse variable (or second-stage variable), since it allows
\end{flushleft}


\begin{flushleft}
us to take some action or make a choice after we know which scenario occurred. In
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
contrast, our choice of x (which is called the first-stage variable) must be made without
\end{flushleft}


\begin{flushleft}
any knowledge of the scenario.
\end{flushleft}


\begin{flushleft}
For simplicity we will consider the case with no constraints. The cost function is given by
\end{flushleft}


\begin{flushleft}
f : Rn × Rq × \{1, . . . , S\} $\rightarrow$ R,
\end{flushleft}


\begin{flushleft}
where f (x, z, i) gives the cost when the first-stage choice x is made, second-stage choice
\end{flushleft}


\begin{flushleft}
z is made, and scenario i occurs. We will take as the overall objective, to be minimized
\end{flushleft}


\begin{flushleft}
over all policies, the expected cost
\end{flushleft}


\begin{flushleft}
S
\end{flushleft}





\begin{flushleft}
$\pi$i f (x, zi , i).
\end{flushleft}





\begin{flushleft}
E f (x, zs , s) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Suppose that f is a convex function of (x, z), for each scenario i = 1, . . . , S. Explain
\end{flushleft}


\begin{flushleft}
how to find an optimal policy, i.e., one that minimizes the expected cost over all possible
\end{flushleft}


\begin{flushleft}
policies, using convex optimization.
\end{flushleft}


\begin{flushleft}
Solution. The variables in the problem are
\end{flushleft}


\begin{flushleft}
x,
\end{flushleft}





\begin{flushleft}
z1 , . . . , zq ,
\end{flushleft}





\begin{flushleft}
i.e., the policy. The (total) dimension of the variables is n + Sq. Our problem is nothing
\end{flushleft}


\begin{flushleft}
more than
\end{flushleft}


\begin{flushleft}
S
\end{flushleft}


\begin{flushleft}
minimize F (x) = i=1 $\pi$i f (x, zi , i),
\end{flushleft}


\begin{flushleft}
which is convex since for each i, f (x, z, i) is convex in (x, zi ), and $\pi$i $\geq$ 0.
\end{flushleft}





\begin{flushleft}
4.65 Optimal operation of a hybrid vehicle. A hybrid vehicle has an internal combustion engine,
\end{flushleft}


\begin{flushleft}
a motor/generator connected to a storage battery, and a conventional (friction) brake. In
\end{flushleft}


\begin{flushleft}
this exercise we consider a (highly simplified) model of a parallel hybrid vehicle, in which
\end{flushleft}


\begin{flushleft}
both the motor/generator and the engine are directly connected to the drive wheels. The
\end{flushleft}


\begin{flushleft}
engine can provide power to the wheels, and the brake can take power from the wheels,
\end{flushleft}


\begin{flushleft}
turning it into heat. The motor/generator can act as a motor, when it uses energy stored
\end{flushleft}


\begin{flushleft}
in the battery to deliver power to the wheels, or as a generator, when it takes power from
\end{flushleft}


\begin{flushleft}
the wheels or engine, and uses the power to charge the battery. When the generator takes
\end{flushleft}


\begin{flushleft}
power from the wheels and charges the battery, it is called regenerative braking; unlike
\end{flushleft}


\begin{flushleft}
ordinary friction braking, the energy taken from the wheels is stored, and can be used
\end{flushleft}


\begin{flushleft}
later. The vehicle is judged by driving it over a known, fixed test track to evaluate its
\end{flushleft}


\begin{flushleft}
fuel efficiency.
\end{flushleft}


\begin{flushleft}
A diagram illustrating the power flow in the hybrid vehicle is shown below. The arrows
\end{flushleft}


\begin{flushleft}
indicate the direction in which the power flow is considered positive. The engine power
\end{flushleft}


\begin{flushleft}
peng , for example, is positive when it is delivering power; the brake power pbr is positive
\end{flushleft}


\begin{flushleft}
when it is taking power from the wheels. The power preq is the required power at the
\end{flushleft}


\begin{flushleft}
wheels. It is positive when the wheels require power (e.g., when the vehicle accelerates,
\end{flushleft}


\begin{flushleft}
climbs a hill, or cruises on level terrain). The required wheel power is negative when the
\end{flushleft}


\begin{flushleft}
vehicle must decelerate rapidly, or descend a hill.
\end{flushleft}


\begin{flushleft}
PSfrag replacements
\end{flushleft}


\begin{flushleft}
Engine
\end{flushleft}





\begin{flushleft}
Brake
\end{flushleft}





\begin{flushleft}
peng
\end{flushleft}





\begin{flushleft}
pbr
\end{flushleft}





\begin{flushleft}
wheels
\end{flushleft}





\begin{flushleft}
pmg
\end{flushleft}


\begin{flushleft}
Motor/
\end{flushleft}


\begin{flushleft}
generator
\end{flushleft}





\begin{flushleft}
preq
\end{flushleft}





\begin{flushleft}
Battery
\end{flushleft}





\newpage
4





\begin{flushleft}
Convex optimization problems
\end{flushleft}





\begin{flushleft}
All of these powers are functions of time, which we discretize in one second intervals, with
\end{flushleft}


\begin{flushleft}
t = 1, 2, . . . , T . The required wheel power preq (1), . . . , preq (T ) is given. (The speed of
\end{flushleft}


\begin{flushleft}
the vehicle on the track is specified, so together with known road slope information, and
\end{flushleft}


\begin{flushleft}
known aerodynamic and other losses, the power required at the wheels can be calculated.)
\end{flushleft}


\begin{flushleft}
Power is conserved, which means we have
\end{flushleft}


\begin{flushleft}
preq (t) = peng (t) + pmg (t) $-$ pbr (t),
\end{flushleft}





\begin{flushleft}
t = 1, . . . , T.
\end{flushleft}





\begin{flushleft}
The brake can only dissipate power, so we have pbr (t) $\geq$ 0 for each t. The engine can only
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
provide power, and only up to a given limit Peng
\end{flushleft}


\begin{flushleft}
, i.e., we have
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
0 $\leq$ peng (t) $\leq$ Peng
\end{flushleft}


,





\begin{flushleft}
t = 1, . . . , T.
\end{flushleft}





\begin{flushleft}
The motor/generator power is also limited: pmg must satisfy
\end{flushleft}


\begin{flushleft}
min
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
Pmg
\end{flushleft}


\begin{flushleft}
$\leq$ pmg (t) $\leq$ Pmg
\end{flushleft}


,





\begin{flushleft}
t = 1, . . . , T.
\end{flushleft}





\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
min
\end{flushleft}


\begin{flushleft}
Here Pmg
\end{flushleft}


\begin{flushleft}
$>$ 0 is the maximum motor power, and $-$Pmg
\end{flushleft}


\begin{flushleft}
$>$ 0 is the maximum generator
\end{flushleft}


\begin{flushleft}
power.
\end{flushleft}


\begin{flushleft}
The battery charge or energy at time t is denoted E(t), t = 1, . . . , T + 1. The battery
\end{flushleft}


\begin{flushleft}
energy satisfies
\end{flushleft}





\begin{flushleft}
E(t + 1) = E(t) $-$ pmg (t) $-$ $\eta$|pmg (t)|,
\end{flushleft}





\begin{flushleft}
t = 1, . . . , T + 1,
\end{flushleft}





\begin{flushleft}
where $\eta$ $>$ 0 is a known parameter. (The term $-$pmg (t) represents the energy removed
\end{flushleft}


\begin{flushleft}
or added the battery by the motor/generator, ignoring any losses. The term $-$$\eta$|p mg (t)|
\end{flushleft}


\begin{flushleft}
represents energy lost through inefficiencies in the battery or motor/generator.)
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
The battery charge must be between 0 (empty) and its limit Ebatt
\end{flushleft}


\begin{flushleft}
(full), at all times. (If
\end{flushleft}


\begin{flushleft}
E(t) = 0, the battery is fully discharged, and no more energy can be extracted from it;
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
when E(t) = Ebatt
\end{flushleft}


\begin{flushleft}
, the battery is full and cannot be charged.) To make the comparison
\end{flushleft}


\begin{flushleft}
with non-hybrid vehicles fair, we fix the initial battery charge to equal the final battery
\end{flushleft}


\begin{flushleft}
charge, so the net energy change is zero over the track: E(1) = E(T + 1). We do not
\end{flushleft}


\begin{flushleft}
specify the value of the initial (and final) energy.
\end{flushleft}


\begin{flushleft}
The objective in the problem is the total fuel consumed by the engine, which is
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
F (peng (t)),
\end{flushleft}





\begin{flushleft}
Ftotal =
\end{flushleft}


\begin{flushleft}
t=1
\end{flushleft}





\begin{flushleft}
where F : R $\rightarrow$ R is the fuel use characteristic of the engine. We assume that F is
\end{flushleft}


\begin{flushleft}
positive, increasing, and convex.
\end{flushleft}


\begin{flushleft}
Formulate this problem as a convex optimization problem, with variables peng (t), pmg (t),
\end{flushleft}


\begin{flushleft}
and pbr (t) for t = 1, . . . , T , and E(t) for t = 1, . . . , T + 1. Explain why your formulation
\end{flushleft}


\begin{flushleft}
is equivalent to the problem described above.
\end{flushleft}


\begin{flushleft}
Solution. We first collect the given objective and constraints to form the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
F (peng (t))
\end{flushleft}


\begin{flushleft}
t=1
\end{flushleft}


\begin{flushleft}
preq (t) = peng (t) +
\end{flushleft}





\begin{flushleft}
pmg (t) $-$ pbr (t)
\end{flushleft}


\begin{flushleft}
E(t + 1) = E(t) $-$ pmg (t)) $-$ $\eta$|pmg (t))|
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
0 $\leq$ E(t) $\leq$ Ebatt
\end{flushleft}


\begin{flushleft}
E(1) = E(T + 1)
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
0 $\leq$ peng (t) $\leq$ Peng
\end{flushleft}


\begin{flushleft}
min
\end{flushleft}


\begin{flushleft}
max
\end{flushleft}


\begin{flushleft}
Pmg
\end{flushleft}


\begin{flushleft}
$\leq$ pmg (t) $\leq$ Pmg
\end{flushleft}


\begin{flushleft}
0 $\leq$ pbr (t),
\end{flushleft}





\begin{flushleft}
where each constraint is imposed for the appropriate range of t. The fuel use function F
\end{flushleft}


\begin{flushleft}
is convex, so the objective function is convex. With the exception of the battery charge
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
equations, each constraint is a linear equality or linear inequality. So in this form the
\end{flushleft}


\begin{flushleft}
problem is not convex.
\end{flushleft}


\begin{flushleft}
We need to show how to deal with the nonconvex constraints
\end{flushleft}


\begin{flushleft}
E(t + 1) = E(t) $-$ pmg (t)) $-$ $\eta$|pmg (t))|.
\end{flushleft}


\begin{flushleft}
One approach is to replace this constraint with the relaxation,
\end{flushleft}


\begin{flushleft}
E(t + 1) $\leq$ E(t) $-$ pmg (t)) $-$ $\eta$|pmg (t))|,
\end{flushleft}


\begin{flushleft}
which is convex, in fact, two linear inequalities. Intuitively, this relaxation means that we
\end{flushleft}


\begin{flushleft}
open the possibility of throwing energy from the battery away at each step. This sounds
\end{flushleft}


\begin{flushleft}
like a bad idea, when fuel efficiency is the goal, and indeed, it is easy to see that if we
\end{flushleft}


\begin{flushleft}
solve the problem with the relaxed battery charge constraints, the optimal E satisfies
\end{flushleft}


\begin{flushleft}
E (t + 1) = E (t) $-$ pmg (t)) $-$ $\eta$|pmg (t))|,
\end{flushleft}


\begin{flushleft}
and therefore solves the original problem. To argue formally that this is the case, suppose
\end{flushleft}


\begin{flushleft}
that the solution of the relaxed problem does throw away some energy at some step t.
\end{flushleft}


\begin{flushleft}
We then construct a new trajectory, where we do not throw away the extra energy, and
\end{flushleft}


\begin{flushleft}
instead, use the energy to power the wheels, and reduce the engine power. This reduces
\end{flushleft}


\begin{flushleft}
the fuel consumption since the fuel consumption characteristic is increasing, which shows
\end{flushleft}


\begin{flushleft}
that the original could not have been optimal.
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 5
\end{flushleft}





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Basic definitions
\end{flushleft}


\begin{flushleft}
5.1 A simple example. Consider the optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x2 + 1
\end{flushleft}


\begin{flushleft}
(x $-$ 2)(x $-$ 4) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
with variable x $\in$ R.
\end{flushleft}


\begin{flushleft}
(a) Analysis of primal problem. Give the feasible set, the optimal value, and the optimal
\end{flushleft}


\begin{flushleft}
solution.
\end{flushleft}


\begin{flushleft}
(b) Lagrangian and dual function. Plot the objective x2 + 1 versus x. On the same plot,
\end{flushleft}


\begin{flushleft}
show the feasible set, optimal point and value, and plot the Lagrangian L(x, $\lambda$) versus
\end{flushleft}


\begin{flushleft}
x for a few positive values of $\lambda$. Verify the lower bound property (p $\geq$ inf x L(x, $\lambda$)
\end{flushleft}


\begin{flushleft}
for $\lambda$ $\geq$ 0). Derive and sketch the Lagrange dual function g.
\end{flushleft}





\begin{flushleft}
(c) Lagrange dual problem. State the dual problem, and verify that it is a concave
\end{flushleft}


\begin{flushleft}
maximization problem. Find the dual optimal value and dual optimal solution $\lambda$ .
\end{flushleft}


\begin{flushleft}
Does strong duality hold?
\end{flushleft}





\begin{flushleft}
(d) Sensitivity analysis. Let p (u) denote the optimal value of the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x2 + 1
\end{flushleft}


\begin{flushleft}
(x $-$ 2)(x $-$ 4) $\leq$ u,
\end{flushleft}





\begin{flushleft}
as a function of the parameter u. Plot p (u). Verify that dp (0)/du = $-$$\lambda$ .
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The feasible set is the interval [2, 4]. The (unique) optimal point is x = 2, and the
\end{flushleft}


\begin{flushleft}
optimal value is p = 5.
\end{flushleft}


\begin{flushleft}
The plot shows f0 and f1 .
\end{flushleft}


30





25





\begin{flushleft}
f0
\end{flushleft}





20





15





10





5





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
f1
\end{flushleft}





0





$-$5


$-$1





0





1





2





3





4





5





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
(b) The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(x, $\lambda$) = (1 + $\lambda$)x2 $-$ 6$\lambda$x + (1 + 8$\lambda$).
\end{flushleft}





\begin{flushleft}
The plot shows the Lagrangian L(x, $\lambda$) = f0 + $\lambda$f1 as a function of x for different
\end{flushleft}


\begin{flushleft}
values of $\lambda$ $\geq$ 0. Note that the minimum value of L(x, $\lambda$) over x (i.e., g($\lambda$)) is always
\end{flushleft}


\begin{flushleft}
less than p . It increases as $\lambda$ varies from 0 toward 2, reaches its maximum at $\lambda$ = 2,
\end{flushleft}


\begin{flushleft}
and then decreases again as $\lambda$ increases above 2. We have equality p = g($\lambda$) for
\end{flushleft}


\begin{flushleft}
$\lambda$ = 2.
\end{flushleft}





\newpage
5


30





 





\begin{flushleft}
f0 + 3.0f1
\end{flushleft}





\begin{flushleft}
  f0 + 2.0f1
\end{flushleft}


 


✠


 


\begin{flushleft}
  f0 + 1.0f1
\end{flushleft}


✠


 


 


 


✠


 





25





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
Duality
\end{flushleft}





20





15





10





5





❅


■


\begin{flushleft}
❅ f0
\end{flushleft}





0





$-$5


$-$1





0





1





2





3





4





5





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
For $\lambda$ $>$ $-$1, the Lagrangian reaches its minimum at x
\end{flushleft}


\begin{flushleft}
˜ = 3$\lambda$/(1 + $\lambda$). For $\lambda$ $\leq$ $-$1 it
\end{flushleft}


\begin{flushleft}
is unbounded below. Thus
\end{flushleft}


\begin{flushleft}
$-$9$\lambda$2 /(1 + $\lambda$) + 1 + 8$\lambda$
\end{flushleft}


$-$$\infty$





\begin{flushleft}
g($\lambda$) =
\end{flushleft}





\begin{flushleft}
$\lambda$ $>$ $-$1
\end{flushleft}


\begin{flushleft}
$\lambda$ $\leq$ $-$1
\end{flushleft}





\begin{flushleft}
which is plotted below.
\end{flushleft}


6


4


2





\begin{flushleft}
g($\lambda$)
\end{flushleft}





0


$-$2


$-$4


$-$6





\begin{flushleft}
PSfrag replacements
\end{flushleft}





$-$8


$-$10


$-$2





$-$1





0





1





2





3





4





\begin{flushleft}
$\lambda$
\end{flushleft}


\begin{flushleft}
We can verify that the dual function is concave, that its value is equal to p = 5 for
\end{flushleft}


\begin{flushleft}
$\lambda$ = 2, and less than p for other values of $\lambda$.
\end{flushleft}


\begin{flushleft}
(c) The Lagrange dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$9$\lambda$2 /(1 + $\lambda$) + 1 + 8$\lambda$
\end{flushleft}


\begin{flushleft}
$\lambda$ $\geq$ 0.
\end{flushleft}





\begin{flushleft}
The dual optimum occurs at $\lambda$ = 2, with d = 5. So for this example we can directly
\end{flushleft}


\begin{flushleft}
observe that strong duality holds (as it must --- Slater's constraint qualification is
\end{flushleft}


\begin{flushleft}
satisfied).
\end{flushleft}


\begin{flushleft}
(d) The perturbed problem is infeasible for u $<$ $-$1, since inf x (x2 $-$ 6x + 8) = $-$1. For
\end{flushleft}


\begin{flushleft}
u $\geq$ $-$1, the feasible set is the interval
\end{flushleft}


$\surd$


$\surd$


\begin{flushleft}
[3 $-$ 1 + u, 3 + 1 + u],
\end{flushleft}


\begin{flushleft}
given by the$\surd$two roots of x2 $-$ 6x + 8 = u. For $-$1 $\leq$ u $\leq$ 8 the optimum is
\end{flushleft}


\begin{flushleft}
x (u) = 3 $-$ 1 + u. For u $\geq$ 8, the optimum is the unconstrained minimum of f0 ,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
i.e., x (u) = 0. In summary,
\end{flushleft}


$\infty$


$\surd$


\begin{flushleft}
11 + u $-$ 6 1 + u
\end{flushleft}


1





\begin{flushleft}
p (u) =
\end{flushleft}





\begin{flushleft}
u $<$ $-$1
\end{flushleft}


\begin{flushleft}
$-$1 $\leq$ u $\leq$ 8
\end{flushleft}


\begin{flushleft}
u $\geq$ 8.
\end{flushleft}





\begin{flushleft}
The figure shows the optimal value function p (u) and its epigraph.
\end{flushleft}


10





\begin{flushleft}
epi p
\end{flushleft}





8





\begin{flushleft}
p (u)
\end{flushleft}





6





4





2





\begin{flushleft}
PSfrag replacements
\end{flushleft}


0





$-$2


$-$2





\begin{flushleft}
p (0) $-$ $\lambda$ u
\end{flushleft}


0





2





4





6





8





10





\begin{flushleft}
u
\end{flushleft}


\begin{flushleft}
Finally, we note that p (u) is a differentiable function of u, and that
\end{flushleft}


\begin{flushleft}
dp (0)
\end{flushleft}


\begin{flushleft}
= $-$2 = $-$$\lambda$ .
\end{flushleft}


\begin{flushleft}
du
\end{flushleft}


\begin{flushleft}
5.2 Weak duality for unbounded and infeasible problems. The weak duality inequality, d $\leq$ p ,
\end{flushleft}


\begin{flushleft}
clearly holds when d = $-$$\infty$ or p = $\infty$. Show that it holds in the other two cases as
\end{flushleft}


\begin{flushleft}
well: If p = $-$$\infty$, then we must have d = $-$$\infty$, and also, if d = $\infty$, then we must have
\end{flushleft}


\begin{flushleft}
p = $\infty$.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) p = $-$$\infty$. The primal problem is unbounded, i.e., there exist feasible x with
\end{flushleft}


\begin{flushleft}
arbitrarily small values of f0 (x). This means that
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
L(x, $\lambda$) = f0 (x) +
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (x)
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
is unbounded below for all $\lambda$
\end{flushleft}


\begin{flushleft}
0, i.e., g($\lambda$) = $-$$\infty$ for $\lambda$
\end{flushleft}


\begin{flushleft}
problem is infeasible (d = $-$$\infty$).
\end{flushleft}





\begin{flushleft}
0. Therefore the dual
\end{flushleft}





\begin{flushleft}
(b) d = $\infty$. The dual problem is unbounded above. This is only possible if the primal
\end{flushleft}


\begin{flushleft}
problem is infeasible. If it were feasible, with fi (˜
\end{flushleft}


\begin{flushleft}
x) $\leq$ 0 for i = 1, . . . , m, then for all
\end{flushleft}


\begin{flushleft}
$\lambda$ 0,
\end{flushleft}


\begin{flushleft}
g($\lambda$) = inf(f0 (x) +
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (x)) $\leq$ f0 (˜
\end{flushleft}


\begin{flushleft}
x) +
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (˜
\end{flushleft}


\begin{flushleft}
x),
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
so the dual problem is bounded above.
\end{flushleft}


\begin{flushleft}
5.3 Problems with one inequality constraint. Express the dual problem of
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
f (x) $\leq$ 0,
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
with c = 0, in terms of the conjugate f ∗ . Explain why the problem you give is convex.
\end{flushleft}


\begin{flushleft}
We do not assume f is convex.
\end{flushleft}


\begin{flushleft}
Solution. For $\lambda$ = 0, g($\lambda$) = inf cT x = $-$$\infty$. For $\lambda$ $>$ 0,
\end{flushleft}


\begin{flushleft}
g($\lambda$)
\end{flushleft}





=





\begin{flushleft}
inf(cT x + $\lambda$f (x))
\end{flushleft}





=


=





\begin{flushleft}
$\lambda$ inf((c/$\lambda$)T x + $\lambda$f (x))
\end{flushleft}


\begin{flushleft}
$-$$\lambda$f1∗ ($-$c/$\lambda$),
\end{flushleft}





\begin{flushleft}
i.e., for $\lambda$ $\geq$ 0, $-$g is the perspective of f1∗ , evaluated at $-$c/$\lambda$. The dual problem is
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$$\lambda$f1∗ ($-$c/$\lambda$)
\end{flushleft}


\begin{flushleft}
$\lambda$ $\geq$ 0.
\end{flushleft}





\begin{flushleft}
Examples and applications
\end{flushleft}


\begin{flushleft}
5.4 Interpretation of LP dual via relaxed problems. Consider the inequality form LP
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
b,
\end{flushleft}





\begin{flushleft}
with A $\in$ Rm×n , b $\in$ Rm . In this exercise we develop a simple geometric interpretation
\end{flushleft}


\begin{flushleft}
of the dual LP (5.22).
\end{flushleft}


\begin{flushleft}
Let w $\in$ Rm
\end{flushleft}


\begin{flushleft}
b, then it also satisfies the
\end{flushleft}


\begin{flushleft}
+ . If x is feasible for the LP, i.e., satisfies Ax
\end{flushleft}


\begin{flushleft}
inequality
\end{flushleft}


\begin{flushleft}
wT Ax $\leq$ wT b.
\end{flushleft}





\begin{flushleft}
Geometrically, for any w 0, the halfspace Hw = \{x | w T Ax $\leq$ wT b\} contains the feasible
\end{flushleft}


\begin{flushleft}
set for the LP. Therefore if we minimize the objective cT x over the halfspace Hw we get
\end{flushleft}


\begin{flushleft}
a lower bound on p .
\end{flushleft}


\begin{flushleft}
(a) Derive an expression for the minimum value of cT x over the halfspace Hw (which
\end{flushleft}


\begin{flushleft}
will depend on the choice of w 0).
\end{flushleft}


\begin{flushleft}
(b) Formulate the problem of finding the best such bound, by maximizing the lower
\end{flushleft}


\begin{flushleft}
bound over w 0.
\end{flushleft}


\begin{flushleft}
(c) Relate the results of (a) and (b) to the Lagrange dual of the LP, given by (5.22).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The optimal value is
\end{flushleft}


\begin{flushleft}
inf cT x =
\end{flushleft}





\begin{flushleft}
x$\in$Hw
\end{flushleft}





\begin{flushleft}
$\lambda$wT b
\end{flushleft}


$-$$\infty$





\begin{flushleft}
c = $\lambda$AT w for some $\lambda$ $\leq$ 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
(See exercise 4.8.)
\end{flushleft}


\begin{flushleft}
(b) We maximize the lower bound by solving
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\lambda$w T b
\end{flushleft}


\begin{flushleft}
c = $\lambda$AT w
\end{flushleft}


\begin{flushleft}
$\lambda$ $\leq$ 0, w
\end{flushleft}





0





\begin{flushleft}
with variables $\lambda$ and w. Note that, as posed, this is not a convex problem.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(c) Defining z = $-$$\lambda$w, we obtain the equivalent problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT z
\end{flushleft}


\begin{flushleft}
AT z + c = 0
\end{flushleft}


\begin{flushleft}
z 0.
\end{flushleft}





\begin{flushleft}
This is the dual of the original LP.
\end{flushleft}


\begin{flushleft}
5.5 Dual of general LP. Find the dual function of the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Gx h
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}





\begin{flushleft}
Give the dual problem, and make the implicit equality constraints explicit.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(x, $\lambda$, $\nu$)
\end{flushleft}





=


=





\begin{flushleft}
cT x + $\lambda$T (Gx $-$ h) + $\nu$ T (Ax $-$ b)
\end{flushleft}


\begin{flushleft}
(cT + $\lambda$T G + $\nu$ T A)x $-$ h$\lambda$T $-$ $\nu$ T b,
\end{flushleft}





\begin{flushleft}
which is an affine function of x. It follows that the dual function is given by
\end{flushleft}


\begin{flushleft}
$-$$\lambda$T h $-$ $\nu$ T b
\end{flushleft}


$-$$\infty$





\begin{flushleft}
g($\lambda$, $\nu$) = inf L(x, $\lambda$, $\nu$) =
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
c + GT $\lambda$ + AT $\nu$ = 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
(b) The dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
g($\lambda$, $\nu$)
\end{flushleft}


\begin{flushleft}
$\lambda$ 0.
\end{flushleft}





\begin{flushleft}
After making the implicit constraints explicit, we obtain
\end{flushleft}


\begin{flushleft}
$-$$\lambda$T h $-$ $\nu$ T b
\end{flushleft}


\begin{flushleft}
c + G T $\lambda$ + AT $\nu$ = 0
\end{flushleft}


\begin{flushleft}
$\lambda$ 0.
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
5.6 Lower bounds in Chebyshev approximation from least-squares. Consider the Chebyshev
\end{flushleft}


\begin{flushleft}
or $\infty$ -norm approximation problem
\end{flushleft}


\begin{flushleft}
Ax $-$ b
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





$\infty$,





(5.103)





\begin{flushleft}
where A $\in$ Rm×n and rank A = n. Let xch denote an optimal solution (there may be
\end{flushleft}


\begin{flushleft}
multiple optimal solutions; xch denotes one of them).
\end{flushleft}


\begin{flushleft}
The Chebyshev problem has no closed-form solution, but the corresponding least-squares
\end{flushleft}


\begin{flushleft}
problem does. Define
\end{flushleft}


\begin{flushleft}
xls = argmin Ax $-$ b
\end{flushleft}





2





\begin{flushleft}
= (AT A)$-$1 AT b.
\end{flushleft}





\begin{flushleft}
We address the following question. Suppose that for a particular A and b we have computed the least-squares solution xls (but not xch ). How suboptimal is xls for the Chebyshev
\end{flushleft}


\begin{flushleft}
problem? In other words, how much larger is Axls $-$ b $\infty$ than Axch $-$ b $\infty$ ?
\end{flushleft}


\begin{flushleft}
(a) Prove the lower bound
\end{flushleft}


\begin{flushleft}
Axls $-$ b
\end{flushleft}





$\infty$





\begin{flushleft}
m
\end{flushleft}





$\leq$





$\surd$





\begin{flushleft}
m Axch $-$ b
\end{flushleft}





\begin{flushleft}
using the fact that for all z $\in$ R ,
\end{flushleft}





1


$\surd$


\begin{flushleft}
z
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





2





\begin{flushleft}
$\leq$ z
\end{flushleft}





$\infty$





\begin{flushleft}
$\leq$ z
\end{flushleft}





2.





$\infty$,





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
(b) In example 5.6 (page 254) we derived a dual for the general norm approximation
\end{flushleft}


\begin{flushleft}
problem. Applying the results to the $\infty$ -norm (and its dual norm, the 1 -norm), we
\end{flushleft}


\begin{flushleft}
can state the following dual for the Chebyshev approximation problem:
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
bT $\nu$
\end{flushleft}


\begin{flushleft}
$\nu$ 1$\leq$1
\end{flushleft}


\begin{flushleft}
AT $\nu$ = 0.
\end{flushleft}





(5.104)





\begin{flushleft}
Any feasible $\nu$ corresponds to a lower bound bT $\nu$ on Axch $-$ b $\infty$ .
\end{flushleft}


\begin{flushleft}
Denote the least-squares residual as rls = b $-$ Axls . Assuming rls = 0, show that
\end{flushleft}


\begin{flushleft}
$\nu$ˆ = $-$rls / rls
\end{flushleft}





1,





\begin{flushleft}
$\nu$˜ = rls / rls
\end{flushleft}





1,





\begin{flushleft}
are both feasible in (5.104). By duality bT $\nu$ˆ and bT $\nu$˜ are lower bounds on Axch $-$
\end{flushleft}


\begin{flushleft}
b $\infty$ . Which is the better bound? How do these bounds compare with the bound
\end{flushleft}


\begin{flushleft}
derived in part (a)?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Simple manipulation yields
\end{flushleft}


\begin{flushleft}
Axcheb $-$ b
\end{flushleft}





$\infty$





1


\begin{flushleft}
Axcheb $-$ b
\end{flushleft}


$\geq$ $\surd$


\begin{flushleft}
m
\end{flushleft}





2





1


$\geq$ $\surd$


\begin{flushleft}
Axls $-$ b
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





2





1


$\geq$ $\surd$


\begin{flushleft}
Axls $-$ b
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





$\infty$.





\begin{flushleft}
(b) From the expression xls = (AT A)$-$1 AT b we note that
\end{flushleft}


\begin{flushleft}
AT rls = AT (b $-$ A(AT A)$-$1 b) = AT b $-$ AT b = 0.
\end{flushleft}


\begin{flushleft}
Therefore AT $\nu$ˆ = 0 and AT $\nu$˜ = 0. Obviously we also have $\nu$ˆ
\end{flushleft}


\begin{flushleft}
so $\nu$ˆ and $\nu$˜ are dual feasible.
\end{flushleft}


\begin{flushleft}
We can write the dual objective value at $\nu$ˆ as
\end{flushleft}


\begin{flushleft}
bT $\nu$ˆ =
\end{flushleft}





\begin{flushleft}
(Axls $-$ b)T rls
\end{flushleft}


\begin{flushleft}
rls
\end{flushleft}


\begin{flushleft}
$-$bT rls
\end{flushleft}


=


=$-$


\begin{flushleft}
rls 1
\end{flushleft}


\begin{flushleft}
rls 1
\end{flushleft}


\begin{flushleft}
rls
\end{flushleft}





1





\begin{flushleft}
= 1 and $\nu$˜
\end{flushleft}





1





= 1,





2


2


1





\begin{flushleft}
and, similarly,
\end{flushleft}


\begin{flushleft}
bT $\nu$˜ =
\end{flushleft}





\begin{flushleft}
rls
\end{flushleft}


\begin{flushleft}
rls
\end{flushleft}





2


2





.





1





\begin{flushleft}
Therefore $\nu$˜ gives a better bound than $\nu$ˆ.
\end{flushleft}


\begin{flushleft}
Finally, to show that the resulting lower bound is better than the bound in part (a),
\end{flushleft}


\begin{flushleft}
we have to verify that
\end{flushleft}


\begin{flushleft}
rls 22
\end{flushleft}


1


$\geq$ $\surd$


\begin{flushleft}
rls $\infty$ .
\end{flushleft}


\begin{flushleft}
rls 1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
This follows from the inequalities
\end{flushleft}


$\surd$


\begin{flushleft}
x 1 $\leq$ m x 2,
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





$\infty$





\begin{flushleft}
$\leq$ x
\end{flushleft}





2





\begin{flushleft}
which hold for general x $\in$ Rm .
\end{flushleft}


\begin{flushleft}
5.7 Piecewise-linear minimization. We consider the convex piecewise-linear minimization
\end{flushleft}


\begin{flushleft}
problem
\end{flushleft}


(5.105)


\begin{flushleft}
minimize maxi=1,...,m (aTi x + bi )
\end{flushleft}


\begin{flushleft}
with variable x $\in$ Rn .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) Derive a dual problem, based on the Lagrange dual of the equivalent problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
maxi=1,...,m yi
\end{flushleft}


\begin{flushleft}
aTi x + bi = yi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
with variables x $\in$ Rn , y $\in$ Rm .
\end{flushleft}





\begin{flushleft}
(b) Formulate the piecewise-linear minimization problem (5.105) as an LP, and form the
\end{flushleft}


\begin{flushleft}
dual of the LP. Relate the LP dual to the dual obtained in part (a).
\end{flushleft}


\begin{flushleft}
(c) Suppose we approximate the objective function in (5.105) by the smooth function
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
exp(aTi x + bi )
\end{flushleft}





\begin{flushleft}
f0 (x) = log
\end{flushleft}





,





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
and solve the unconstrained geometric program
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log
\end{flushleft}





\begin{flushleft}
exp(aTi x + bi ) .
\end{flushleft}





(5.106)





\begin{flushleft}
A dual of this problem is given by (5.62). Let ppwl and pgp be the optimal values
\end{flushleft}


\begin{flushleft}
of (5.105) and (5.106), respectively. Show that
\end{flushleft}


\begin{flushleft}
0 $\leq$ pgp $-$ ppwl $\leq$ log m.
\end{flushleft}


\begin{flushleft}
(d) Derive similar bounds for the difference between ppwl and the optimal value of
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(1/$\gamma$) log
\end{flushleft}





\begin{flushleft}
exp($\gamma$(aTi x + bi )) ,
\end{flushleft}





\begin{flushleft}
where $\gamma$ $>$ 0 is a parameter. What happens as we increase $\gamma$?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The dual function is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g($\lambda$) = inf
\end{flushleft}





\begin{flushleft}
max yi +
\end{flushleft}





\begin{flushleft}
x,y
\end{flushleft}





\begin{flushleft}
i=1,...,m
\end{flushleft}





\begin{flushleft}
The infimum over x is finite only if
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
inf (max yi $-$ $\lambda$T y) =
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
To prove this, we first note that if $\lambda$
\end{flushleft}


\begin{flushleft}
$\lambda$T y =
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$j y j $\leq$
\end{flushleft}





\begin{flushleft}
$\lambda$i (aTi x + bi $-$ yi )
\end{flushleft}





.





\begin{flushleft}
$\lambda$i ai = 0. To minimize over y we note that
\end{flushleft}


0


$-$$\infty$





\begin{flushleft}
$\lambda$ 0, 1T $\lambda$ = 1
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
0, 1T $\lambda$ = 1, then
\end{flushleft}


\begin{flushleft}
$\lambda$j max yi = max yi ,
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
with equality if y = 0, so in that case
\end{flushleft}


\begin{flushleft}
inf (max yi $-$ $\lambda$T y) = 0.
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
If $\lambda$
\end{flushleft}


\begin{flushleft}
0, say $\lambda$j $<$ 0, then choosing yi = 0, i = j, and yj = $-$t, with t $\geq$ 0, and
\end{flushleft}


\begin{flushleft}
letting t go to infinity, gives
\end{flushleft}


\begin{flushleft}
max yi $-$ $\lambda$T y = 0 + t$\lambda$k $\rightarrow$ $-$$\infty$.
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
Finally, if 1T $\lambda$ = 1, choosing y = t1, gives
\end{flushleft}


\begin{flushleft}
max yi $-$ $\lambda$T y = t(1 $-$ 1T $\lambda$) $\rightarrow$ $-$$\infty$,
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
if t $\rightarrow$ $\infty$ and 1 $<$ 1 $\lambda$, or if t $\rightarrow$ $-$$\infty$ and 1 $>$ 1T $\lambda$.
\end{flushleft}


\begin{flushleft}
Summing up, we have
\end{flushleft}


\begin{flushleft}
bT $\lambda$
\end{flushleft}


$-$$\infty$





\begin{flushleft}
g($\lambda$) =
\end{flushleft}





\begin{flushleft}
$\lambda$ a = 0,
\end{flushleft}


\begin{flushleft}
i i i
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
$\lambda$
\end{flushleft}





0,





\begin{flushleft}
1T $\lambda$ = 1
\end{flushleft}





\begin{flushleft}
The resulting dual problem is
\end{flushleft}


\begin{flushleft}
bT $\lambda$
\end{flushleft}


\begin{flushleft}
AT $\lambda$ = 0
\end{flushleft}


\begin{flushleft}
1T $\lambda$ = 1
\end{flushleft}


\begin{flushleft}
$\lambda$ 0.
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(b) The problem is equivalent to the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
Ax + b
\end{flushleft}





\begin{flushleft}
t1.
\end{flushleft}





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
bT z
\end{flushleft}


\begin{flushleft}
AT z = 0,
\end{flushleft}





\begin{flushleft}
1T z = 1,
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





0,





\begin{flushleft}
which is identical to the dual derived in (a).
\end{flushleft}


\begin{flushleft}
(c) Suppose z is dual optimal for the dual GP (5.62),
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
bT z $-$ i=1 zi log zi
\end{flushleft}


\begin{flushleft}
1T z = 1
\end{flushleft}


\begin{flushleft}
AT z = 0.
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
Then z is also feasible for the dual of the piecewise-linear formulation, with objective
\end{flushleft}


\begin{flushleft}
value
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
bT z = pgp +
\end{flushleft}





\begin{flushleft}
zi log zi .
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
This provides a lower bound on ppwl :
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
ppwl $\geq$ pgp +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
zi log zi $\geq$ pgp $-$ log m.
\end{flushleft}





\begin{flushleft}
The bound follows from
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
1T z=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
zi log zi = $-$ log m.
\end{flushleft}





\begin{flushleft}
On the other hand, we also have
\end{flushleft}


\begin{flushleft}
max(aTi x + bi ) $\leq$ log
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
exp(aTi x + bi )
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
for all x, and therefore ppwl $\leq$ pgp .
\end{flushleft}


\begin{flushleft}
In conclusion,
\end{flushleft}


\begin{flushleft}
pgp $-$ log m $\leq$ ppwl $\leq$ pgp .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(d) We first reformulate the problem as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(1/$\gamma$) log i=1 exp($\gamma$yi )
\end{flushleft}


\begin{flushleft}
Ax + b = y.
\end{flushleft}





\begin{flushleft}
The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(x, y, z) =
\end{flushleft}





1


\begin{flushleft}
log
\end{flushleft}


\begin{flushleft}
$\gamma$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
exp($\gamma$yi ) + z T (Ax + b $-$ y).
\end{flushleft}





\begin{flushleft}
L is bounded below as a function of x only if AT z = 0. To find the optimum over
\end{flushleft}


\begin{flushleft}
y, we set the gradient equal to zero:
\end{flushleft}


\begin{flushleft}
e$\gamma$yi
\end{flushleft}


\begin{flushleft}
= zi .
\end{flushleft}


\begin{flushleft}
e$\gamma$yi
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
This is solvable for yi if 1T z = 1 and z
\end{flushleft}





\begin{flushleft}
0. The Lagrange dual function is
\end{flushleft}





1


\begin{flushleft}
g(z) = b z $-$
\end{flushleft}


\begin{flushleft}
$\gamma$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
zi log zi ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
and the dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
bT z $-$ (1/$\gamma$)
\end{flushleft}


\begin{flushleft}
AT z = 0
\end{flushleft}


\begin{flushleft}
1T z = 1.
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
zi log zi
\end{flushleft}





\begin{flushleft}
Let pgp ($\gamma$) be the optimal value of the GP. Following the same argument as above,
\end{flushleft}


\begin{flushleft}
we can conclude that
\end{flushleft}


\begin{flushleft}
pgp ($\gamma$) $-$
\end{flushleft}





1


\begin{flushleft}
log m $\leq$ ppwl $\leq$ pgp ($\gamma$).
\end{flushleft}


\begin{flushleft}
$\gamma$
\end{flushleft}





\begin{flushleft}
In other words, pgp ($\gamma$) approaches ppwl as $\gamma$ increases.
\end{flushleft}


\begin{flushleft}
5.8 Relate the two dual problems derived in example 5.9 on page 257.
\end{flushleft}


\begin{flushleft}
Solution. Suppose for example that $\nu$ is feasible in (5.71). Then choosing $\lambda$1 = (AT $\nu$ +
\end{flushleft}


\begin{flushleft}
c)$-$ and $\lambda$2 = (AT $\nu$ + c)+ , yields a feasible solution in (5.69), with the same objective
\end{flushleft}


\begin{flushleft}
value. Conversely, suppose $\nu$, $\lambda$1 and $\lambda$2 are feasible in (5.69). The equality constraint
\end{flushleft}


\begin{flushleft}
implies that
\end{flushleft}


\begin{flushleft}
$\lambda$1 = (AT $\nu$ + c)$-$ + v, $\lambda$2 = (AT $\nu$ + c)+ + v,
\end{flushleft}


\begin{flushleft}
for some v 0. Therefore we can write (5.69) as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT $\nu$ $-$ uT (AT $\nu$ + c)$-$ + lT (AT $\nu$ + c)+ $-$ (u $-$ l)T v
\end{flushleft}


\begin{flushleft}
v 0,
\end{flushleft}





\begin{flushleft}
and it is clear that at the optimum v = 0. Therefore the optimum $\nu$ in (5.69) is also
\end{flushleft}


\begin{flushleft}
optimal in (5.71).
\end{flushleft}


\begin{flushleft}
5.9 Suboptimality of a simple covering ellipsoid. Recall the problem of determining the minimum volume ellipsoid, centered at the origin, that contains the points a1 , . . . , am $\in$ Rn
\end{flushleft}


\begin{flushleft}
(problem (5.14), page 222):
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (X) = log det(X $-$1 )
\end{flushleft}


\begin{flushleft}
aTi Xai $\leq$ 1, i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
with dom f0 = Sn
\end{flushleft}


\begin{flushleft}
++ . We assume that the vectors a1 , . . . , am span R (which implies that
\end{flushleft}


\begin{flushleft}
the problem is bounded below).
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
(a) Show that the matrix
\end{flushleft}


$-$1





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
Xsim =
\end{flushleft}





\begin{flushleft}
ak aTk
\end{flushleft}





,





\begin{flushleft}
ai
\end{flushleft}


1





0,





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
is feasible. Hint. Show that
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
a aT
\end{flushleft}


\begin{flushleft}
k=1 k k
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
ai
\end{flushleft}





\begin{flushleft}
and use Schur complements (§A.5.5) to prove that aTi Xai $\leq$ 1 for i = 1, . . . , m.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
a aT
\end{flushleft}


\begin{flushleft}
k=1 k k
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
ai
\end{flushleft}





\begin{flushleft}
ak
\end{flushleft}


1





\begin{flushleft}
k=i
\end{flushleft}





=





\begin{flushleft}
ak aTk
\end{flushleft}





0


0





0





+





\begin{flushleft}
ai
\end{flushleft}


1





\begin{flushleft}
ai
\end{flushleft}


1





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
is the sum of two positive semidefinite matrices, hence positive semidefinite. The
\end{flushleft}


\begin{flushleft}
Schur complement of the 1, 1 block of this matrix is therefore also positive semidefinite:
\end{flushleft}


$-$1





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
1 $-$ aTi
\end{flushleft}





\begin{flushleft}
ak aTk
\end{flushleft}





\begin{flushleft}
ai $\geq$ 0,
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
which is the desired conclusion.
\end{flushleft}


\begin{flushleft}
(b) Now we establish a bound on how suboptimal the feasible point Xsim is, via the dual
\end{flushleft}


\begin{flushleft}
problem,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
$\lambda$ a aT $-$ 1 T $\lambda$ + n
\end{flushleft}


\begin{flushleft}
maximize log det
\end{flushleft}


\begin{flushleft}
i=1 i i i
\end{flushleft}


\begin{flushleft}
subject to $\lambda$ 0,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
0. (This dual is derived on page 222.)
\end{flushleft}


\begin{flushleft}
with the implicit constraint
\end{flushleft}


\begin{flushleft}
$\lambda$ a aT
\end{flushleft}


\begin{flushleft}
i=1 i i i
\end{flushleft}


\begin{flushleft}
To derive a bound, we restrict our attention to dual variables of the form $\lambda$ = t1,
\end{flushleft}


\begin{flushleft}
where t $>$ 0. Find (analytically) the optimal value of t, and evaluate the dual
\end{flushleft}


\begin{flushleft}
objective at this $\lambda$. Use this to prove that the volume of the ellipsoid \{u | uT Xsim u $\leq$
\end{flushleft}


\begin{flushleft}
1\} is no more than a factor (m/n)n/2 more than the volume of the minimum volume
\end{flushleft}


\begin{flushleft}
ellipsoid.
\end{flushleft}


\begin{flushleft}
Solution. The dual function evaluated at $\lambda$ = t1 is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
ai aTi
\end{flushleft}





\begin{flushleft}
g($\lambda$) = log det
\end{flushleft}





\begin{flushleft}
+ n log t $-$ mt + n.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Now we'll maximize over t $>$ 0 to get the best lower bound. Setting the derivative
\end{flushleft}


\begin{flushleft}
with respect to t equal to zero yields the optimal value t = n/m. Using this $\lambda$ we
\end{flushleft}


\begin{flushleft}
get the dual objective value
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
ai aTi
\end{flushleft}





\begin{flushleft}
g($\lambda$) = log det
\end{flushleft}





\begin{flushleft}
+ n log(n/m).
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
The primal objective value for X = Xsim is given by
\end{flushleft}


$-$1





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$ log det
\end{flushleft}





\begin{flushleft}
ai aTi
\end{flushleft}





,





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
so the duality gap associated with Xsim and $\lambda$ is n log(m/n). (Recall that m $\geq$
\end{flushleft}


\begin{flushleft}
n, by our assumption that a1 , . . . , am span Rn .) It follows that, in terms of the
\end{flushleft}


\begin{flushleft}
objective function, Xsim is no more than n log(m/n) suboptimal. The volume V of
\end{flushleft}


\begin{flushleft}
the ellipsoid E associated with the matrix X is given by V = exp($-$O/2), where O
\end{flushleft}


\begin{flushleft}
is the associated objective function, O = $-$ log det X. The bound follows.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
5.10 Optimal experiment design. The following problems arise in experiment design (see §7.5).
\end{flushleft}


\begin{flushleft}
(a) D-optimal design.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
xi vi viT
\end{flushleft}


\begin{flushleft}
1 x = 1.
\end{flushleft}





\begin{flushleft}
log det
\end{flushleft}


\begin{flushleft}
x 0,
\end{flushleft}





$-$1





\begin{flushleft}
(b) A-optimal design.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





$-$1





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





0,





\begin{flushleft}
xi vi viT
\end{flushleft}


\begin{flushleft}
1T x = 1.
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
The domain of both problems is \{x |
\end{flushleft}


\begin{flushleft}
x v vT
\end{flushleft}


\begin{flushleft}
0\}. The variable is x $\in$ Rp ; the
\end{flushleft}


\begin{flushleft}
i=1 i i i
\end{flushleft}


\begin{flushleft}
vectors v1 , . . . , vp $\in$ Rn are given.
\end{flushleft}


\begin{flushleft}
Derive dual problems by first introducing a new variable X $\in$ Sn and an equality conp
\end{flushleft}


\begin{flushleft}
straint X =
\end{flushleft}


\begin{flushleft}
x v v T , and then applying Lagrange duality. Simplify the dual probi=1 i i i
\end{flushleft}


\begin{flushleft}
lems as much as you can.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) D-optimal design.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log det(X $-$1 )
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
X = i=1 xi vi viT
\end{flushleft}


\begin{flushleft}
x 0, 1T x = 1.
\end{flushleft}





\begin{flushleft}
The Lagrangian is
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
L(x, Z, z, $\nu$)
\end{flushleft}





=





=





\begin{flushleft}
log det(X $-$1 ) + tr(ZX) $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
log det(X $-$1 ) + tr(ZX) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
xi viT Zvi $-$ z T x + $\nu$(1T x $-$ 1)
\end{flushleft}


\begin{flushleft}
xi ($-$viT Zvi $-$ zi + $\nu$) $-$ $\nu$.
\end{flushleft}





\begin{flushleft}
The minimum over xi is bounded below only if $\nu$ $-$ viT Zvi = zi . Setting the gradient
\end{flushleft}


\begin{flushleft}
with respect to X equal to zero gives X $-$1 = Z. We obtain the dual function
\end{flushleft}


\begin{flushleft}
g(Z, z) =
\end{flushleft}





\begin{flushleft}
log det Z + n $-$ $\nu$
\end{flushleft}


$-$$\infty$





\begin{flushleft}
$\nu$ $-$ viT Zvi = zi ,
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p
\end{flushleft}





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log det Z + n $-$ $\nu$
\end{flushleft}


\begin{flushleft}
viT Zvi $\leq$ $\nu$, i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
with domain Sn
\end{flushleft}


\begin{flushleft}
++ × R. We can eliminate $\nu$ by first making a change of variables
\end{flushleft}


\begin{flushleft}
W = (1/$\nu$)Z, which gives
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log det W + n + n log $\nu$ $-$ $\nu$
\end{flushleft}


\begin{flushleft}
ˆ vi $\leq$ 1, i = 1, . . . , p.
\end{flushleft}


\begin{flushleft}
viT W
\end{flushleft}





\begin{flushleft}
Finally, we note that we can easily optimize n log $\nu$ $-$ $\nu$ over $\nu$. The optimum is
\end{flushleft}


\begin{flushleft}
$\nu$ = n, and substituting gives
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log det W + n log n
\end{flushleft}


\begin{flushleft}
viT W vi $\leq$ 1, i = 1, . . . , p.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
(b) A-optimal design.
\end{flushleft}


\begin{flushleft}
tr(X $-$1 )
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
X=
\end{flushleft}


\begin{flushleft}
x v vT
\end{flushleft}


\begin{flushleft}
i=1 i i i
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
x 0, 1 x = 1.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





$-$1





\begin{flushleft}
The Lagrangian is
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
L(X, Z, z, $\nu$)
\end{flushleft}





\begin{flushleft}
tr(X $-$1 ) + tr(ZX) $-$
\end{flushleft}





=





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
tr(X $-$1 ) + tr(ZX) +
\end{flushleft}





=





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
xi viT Zvi $-$ z T x + $\nu$(1T x $-$ 1)
\end{flushleft}


\begin{flushleft}
xi ($-$viT Zvi $-$ zi + $\nu$) $-$ $\nu$.
\end{flushleft}





\begin{flushleft}
The minimum over x is unbounded below unless viT Zvi + zi = $\nu$. The minimum
\end{flushleft}


\begin{flushleft}
over X can be found by setting the gradient equal to zero: X $-$2 = Z, or X = Z $-$1/2
\end{flushleft}


\begin{flushleft}
if Z 0, which gives
\end{flushleft}


\begin{flushleft}
2 tr(Z 1/2 )
\end{flushleft}


$-$$\infty$





\begin{flushleft}
inf (tr(X $-$1 ) + tr(ZX)) =
\end{flushleft}





\begin{flushleft}
X
\end{flushleft}





0





\begin{flushleft}
Z 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The dual function is
\end{flushleft}


\begin{flushleft}
$-$$\nu$ + 2 tr(Z 1/2 )
\end{flushleft}


$-$$\infty$





\begin{flushleft}
g(Z, z, $\nu$) =
\end{flushleft}





\begin{flushleft}
Z 0, viT Zvi + zi = $\nu$
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
$-$$\nu$ + 2 tr(Z 1/2 )
\end{flushleft}


\begin{flushleft}
viT Zvi $\leq$ nu, i = 1, . . . , p
\end{flushleft}


\begin{flushleft}
Z 0.
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
As a first simplification, we define W = (1/$\nu$)Z, and write the problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





$\surd$


\begin{flushleft}
$-$$\nu$ + 2 $\nu$ tr(W 1/2 )
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
vi W vi $\leq$ 1, i = 1, . . . , p
\end{flushleft}


\begin{flushleft}
W
\end{flushleft}


0.





\begin{flushleft}
By optimizing over $\nu$ $>$ 0, we obtain
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
(tr(W 1/2 ))2
\end{flushleft}


\begin{flushleft}
viT W vi $\leq$ 1,
\end{flushleft}


\begin{flushleft}
W
\end{flushleft}


0.





\begin{flushleft}
i = 1, . . . , p
\end{flushleft}





\begin{flushleft}
5.11 Derive a dual problem for
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
Ai x + bi
\end{flushleft}





2





\begin{flushleft}
+ (1/2) x $-$ x0
\end{flushleft}





2


2.





\begin{flushleft}
The problem data are Ai $\in$ Rmi ×n , bi $\in$ Rmi , and x0 $\in$ Rn . First introduce new variables
\end{flushleft}


\begin{flushleft}
yi $\in$ Rmi and equality constraints yi = Ai x + bi .
\end{flushleft}


\begin{flushleft}
Solution. The Lagrangian is
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
L(x, z1 , . . . , zN ) =
\end{flushleft}





\begin{flushleft}
yi
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





2 +





1


\begin{flushleft}
x $-$ x0
\end{flushleft}


2





\begin{flushleft}
N
\end{flushleft}


2


2





$-$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ziT (yi $-$ Ai x $-$ bi ).
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We first minimize over yi . We have
\end{flushleft}


\begin{flushleft}
inf ( yi
\end{flushleft}


\begin{flushleft}
yi
\end{flushleft}





2





\begin{flushleft}
zi 2 $\leq$ 1
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





0


$-$$\infty$





\begin{flushleft}
+ ziT yi ) =
\end{flushleft}





\begin{flushleft}
(If zi 2 $>$ 1, choose yi = $-$tzi and let t $\rightarrow$ $\infty$, to show that the function is unbounded
\end{flushleft}


\begin{flushleft}
below. If zi 2 $\leq$ 1, it follows from the Cauchy-Schwarz inequality that yi 2 + ziT yi $\geq$ 0,
\end{flushleft}


\begin{flushleft}
so the minimum is reached when yi = 0.)
\end{flushleft}


\begin{flushleft}
We can minimize over x by setting the gradient with respect to x equal to zero. This
\end{flushleft}


\begin{flushleft}
yields
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
ATi z.
\end{flushleft}





\begin{flushleft}
x = x0 +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Substituting in the Lagrangian gives the dual function
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
(Ai x0
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
g(z1 , . . . , zN ) =
\end{flushleft}





$-$$\infty$





\begin{flushleft}
+ b i )T z i $-$
\end{flushleft}





1


2





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ATi zi
\end{flushleft}





2


2





\begin{flushleft}
zi 2 $\leq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
(Ai x0
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
zi
\end{flushleft}





2





\begin{flushleft}
+ b i )T z i $-$
\end{flushleft}





1


2





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ATi zi
\end{flushleft}





2





\begin{flushleft}
$\leq$ 1, i = 1, . . . , N.
\end{flushleft}





\begin{flushleft}
5.12 Analytic centering. Derive a dual problem for
\end{flushleft}


$-$





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(bi $-$ aTi x)
\end{flushleft}





\begin{flushleft}
with domain \{x | aTi x $<$ bi , i = 1, . . . , m\}. First introduce new variables yi and equality
\end{flushleft}


\begin{flushleft}
constraints yi = bi $-$ aTi x.
\end{flushleft}


\begin{flushleft}
(The solution of this problem is called the analytic center of the linear inequalities a Ti x $\leq$
\end{flushleft}


\begin{flushleft}
bi , i = 1, . . . , m. Analytic centers have geometric applications (see §8.5.3), and play an
\end{flushleft}


\begin{flushleft}
important role in barrier methods (see chapter 11).)
\end{flushleft}


\begin{flushleft}
Solution. We derive the dual of the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$ i=1 log yi
\end{flushleft}


\begin{flushleft}
y = b $-$ Ax,
\end{flushleft}





\begin{flushleft}
where A $\in$ Rm×n has aTi as its ith row. The Lagrangian is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
L(x, y, $\nu$) = $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log yi + $\nu$ T (y $-$ b + Ax)
\end{flushleft}





\begin{flushleft}
and the dual function is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g($\nu$) = inf
\end{flushleft}





\begin{flushleft}
x,y
\end{flushleft}





$-$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log yi + $\nu$ T (y $-$ b + Ax)
\end{flushleft}





.





\begin{flushleft}
The term $\nu$ T Ax is unbounded below as a function of x unless AT $\nu$ = 0. The terms in y
\end{flushleft}


\begin{flushleft}
are unbounded below if $\nu$
\end{flushleft}


\begin{flushleft}
0, and achieve their minimum for yi = 1/$\nu$i otherwise. We
\end{flushleft}


\begin{flushleft}
therefore find the dual function
\end{flushleft}


\begin{flushleft}
g($\nu$) =
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





$-$$\infty$





\begin{flushleft}
log $\nu$i + m $-$ bT $\nu$
\end{flushleft}





\begin{flushleft}
AT $\nu$ = 0,
\end{flushleft}


\begin{flushleft}
otherwise
\end{flushleft}





\begin{flushleft}
and the dual problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
log $\nu$i $-$ bT $\nu$ + m
\end{flushleft}


\begin{flushleft}
A $\nu$ = 0.
\end{flushleft}





\begin{flushleft}
$\nu$
\end{flushleft}





0





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
5.13 Lagrangian relaxation of Boolean LP. A Boolean linear program is an optimization problem of the form
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
subject to Ax b
\end{flushleft}


\begin{flushleft}
xi $\in$ \{0, 1\}, i = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
and is, in general, very difficult to solve. In exercise 4.15 we studied the LP relaxation of
\end{flushleft}


\begin{flushleft}
this problem,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


(5.107)


\begin{flushleft}
subject to Ax b
\end{flushleft}


\begin{flushleft}
0 $\leq$ xi $\leq$ 1, i = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
which is far easier to solve, and gives a lower bound on the optimal value of the Boolean
\end{flushleft}


\begin{flushleft}
LP. In this problem we derive another lower bound for the Boolean LP, and work out the
\end{flushleft}


\begin{flushleft}
relation between the two lower bounds.
\end{flushleft}


\begin{flushleft}
(a) Lagrangian relaxation. The Boolean LP can be reformulated as the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax b
\end{flushleft}


\begin{flushleft}
xi (1 $-$ xi ) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
which has quadratic equality constraints. Find the Lagrange dual of this problem.
\end{flushleft}


\begin{flushleft}
The optimal value of the dual problem (which is convex) gives a lower bound on
\end{flushleft}


\begin{flushleft}
the optimal value of the Boolean LP. This method of finding a lower bound on the
\end{flushleft}


\begin{flushleft}
optimal value is called Lagrangian relaxation.
\end{flushleft}


\begin{flushleft}
(b) Show that the lower bound obtained via Lagrangian relaxation, and via the LP
\end{flushleft}


\begin{flushleft}
relaxation (5.107), are the same. Hint. Derive the dual of the LP relaxation (5.107).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(x, $\mu$, $\nu$)
\end{flushleft}





\begin{flushleft}
cT x + $\mu$T (Ax $-$ b) $-$ $\nu$ T x + xT diag($\nu$)x
\end{flushleft}





=





\begin{flushleft}
xT diag($\nu$)x + (c + AT $\mu$ $-$ $\nu$)T x $-$ bT $\mu$.
\end{flushleft}





=





\begin{flushleft}
Minimizing over x gives the dual function
\end{flushleft}


\begin{flushleft}
g($\mu$, $\nu$) =
\end{flushleft}





\begin{flushleft}
$-$bT $\mu$ $-$ (1/4)
\end{flushleft}


$-$$\infty$





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(c
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}





\begin{flushleft}
+ aTi $\mu$ $-$ $\nu$i )2 /$\nu$i
\end{flushleft}





\begin{flushleft}
$\nu$ 0
\end{flushleft}


\begin{flushleft}
otherwise
\end{flushleft}





\begin{flushleft}
where ai is the ith column of A, and we adopt the convention that a2 /0 = $\infty$ if
\end{flushleft}


\begin{flushleft}
a = 0, and a2 /0 = 0 if a = 0.
\end{flushleft}


\begin{flushleft}
The resulting dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT $\mu$ $-$ (1/4)
\end{flushleft}


\begin{flushleft}
$\nu$ 0.
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(c
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}





\begin{flushleft}
+ aTi $\mu$ $-$ $\nu$i )2 /$\nu$i
\end{flushleft}





\begin{flushleft}
In order to simplify this dual, we optimize analytically over $\nu$, by noting that
\end{flushleft}


\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
$\nu$i $\geq$0
\end{flushleft}





$-$





\begin{flushleft}
(ci + aTi $\mu$ $-$ $\nu$i )2
\end{flushleft}


\begin{flushleft}
$\nu$i
\end{flushleft}





=


=





\begin{flushleft}
(ci + aTi $\mu$)
\end{flushleft}


0





\begin{flushleft}
ci + aTi $\mu$ $\leq$ 0
\end{flushleft}


\begin{flushleft}
ci + aTi $\mu$ $\geq$ 0
\end{flushleft}





\begin{flushleft}
min\{0, (ci + aTi $\mu$)\}.
\end{flushleft}





\begin{flushleft}
This allows us to eliminate $\nu$ from the dual problem, and simplify it as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT $\mu$ +
\end{flushleft}


\begin{flushleft}
$\mu$ 0.
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
min\{0, ci + aTi $\mu$\}
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) We follow the hint. The Lagrangian and dual function of the LP relaxation re
\end{flushleft}


\begin{flushleft}
L(x, u, v, w)
\end{flushleft}





\begin{flushleft}
cT x + uT (Ax $-$ b) $-$ v T x + wT (x $-$ 1)
\end{flushleft}





=





\begin{flushleft}
(c + AT u $-$ v + w)T x $-$ bT u $-$ 1T w
\end{flushleft}





=


\begin{flushleft}
g(u, v, w)
\end{flushleft}





\begin{flushleft}
$-$bT u $-$ 1T w
\end{flushleft}


$-$$\infty$





=





\begin{flushleft}
AT u $-$ v + w + c = 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT u $-$ 1T w
\end{flushleft}


\begin{flushleft}
AT u $-$ v + w + c = 0
\end{flushleft}


\begin{flushleft}
u 0, v 0, w 0,
\end{flushleft}





\begin{flushleft}
which is equivalent to the Lagrange relaxation problem derived above. We conclude
\end{flushleft}


\begin{flushleft}
that the two relaxations give the same value.
\end{flushleft}


\begin{flushleft}
5.14 A penalty method for equality constraints. We consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
Ax = b,
\end{flushleft}





(5.108)





\begin{flushleft}
where f0 : Rn $\rightarrow$ R is convex and differentiable, and A $\in$ Rm×n with rank A = m.
\end{flushleft}


\begin{flushleft}
In a quadratic penalty method, we form an auxiliary function
\end{flushleft}


\begin{flushleft}
$\phi$(x) = f (x) + $\alpha$ Ax $-$ b 22 ,
\end{flushleft}


\begin{flushleft}
where $\alpha$ $>$ 0 is a parameter. This auxiliary function consists of the objective plus the
\end{flushleft}


\begin{flushleft}
˜, should
\end{flushleft}


\begin{flushleft}
penalty term $\alpha$ Ax $-$ b 22 . The idea is that a minimizer of the auxiliary function, x
\end{flushleft}


\begin{flushleft}
be an approximate solution of the original problem. Intuition suggests that the larger the
\end{flushleft}


\begin{flushleft}
penalty weight $\alpha$, the better the approximation x
\end{flushleft}


\begin{flushleft}
˜ to a solution of the original problem.
\end{flushleft}


\begin{flushleft}
Suppose x
\end{flushleft}


\begin{flushleft}
˜ is a minimizer of $\phi$. Show how to find, from x
\end{flushleft}


\begin{flushleft}
˜, a dual feasible point for (5.108).
\end{flushleft}


\begin{flushleft}
Find the corresponding lower bound on the optimal value of (5.108).
\end{flushleft}


\begin{flushleft}
Solution. If x
\end{flushleft}


\begin{flushleft}
˜ minimizes $\phi$, then
\end{flushleft}


\begin{flushleft}
$\nabla$f0 (˜
\end{flushleft}


\begin{flushleft}
x) + 2$\alpha$AT (A˜
\end{flushleft}


\begin{flushleft}
x $-$ b) = 0.
\end{flushleft}


\begin{flushleft}
Therefore x
\end{flushleft}


\begin{flushleft}
˜ is also a minimizer of
\end{flushleft}


\begin{flushleft}
f0 (x) + $\nu$ T (Ax $-$ b)
\end{flushleft}


\begin{flushleft}
where $\nu$ = 2$\alpha$(A˜
\end{flushleft}


\begin{flushleft}
x $-$ b). Therefore $\nu$ is dual feasible with
\end{flushleft}


\begin{flushleft}
g($\nu$)
\end{flushleft}





=





\begin{flushleft}
inf (f0 (x) + $\nu$ T (Ax $-$ b))
\end{flushleft}





=





\begin{flushleft}
f0 (˜
\end{flushleft}


\begin{flushleft}
x) + 2$\alpha$ A˜
\end{flushleft}


\begin{flushleft}
x $-$ b 22 .
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
Therefore,
\end{flushleft}


\begin{flushleft}
for all x that satisfy Ax = b.
\end{flushleft}





\begin{flushleft}
f0 (x) $\geq$ f0 (˜
\end{flushleft}


\begin{flushleft}
x) + 2$\alpha$ A˜
\end{flushleft}


\begin{flushleft}
x$-$b
\end{flushleft}





2


2





\begin{flushleft}
5.15 Consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





(5.109)





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
where the functions fi : Rn $\rightarrow$ R are differentiable and convex. Let h1 , . . . , hm : R $\rightarrow$ R
\end{flushleft}


\begin{flushleft}
be increasing differentiable convex functions. Show that
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
hi (fi (x))
\end{flushleft}





\begin{flushleft}
$\phi$(x) = f0 (x) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
is convex. Suppose x
\end{flushleft}


\begin{flushleft}
˜ minimizes $\phi$. Show how to find from x
\end{flushleft}


\begin{flushleft}
˜ a feasible point for the dual
\end{flushleft}


\begin{flushleft}
of (5.109). Find the corresponding lower bound on the optimal value of (5.109).
\end{flushleft}


\begin{flushleft}
Solution. x
\end{flushleft}


\begin{flushleft}
˜ satisfies
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
0 = $\nabla$f0 (˜
\end{flushleft}


\begin{flushleft}
x) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
x)))$\nabla$fi (˜
\end{flushleft}


\begin{flushleft}
x)) = $\nabla$f0 (˜
\end{flushleft}


\begin{flushleft}
x) +
\end{flushleft}


\begin{flushleft}
(hi (fi (˜
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i $\nabla$fi (˜
\end{flushleft}


\begin{flushleft}
x))
\end{flushleft}





\begin{flushleft}
where $\lambda$i = hi (fi (˜
\end{flushleft}


\begin{flushleft}
x)). $\lambda$ is dual feasible: $\lambda$i $\geq$ 0, since hi is increasing, and
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g($\lambda$)
\end{flushleft}





=





\begin{flushleft}
f0 (˜
\end{flushleft}


\begin{flushleft}
x) +
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (˜
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
hi (fi (˜
\end{flushleft}


\begin{flushleft}
x))fi (˜
\end{flushleft}


\begin{flushleft}
x).
\end{flushleft}





\begin{flushleft}
f0 (˜
\end{flushleft}


\begin{flushleft}
x) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
5.16 An exact penalty method for inequality constraints. Consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





(5.110)





\begin{flushleft}
where the functions fi : Rn $\rightarrow$ R are differentiable and convex. In an exact penalty
\end{flushleft}


\begin{flushleft}
method, we solve the auxiliary problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$\phi$(x) = f0 (x) + $\alpha$ maxi=1,...,m max\{0, fi (x)\},
\end{flushleft}





(5.111)





\begin{flushleft}
where $\alpha$ $>$ 0 is a parameter. The second term in $\phi$ penalizes deviations of x from feasibility.
\end{flushleft}


\begin{flushleft}
The method is called an exact penalty method if for sufficiently large $\alpha$, solutions of the
\end{flushleft}


\begin{flushleft}
auxiliary problem (5.111) also solve the original problem (5.110).
\end{flushleft}


\begin{flushleft}
(a) Show that $\phi$ is convex.
\end{flushleft}


\begin{flushleft}
(b) The auxiliary problem can be expressed as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x) + $\alpha$y
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ y, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
0$\leq$y
\end{flushleft}





\begin{flushleft}
where the variables are x and y $\in$ R. Find the Lagrange dual of this problem, and
\end{flushleft}


\begin{flushleft}
express it in terms of the Lagrange dual function g of (5.110).
\end{flushleft}


\begin{flushleft}
(c) Use the result in (b) to prove the following property. Suppose $\lambda$ is an optimal
\end{flushleft}


\begin{flushleft}
solution of the Lagrange dual of (5.110), and that strong duality holds. If $\alpha$ $>$
\end{flushleft}


\begin{flushleft}
1T $\lambda$ , then any solution of the auxiliary problem (5.111) is also an optimal solution
\end{flushleft}


\begin{flushleft}
of (5.110).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The first term is convex. The second term is convex since it can be expressed as
\end{flushleft}


\begin{flushleft}
max\{f1 (x), . . . , fm (x), 0\},
\end{flushleft}


\begin{flushleft}
i.e., the pointwise maximum of a number of convex functions.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) The Lagrangian is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
L(x, y, $\lambda$, $\mu$) = f0 (x) + $\alpha$y +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i (fi (x) $-$ y) $-$ $\mu$y.
\end{flushleft}





\begin{flushleft}
The dual function is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
inf L(x, y, $\lambda$, $\mu$)
\end{flushleft}


\begin{flushleft}
x,y
\end{flushleft}





=





\begin{flushleft}
$\lambda$i (fi (x) $-$ y) $-$ $\mu$y
\end{flushleft}





\begin{flushleft}
inf f0 (x) + $\alpha$y +
\end{flushleft}


\begin{flushleft}
x,y
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
inf (f0 (x) +
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
g($\lambda$)
\end{flushleft}


$-$$\infty$





=





\begin{flushleft}
$\lambda$i fi (x)) + inf ($\alpha$ $-$
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i $-$ $\mu$)y
\end{flushleft}





\begin{flushleft}
1T $\lambda$ + $\mu$ = $\alpha$
\end{flushleft}


\begin{flushleft}
otherwise,
\end{flushleft}





\begin{flushleft}
and the dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
g($\lambda$)
\end{flushleft}


\begin{flushleft}
1T $\lambda$ + $\mu$ = $\alpha$
\end{flushleft}


\begin{flushleft}
$\lambda$ 0, $\mu$ $\geq$ 0,
\end{flushleft}





\begin{flushleft}
or, equivalently,
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
g($\lambda$)
\end{flushleft}


\begin{flushleft}
1T $\lambda$ $\leq$ $\alpha$
\end{flushleft}


\begin{flushleft}
$\lambda$ 0.
\end{flushleft}





\begin{flushleft}
(c) If 1T $\lambda$ $<$ $\alpha$, then $\lambda$ is also optimal for the dual problem derived in part (b). By
\end{flushleft}


\begin{flushleft}
complementary slackness y = 0 in any optimal solution of the primal problem, so the
\end{flushleft}


\begin{flushleft}
optimal x satisfies fi (x) $\leq$ 0, i = 1, . . . , m, i.e., it is feasible in the original problem,
\end{flushleft}


\begin{flushleft}
and therefore also optimal.
\end{flushleft}


\begin{flushleft}
5.17 Robust linear programming with polyhedral uncertainty. Consider the robust LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
supa$\in$Pi aT x $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
with variable x $\in$ Rn , where Pi = \{a | Ci a
\end{flushleft}


\begin{flushleft}
di \}. The problem data are c $\in$ Rn ,
\end{flushleft}


\begin{flushleft}
Ci $\in$ Rmi ×n , di $\in$ Rmi , and b $\in$ Rm . We assume the polyhedra Pi are nonempty.
\end{flushleft}


\begin{flushleft}
Show that this problem is equivalent to the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
dTi zi $\leq$ bi , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
CiT zi = x, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
zi 0, i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
with variables x $\in$ Rn and zi $\in$ Rmi , i = 1, . . . , m. Hint. Find the dual of the problem
\end{flushleft}


\begin{flushleft}
of maximizing aTi x over ai $\in$ Pi (with variable ai ).
\end{flushleft}


\begin{flushleft}
Solution. The problem can be expressed as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
if we define fi (x) as the optimal value of the LP
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
xT a
\end{flushleft}


\begin{flushleft}
Ci a
\end{flushleft}





\begin{flushleft}
d,
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
where a is the variable, and x is treated as a problem parameter. It is readily shown that
\end{flushleft}


\begin{flushleft}
the Lagrange dual of this LP is given by
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
dTi z
\end{flushleft}


\begin{flushleft}
CiT z = x
\end{flushleft}


\begin{flushleft}
z 0.
\end{flushleft}





\begin{flushleft}
The optimal value of this LP is also equal to fi (x), so we have fi (x) $\leq$ bi if and only if
\end{flushleft}


\begin{flushleft}
there exists a zi with
\end{flushleft}


\begin{flushleft}
CiT zi = x,
\end{flushleft}


\begin{flushleft}
zi 0.
\end{flushleft}


\begin{flushleft}
dTi z $\leq$ bi ,
\end{flushleft}


\begin{flushleft}
5.18 Separating hyperplane between two polyhedra. Formulate the following problem as an LP
\end{flushleft}


\begin{flushleft}
or an LP feasibility problem. Find a separating hyperplane that strictly separates two
\end{flushleft}


\begin{flushleft}
polyhedra
\end{flushleft}


\begin{flushleft}
P1 = \{x | Ax b\},
\end{flushleft}


\begin{flushleft}
P2 = \{x | Cx d\},
\end{flushleft}


\begin{flushleft}
i.e., find a vector a $\in$ Rn and a scalar $\gamma$ such that
\end{flushleft}


\begin{flushleft}
aT x $>$ $\gamma$ for x $\in$ P1 ,
\end{flushleft}





\begin{flushleft}
aT x $<$ $\gamma$ for x $\in$ P2 .
\end{flushleft}





\begin{flushleft}
You can assume that P1 and P2 do not intersect.
\end{flushleft}


\begin{flushleft}
Hint. The vector a and scalar $\gamma$ must satisfy
\end{flushleft}


\begin{flushleft}
inf aT x $>$ $\gamma$ $>$ sup aT x.
\end{flushleft}





\begin{flushleft}
x$\in$P1
\end{flushleft}





\begin{flushleft}
x$\in$P2
\end{flushleft}





\begin{flushleft}
Use LP duality to simplify the infimum and supremum in these conditions.
\end{flushleft}


\begin{flushleft}
Solution. Define p1 (a) and p2 (a) as
\end{flushleft}


\begin{flushleft}
p1 (a) = inf\{aT x | Ax
\end{flushleft}





\begin{flushleft}
b\},
\end{flushleft}





\begin{flushleft}
p2 (a) = sup\{aT x | Cx
\end{flushleft}





\begin{flushleft}
d\}.
\end{flushleft}





\begin{flushleft}
A hyperplane aT x = $\gamma$ strictly separates the two polyhedra if
\end{flushleft}


\begin{flushleft}
p2 (a) $<$ $\gamma$ $<$ p1 (a).
\end{flushleft}


\begin{flushleft}
For example, we can find a by solving
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
p1 (a) $-$ p2 (a)
\end{flushleft}


\begin{flushleft}
a 1$\leq$1
\end{flushleft}





\begin{flushleft}
and selecting $\gamma$ = (p1 (a) + p2 (a))/2. (The bound a 1 is added because the objective is
\end{flushleft}


\begin{flushleft}
homogeneous in a, so it unbounded unless we add a constraint on a.)
\end{flushleft}


\begin{flushleft}
Using LP duality we have
\end{flushleft}


\begin{flushleft}
p1 (a)
\end{flushleft}





=





\begin{flushleft}
p2 (a)
\end{flushleft}





=


=





\begin{flushleft}
sup\{$-$bT z1 | AT z1 + a = 0, z1
\end{flushleft}





0\}





\begin{flushleft}
sup\{$-$dT z2 | C T z2 $-$ a = 0, z2
\end{flushleft}





0\},





\begin{flushleft}
inf\{$-$aT x | Cx
\end{flushleft}





\begin{flushleft}
d\}
\end{flushleft}





\begin{flushleft}
so we can reformulate the problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT z1 $-$ dT z2
\end{flushleft}


\begin{flushleft}
A T z1 + a = 0
\end{flushleft}


\begin{flushleft}
C T z2 $-$ a = 0
\end{flushleft}


\begin{flushleft}
z1 0, z2 0
\end{flushleft}


\begin{flushleft}
a 1 $\leq$ 1.
\end{flushleft}





\begin{flushleft}
The variables are a, z1 and z2 .
\end{flushleft}


\begin{flushleft}
Another solution is based on theorems of alternative. The hyperplane separates the two
\end{flushleft}


\begin{flushleft}
polyhedra if the following two sets of linear inequalities are infeasible:
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
$\bullet$ Ax
\end{flushleft}





\begin{flushleft}
$\bullet$ Cx
\end{flushleft}





\begin{flushleft}
b, aT x $\leq$ $\gamma$
\end{flushleft}





\begin{flushleft}
d, aT x $\geq$ $\gamma$.
\end{flushleft}





\begin{flushleft}
Using a theorem of alternatives this is equivalent to requiring that the following two sets
\end{flushleft}


\begin{flushleft}
of inequalities are both feasible:
\end{flushleft}


\begin{flushleft}
$\bullet$ z1
\end{flushleft}





\begin{flushleft}
$\bullet$ z2
\end{flushleft}





\begin{flushleft}
0, w1 $\geq$ 0, AT z1 + aw1 = 0, bT z1 $-$ $\gamma$w1 $<$ 0
\end{flushleft}





\begin{flushleft}
0, w2 $\geq$ 0, C T z2 $-$ aw2 = 0, dT z2 + $\gamma$w2 $<$ 0
\end{flushleft}





\begin{flushleft}
w1 and w2 must be nonzero. If w1 = 0, then AT z1 = 0, bT z1 $<$ 0. which means P1
\end{flushleft}


\begin{flushleft}
is empty, and similarly, w2 = 0 means P2 is empty. We can therefore simplify the two
\end{flushleft}


\begin{flushleft}
conditions as
\end{flushleft}


\begin{flushleft}
$\bullet$ z1
\end{flushleft}





\begin{flushleft}
$\bullet$ z2
\end{flushleft}





\begin{flushleft}
0, AT z1 + a = 0, bT z1 $<$ $\gamma$
\end{flushleft}


\begin{flushleft}
0, C T z2 $-$ a = 0, dT z2 $<$ $-$$\gamma$,
\end{flushleft}





\begin{flushleft}
which is basically the same as the conditions derived above.
\end{flushleft}


\begin{flushleft}
5.19 The sum of the largest elements of a vector. Define f : Rn $\rightarrow$ R as
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
x[i] ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where r is an integer between 1 and n, and x[1] $\geq$ x[2] $\geq$ · · · $\geq$ x[r] are the components of
\end{flushleft}


\begin{flushleft}
x sorted in decreasing order. In other words, f (x) is the sum of the r largest elements of
\end{flushleft}


\begin{flushleft}
x. In this problem we study the constraint
\end{flushleft}


\begin{flushleft}
f (x) $\leq$ $\alpha$.
\end{flushleft}


\begin{flushleft}
As we have seen in chapter 3, page 80, this is a convex constraint, and equivalent to a set
\end{flushleft}


\begin{flushleft}
of n!/(r!(n $-$ r)!) linear inequalities
\end{flushleft}


\begin{flushleft}
xi1 + · · · + xir $\leq$ $\alpha$,
\end{flushleft}





\begin{flushleft}
1 $\leq$ i1 $<$ i2 $<$ · · · $<$ ir $\leq$ n.
\end{flushleft}





\begin{flushleft}
The purpose of this problem is to derive a more compact representation.
\end{flushleft}


\begin{flushleft}
(a) Given a vector x $\in$ Rn , show that f (x) is equal to the optimal value of the LP
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
xT y
\end{flushleft}


\begin{flushleft}
0 y 1
\end{flushleft}


\begin{flushleft}
1T y = r
\end{flushleft}





\begin{flushleft}
with y $\in$ Rn as variable.
\end{flushleft}





\begin{flushleft}
(b) Derive the dual of the LP in part (a). Show that it can be written as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
rt + 1T u
\end{flushleft}


\begin{flushleft}
t1 + u x
\end{flushleft}


\begin{flushleft}
u 0,
\end{flushleft}





\begin{flushleft}
where the variables are t $\in$ R, u $\in$ Rn . By duality this LP has the same optimal
\end{flushleft}


\begin{flushleft}
value as the LP in (a), i.e., f (x). We therefore have the following result: x satisfies
\end{flushleft}


\begin{flushleft}
f (x) $\leq$ $\alpha$ if and only if there exist t $\in$ R, u $\in$ Rn such that
\end{flushleft}


\begin{flushleft}
rt + 1T u $\leq$ $\alpha$,
\end{flushleft}





\begin{flushleft}
t1 + u
\end{flushleft}





\begin{flushleft}
x,
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





0.





\begin{flushleft}
These conditions form a set of 2n + 1 linear inequalities in the 2n + 1 variables x, u, t.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
(c) As an application, we consider an extension of the classical Markowitz portfolio
\end{flushleft}


\begin{flushleft}
optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
xT $\Sigma$x
\end{flushleft}


\begin{flushleft}
pT x $\geq$ rmin
\end{flushleft}


\begin{flushleft}
1T x = 1, x
\end{flushleft}





0





\begin{flushleft}
discussed in chapter 4, page 155. The variable is the portfolio x $\in$ Rn ; p and $\Sigma$ are
\end{flushleft}


\begin{flushleft}
the mean and covariance matrix of the price change vector p.
\end{flushleft}


\begin{flushleft}
Suppose we add a diversification constraint, requiring that no more than 80\% of
\end{flushleft}


\begin{flushleft}
the total budget can be invested in any 10\% of the assets. This constraint can be
\end{flushleft}


\begin{flushleft}
expressed as
\end{flushleft}


\begin{flushleft}
0.1n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
x[i] $\leq$ 0.8.
\end{flushleft}





\begin{flushleft}
Formulate the portfolio optimization problem with diversification constraint as a
\end{flushleft}


\begin{flushleft}
QP.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) See also chapter 4, exercise 4.8.
\end{flushleft}


\begin{flushleft}
For simplicity we assume that the elements of x are sorted in decreasing order:
\end{flushleft}


\begin{flushleft}
x1 $\geq$ x 2 $\geq$ · · · $\geq$ x n .
\end{flushleft}


\begin{flushleft}
It is easy to see that the optimal value is
\end{flushleft}


\begin{flushleft}
x1 + x 2 + · · · + x r ,
\end{flushleft}


\begin{flushleft}
obtained by choosing y1 = y2 = · · · = yr = 1 and yr+1 = · · · = yn = 0.
\end{flushleft}


\begin{flushleft}
(b) We first change the objective from maximization to minimization:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$xT y
\end{flushleft}


\begin{flushleft}
0 y 1
\end{flushleft}


\begin{flushleft}
1T y = r.
\end{flushleft}





\begin{flushleft}
We introduce a Lagrange multiplier $\lambda$ for the lower bound, u for the upper bound,
\end{flushleft}


\begin{flushleft}
and t for the equality constraint. The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(y, $\lambda$, u, t)
\end{flushleft}





=


=





\begin{flushleft}
$-$xT y $-$ $\lambda$T y + uT (y $-$ 1) + t(1T y $-$ r)
\end{flushleft}


\begin{flushleft}
$-$1T u $-$ rt + ($-$x $-$ $\lambda$ + u + t1)T y.
\end{flushleft}





\begin{flushleft}
Minimizing over y yields the dual function
\end{flushleft}


\begin{flushleft}
g($\lambda$, u, t) =
\end{flushleft}





\begin{flushleft}
$-$1T u $-$ rt
\end{flushleft}


$-$$\infty$





\begin{flushleft}
$-$x $-$ $\lambda$ + u + t1 = 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The dual problem is to maximize g subject to $\lambda$
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
0 and u
\end{flushleft}





0:





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
$-$1 u $-$ rt
\end{flushleft}


\begin{flushleft}
$-$$\lambda$ + u + t1 = x
\end{flushleft}


\begin{flushleft}
$\lambda$ 0, u 0,
\end{flushleft}





\begin{flushleft}
or after changing the objective to minimization (i.e., undoing the sign change we
\end{flushleft}


\begin{flushleft}
started with),
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
1T u + rt
\end{flushleft}


\begin{flushleft}
subject to u + t1 x
\end{flushleft}


\begin{flushleft}
u 0.
\end{flushleft}


\begin{flushleft}
We eliminated $\lambda$ by noting that it acts as a slack variable in the first constraint.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(c)
\end{flushleft}


\begin{flushleft}
xT $\Sigma$x
\end{flushleft}


\begin{flushleft}
pT x $\geq$ rmin
\end{flushleft}


\begin{flushleft}
1T x = 1, x 0
\end{flushleft}


\begin{flushleft}
n/20 t + 1T u $\leq$ 0.9
\end{flushleft}


\begin{flushleft}
$\lambda$1 + u 0
\end{flushleft}


\begin{flushleft}
u 0,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
with variables x, u, t, v.
\end{flushleft}


\begin{flushleft}
5.20 Dual of channel capacity problem. Derive a dual for the problem
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$cT x + i=1 yi log yi
\end{flushleft}


\begin{flushleft}
Px = y
\end{flushleft}


\begin{flushleft}
x 0, 1T x = 1,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
where P $\in$ Rm×n has nonnegative elements, and its columns add up to one (i.e., P T 1 =
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
1). The variables are x $\in$ Rn , y $\in$ Rm . (For cj =
\end{flushleft}


\begin{flushleft}
p log pij , the optimal value is,
\end{flushleft}


\begin{flushleft}
i=1 ij
\end{flushleft}


\begin{flushleft}
up to a factor log 2, the negative of the capacity of a discrete memoryless channel with
\end{flushleft}


\begin{flushleft}
channel transition probability matrix P ; see exercise 4.57.)
\end{flushleft}


\begin{flushleft}
Simplify the dual problem as much as possible.
\end{flushleft}


\begin{flushleft}
Solution. The Lagrangian is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
L(x, y, $\lambda$, $\nu$, z)
\end{flushleft}





=





\begin{flushleft}
$-$cT x +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
yi log yi $-$ $\lambda$T x + $\nu$(1T x $-$ 1) + z T (P x $-$ y)
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
($-$c $-$ $\lambda$ + $\nu$1 + P T z)T x +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
yi log yi $-$ z T y $-$ $\nu$.
\end{flushleft}





\begin{flushleft}
The minimum over x is bounded below if and only if
\end{flushleft}


\begin{flushleft}
$-$c $-$ $\lambda$ + $\nu$1 + P T z = 0.
\end{flushleft}





\begin{flushleft}
To minimize over y, we set the derivative with respect to yi equal to zero, which gives
\end{flushleft}


\begin{flushleft}
log yi + 1 $-$ zi = 0, and conclude that
\end{flushleft}


\begin{flushleft}
inf (yi log yi $-$ zi yi ) = $-$ezi $-$1 .
\end{flushleft}





\begin{flushleft}
yi $\geq$0
\end{flushleft}





\begin{flushleft}
The dual function is
\end{flushleft}


\begin{flushleft}
g($\lambda$, $\nu$, z) =
\end{flushleft}





$-$


$-$$\infty$





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ezi $-$1 $-$ $\nu$
\end{flushleft}





\begin{flushleft}
The dual problem is
\end{flushleft}





\begin{flushleft}
$-$c $-$ $\lambda$ + $\nu$1 + P T z = 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
maximize $-$ i=1 exp(zi $-$ 1) $-$ $\nu$
\end{flushleft}


\begin{flushleft}
subject to P T z $-$ c + $\nu$1 0.
\end{flushleft}


\begin{flushleft}
This can be simplified by introducing a variable w = z + $\nu$1 (and using the fact that
\end{flushleft}


\begin{flushleft}
1 = P T 1), which gives
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$ i=1 exp(wi $-$ $\nu$ $-$ 1) $-$ $\nu$
\end{flushleft}


\begin{flushleft}
P T w c.
\end{flushleft}





\begin{flushleft}
Finally we can easily maximize the objective function over $\nu$ by setting the derivative
\end{flushleft}


\begin{flushleft}
equal to zero (the optimal value is $\nu$ = $-$ log( i e1$-$wi ), which leads to
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$ log( i=1 exp wi ) $-$ 1
\end{flushleft}


\begin{flushleft}
P T w c.
\end{flushleft}





\begin{flushleft}
This is a geometric program, in convex form, with linear inequality constraints (i.e.,
\end{flushleft}


\begin{flushleft}
monomial inequality constraints in the associated geometric program).
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
Strong duality and Slater's condition
\end{flushleft}


\begin{flushleft}
5.21 A convex problem in which strong duality fails. Consider the optimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
e$-$x
\end{flushleft}


\begin{flushleft}
x2 /y $\leq$ 0
\end{flushleft}





\begin{flushleft}
with variables x and y, and domain D = \{(x, y) | y $>$ 0\}.
\end{flushleft}


\begin{flushleft}
(a) Verify that this is a convex optimization problem. Find the optimal value.
\end{flushleft}


\begin{flushleft}
(b) Give the Lagrange dual problem, and find the optimal solution $\lambda$ and optimal value
\end{flushleft}


\begin{flushleft}
d of the dual problem. What is the optimal duality gap?
\end{flushleft}


\begin{flushleft}
(c) Does Slater's condition hold for this problem?
\end{flushleft}


\begin{flushleft}
(d) What is the optimal value p (u) of the perturbed problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
e$-$x
\end{flushleft}


\begin{flushleft}
x2 /y $\leq$ u
\end{flushleft}





\begin{flushleft}
as a function of u? Verify that the global sensitivity inequality
\end{flushleft}


\begin{flushleft}
p (u) $\geq$ p (0) $-$ $\lambda$ u
\end{flushleft}


\begin{flushleft}
does not hold.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) p = 1.
\end{flushleft}


\begin{flushleft}
(b) The Lagrangian is L(x, y, $\lambda$) = e$-$x + $\lambda$x2 /y. The dual function is
\end{flushleft}


\begin{flushleft}
g($\lambda$) = inf (e$-$x + $\lambda$x2 /y) =
\end{flushleft}


\begin{flushleft}
x,y$>$0
\end{flushleft}





0


$-$$\infty$





\begin{flushleft}
$\lambda$$\geq$0
\end{flushleft}


\begin{flushleft}
$\lambda$ $<$ 0,
\end{flushleft}





\begin{flushleft}
so we can write the dual problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





0


\begin{flushleft}
$\lambda$ $\geq$ 0,
\end{flushleft}





\begin{flushleft}
with optimal value d = 0. The optimal duality gap is p $-$ d = 1.
\end{flushleft}





\begin{flushleft}
(c) Slater's condition is not satisfied.
\end{flushleft}





\begin{flushleft}
(d) p (u) = 1 if u = 0, p (u) = 0 if u $>$ 0 and p (u) = $\infty$ if u $<$ 0.
\end{flushleft}


\begin{flushleft}
5.22 Geometric interpretation of duality. For each of the following optimization problems,
\end{flushleft}


\begin{flushleft}
draw a sketch of the sets
\end{flushleft}


\begin{flushleft}
G
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





=


=





\begin{flushleft}
\{(u, t) | $\exists$x $\in$ D, f0 (x) = t, f1 (x) = u\},
\end{flushleft}


\begin{flushleft}
\{(u, t) | $\exists$x $\in$ D, f0 (x) $\leq$ t, f1 (x) $\leq$ u\},
\end{flushleft}





\begin{flushleft}
give the dual problem, and solve the primal and dual problems. Is the problem convex?
\end{flushleft}


\begin{flushleft}
Is Slater's condition satisfied? Does strong duality hold?
\end{flushleft}


\begin{flushleft}
The domain of the problem is R unless otherwise stated.
\end{flushleft}


\begin{flushleft}
(a) Minimize x subject to x2 $\leq$ 1.
\end{flushleft}





\begin{flushleft}
(b) Minimize x subject to x2 $\leq$ 0.
\end{flushleft}





\begin{flushleft}
(c) Minimize x subject to |x| $\leq$ 0.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(d) Minimize x subject to f1 (x) $\leq$ 0 where
\end{flushleft}


\begin{flushleft}
f1 (x) =
\end{flushleft}





\begin{flushleft}
$-$x + 2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
$-$x $-$ 2
\end{flushleft}





\begin{flushleft}
x$\geq$1
\end{flushleft}


\begin{flushleft}
$-$1 $\leq$ x $\leq$ 1
\end{flushleft}


\begin{flushleft}
x $\leq$ $-$1.
\end{flushleft}





\begin{flushleft}
(e) Minimize x3 subject to $-$x + 1 $\leq$ 0.
\end{flushleft}


\begin{flushleft}
(f) Minimize x3 subject to $-$x + 1 $\leq$ 0 with domain D = R+ .
\end{flushleft}





\begin{flushleft}
Solution. For the first four problems G is the curve
\end{flushleft}





\begin{flushleft}
G = \{(u, t) | u $\in$ D, u = f1 (t)\}.
\end{flushleft}


\begin{flushleft}
For problem (e), G is the curve
\end{flushleft}


\begin{flushleft}
G = \{(u, t) | t = (1 $-$ u)3 \}.
\end{flushleft}


\begin{flushleft}
For problem (f), G is the curve
\end{flushleft}


\begin{flushleft}
G = \{(u, t) | u $\leq$ 1, t = (1 $-$ u)3 \}.
\end{flushleft}


\begin{flushleft}
A is the set of points above and to the right of G.
\end{flushleft}


\begin{flushleft}
(a) x = $-$1. $\lambda$ = 1. p = $-$1. d = $-$1. Convex. Strong duality. Slater's condition
\end{flushleft}


\begin{flushleft}
holds.
\end{flushleft}


\begin{flushleft}
This is the generic convex case.
\end{flushleft}


\begin{flushleft}
(b) x = 0. p = 0. d = 0. Dual optimum is not achieved. Convex. Strong duality.
\end{flushleft}


\begin{flushleft}
Slater's condition does not hold.
\end{flushleft}


\begin{flushleft}
We have strong duality although Slater's condition does not hold. However the dual
\end{flushleft}


\begin{flushleft}
optimum is not attained.
\end{flushleft}


\begin{flushleft}
(c) x = 0. p = 0. $\lambda$ = 1. d = 0. Convex. Strong duality. Slater's condition not
\end{flushleft}


\begin{flushleft}
satisfied.
\end{flushleft}


\begin{flushleft}
We have strong duality and the dual is attained, although Slater's condition does
\end{flushleft}


\begin{flushleft}
not hold.
\end{flushleft}


\begin{flushleft}
(d) x = $-$2. p = $-$2. $\lambda$ = 1. d = $-$2. Not convex. Strong duality.
\end{flushleft}


\begin{flushleft}
We have strong duality, although this is a very nonconvex problem.
\end{flushleft}


\begin{flushleft}
(e) x = 1. p = 1. d = $-$$\infty$. Not convex. No strong duality.
\end{flushleft}


\begin{flushleft}
The problem has a convex feasibility set, and the objective is convex on the feasible
\end{flushleft}


\begin{flushleft}
set. However the problem is not convex, according to the definition used in this
\end{flushleft}


\begin{flushleft}
book. Lagrange duality gives a trivial bound $-$$\infty$.
\end{flushleft}


\begin{flushleft}
(f) x = 1. p = 1. $\lambda$ = 1. d = 1. Convex. Strong duality. Slater's condition is
\end{flushleft}


\begin{flushleft}
satisfied.
\end{flushleft}


\begin{flushleft}
Adding the domain condition seems redundant at first. However the new problem
\end{flushleft}


\begin{flushleft}
is convex (according to our definition). Now strong duality holds and the dual
\end{flushleft}


\begin{flushleft}
optimum is attained.
\end{flushleft}


\begin{flushleft}
5.23 Strong duality in linear programming. We prove that strong duality holds for the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
and its dual
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
$-$bT z
\end{flushleft}


\begin{flushleft}
AT z + c = 0,
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





0,





\begin{flushleft}
provided at least one of the problems is feasible. In other words, the only possible exception to strong duality occurs when p = $\infty$ and d = $-$$\infty$.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
(a) Suppose p is finite and x is an optimal solution. (If finite, the optimal value of an
\end{flushleft}


\begin{flushleft}
LP is attained.) Let I $\subseteq$ \{1, 2, . . . , m\} be the set of active constraints at x :
\end{flushleft}


\begin{flushleft}
aTi x = bi ,
\end{flushleft}





\begin{flushleft}
aTi x $<$ bi ,
\end{flushleft}





\begin{flushleft}
i $\in$ I,
\end{flushleft}





\begin{flushleft}
i $\in$ I.
\end{flushleft}





\begin{flushleft}
Show that there exists a z $\in$ Rm that satisfies
\end{flushleft}


\begin{flushleft}
zi $\geq$ 0,
\end{flushleft}





\begin{flushleft}
i $\in$ I,
\end{flushleft}





\begin{flushleft}
i $\in$ I,
\end{flushleft}





\begin{flushleft}
zi = 0,
\end{flushleft}





\begin{flushleft}
zi ai + c = 0.
\end{flushleft}


\begin{flushleft}
i$\in$I
\end{flushleft}





\begin{flushleft}
Show that z is dual optimal with objective value cT x .
\end{flushleft}


\begin{flushleft}
Hint. Assume there exists no such z, i.e., $-$c $\in$ \{ i$\in$I zi ai | zi $\geq$ 0\}. Reduce
\end{flushleft}


\begin{flushleft}
this to a contradiction by applying the strict separating hyperplane theorem of
\end{flushleft}


\begin{flushleft}
example 2.20, page 49. Alternatively, you can use Farkas' lemma (see §5.8.3).
\end{flushleft}





\begin{flushleft}
(b) Suppose p = $\infty$ and the dual problem is feasible. Show that d = $\infty$. Hint. Show
\end{flushleft}


\begin{flushleft}
that there exists a nonzero v $\in$ Rm such that AT v = 0, v 0, bT v $<$ 0. If the dual
\end{flushleft}


\begin{flushleft}
is feasible, it is unbounded in the direction v.
\end{flushleft}


\begin{flushleft}
(c) Consider the example
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
subject to
\end{flushleft}





0


1





\begin{flushleft}
x
\end{flushleft}





$-$1


1





.





\begin{flushleft}
Formulate the dual LP, and solve the primal and dual problems. Show that p = $\infty$
\end{flushleft}


\begin{flushleft}
and d = $-$$\infty$.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Without loss of generality we can assume that I = \{1, 2, . . . , k\}. Let A¯ $\in$ Rk×n be
\end{flushleft}


\begin{flushleft}
the matrix formed by the first k rows of A. We assume there is no z¯ 0 such that
\end{flushleft}


\begin{flushleft}
c + A¯T z¯ = 0, i.e.,
\end{flushleft}


\begin{flushleft}
$-$c $\in$ S = \{A¯T z¯ | z¯ 0\}.
\end{flushleft}


\begin{flushleft}
By the strict separating hyperplane theorem, applied to $-$c and S, there exists a u
\end{flushleft}


\begin{flushleft}
such that
\end{flushleft}


\begin{flushleft}
$-$uT c $>$ uT A¯T z¯
\end{flushleft}





\begin{flushleft}
for all z¯
\end{flushleft}


\begin{flushleft}
0. This means cT u $<$ 0 (evaluate the righthand side at z¯ = 0), and
\end{flushleft}


¯


\begin{flushleft}
Au
\end{flushleft}


0.


\begin{flushleft}
Now consider x = x + tu. We have
\end{flushleft}


\begin{flushleft}
aTi x = aTi x + taTi u = bi + taTi u $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i $\in$ I,
\end{flushleft}





\begin{flushleft}
aTi x = aTi x + taTi u $<$ bi + taTi u $<$ bi ,
\end{flushleft}





\begin{flushleft}
i $\in$ I,
\end{flushleft}





\begin{flushleft}
for all t $\geq$ 0, and
\end{flushleft}





\begin{flushleft}
for sufficiently small positive t. Finally
\end{flushleft}


\begin{flushleft}
cT x = cT x + tcT u $<$ cT x
\end{flushleft}


\begin{flushleft}
for all positive t. This is a contradiction, because we have constructed primal feasible
\end{flushleft}


\begin{flushleft}
points with a lower objective value than x .
\end{flushleft}


\begin{flushleft}
We conclude that there exists a z¯ 0 with A¯T z¯ + c = 0. Choosing z = (¯
\end{flushleft}


\begin{flushleft}
z , 0) yields
\end{flushleft}


\begin{flushleft}
a dual feasible point. Its objective value is
\end{flushleft}


\begin{flushleft}
$-$bT z = $-$(x )T A¯T z = cT x .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) The primal problem is infeasible, i.e.,
\end{flushleft}


\begin{flushleft}
$-$b $\in$ S = \{Ax + s | s
\end{flushleft}





0\}.





\begin{flushleft}
The righthand side is a closed convex set, so we can apply the strict separating
\end{flushleft}


\begin{flushleft}
hyperplane theorem and conclude there exists a v $\in$ Rm such that $-$v T b $>$ v T (Ax +
\end{flushleft}


\begin{flushleft}
s) for all x and all s 0. This is equivalent to
\end{flushleft}


\begin{flushleft}
bT v $<$ 0,
\end{flushleft}





\begin{flushleft}
AT v = 0,
\end{flushleft}





\begin{flushleft}
v
\end{flushleft}





0.





\begin{flushleft}
This only leaves two possibilities. Either the dual problem is infeasible, or it is
\end{flushleft}


\begin{flushleft}
feasible and unbounded above. (If z0 is dual feasible, then z = z0 + tv is dual
\end{flushleft}


\begin{flushleft}
feasible for all t $\geq$ 0, with $-$bT z = $-$bT z0 + tbT v).
\end{flushleft}





\begin{flushleft}
(c) The dual LP is
\end{flushleft}





\begin{flushleft}
z 1 $-$ z2
\end{flushleft}


\begin{flushleft}
z2 + 1 = 0
\end{flushleft}


\begin{flushleft}
z1 , z2 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
which is also infeasible (d = $-$$\infty$).
\end{flushleft}





\begin{flushleft}
5.24 Weak max-min inequality. Show that the weak max-min inequality
\end{flushleft}


\begin{flushleft}
sup inf f (w, z) $\leq$ inf sup f (w, z)
\end{flushleft}





\begin{flushleft}
z$\in$Z w$\in$W
\end{flushleft}





\begin{flushleft}
w$\in$W z$\in$Z
\end{flushleft}





\begin{flushleft}
always holds, with no assumptions on f : Rn × Rm $\rightarrow$ R, W $\subseteq$ Rn , or Z $\subseteq$ Rm .
\end{flushleft}


\begin{flushleft}
Solution. If W and Z are empty, the inequality reduces to $-$$\infty$ $\leq$ $\infty$.
\end{flushleft}


\begin{flushleft}
If W is nonempty, with w
\end{flushleft}


\begin{flushleft}
˜ $\in$ W , we have
\end{flushleft}


\begin{flushleft}
inf f (w, z) $\leq$ f (w,
\end{flushleft}


\begin{flushleft}
˜ z)
\end{flushleft}





\begin{flushleft}
w$\in$W
\end{flushleft}





\begin{flushleft}
for all z $\in$ Z. Taking the supremum over z $\in$ Z on both sides we get
\end{flushleft}


\begin{flushleft}
sup inf f (w, z) $\leq$ sup f (w,
\end{flushleft}


\begin{flushleft}
˜ z).
\end{flushleft}





\begin{flushleft}
z$\in$Z w$\in$W
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
Taking the inf over w
\end{flushleft}


\begin{flushleft}
˜ $\in$ W we get the max-min inequality.
\end{flushleft}


\begin{flushleft}
The proof for nonempty Z is similar.
\end{flushleft}


\begin{flushleft}
5.25 [BL00, page 95] Convex-concave functions and the saddle-point property. We derive conditions under which the saddle-point property
\end{flushleft}


\begin{flushleft}
sup inf f (w, z) = inf sup f (w, z)
\end{flushleft}


\begin{flushleft}
z$\in$Z w$\in$W
\end{flushleft}





(5.112)





\begin{flushleft}
w$\in$W z$\in$Z
\end{flushleft}





\begin{flushleft}
holds, where f : Rn × Rm $\rightarrow$ R, W × Z $\subseteq$ dom f , and W and Z are nonempty. We will
\end{flushleft}


\begin{flushleft}
assume that the function
\end{flushleft}


\begin{flushleft}
gz (w) =
\end{flushleft}





\begin{flushleft}
f (w, z)
\end{flushleft}


$\infty$





\begin{flushleft}
w$\in$W
\end{flushleft}


\begin{flushleft}
otherwise
\end{flushleft}





\begin{flushleft}
is closed and convex for all z $\in$ Z, and the function
\end{flushleft}


\begin{flushleft}
hw (z) =
\end{flushleft}


\begin{flushleft}
is closed and convex for all w $\in$ W .
\end{flushleft}





\begin{flushleft}
$-$f (w, z)
\end{flushleft}


$\infty$





\begin{flushleft}
z$\in$Z
\end{flushleft}


\begin{flushleft}
otherwise
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
(a) The righthand side of (5.112) can be expressed as p(0), where
\end{flushleft}


\begin{flushleft}
p(u) = inf sup (f (w, z) + uT z).
\end{flushleft}


\begin{flushleft}
w$\in$W z$\in$Z
\end{flushleft}





\begin{flushleft}
Show that p is a convex function.
\end{flushleft}


\begin{flushleft}
(b) Show that the conjugate of p is given by
\end{flushleft}


\begin{flushleft}
$-$ inf w$\in$W f (w, v)
\end{flushleft}


$\infty$





\begin{flushleft}
p∗ (v) =
\end{flushleft}





\begin{flushleft}
v$\in$Z
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
(c) Show that the conjugate of p∗ is given by
\end{flushleft}


\begin{flushleft}
p∗∗ (u) = sup inf (f (w, z) + uT z).
\end{flushleft}


\begin{flushleft}
z$\in$Z w$\in$W
\end{flushleft}





\begin{flushleft}
Combining this with (a), we can express the max-min equality (5.112) as p∗∗ (0) =
\end{flushleft}


\begin{flushleft}
p(0).
\end{flushleft}


\begin{flushleft}
(d) From exercises 3.28 and 3.39 (d), we know that p∗∗ (0) = p(0) if 0 $\in$ int dom p.
\end{flushleft}


\begin{flushleft}
Conclude that this is the case if W and Z are bounded.
\end{flushleft}


\begin{flushleft}
(e) As another consequence of exercises 3.28 and 3.39, we have p∗∗ (0) = p(0) if 0 $\in$
\end{flushleft}


\begin{flushleft}
dom p and p is closed. Show that p is closed if the sublevel sets of gz are bounded.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) For fixed z, Fz (u, w) = gz (w)$-$uT z is a (closed) convex function of (w, u). Therefore
\end{flushleft}


\begin{flushleft}
F (w, u) = sup (gz (w) + uT z)
\end{flushleft}


\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
is a convex function of (w, u). (It is also closed because it epigraph is the intersection
\end{flushleft}


\begin{flushleft}
of closed sets, the epigraphs of the functions Fz .)
\end{flushleft}


\begin{flushleft}
Minimizing F over w yields a convex function
\end{flushleft}


\begin{flushleft}
inf F (w, u)
\end{flushleft}





=





\begin{flushleft}
inf sup (gz (w) + uT z)
\end{flushleft}





=





\begin{flushleft}
inf sup (f (w, z) + uT z)
\end{flushleft}





\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
w$\in$W z$\in$Z
\end{flushleft}





=





\begin{flushleft}
p(u).
\end{flushleft}





\begin{flushleft}
(b) The conjugate is
\end{flushleft}


\begin{flushleft}
p∗ (v)
\end{flushleft}





=





\begin{flushleft}
sup(v T u $-$ p(u))
\end{flushleft}





=





\begin{flushleft}
sup(v T u $-$ inf sup(f (w, z) + uT z))
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
w$\in$W z$\in$Z
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





=





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
sup sup (v u $-$ sup(f (w, z) + uT z))
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
w$\in$W
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





=





\begin{flushleft}
sup sup ($-$ sup(f (w, z) + (z $-$ v)T u))
\end{flushleft}





=





\begin{flushleft}
sup sup inf ($-$f (w, z) + (v $-$ z)T u)
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





=





\begin{flushleft}
w$\in$W
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
w$\in$W z$\in$Z
\end{flushleft}





\begin{flushleft}
sup sup inf ($-$f (w, z) + (v $-$ z)T u).
\end{flushleft}





\begin{flushleft}
w$\in$W
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
By assumption, for all w, the set
\end{flushleft}


\begin{flushleft}
Cw = epi hw = \{(z, t) | z $\in$ Z, t $\geq$ $-$f (z, w)\}
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
is closed and convex. We show that this implies that
\end{flushleft}


\begin{flushleft}
sup inf ($-$f (w, z) + (z $-$ v)T u) =
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
$-$f (w, v)
\end{flushleft}


$\infty$





\begin{flushleft}
v$\in$Z
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
First assume v $\in$ Z. It is clear that
\end{flushleft}


\begin{flushleft}
inf ($-$f (w, z) + z T u) $\leq$ $-$f (w, v) + v T u
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
(5.25.A)
\end{flushleft}





\begin{flushleft}
for all u. Since hw is closed and convex, there exists a nonvertical supporting
\end{flushleft}


\begin{flushleft}
hyperplane to its epigraph Cw at the point (z, f (z, w)), i.e., there exists a u
\end{flushleft}


\begin{flushleft}
˜ such
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


\begin{flushleft}
uT z $-$ t) = u
\end{flushleft}


\begin{flushleft}
˜T v $-$ f (v, w).
\end{flushleft}


\begin{flushleft}
(5.25.B)
\end{flushleft}


\begin{flushleft}
inf (˜
\end{flushleft}


\begin{flushleft}
uT z $-$ f (z, w)) = inf (˜
\end{flushleft}


\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
(z,t)$\in$Cw
\end{flushleft}





\begin{flushleft}
Combining (5.25.A) and (5.25.B) we conclude that
\end{flushleft}


\begin{flushleft}
inf ($-$f (w, z) + (z $-$ v)T u) $\leq$ $-$f (w, v)
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
for all u, with equality for u = u
\end{flushleft}


\begin{flushleft}
˜. Therefore
\end{flushleft}


\begin{flushleft}
sup inf ($-$f (w, z) + z T u $-$ v T u) = $-$f (w, v).
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
Next assume v = Z. For all w, and all t, (v, t) = Cw , hence it can be strictly
\end{flushleft}


\begin{flushleft}
separated from Cw by a nonvertical hyperplane: for all t and w $\in$ W there exists a
\end{flushleft}


\begin{flushleft}
u such that
\end{flushleft}


\begin{flushleft}
t + uT v $<$ inf ($-$f (w, z) + uT z),
\end{flushleft}


\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
t $<$ inf ($-$f (w, z) + uT (z $-$ v)).
\end{flushleft}


\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
This holds for all t, so
\end{flushleft}


\begin{flushleft}
sup inf ($-$f (w, z) + uT (z $-$ v)) = $\infty$.
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
(c) The conjugate of p∗ is
\end{flushleft}


\begin{flushleft}
p∗∗ (u)
\end{flushleft}





=





\begin{flushleft}
sup(uT v + inf f (w, v))
\end{flushleft}


\begin{flushleft}
v$\in$Z
\end{flushleft}





=





\begin{flushleft}
w$\in$W
\end{flushleft}





\begin{flushleft}
sup inf (f (w, v) + uT v).
\end{flushleft}





\begin{flushleft}
v$\in$Z w$\in$W
\end{flushleft}





\begin{flushleft}
(d) We noted in part (a) that F (w, u) = supz$\in$Z (f (w, z) + z T u) is a closed convex
\end{flushleft}


\begin{flushleft}
function. If Z is bounded, then the maximum in the definition is attained for all
\end{flushleft}


\begin{flushleft}
(w, u) $\in$ W × Rm , so W × Rm $\subseteq$ dom Fz .
\end{flushleft}


\begin{flushleft}
If W is bounded, the minimum in p(u) = inf w$\in$W F (w, u) is also attained for all u,
\end{flushleft}


\begin{flushleft}
so dom p = Rm .
\end{flushleft}


\begin{flushleft}
(e) epi p is the projection of epi F $\subseteq$ Rn × Rm × R (a closed set) on Rm × R.
\end{flushleft}


\begin{flushleft}
Now in general, the projection of a closed convex set C $\in$ Rp × Rq on Rp is closed
\end{flushleft}


\begin{flushleft}
if C does not contain any half-lines of the form \{(¯
\end{flushleft}


\begin{flushleft}
x, y¯ + sv) $\in$ Rp × Rq | s $\geq$ 0\} with
\end{flushleft}


\begin{flushleft}
v = 0 (i.e., no directions of recession of the form (0, v)).
\end{flushleft}


\begin{flushleft}
Applying this result to the epigraph of F and its projection epi p, we conclude that
\end{flushleft}


\begin{flushleft}
epi p is closed if epi F does not contain any half-lines \{(w,
\end{flushleft}


\begin{flushleft}
¯ u
\end{flushleft}


\begin{flushleft}
¯, t¯) + s(v, 0, 0) | s $\geq$ 0\}.
\end{flushleft}


\begin{flushleft}
This is the case if the sublevel sets of gz are bounded.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
Optimality conditions
\end{flushleft}


\begin{flushleft}
5.26 Consider the QCQP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x21 + x22
\end{flushleft}


\begin{flushleft}
(x1 $-$ 1)2 + (x2 $-$ 1)2 $\leq$ 1
\end{flushleft}


\begin{flushleft}
(x1 $-$ 1)2 + (x2 + 1)2 $\leq$ 1
\end{flushleft}





\begin{flushleft}
with variable x $\in$ R2 .
\end{flushleft}





\begin{flushleft}
(a) Sketch the feasible set and level sets of the objective. Find the optimal point x and
\end{flushleft}


\begin{flushleft}
optimal value p .
\end{flushleft}


\begin{flushleft}
(b) Give the KKT conditions. Do there exist Lagrange multipliers $\lambda$1 and $\lambda$2 that prove
\end{flushleft}


\begin{flushleft}
that x is optimal?
\end{flushleft}


\begin{flushleft}
(c) Derive and solve the Lagrange dual problem. Does strong duality hold?
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The figure shows the feasible set (the intersection of the two shaded disks) and some
\end{flushleft}


\begin{flushleft}
contour lines of the objective function. There is only one feasible point, (1, 0), so it
\end{flushleft}


\begin{flushleft}
is optimal for the primal problem, and we have p = 1.
\end{flushleft}


2


1.5





\begin{flushleft}
f1 (x) $\leq$ 0
\end{flushleft}





1


0.5


0





\begin{flushleft}
x
\end{flushleft}





$\clubsuit$





$-$0.5





\begin{flushleft}
PSfrag replacements
\end{flushleft}





$-$1


$-$1.5


$-$2





\begin{flushleft}
f2 (x) $\leq$ 0
\end{flushleft}


$-$2





$-$1





0





1





2





\begin{flushleft}
(b) The KKT conditions are
\end{flushleft}


\begin{flushleft}
(x1 $-$ 1)2 + (x2 $-$ 1)2 $\leq$ 1, (x1 $-$ 1)2 + (x2 + 1)2 $\leq$ 1,
\end{flushleft}


\begin{flushleft}
$\lambda$1 $\geq$ 0,
\end{flushleft}


\begin{flushleft}
$\lambda$2 $\geq$ 0
\end{flushleft}


\begin{flushleft}
2x1 + 2$\lambda$1 (x1 $-$ 1) + 2$\lambda$2 (x1 $-$ 1) = 0
\end{flushleft}


\begin{flushleft}
2x2 + 2$\lambda$1 (x2 $-$ 1) + 2$\lambda$2 (x2 + 1) = 0
\end{flushleft}


\begin{flushleft}
$\lambda$1 ((x1 $-$ 1)2 + (x2 $-$ 1)2 $-$ 1) = $\lambda$2 ((x1 $-$ 1)2 + (x2 + 1)2 $-$ 1) = 0.
\end{flushleft}


\begin{flushleft}
At x = (1, 0), these conditions reduce to
\end{flushleft}


\begin{flushleft}
$\lambda$1 $\geq$ 0,
\end{flushleft}





\begin{flushleft}
$\lambda$2 $\geq$ 0,
\end{flushleft}





2 = 0,





\begin{flushleft}
$-$2$\lambda$1 + 2$\lambda$2 = 0,
\end{flushleft}





\begin{flushleft}
which (clearly, in view of the third equation) have no solution.
\end{flushleft}


\begin{flushleft}
(c) The Lagrange dual function is given by
\end{flushleft}


\begin{flushleft}
g($\lambda$1 , $\lambda$2 ) = inf L(x1 , x2 , $\lambda$1 , $\lambda$2 )
\end{flushleft}


\begin{flushleft}
x1 ,x2
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
L(x1 , x2 , $\lambda$1 , $\lambda$2 )
\end{flushleft}


=


=





\begin{flushleft}
x21 + x22 + $\lambda$1 ((x1 $-$ 1)2 + (x2 $-$ 1)2 $-$ 1) + $\lambda$2 ((x1 $-$ 1)2 + (x2 + 1)2 $-$ 1)
\end{flushleft}





\begin{flushleft}
(1 + $\lambda$1 + $\lambda$2 )x21 + (1 + $\lambda$1 + $\lambda$2 )x22 $-$ 2($\lambda$1 + $\lambda$2 )x1 $-$ 2($\lambda$1 $-$ $\lambda$2 )x2 + $\lambda$1 + $\lambda$2 .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
L reaches its minimum for
\end{flushleft}


\begin{flushleft}
x1 =
\end{flushleft}





\begin{flushleft}
$\lambda$1 + $\lambda$ 2
\end{flushleft}


,


\begin{flushleft}
1 + $\lambda$ 1 + $\lambda$2
\end{flushleft}





\begin{flushleft}
x2 =
\end{flushleft}





\begin{flushleft}
$\lambda$1 $-$ $\lambda$ 2
\end{flushleft}


,


\begin{flushleft}
1 + $\lambda$ 1 + $\lambda$2
\end{flushleft}





\begin{flushleft}
and we find
\end{flushleft}


2





\begin{flushleft}
g($\lambda$1 , $\lambda$2 )
\end{flushleft}





2





\begin{flushleft}
2 ) +($\lambda$1 $-$$\lambda$2 )
\end{flushleft}


\begin{flushleft}
$-$ ($\lambda$1 +$\lambda$1+$\lambda$
\end{flushleft}


\begin{flushleft}
+ $\lambda$1 + $\lambda$2
\end{flushleft}


\begin{flushleft}
1 +$\lambda$2
\end{flushleft}


$-$$\infty$





=





\begin{flushleft}
1 + $\lambda$ 1 + $\lambda$2 $\geq$ 0
\end{flushleft}


\begin{flushleft}
otherwise,
\end{flushleft}





\begin{flushleft}
where we interpret a/0 = 0 if a = 0 and as $-$$\infty$ if a $<$ 0. The Lagrange dual problem
\end{flushleft}


\begin{flushleft}
is given by
\end{flushleft}


\begin{flushleft}
maximize ($\lambda$1 + $\lambda$2 $-$ ($\lambda$1 $-$ $\lambda$2 )2 )/(1 + $\lambda$1 + $\lambda$2 )
\end{flushleft}


\begin{flushleft}
subject to $\lambda$1 , $\lambda$2 $\geq$ 0.
\end{flushleft}





\begin{flushleft}
Since g is symmetric, the optimum (if it exists) occurs with $\lambda$1 = $\lambda$2 . The dual
\end{flushleft}


\begin{flushleft}
function then simplifies to
\end{flushleft}


\begin{flushleft}
2$\lambda$1
\end{flushleft}


\begin{flushleft}
g($\lambda$1 , $\lambda$1 ) =
\end{flushleft}


.


\begin{flushleft}
2$\lambda$1 + 1
\end{flushleft}


\begin{flushleft}
We see that g($\lambda$1 , $\lambda$2 ) tends to 1 as $\lambda$1 $\rightarrow$ $\infty$. We have d = p = 1, but the dual
\end{flushleft}


\begin{flushleft}
optimum is not attained.
\end{flushleft}


\begin{flushleft}
Recall that the KKT conditions only hold if (1) strong duality holds, (2) the primal
\end{flushleft}


\begin{flushleft}
optimum is attained, and (3) the dual optimum is attained. In this example, the
\end{flushleft}


\begin{flushleft}
KKT conditions fail because the dual optimum is not attained.
\end{flushleft}


\begin{flushleft}
5.27 Equality constrained least-squares. Consider the equality constrained least-squares problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
Ax $-$ b 22
\end{flushleft}


\begin{flushleft}
subject to Gx = h
\end{flushleft}


\begin{flushleft}
where A $\in$ Rm×n with rank A = n, and G $\in$ Rp×n with rank G = p.
\end{flushleft}


\begin{flushleft}
Give the KKT conditions, and derive expressions for the primal solution x and the dual
\end{flushleft}


\begin{flushleft}
solution $\nu$ .
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(x, $\nu$)
\end{flushleft}





=


=





\begin{flushleft}
Ax $-$ b
\end{flushleft}





2


2





\begin{flushleft}
+ $\nu$ T (Gx $-$ h)
\end{flushleft}





\begin{flushleft}
xT AT Ax + (GT $\nu$ $-$ 2AT b)T x $-$ $\nu$ T h,
\end{flushleft}





\begin{flushleft}
with minimizer x = $-$(1/2)(AT A)$-$1 (GT $\nu$ $-$ 2AT b). The dual function is
\end{flushleft}


\begin{flushleft}
g($\nu$) = $-$(1/4)(GT $\nu$ $-$ 2AT b)T (AT A)$-$1 (GT $\nu$ $-$ 2AT b) $-$ $\nu$ T h
\end{flushleft}


\begin{flushleft}
(b) The optimality conditions are
\end{flushleft}


\begin{flushleft}
2AT (Ax $-$ b) + GT $\nu$ = 0,
\end{flushleft}





\begin{flushleft}
Gx = h.
\end{flushleft}





\begin{flushleft}
(c) From the first equation,
\end{flushleft}


\begin{flushleft}
x = (AT A)$-$1 (AT b $-$ (1/2)GT $\nu$ ).
\end{flushleft}


\begin{flushleft}
Plugging this expression for x into the second equation gives
\end{flushleft}


\begin{flushleft}
G(AT A)$-$1 AT b $-$ (1/2)G(AT A)$-$1 GT $\nu$ = h
\end{flushleft}


\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
$\nu$ = $-$2(G(AT A)$-$1 GT )$-$1 (h $-$ G(AT A)$-$1 AT b).
\end{flushleft}





\begin{flushleft}
Substituting in the first expression gives an analytical expression for x .
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
5.28 Prove (without using any linear programming code) that the optimal solution of the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
47x
\end{flushleft}


4


\begin{flushleft}
 1 + 93x2 + 17x3 $-$ 93x
\end{flushleft}


$-$1


$-$6


1


\begin{flushleft}
3 
\end{flushleft}


\begin{flushleft}
 $-$1 $-$2
\end{flushleft}


7


\begin{flushleft}
1 
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
3 $-$10 $-$1  
\end{flushleft}


\begin{flushleft}
 0
\end{flushleft}


\begin{flushleft}
 $-$6 $-$11 $-$2 12 
\end{flushleft}


1


6


$-$1 $-$3





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
x2 
\end{flushleft}


\begin{flushleft}
x3 
\end{flushleft}


\begin{flushleft}
x4
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





$-$3


5


$-$8


$-$7


4





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
is unique, and given by x = (1, 1, 1, 1).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
Clearly, x = (1, 1, 1, 1) is feasible (it satisfies the first four constraints with equality).
\end{flushleft}


\begin{flushleft}
The point z = (3, 2, 2, 7, 0) is a certificate of optimality of x = (1, 1, 1, 1):
\end{flushleft}


\begin{flushleft}
$\bullet$ z is dual feasible: z
\end{flushleft}


\begin{flushleft}
0 and AT z + c = 0.
\end{flushleft}


\begin{flushleft}
$\bullet$ z satisfies the complementary slackness condition:
\end{flushleft}


\begin{flushleft}
zi (aTi x $-$ bi ) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
since the first four components of Ax $-$ b and the last component of z are zero.
\end{flushleft}





\begin{flushleft}
5.29 The problem
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$3x21 + x22 + 2x23 + 2(x1 + x2 + x3 )
\end{flushleft}


\begin{flushleft}
x21 + x22 + x23 = 1,
\end{flushleft}





\begin{flushleft}
is a special case of (5.32), so strong duality holds even though the problem is not convex.
\end{flushleft}


\begin{flushleft}
Derive the KKT conditions. Find all solutions x, $\nu$ that satisfy the KKT conditions.
\end{flushleft}


\begin{flushleft}
Which pair corresponds to the optimum?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The KKT conditions are
\end{flushleft}


\begin{flushleft}
x21 +x22 +x23 = 1,
\end{flushleft}





\begin{flushleft}
($-$3+$\nu$)x1 +1 = 0,
\end{flushleft}





\begin{flushleft}
(1+$\nu$)x2 +1 = 0,
\end{flushleft}





\begin{flushleft}
(2+$\nu$)x3 +1 = 0.
\end{flushleft}





\begin{flushleft}
(b) A first observation is that the KKT conditions imply $\nu$ = 2, $\nu$ = $-$1, $\nu$ = 3. We can
\end{flushleft}


\begin{flushleft}
therefore eliminate x and reduce the KKT conditions to a nonlinear equation in $\nu$:
\end{flushleft}


1


1


1


+


+


=1


\begin{flushleft}
($-$3 + $\nu$)2
\end{flushleft}


\begin{flushleft}
(1 + $\nu$)2
\end{flushleft}


\begin{flushleft}
(2 + $\nu$)2
\end{flushleft}


\begin{flushleft}
The lefthand side is plotted in the figure.
\end{flushleft}


10


8


6


4


2


0





\begin{flushleft}
PSfrag replacements $-$8
\end{flushleft}





$-$6





$-$4





$-$2





0





\begin{flushleft}
$\nu$
\end{flushleft}





2





4





6





8





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
There are four solutions:
\end{flushleft}


\begin{flushleft}
$\nu$ = $-$3.15,
\end{flushleft}





\begin{flushleft}
$\nu$ = 0.22,
\end{flushleft}





\begin{flushleft}
$\nu$ = 1.89,
\end{flushleft}





\begin{flushleft}
$\nu$ = 4.04,
\end{flushleft}





\begin{flushleft}
corresponding to
\end{flushleft}


\begin{flushleft}
x = (0.16, 0.47, $-$0.87),
\end{flushleft}





\begin{flushleft}
x = (0.36, $-$0.82, 0.45),
\end{flushleft}





\begin{flushleft}
x = (0.90, $-$0.35, 0.26),
\end{flushleft}





\begin{flushleft}
x = ($-$0.97, $-$0.20, 0.17).
\end{flushleft}





\begin{flushleft}
(c) $\nu$ is the largest of the four values: $\nu$ = 4.0352. This can be seen several ways. The
\end{flushleft}


\begin{flushleft}
simplest way is to compare the objective values of the four solutions x, which are
\end{flushleft}


\begin{flushleft}
f0 (x) = 1.17,
\end{flushleft}





\begin{flushleft}
f0 (x) = $-$0.56,
\end{flushleft}





\begin{flushleft}
f0 (x) = 0.67,
\end{flushleft}





\begin{flushleft}
f0 (x) = $-$4.70.
\end{flushleft}





\begin{flushleft}
We can also evaluate the dual objective at the four candidate values for $\nu$. Finally
\end{flushleft}


\begin{flushleft}
we can note that we must have
\end{flushleft}


\begin{flushleft}
$\nabla$2 f0 (x ) + $\nu$ $\nabla$2 f1 (x )
\end{flushleft}





0,





\begin{flushleft}
because x is a minimizer of L(x, $\nu$ ). In other words
\end{flushleft}


$-$3


0


0





0


1


0





0


0


2





\begin{flushleft}
+$\nu$
\end{flushleft}





1


0


0





0


1


0





0


0


1





0,





\begin{flushleft}
and therefore $\nu$ $\geq$ 3.
\end{flushleft}


\begin{flushleft}
5.30 Derive the KKT conditions for the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr X $-$ log det X
\end{flushleft}


\begin{flushleft}
Xs = y,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
with variable X $\in$ Sn and domain Sn
\end{flushleft}


\begin{flushleft}
++ . y $\in$ R and s $\in$ R are given, with s y = 1.
\end{flushleft}


\begin{flushleft}
Verify that the optimal solution is given by
\end{flushleft}





\begin{flushleft}
X = I + yy T $-$
\end{flushleft}





1


\begin{flushleft}
ssT .
\end{flushleft}


\begin{flushleft}
sT s
\end{flushleft}





\begin{flushleft}
Solution. We introduce a Lagrange multiplier z $\in$ Rn for the equality constraint. The
\end{flushleft}


\begin{flushleft}
KKT optimality conditions are:
\end{flushleft}


\begin{flushleft}
X
\end{flushleft}





0,





\begin{flushleft}
Xs = y,
\end{flushleft}





\begin{flushleft}
X $-$1 = I +
\end{flushleft}





1


\begin{flushleft}
(zsT + sz T ).
\end{flushleft}


2





\begin{flushleft}
(5.30.A)
\end{flushleft}





\begin{flushleft}
We first determine z from the condition Xs = y. Multiplying the gradient equation on
\end{flushleft}


\begin{flushleft}
the right with y gives
\end{flushleft}


1


\begin{flushleft}
s = X $-$1 y = y + (z + (z T y)s).
\end{flushleft}


\begin{flushleft}
(5.30.B)
\end{flushleft}


2


\begin{flushleft}
By taking the inner product with y on both sides and simplifying, we get z T y = 1 $-$ y T y.
\end{flushleft}


\begin{flushleft}
Substituting in (5.30.B) we get
\end{flushleft}


\begin{flushleft}
z = $-$2y + (1 + y T y)s,
\end{flushleft}


\begin{flushleft}
and substituting this expression for z in (5.30.A) gives
\end{flushleft}


\begin{flushleft}
X $-$1
\end{flushleft}





=


=





1


\begin{flushleft}
($-$2ysT $-$ 2sy T + 2(1 + y T y)ssT )
\end{flushleft}


2


\begin{flushleft}
I + (1 + y T y)ssT $-$ ysT $-$ sy T .
\end{flushleft}





\begin{flushleft}
I+
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
Finally we verify that this is the inverse of the matrix X given above:
\end{flushleft}


\begin{flushleft}
I + (1 + y T y)ssT $-$ ysT $-$ sy T X
\end{flushleft}


=


=





\begin{flushleft}
(I + yy T $-$ (1/sT s)ssT ) + (1 + y T y)(ssT + sy T $-$ ssT )
\end{flushleft}





\begin{flushleft}
$-$ (ysT + yy T $-$ ysT ) $-$ (sy T + (y T y)sy T $-$ (1/sT s)ssT )
\end{flushleft}


\begin{flushleft}
I.
\end{flushleft}





\begin{flushleft}
To complete the solution, we prove that X
\end{flushleft}


\begin{flushleft}
X = I + yy T $-$
\end{flushleft}





\begin{flushleft}
ssT
\end{flushleft}


=


\begin{flushleft}
sT s
\end{flushleft}





\begin{flushleft}
I+
\end{flushleft}





\begin{flushleft}
0. An easy way to see this is to note that
\end{flushleft}





\begin{flushleft}
ssT
\end{flushleft}


\begin{flushleft}
ysT
\end{flushleft}


\begin{flushleft}
$-$ T
\end{flushleft}


\begin{flushleft}
s 2
\end{flushleft}


\begin{flushleft}
s s
\end{flushleft}





\begin{flushleft}
I+
\end{flushleft}





\begin{flushleft}
ysT
\end{flushleft}


\begin{flushleft}
ssT
\end{flushleft}


\begin{flushleft}
$-$ T
\end{flushleft}


\begin{flushleft}
s 2
\end{flushleft}


\begin{flushleft}
s s
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





.





\begin{flushleft}
5.31 Supporting hyperplane interpretation of KKT conditions. Consider a convex problem with
\end{flushleft}


\begin{flushleft}
no equality constraints,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Assume that x $\in$ Rn and $\lambda$ $\in$ Rm satisfy the KKT conditions
\end{flushleft}


\begin{flushleft}
fi (x )
\end{flushleft}


\begin{flushleft}
$\lambda$i
\end{flushleft}


\begin{flushleft}
$\lambda$i fi (x )
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
$\lambda$ $\nabla$fi (x )
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}





\begin{flushleft}
$\nabla$f0 (x ) +
\end{flushleft}


\begin{flushleft}
Show that
\end{flushleft}





$\leq$


$\geq$


=


=





0,


0,


0,


0.





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
$\nabla$f0 (x )T (x $-$ x ) $\geq$ 0
\end{flushleft}





\begin{flushleft}
for all feasible x. In other words the KKT conditions imply the simple optimality criterion
\end{flushleft}


\begin{flushleft}
of §4.2.3.
\end{flushleft}


\begin{flushleft}
Solution. Suppose x is feasible. Since fi are convex and fi (x) $\leq$ 0 we have
\end{flushleft}


\begin{flushleft}
0 $\geq$ fi (x) $\geq$ fi (x ) + $\nabla$fi (x )T (x $-$ x ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Using $\lambda$i $\geq$ 0, we conclude that
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





0





$\geq$





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (x ) + $\nabla$fi (x )T (x $-$ x )
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (x ) +
\end{flushleft}





=





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





=





\begin{flushleft}
$\lambda$i $\nabla$fi (x )T (x $-$ x )
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
$-$$\nabla$f0 (x ) (x $-$ x ).
\end{flushleft}





\begin{flushleft}
In the last line, we use the complementary slackness condition $\lambda$i fi (x ) = 0, and the last
\end{flushleft}


\begin{flushleft}
KKT condition. This shows that $\nabla$f0 (x )T (x$-$x ) $\geq$ 0, i.e., $\nabla$f0 (x ) defines a supporting
\end{flushleft}


\begin{flushleft}
hyperplane to the feasible set at x .
\end{flushleft}





\begin{flushleft}
Perturbation and sensitivity analysis
\end{flushleft}


\begin{flushleft}
5.32 Optimal value of perturbed problem. Let f0 , f1 , . . . , fm : Rn $\rightarrow$ R be convex. Show that
\end{flushleft}


\begin{flushleft}
the function
\end{flushleft}


\begin{flushleft}
p (u, v) = inf\{f0 (x) | $\exists$x $\in$ D, fi (x) $\leq$ ui , i = 1, . . . , m, Ax $-$ b = v\}
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
is convex. This function is the optimal cost of the perturbed problem, as a function of
\end{flushleft}


\begin{flushleft}
the perturbations u and v (see §5.6.1).
\end{flushleft}


\begin{flushleft}
Solution. Define the function
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ ui , i = 1, . . . , m,
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


$\infty$





\begin{flushleft}
G(x, u, v) =
\end{flushleft}





\begin{flushleft}
Ax $-$ b = v
\end{flushleft}





\begin{flushleft}
G is convex on its domain
\end{flushleft}


\begin{flushleft}
dom G = \{(x, u, v) | x $\in$ D, fi (x) $\leq$ ui , i = 1, . . . , m, Ax $-$ b = v\},
\end{flushleft}


\begin{flushleft}
which is easily shown to be convex. Therefore G is convex, jointly in x, u, v. Therefore
\end{flushleft}


\begin{flushleft}
p (u, v) = inf G(x, u, v)
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
is convex.
\end{flushleft}


\begin{flushleft}
5.33 Parametrized
\end{flushleft}





\begin{flushleft}
1 -norm
\end{flushleft}





\begin{flushleft}
approximation. Consider the
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
with variable x $\in$ R3 , and
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
A=
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





$-$2


$-$5


$-$7


$-$1


1


2





7


$-$1


3


4


5


$-$5





1


3


$-$5


$-$4


5


$-$1





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
Ax + b + d
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
1 -norm
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
b=
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





$-$4


3


9


0


$-$11


5





1





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
We denote by p ( ) the optimal value as a function of .
\end{flushleft}


\begin{flushleft}
(a) Suppose
\end{flushleft}





\begin{flushleft}
minimization problem
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
d=
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





$-$10


$-$13


$-$27


$-$10


$-$7


14





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
.
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
= 0. Prove that x = 1 is optimal. Are there any other optimal points?
\end{flushleft}





\begin{flushleft}
(b) Show that p ( ) is affine on an interval that includes
\end{flushleft}





= 0.





\begin{flushleft}
Solution. The dual problem of
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
Ax + b
\end{flushleft}





1





\begin{flushleft}
is given by
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
bT z
\end{flushleft}


\begin{flushleft}
AT z = 0
\end{flushleft}


\begin{flushleft}
z $\infty$ $\leq$ 1.
\end{flushleft}





\begin{flushleft}
If x and z are both feasible, then
\end{flushleft}


\begin{flushleft}
Ax + b
\end{flushleft}





1





\begin{flushleft}
$\geq$ z T (Ax + b) = bT z
\end{flushleft}





\begin{flushleft}
(this follows from the inequality uT v $\leq$ u $\infty$ v 1 ). We have equality ( Ax + b 1 = bT z)
\end{flushleft}


\begin{flushleft}
only if zi (Ax + b)i = |(Ax + b)i | for all i. In other words, the optimality conditions are: x
\end{flushleft}


\begin{flushleft}
and z are optimal if and only if AT z = 0, z $\infty$ $\leq$ 1 and the following {`}complementarity
\end{flushleft}


\begin{flushleft}
conditions' hold:
\end{flushleft}


\begin{flushleft}
$-$1 $<$ zi $<$ 1
\end{flushleft}


\begin{flushleft}
=$\Rightarrow$ (Ax + b)i = 0
\end{flushleft}


\begin{flushleft}
(Ax + b)i $>$ 0 =$\Rightarrow$ zi = 1
\end{flushleft}


\begin{flushleft}
(Ax + b)i $<$ 0 =$\Rightarrow$ zi = $-$1.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
(a) b + Ax = (2, 0, 0, $-$1, 0, 1), so the optimality conditions tell us that the dual optimal
\end{flushleft}


\begin{flushleft}
solution must satisfy z1 = 1, z4 = $-$1, and z5 = 1. It remains to find the other 3
\end{flushleft}


\begin{flushleft}
components z2 , z3 , z6 . We can do this by solving
\end{flushleft}


\begin{flushleft}
AT z =
\end{flushleft}





$-$5


$-$1


3





$-$7


3


$-$5





\begin{flushleft}
z2
\end{flushleft}


\begin{flushleft}
z3
\end{flushleft}


\begin{flushleft}
z5
\end{flushleft}





1


5


5





$-$2


7


1





+





$-$1


4


$-$4





2


$-$5


$-$1





1


$-$1


1





= 0,





\begin{flushleft}
in the three variables z2 , z3 , z6 . The solution is z = (1, $-$0.5, 0.5, $-$1, 0, 1). By
\end{flushleft}


\begin{flushleft}
construction z satisfies AT z = 0, and the complementarity conditions. It also
\end{flushleft}


\begin{flushleft}
satisfies z $\infty$ $\leq$ 1, hence it is optimal.
\end{flushleft}





\begin{flushleft}
(b) All primal optimal points x must satisfy the complementarity conditions with the
\end{flushleft}


\begin{flushleft}
dual optimal z we have constructed. This implies that
\end{flushleft}


\begin{flushleft}
(Ax + b)2 = (Ax + b)3 = (Ax + b)5 = 0.
\end{flushleft}


\begin{flushleft}
This forms a set of three linearly independent equations in three variables. Therefore
\end{flushleft}


\begin{flushleft}
the solution is unique.
\end{flushleft}


\begin{flushleft}
(c) z remains dual feasible for nonzero . It will be optimal as long as at the optimal
\end{flushleft}


\begin{flushleft}
x ( ),
\end{flushleft}


\begin{flushleft}
(b + d + Ax ( ))k = 0, k = 2, 3, 5.
\end{flushleft}


\begin{flushleft}
Solving this three equations for x ( ) yields
\end{flushleft}


\begin{flushleft}
x ( ) = (1, 1, 1) + ($-$3, 2, 0).
\end{flushleft}


\begin{flushleft}
To find the limits on , we note that z and x ( ) are optimal as long as
\end{flushleft}


\begin{flushleft}
(A(x ( ) + b + d)1 = 2 + 10 $\geq$ 0
\end{flushleft}


\begin{flushleft}
(A(x ( ) + b + d)4 = $-$1 + $\leq$ 0
\end{flushleft}


\begin{flushleft}
(A(x ( ) + b + d)6 = 1 $-$ 2 $\geq$ 0
\end{flushleft}


\begin{flushleft}
i.e., $-$1/5 $\leq$ $\leq$ 1/2.
\end{flushleft}


\begin{flushleft}
The optimal value is
\end{flushleft}


\begin{flushleft}
p ( ) = (b + d)T z = 4 + 7 .
\end{flushleft}


\begin{flushleft}
5.34 Consider the pair of primal and dual LPs
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
A=
\end{flushleft}





$-$4


$-$17


1


3


$-$11





12


12


0


3


2





$-$2


7


$-$6


22


$-$1





1


11


1


$-$1


$-$8





\begin{flushleft}
(c + d)T x
\end{flushleft}


\begin{flushleft}
Ax b + f
\end{flushleft}





\begin{flushleft}
$-$(b + f )T z
\end{flushleft}


\begin{flushleft}
AT z + c + d = 0
\end{flushleft}


\begin{flushleft}
z 0
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
c = (49, $-$34, $-$50, $-$5), d = (3, 8, 21, 25), and
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
b=
\end{flushleft}





8


13


$-$4


27


$-$18





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
is a parameter.
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
f =
\end{flushleft}





6


15


$-$13


48


8





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
(a) Prove that x = (1, 1, 1, 1) is optimal when = 0, by constructing a dual optimal
\end{flushleft}


\begin{flushleft}
point z that has the same objective value as x . Are there any other primal or dual
\end{flushleft}


\begin{flushleft}
optimal solutions?
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) Give an explicit expression for the optimal value p ( ) as a function of on an
\end{flushleft}


\begin{flushleft}
interval that contains = 0. Specify the interval on which your expression is valid.
\end{flushleft}


\begin{flushleft}
Also give explicit expressions for the primal solution x ( ) and the dual solution
\end{flushleft}


\begin{flushleft}
z ( ) as a function of , on the same interval.
\end{flushleft}


\begin{flushleft}
Hint. First calculate x ( ) and z ( ), assuming that the primal and dual constraints
\end{flushleft}


\begin{flushleft}
that are active at the optimum for = 0, remain active at the optimum for values
\end{flushleft}


\begin{flushleft}
of around 0. Then verify that this assumption is correct.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) All constraints except the first are active at x = (1, 1, 1, 1), so complementary slackness implies that z1 = 0 at the dual optimum.
\end{flushleft}


\begin{flushleft}
For this problem, the complementary slackness condition uniquely determines z: We
\end{flushleft}


\begin{flushleft}
must have
\end{flushleft}


\begin{flushleft}
A¯T z¯ + c = 0,
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}

\end{flushleft}





$-$17


1


\begin{flushleft}

\end{flushleft}


¯


\begin{flushleft}
A=
\end{flushleft}


3


$-$11





12


0


3


2





7


$-$6


22


$-$1





\begin{flushleft}

\end{flushleft}





11


\begin{flushleft}
1 
\end{flushleft}


,


\begin{flushleft}
$-$1 
\end{flushleft}


$-$8





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
z2
\end{flushleft}


\begin{flushleft}
 z3 
\end{flushleft}


\begin{flushleft}
z¯ = 
\end{flushleft}


\begin{flushleft}
z4 
\end{flushleft}


\begin{flushleft}
z5
\end{flushleft}





\begin{flushleft}
A¯ is nonsingular, so A¯T z¯ + c = 0 has a unique solution: z¯ = (2, 1, 2, 2). All components are nonnegative, so we conclude that z = (0, 2, 1, 2, 2) is dual feasible.
\end{flushleft}


\begin{flushleft}
(b) We expect that for small the same primal and dual constraints remain active.
\end{flushleft}


\begin{flushleft}
Let us first construct x ( ) and z ( ) under that assumption, and then verify using
\end{flushleft}


\begin{flushleft}
complementary slackness that they are optimal for the perturbed problem.
\end{flushleft}


\begin{flushleft}
To keep the last four constraints of x ( ) active, we must have
\end{flushleft}


\begin{flushleft}
x ( ) = (1, 1, 1, 1) + ∆x
\end{flushleft}


¯


\begin{flushleft}
where A∆x
\end{flushleft}


\begin{flushleft}
= (f2 , f3 , f4 , f5 ). We find ∆x = (0, 1, 2, $-$1). x ( ) is primal feasible as
\end{flushleft}


\begin{flushleft}
long as
\end{flushleft}


\begin{flushleft}
A((1, 1, 1, 1) + (0, 1, 2, $-$1) $\leq$ b + f.
\end{flushleft}


\begin{flushleft}
By construction, this holds with equality for constraints 2--5. For the first inequality
\end{flushleft}


\begin{flushleft}
we obtain
\end{flushleft}


7+7 $\leq$8+6 .





\begin{flushleft}
i.e., $\leq$ 1.
\end{flushleft}


\begin{flushleft}
If we keep the first component of z ( ) zero, the other components follow from
\end{flushleft}


\begin{flushleft}
AT z ( ) + c + d = 0. We must have
\end{flushleft}


\begin{flushleft}
z ( ) = (0, 2, 1, 2, 2) + ∆z
\end{flushleft}


\begin{flushleft}
where AT ∆z + f = 0 and ∆z1 = 0. We find ∆z = (0, $-$1, 2, 0, 2). By construction,
\end{flushleft}


\begin{flushleft}
z ( ) satisfies the equality constraints AT z ( ) + c + f = 0, so it is dual feasible if
\end{flushleft}


\begin{flushleft}
its components are nonnegative:
\end{flushleft}


\begin{flushleft}
z ( ) = (0, 2 $-$ , 1 + 2 , 2, 2 + 2 ) $\geq$ 0,
\end{flushleft}


\begin{flushleft}
i.e., $-$1/2 $\leq$ $\leq$ 2.
\end{flushleft}


\begin{flushleft}
In conclusion, we constructed x ( ) and z ( ) that are primal and dual feasible for
\end{flushleft}


\begin{flushleft}
the perturbed problem, and complementary. Therefore they must be optimal for
\end{flushleft}


\begin{flushleft}
the perturbed problems in the interval $-$1/2 $\leq$ $\leq$ 1..
\end{flushleft}





\begin{flushleft}
(c) The optimal value is quadratic
\end{flushleft}





\begin{flushleft}
p ( ) = (c + d)T x ( ) = $-$(b + f )T z ( ) = $-$40 $-$ 72 + 25 2 .
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
5.35 Sensitivity analysis for GPs. Consider a GP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 1,
\end{flushleft}


\begin{flushleft}
hi (x) = 1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
where f0 , . . . , fm are posynomials, h1 , . . . , hp are monomials, and the domain of the problem is Rn
\end{flushleft}


\begin{flushleft}
++ . We define the perturbed GP as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ eui ,
\end{flushleft}


\begin{flushleft}
hi (x) = evi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
and we denote the optimal value of the perturbed GP as p (u, v). We can think of ui and
\end{flushleft}


\begin{flushleft}
vi as relative, or fractional, perturbations of the constraints. For example, u1 = $-$0.01
\end{flushleft}


\begin{flushleft}
corresponds to tightening the first inequality constraint by (approximately) 1\%.
\end{flushleft}


\begin{flushleft}
Let $\lambda$ and $\nu$ be optimal dual variables for the convex form GP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log f0 (y)
\end{flushleft}


\begin{flushleft}
log fi (y) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
log hi (y) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
with variables yi = log xi . Assuming that p (u, v) is differentiable at u = 0, v = 0, relate
\end{flushleft}


\begin{flushleft}
$\lambda$ and $\nu$ to the derivatives of p (u, v) at u = 0, v = 0. Justify the statement {``}Relaxing
\end{flushleft}


\begin{flushleft}
the ith constraint by $\alpha$ percent will give an improvement in the objective of around $\alpha$$\lambda$i
\end{flushleft}


\begin{flushleft}
percent, for $\alpha$ small.''
\end{flushleft}


\begin{flushleft}
Solution. $-$$\lambda$ , $-$$\nu$ are {`}shadow prices' for the perturbed problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log f0 (y)
\end{flushleft}


\begin{flushleft}
log fi (y) $\leq$ ui ,
\end{flushleft}


\begin{flushleft}
log hi (y) = vi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
i.e., if the optimal value log p (u, v) is differentiable at the origin, they are the derivatives
\end{flushleft}


\begin{flushleft}
of the optimal value,
\end{flushleft}


\begin{flushleft}
$-$$\lambda$i =
\end{flushleft}





\begin{flushleft}
$\partial$p (0, 0)/$\partial$ui
\end{flushleft}


\begin{flushleft}
$\partial$ log p (0, 0)
\end{flushleft}


=


\begin{flushleft}
$\partial$ui
\end{flushleft}


\begin{flushleft}
p (0, 0)
\end{flushleft}





\begin{flushleft}
$-$ $\nu$i =
\end{flushleft}





\begin{flushleft}
$\partial$p (0, 0)/$\partial$vi
\end{flushleft}


\begin{flushleft}
$\partial$ log p∗ (0, 0)
\end{flushleft}


=


.


\begin{flushleft}
$\partial$vi
\end{flushleft}


\begin{flushleft}
p (0, 0)
\end{flushleft}





\begin{flushleft}
Theorems of alternatives
\end{flushleft}


\begin{flushleft}
5.36 Alternatives for linear equalities. Consider the linear equations Ax = b, where A $\in$ R m×n .
\end{flushleft}


\begin{flushleft}
From linear algebra we know that this equation has a solution if and only b $\in$ R(A), which
\end{flushleft}


\begin{flushleft}
occurs if and only if b $\perp$ N (AT ). In other words, Ax = b has a solution if and only if
\end{flushleft}


\begin{flushleft}
there exists no y $\in$ Rm such that AT y = 0 and bT y = 0.
\end{flushleft}


\begin{flushleft}
Derive this result from the theorems of alternatives in §5.8.2.
\end{flushleft}


\begin{flushleft}
Solution. We first note that we can't directly apply the results on strong alternatives
\end{flushleft}


\begin{flushleft}
for systems of the form
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
Ax = b
\end{flushleft}





\begin{flushleft}
fi (x) $<$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
Ax = b,
\end{flushleft}





\begin{flushleft}
or
\end{flushleft}


\begin{flushleft}
because the theorems all assume that Ax = b is feasible.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We can apply the theorem for strict inequalities to
\end{flushleft}


\begin{flushleft}
t $<$ $-$1,
\end{flushleft}





\begin{flushleft}
Ax + bt = b.
\end{flushleft}





\begin{flushleft}
(5.36.A)
\end{flushleft}





\begin{flushleft}
This is feasible if and only if Ax = b is feasible: Indeed, if A˜
\end{flushleft}


\begin{flushleft}
x = b is feasible, then
\end{flushleft}


\begin{flushleft}
A(3˜
\end{flushleft}


\begin{flushleft}
x) $-$ 2b = b. so x = 3˜
\end{flushleft}


\begin{flushleft}
x, t = $-$2 satisfies (5.36.A). Conversely, if x
\end{flushleft}


\begin{flushleft}
˜, t˜ satisfies (5.36.A)
\end{flushleft}


\begin{flushleft}
then 1 $-$ t˜ $>$ 2 and
\end{flushleft}


\begin{flushleft}
A(˜
\end{flushleft}


\begin{flushleft}
x/(1 $-$ t˜)) = b,
\end{flushleft}





\begin{flushleft}
so Ax = b is feasible.
\end{flushleft}


\begin{flushleft}
Moreover Ax + bt = b is always feasible (choose x = 0, t = 1, so we can apply the theorem
\end{flushleft}


\begin{flushleft}
of alternatives for strict inequalities to (5.36.A). The dual function is
\end{flushleft}


\begin{flushleft}
$\lambda$ $-$ bT $\nu$
\end{flushleft}


$-$$\infty$





\begin{flushleft}
g($\lambda$, $\nu$) = inf ($\lambda$(t + 1) + $\nu$ T (Ax + bt $-$ b)) =
\end{flushleft}


\begin{flushleft}
x,t
\end{flushleft}





\begin{flushleft}
The alternative reduces to
\end{flushleft}





\begin{flushleft}
AT $\nu$ = 0,
\end{flushleft}





\begin{flushleft}
AT $\nu$ = 0, $\lambda$ + bT $\nu$ = 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
bT $\nu$ $<$ 0.
\end{flushleft}





\begin{flushleft}
5.37 [BT97] Existence of equilibrium distribution in finite state Markov chain. Let P $\in$ R n×n
\end{flushleft}


\begin{flushleft}
be a matrix that satisfies
\end{flushleft}


\begin{flushleft}
pij $\geq$ 0,
\end{flushleft}





\begin{flushleft}
P T 1 = 1,
\end{flushleft}





\begin{flushleft}
i, j = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
i.e., the coefficients are nonnegative and the columns sum to one. Use Farkas' lemma to
\end{flushleft}


\begin{flushleft}
prove there exists a y $\in$ Rn such that
\end{flushleft}


\begin{flushleft}
P y = y,
\end{flushleft}





\begin{flushleft}
y
\end{flushleft}





\begin{flushleft}
1T y = 1.
\end{flushleft}





0,





\begin{flushleft}
(We can interpret y as an equilibrium distribution of the Markov chain with n states and
\end{flushleft}


\begin{flushleft}
transition probability matrix P .)
\end{flushleft}


\begin{flushleft}
Solution. Suppose there exists no such y, i.e.,
\end{flushleft}


\begin{flushleft}
P $-$I
\end{flushleft}


\begin{flushleft}
1T
\end{flushleft}





\begin{flushleft}
y=
\end{flushleft}





0


1





,





\begin{flushleft}
y
\end{flushleft}





0,





\begin{flushleft}
is infeasible. From Farkas' lemma there exist z $\in$ Rn and w $\in$ R such that
\end{flushleft}


\begin{flushleft}
(P $-$ I)T z + w1
\end{flushleft}





0,





\begin{flushleft}
PTz
\end{flushleft}





\begin{flushleft}
z.
\end{flushleft}





\begin{flushleft}
w $<$ 0,
\end{flushleft}





\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
Since the elements of P are nonnegative with unit column sums we must have
\end{flushleft}


\begin{flushleft}
(P T z)i $\leq$ max zj
\end{flushleft}


\begin{flushleft}
j
\end{flushleft}





\begin{flushleft}
which contradicts P T z
\end{flushleft}





1.





\begin{flushleft}
5.38 [BT97] Option pricing. We apply the results of example 5.10, page 263, to a simple
\end{flushleft}


\begin{flushleft}
problem with three assets: a riskless asset with fixed return r $>$ 1 over the investment
\end{flushleft}


\begin{flushleft}
period of interest (for example, a bond), a stock, and an option on the stock. The option
\end{flushleft}


\begin{flushleft}
gives us the right to purchase the stock at the end of the period, for a predetermined
\end{flushleft}


\begin{flushleft}
price K.
\end{flushleft}


\begin{flushleft}
We consider two scenarios. In the first scenario, the price of the stock goes up from
\end{flushleft}


\begin{flushleft}
S at the beginning of the period, to Su at the end of the period, where u $>$ r. In this
\end{flushleft}


\begin{flushleft}
scenario, we exercise the option only if Su $>$ K, in which case we make a profit of Su $-$ K.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
Otherwise, we do not exercise the option, and make zero profit. The value of the option
\end{flushleft}


\begin{flushleft}
at the end of the period, in the first scenario, is therefore max\{0, Su $-$ K\}.
\end{flushleft}


\begin{flushleft}
In the second scenario, the price of the stock goes down from S to Sd, where d $<$ 1. The
\end{flushleft}


\begin{flushleft}
value at the end of the period is max\{0, Sd $-$ K\}.
\end{flushleft}


\begin{flushleft}
In the notation of example 5.10,
\end{flushleft}


\begin{flushleft}
V =
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
uS
\end{flushleft}


\begin{flushleft}
dS
\end{flushleft}





\begin{flushleft}
max\{0, Su $-$ K\}
\end{flushleft}


\begin{flushleft}
max\{0, Sd $-$ K\}
\end{flushleft}





,





\begin{flushleft}
p1 = 1,
\end{flushleft}





\begin{flushleft}
p2 = S,
\end{flushleft}





\begin{flushleft}
p3 = C,
\end{flushleft}





\begin{flushleft}
where C is the price of the option.
\end{flushleft}


\begin{flushleft}
Show that for given r, S, K, u, d, the option price C is uniquely determined by the
\end{flushleft}


\begin{flushleft}
no-arbitrage condition. In other words, the market for the option is complete.
\end{flushleft}


\begin{flushleft}
Solution. The condition V T y = p reduces to
\end{flushleft}


\begin{flushleft}
y1 + y2 = 1/r,
\end{flushleft}





\begin{flushleft}
uy1 + dy2 = 1,
\end{flushleft}





\begin{flushleft}
y1 max\{0, Su $-$ K\} + y2 max\{0, Sd $-$ K\} = C.
\end{flushleft}





\begin{flushleft}
The first two equations determine y1 and y2 uniquely:
\end{flushleft}


\begin{flushleft}
y1 =
\end{flushleft}





\begin{flushleft}
r$-$d
\end{flushleft}


,


\begin{flushleft}
r(u $-$ d)
\end{flushleft}





\begin{flushleft}
y2 =
\end{flushleft}





\begin{flushleft}
u$-$r
\end{flushleft}


,


\begin{flushleft}
r(u $-$ d)
\end{flushleft}





\begin{flushleft}
and these values are positive because u $>$ r $>$ d. Hence
\end{flushleft}


\begin{flushleft}
C=
\end{flushleft}





\begin{flushleft}
(r $-$ d) max\{0, Su $-$ K\} + (u $-$ r) max\{0, Sd $-$ K\}
\end{flushleft}


.


\begin{flushleft}
r(u $-$ d)
\end{flushleft}





\begin{flushleft}
Generalized inequalities
\end{flushleft}


\begin{flushleft}
5.39 SDP relaxations of two-way partitioning problem. We consider the two-way partitioning
\end{flushleft}


\begin{flushleft}
problem (5.7), described on page 219,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
xT W x
\end{flushleft}


\begin{flushleft}
x2i = 1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n,
\end{flushleft}





(5.113)





\begin{flushleft}
with variable x $\in$ Rn . The Lagrange dual of this (nonconvex) problem is given by the
\end{flushleft}


\begin{flushleft}
SDP
\end{flushleft}


\begin{flushleft}
maximize $-$1T $\nu$
\end{flushleft}


(5.114)


\begin{flushleft}
subject to W + diag($\nu$) 0
\end{flushleft}


\begin{flushleft}
with variable $\nu$ $\in$ Rn . The optimal value of this SDP gives a lower bound on the optimal
\end{flushleft}


\begin{flushleft}
value of the partitioning problem (5.113). In this exercise we derive another SDP that
\end{flushleft}


\begin{flushleft}
gives a lower bound on the optimal value of the two-way partitioning problem, and explore
\end{flushleft}


\begin{flushleft}
the connection between the two SDPs.
\end{flushleft}


\begin{flushleft}
(a) Two-way partitioning problem in matrix form. Show that the two-way partitioning
\end{flushleft}


\begin{flushleft}
problem can be cast as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr(W X)
\end{flushleft}


\begin{flushleft}
X 0, rank X = 1
\end{flushleft}


\begin{flushleft}
Xii = 1, i = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
with variable X $\in$ Sn . Hint. Show that if X is feasible, then it has the form
\end{flushleft}


\begin{flushleft}
X = xxT , where x $\in$ Rn satisfies xi $\in$ \{$-$1, 1\} (and vice versa).
\end{flushleft}


\begin{flushleft}
(b) SDP relaxation of two-way partitioning problem. Using the formulation in part (a),
\end{flushleft}


\begin{flushleft}
we can form the relaxation
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr(W X)
\end{flushleft}


\begin{flushleft}
X 0
\end{flushleft}


\begin{flushleft}
Xii = 1,
\end{flushleft}





(5.115)


\begin{flushleft}
i = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
with variable X $\in$ Sn . This problem is an SDP, and therefore can be solved efficiently. Explain why its optimal value gives a lower bound on the optimal value of
\end{flushleft}


\begin{flushleft}
the two-way partitioning problem (5.113). What can you say if an optimal point
\end{flushleft}


\begin{flushleft}
X for this SDP has rank one?
\end{flushleft}


\begin{flushleft}
(c) We now have two SDPs that give a lower bound on the optimal value of the two-way
\end{flushleft}


\begin{flushleft}
partitioning problem (5.113): the SDP relaxation (5.115) found in part (b), and the
\end{flushleft}


\begin{flushleft}
Lagrange dual of the two-way partitioning problem, given in (5.114). What is the
\end{flushleft}


\begin{flushleft}
relation between the two SDPs? What can you say about the lower bounds found
\end{flushleft}


\begin{flushleft}
by them? Hint: Relate the two SDPs via duality.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Follows from tr(W xxT ) = xT W x and (xxT )ii = x2i .
\end{flushleft}


\begin{flushleft}
(b) It gives a lower bound because we minimize the same objective over a larger set. If
\end{flushleft}


\begin{flushleft}
X is rank one, it is optimal.
\end{flushleft}


\begin{flushleft}
(c) We write the problem as a minimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T $\nu$
\end{flushleft}


\begin{flushleft}
W + diag($\nu$)
\end{flushleft}





0.





\begin{flushleft}
Introducing a Lagrange multiplier X $\in$ Sn for the matrix inequality, we obtain the
\end{flushleft}


\begin{flushleft}
Lagrangian
\end{flushleft}


\begin{flushleft}
L($\nu$, X)
\end{flushleft}





=





\begin{flushleft}
1T $\nu$ $-$ tr(X(W + diag($\nu$)))
\end{flushleft}





=





\begin{flushleft}
1T $\nu$ $-$ tr(XW ) $-$
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\nu$i Xii
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=





\begin{flushleft}
$-$ tr(XW ) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\nu$i (1 $-$ Xii ).
\end{flushleft}





\begin{flushleft}
This is bounded below as a function of $\nu$ only if Xii = 1 for all i, so we obtain the
\end{flushleft}


\begin{flushleft}
dual problem
\end{flushleft}


\begin{flushleft}
maximize $-$ tr(W X)
\end{flushleft}


\begin{flushleft}
subject to X 0
\end{flushleft}


\begin{flushleft}
Xii = 1, i = 1, . . . , n.
\end{flushleft}


\begin{flushleft}
Changing the sign again, and switching from maximization to minimization, yields
\end{flushleft}


\begin{flushleft}
the problem in part (a).
\end{flushleft}


\begin{flushleft}
5.40 E-optimal experiment design. A variation on the two optimal experiment design problems
\end{flushleft}


\begin{flushleft}
of exercise 5.10 is the E-optimal design problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
xi vi viT
\end{flushleft}


\begin{flushleft}
1 x = 1.
\end{flushleft}





\begin{flushleft}
$\lambda$max
\end{flushleft}


\begin{flushleft}
x 0,
\end{flushleft}





$-$1





\begin{flushleft}
(See also §7.5.) Derive a dual for this problem, by first reformulating it as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1/t
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
xi vi viT
\end{flushleft}


\begin{flushleft}
tI
\end{flushleft}


\begin{flushleft}
0, 1T x = 1,
\end{flushleft}





\begin{flushleft}
with variables t $\in$ R, x $\in$ Rp and domain R++ × Rp , and applying Lagrange duality.
\end{flushleft}


\begin{flushleft}
Simplify the dual problem as much as you can.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
1/t
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
tI
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
x v vT
\end{flushleft}


\begin{flushleft}
i=1 i i i
\end{flushleft}


\begin{flushleft}
x 0, 1T x = 1.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
The Lagrangian is
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
L(t, x, Z, z, $\nu$)
\end{flushleft}





=





=





\begin{flushleft}
1/t $-$ tr
\end{flushleft}





\begin{flushleft}
Z(
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
xi vi viT $-$ tI)
\end{flushleft}





\begin{flushleft}
1/t + t tr Z +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$ z T x + $\nu$(1T x $-$ 1)
\end{flushleft}





\begin{flushleft}
xi ($-$viT Zvi $-$ zi + $\nu$) $-$ $\nu$.
\end{flushleft}





\begin{flushleft}
The minimum over xi is bounded below only if $-$viT Zvi $-$ zi + $\nu$ = 0. To minimize over t
\end{flushleft}


\begin{flushleft}
we note that
\end{flushleft}


$\surd$


\begin{flushleft}
2 tr Z Z 0
\end{flushleft}


\begin{flushleft}
inf (1/t + t tr Z) =
\end{flushleft}


$-$$\infty$


\begin{flushleft}
otherwise.
\end{flushleft}


\begin{flushleft}
t$>$0
\end{flushleft}


\begin{flushleft}
The dual function is
\end{flushleft}


\begin{flushleft}
g(Z, z, $\nu$) =
\end{flushleft}





$\surd$


\begin{flushleft}
2 tr Z $-$ $\nu$
\end{flushleft}


$-$$\infty$





\begin{flushleft}
viT Zvi + zi = $\nu$,
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
Z
\end{flushleft}





0





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





$\surd$


\begin{flushleft}
2 tr Z $-$ $\nu$
\end{flushleft}


\begin{flushleft}
viT Zvi $\leq$ $\nu$,
\end{flushleft}


\begin{flushleft}
Z 0.
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p
\end{flushleft}





\begin{flushleft}
We can define W = (1/$\nu$)Z,
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





$\surd$ $\surd$


\begin{flushleft}
2 $\nu$ tr W $-$ $\nu$
\end{flushleft}


\begin{flushleft}
viT W vi $\geq$ 1, i = 1, . . . , p
\end{flushleft}


\begin{flushleft}
W
\end{flushleft}


0.





\begin{flushleft}
Finally, optimizing over $\nu$, gives $\nu$ = tr W , so the problem simplifies further to
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr W
\end{flushleft}


\begin{flushleft}
viT W vi $\leq$ 1,
\end{flushleft}


\begin{flushleft}
W
\end{flushleft}


0.





\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
5.41 Dual of fastest mixing Markov chain problem. On page 174, we encountered the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
$-$tI P $-$ (1/n)11T
\end{flushleft}


\begin{flushleft}
tI
\end{flushleft}


\begin{flushleft}
P1 = 1
\end{flushleft}


\begin{flushleft}
Pij $\geq$ 0, i, j = 1, . . . , n
\end{flushleft}


\begin{flushleft}
Pij = 0 for (i, j) $\in$ E,
\end{flushleft}





\begin{flushleft}
with variables t $\in$ R, P $\in$ Sn .
\end{flushleft}


\begin{flushleft}
Show that the dual of this problem can be expressed as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T z $-$ (1/n)1T Y 1
\end{flushleft}


\begin{flushleft}
Y 2∗ $\leq$ 1
\end{flushleft}


\begin{flushleft}
(zi + zj ) $\leq$ Yij for (i, j) $\in$ E
\end{flushleft}





\begin{flushleft}
with variables z $\in$ Rn and Y $\in$ Sn . The norm · 2∗ is the dual of the spectral norm
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
on Sn : Y 2∗ =
\end{flushleft}


\begin{flushleft}
|$\lambda$i (Y )|, the sum of the absolute values of the eigenvalues of Y .
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
(See §A.1.6, page 639.)
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Solution. We represent the Lagrange multiplier for the last constraint as $\Lambda$ $\in$ S n , with
\end{flushleft}


\begin{flushleft}
$\lambda$ij = 0 for (i, j) $\in$ E.
\end{flushleft}


\begin{flushleft}
The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(t, P, U, V, z, W, $\Lambda$)
\end{flushleft}


=


=





\begin{flushleft}
t + tr(U ($-$tI $-$ P + (1/n)11T )) + tr(V (P $-$ (1/n)11T $-$ tI))
\end{flushleft}


\begin{flushleft}
+ z T (1 $-$ P 1) $-$ tr(W P ) + tr($\Lambda$P )
\end{flushleft}





\begin{flushleft}
(1 $-$ tr U $-$ tr V )t + tr(P ($-$U + V $-$ W + $\Lambda$ $-$ (1/2)(1z T $-$ z1T ))
\end{flushleft}


\begin{flushleft}
+ 1T z + (1/n)(1T U 1 $-$ 1T V 1).
\end{flushleft}





\begin{flushleft}
Minimizing over t and P gives the conditions
\end{flushleft}


\begin{flushleft}
(1/2)(1z T + z1T )) = V $-$ U $-$ W + $\Lambda$.
\end{flushleft}





\begin{flushleft}
tr U + tr V = 1,
\end{flushleft}


\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T z $-$ (1/n)1T (V $-$ U )1
\end{flushleft}


\begin{flushleft}
U 0, V
\end{flushleft}


\begin{flushleft}
0, tr(U + V ) = 1
\end{flushleft}


\begin{flushleft}
(zi + zj ) $\leq$ Vij $-$ Uij for (i, j) $\in$ E.
\end{flushleft}





\begin{flushleft}
This problem is equivalent to
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T z $-$ (1/n)1T Y 1
\end{flushleft}


\begin{flushleft}
Y ∗$\leq$1
\end{flushleft}


\begin{flushleft}
(zi + zj ) $\leq$ Yij for (i, j) $\in$ E
\end{flushleft}





\begin{flushleft}
with variables z $\in$ Rn , Y $\in$ Sn .
\end{flushleft}





\begin{flushleft}
5.42 Lagrange dual of conic form problem in inequality form. Find the Lagrange dual problem
\end{flushleft}


\begin{flushleft}
of the conic form problem in inequality form
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
b
\end{flushleft}





\begin{flushleft}
where A $\in$ Rm×n , b $\in$ Rm , and K is a proper cone in Rm . Make any implicit equality
\end{flushleft}


\begin{flushleft}
constraints explicit.
\end{flushleft}


\begin{flushleft}
Solution. We associate with the inequality a multiplier $\lambda$ $\in$ Rm , and form the Lagrangian
\end{flushleft}


\begin{flushleft}
L(x, $\lambda$) = cT x + $\lambda$T (Ax $-$ b).
\end{flushleft}


\begin{flushleft}
The dual function is
\end{flushleft}


\begin{flushleft}
g($\lambda$)
\end{flushleft}





=


=





\begin{flushleft}
inf cT x + $\lambda$T (Ax $-$ b)
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
$-$bT $\lambda$
\end{flushleft}


$-$$\infty$





\begin{flushleft}
AT $\lambda$ + c = 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The dual problem is to maximize g($\lambda$) over all $\lambda$
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
0 or, equivalently,
\end{flushleft}





\begin{flushleft}
$-$bT $\lambda$
\end{flushleft}


\begin{flushleft}
AT $\lambda$ + c = 0
\end{flushleft}


\begin{flushleft}
$\lambda$ K ∗ 0.
\end{flushleft}





\newpage
5





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
5.43 Dual of SOCP. Show that the dual of the SOCP
\end{flushleft}


\begin{flushleft}
fT x
\end{flushleft}


\begin{flushleft}
A i x + bi
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





2





\begin{flushleft}
$\leq$ cTi x + di ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
with variables x $\in$ Rn , can be expressed as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(bT u + di vi )
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
(A
\end{flushleft}


\begin{flushleft}
i ui + c i v i ) + f = 0
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
ui 2 $\leq$ vi , i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
with variables ui $\in$ Rni , vi $\in$ R, i = 1, . . . , m. The problem data are f $\in$ Rn , Ai $\in$ Rni ×n ,
\end{flushleft}


\begin{flushleft}
bi $\in$ Rni , ci $\in$ R and di $\in$ R, i = 1, . . . , m.
\end{flushleft}


\begin{flushleft}
Derive the dual in the following two ways.
\end{flushleft}


\begin{flushleft}
(a) Introduce new variables yi $\in$ Rni and ti $\in$ R and equalities yi = Ai x + bi , ti =
\end{flushleft}


\begin{flushleft}
cTi x + di , and derive the Lagrange dual.
\end{flushleft}


\begin{flushleft}
(b) Start from the conic formulation of the SOCP and use the conic dual. Use the fact
\end{flushleft}


\begin{flushleft}
that the second-order cone is self-dual.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We introduce the new variables, and write the problem as
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
yi 2 $\leq$ ti , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
yi = Ai x + bi , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
ti = cTi x + di , i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(x, y, t, $\lambda$, $\nu$, $\mu$)
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
cT x +
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
(c $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





$-$





\begin{flushleft}
2 $-$ ti ) +
\end{flushleft}





\begin{flushleft}
$\lambda$i ( y i
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\mu$i c i ) T x +
\end{flushleft}





\begin{flushleft}
($\lambda$i yi
\end{flushleft}





2





\begin{flushleft}
+ $\nu$iT yi ) +
\end{flushleft}





\begin{flushleft}
($-$$\lambda$i + $\mu$i )ti
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\mu$i (ti $-$ cTi x $-$ di )
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
ATi $\nu$i $-$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\nu$iT (yi $-$ Ai x $-$ bi ) +
\end{flushleft}





\begin{flushleft}
(bTi $\nu$i + di $\mu$i ).
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
The minimum over x is bounded below if and only if
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
(ATi $\nu$i + $\mu$i ci ) = c.
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
To minimize over yi , we note that
\end{flushleft}


\begin{flushleft}
inf ($\lambda$i yi
\end{flushleft}


\begin{flushleft}
yi
\end{flushleft}





2





\begin{flushleft}
+ $\nu$iT yi ) =
\end{flushleft}





0


$-$$\infty$





\begin{flushleft}
$\nu$ i 2 $\leq$ $\lambda$i
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The minimum over ti is bounded below if and only if $\lambda$i = $\mu$i . The Lagrangian is
\end{flushleft}


\begin{flushleft}
g($\lambda$, $\nu$, $\mu$) =
\end{flushleft}





$-$


$-$$\infty$





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(bT $\nu$
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}





\begin{flushleft}
+ d i $\mu$i )
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(ATi $\nu$i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
$\nu$ i 2 $\leq$ $\lambda$i ,
\end{flushleft}





\begin{flushleft}
otherwise
\end{flushleft}





\begin{flushleft}
+ $\mu$i ci ) = c,
\end{flushleft}


\begin{flushleft}
$\mu$=$\lambda$
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
which leads to the dual problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(bT $\nu$ + di $\lambda$i )
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(ATi $\nu$i + $\lambda$i ci ) = c
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
$\nu$i 2 $\leq$ $\lambda$i , i = 1, . . . , m.
\end{flushleft}





$-$





\begin{flushleft}
(b) We express the SOCP as a conic form problem
\end{flushleft}


\begin{flushleft}
cT x
\end{flushleft}


\begin{flushleft}
$-$(Ai x + bi , cTi x + di )
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
Ki
\end{flushleft}





0,





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
The conic dual is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(bT u + di vi )
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(ATi ui + vi ci ) = c
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
(ui , vi ) K ∗ 0, i = 1, . . . , m.
\end{flushleft}





$-$





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
5.44 Strong alternatives for nonstrict LMIs. In example 5.14, page 270, we mentioned that
\end{flushleft}


\begin{flushleft}
the system
\end{flushleft}


\begin{flushleft}
Z 0,
\end{flushleft}


\begin{flushleft}
tr(GZ) $>$ 0,
\end{flushleft}


\begin{flushleft}
tr(Fi Z) = 0, i = 1, . . . , n,
\end{flushleft}


(5.116)


\begin{flushleft}
is a strong alternative for the nonstrict LMI
\end{flushleft}


\begin{flushleft}
F (x) = x1 F1 + · · · + xn Fn + G
\end{flushleft}





0,





(5.117)





\begin{flushleft}
if the matrices Fi satisfy
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
v i Fi
\end{flushleft}





0 =$\Rightarrow$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
vi Fi = 0.
\end{flushleft}





(5.118)





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
In this exercise we prove this result, and give an example to illustrate that the systems
\end{flushleft}


\begin{flushleft}
are not always strong alternatives.
\end{flushleft}


\begin{flushleft}
(a) Suppose (5.118) holds, and that the optimal value of the auxiliary SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
s
\end{flushleft}


\begin{flushleft}
F (x)
\end{flushleft}





\begin{flushleft}
sI
\end{flushleft}





\begin{flushleft}
is positive. Show that the optimal value is attained. If follows from the discussion
\end{flushleft}


\begin{flushleft}
in §5.9.4 that the systems (5.117) and (5.116) are strong alternatives.
\end{flushleft}


\begin{flushleft}
Hint. The proof simplifies if you assume, without loss of generality, that the matrices
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
F1 , . . . , Fn are independent, so (5.118) may be replaced by i=1 vi Fi 0 $\Rightarrow$ v = 0.
\end{flushleft}





\begin{flushleft}
(b) Take n = 1, and
\end{flushleft}





\begin{flushleft}
G=
\end{flushleft}





0


1





1


0





,





\begin{flushleft}
F1 =
\end{flushleft}





0


0





0


1





.





\begin{flushleft}
Show that (5.117) and (5.116) are both infeasible.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Suppose that the optimal value is finite but not attained, i.e., there exists a sequence
\end{flushleft}


\begin{flushleft}
(x(k) , s(k) ), k = 0, 1, 2, . . . , with
\end{flushleft}


\begin{flushleft}
(k)
\end{flushleft}





\begin{flushleft}
x1 F1 + · · · + x(k)
\end{flushleft}


\begin{flushleft}
n Fn + G
\end{flushleft}





\begin{flushleft}
s(k) I
\end{flushleft}





\begin{flushleft}
for all k, and s(k) $\rightarrow$ s $>$ 0. We show that the norms x(k)
\end{flushleft}





\begin{flushleft}
(5.44.A)
\end{flushleft}


2





\begin{flushleft}
are bounded.
\end{flushleft}





\newpage
5


\begin{flushleft}
Suppose they are not. Dividing (5.44.A) by x(k)
\end{flushleft}


\begin{flushleft}
(1/ x(k)
\end{flushleft}





\begin{flushleft}
2 )G
\end{flushleft}





2,





\begin{flushleft}
Duality
\end{flushleft}





\begin{flushleft}
we have
\end{flushleft}





\begin{flushleft}
(k)
\end{flushleft}





\begin{flushleft}
+ v1 F1 + · · · + vn(k) Fn
\end{flushleft}





\begin{flushleft}
w(k) I,
\end{flushleft}





\begin{flushleft}
where v (k) = x(k) / x(k) 2 , w(k) = s(k) / x(k) 2 . The sequence (v (k) , w(k) ) is bounded,
\end{flushleft}


\begin{flushleft}
so it has a convergent subsequence. Let v¯, w
\end{flushleft}


\begin{flushleft}
¯ be its limit. We have
\end{flushleft}


\begin{flushleft}
v¯1 F1 + · · · + v¯n Fn
\end{flushleft}





0,





\begin{flushleft}
since w
\end{flushleft}


\begin{flushleft}
¯ must be zero. By assumption, this implies that v = 0, which contradicts our
\end{flushleft}


\begin{flushleft}
assumption that the sequence x(k) is unbounded.
\end{flushleft}


\begin{flushleft}
Since it is bounded, the sequence x(k) must have a convergent subsequence. Taking
\end{flushleft}


\begin{flushleft}
limits in (5.44.A), we get
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
¯ 1 F1 + · · · + x
\end{flushleft}


\begin{flushleft}
¯ n Fn + G
\end{flushleft}





\begin{flushleft}
s I,
\end{flushleft}





\begin{flushleft}
i.e., the optimum is attained.
\end{flushleft}


\begin{flushleft}
(b) The LMI is
\end{flushleft}


\begin{flushleft}
x1
\end{flushleft}


1





1


0





0,





\begin{flushleft}
which is infeasible. The alternative system is
\end{flushleft}


\begin{flushleft}
z11
\end{flushleft}


\begin{flushleft}
z12
\end{flushleft}


\begin{flushleft}
which is also impossible.
\end{flushleft}





\begin{flushleft}
z12
\end{flushleft}


\begin{flushleft}
z22
\end{flushleft}





0,





\begin{flushleft}
z22 = 0,
\end{flushleft}





\begin{flushleft}
z12 $>$ 0,
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 6
\end{flushleft}





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Norm approximation and least-norm problems
\end{flushleft}


\begin{flushleft}
6.1 Quadratic bounds for log barrier penalty. Let $\phi$ : R $\rightarrow$ R be the log barrier penalty
\end{flushleft}


\begin{flushleft}
function with limit a $>$ 0:
\end{flushleft}


\begin{flushleft}
$-$a2 log(1 $-$ (u/a)2 )
\end{flushleft}


$\infty$





\begin{flushleft}
$\phi$(u) =
\end{flushleft}


\begin{flushleft}
Show that if u $\in$ Rm satisfies u
\end{flushleft}





$\infty$





\begin{flushleft}
$<$ a, then
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





2


2





$\leq$





\begin{flushleft}
|u| $<$ a
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\phi$( u $\infty$ )
\end{flushleft}


\begin{flushleft}
u 22 .
\end{flushleft}


\begin{flushleft}
u 2$\infty$
\end{flushleft}





\begin{flushleft}
$\phi$(ui ) $\leq$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
This means that
\end{flushleft}


\begin{flushleft}
$\phi$(ui ) is well approximated by u
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
a. For example, if u $\infty$ /a = 0.25, then
\end{flushleft}





2


2





\begin{flushleft}
if u
\end{flushleft}





$\infty$





\begin{flushleft}
is small compared to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





2


2





$\leq$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\phi$(ui ) $\leq$ 1.033 · u 22 .
\end{flushleft}





\begin{flushleft}
Solution. The left inequality follows from log(1 + x) $\leq$ x for all x $>$ $-$1.
\end{flushleft}


\begin{flushleft}
The right inequality follows from convexity of $-$ log(1 $-$ x):
\end{flushleft}


\begin{flushleft}
$-$ log(1 $-$ u2i /a2 ) $\leq$ $-$
\end{flushleft}





\begin{flushleft}
u2i
\end{flushleft}


\begin{flushleft}
log(1 $-$ u
\end{flushleft}


\begin{flushleft}
u 2$\infty$
\end{flushleft}





2


2


\begin{flushleft}
$\infty$ /a )
\end{flushleft}





\begin{flushleft}
and therefore
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$a2
\end{flushleft}


6.2





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(1 $-$ u2i /a2 ) $\leq$ $-$a2
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





2


2


2


$\infty$





\begin{flushleft}
log(1 $-$ u
\end{flushleft}





2


2


\begin{flushleft}
$\infty$ /a ).
\end{flushleft}





\begin{flushleft}
1 -, 2 -, and $\infty$ -norm approximation by a constant vector. What is the solution of the
\end{flushleft}


\begin{flushleft}
norm approximation problem with one scalar variable x $\in$ R,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
for the 1 -,
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





2 -,





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
x1 $-$ b ,
\end{flushleft}





\begin{flushleft}
$\infty$ -norms?
\end{flushleft}





\begin{flushleft}
(a)
\end{flushleft}





\begin{flushleft}
2 -norm:
\end{flushleft}





\begin{flushleft}
the average 1T b/m.
\end{flushleft}





\begin{flushleft}
(b)
\end{flushleft}





\begin{flushleft}
1 -norm:
\end{flushleft}





\begin{flushleft}
the (or a) median of the coefficients of b.
\end{flushleft}





\begin{flushleft}
(c)
\end{flushleft}





\begin{flushleft}
$\infty$ -norm:
\end{flushleft}





\begin{flushleft}
the midrange point (max bi $-$ min bi )/2.
\end{flushleft}





\begin{flushleft}
6.3 Formulate the following approximation problems as LPs, QPs, SOCPs, or SDPs. The
\end{flushleft}


\begin{flushleft}
problem data are A $\in$ Rm×n and b $\in$ Rm . The rows of A are denoted aTi .
\end{flushleft}


\begin{flushleft}
(a) Deadzone-linear penalty approximation: minimize
\end{flushleft}


\begin{flushleft}
$\phi$(u) =
\end{flushleft}


\begin{flushleft}
where a $>$ 0.
\end{flushleft}





0


\begin{flushleft}
|u| $-$ a
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|u| $\leq$ a
\end{flushleft}


\begin{flushleft}
|u| $>$ a,
\end{flushleft}





\begin{flushleft}
$\phi$(aTi x $-$ bi ), where
\end{flushleft}





\newpage
6


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(b) Log-barrier penalty approximation: minimize
\end{flushleft}





\begin{flushleft}
Approximation and fitting
\end{flushleft}


\begin{flushleft}
$\phi$(aTi x $-$ bi ), where
\end{flushleft}





\begin{flushleft}
$-$a2 log(1 $-$ (u/a)2 )
\end{flushleft}


$\infty$





\begin{flushleft}
$\phi$(u) =
\end{flushleft}





\begin{flushleft}
with a $>$ 0.
\end{flushleft}


\begin{flushleft}
(c) Huber penalty approximation: minimize
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\phi$(aTi x $-$ bi ), where
\end{flushleft}





\begin{flushleft}
u2
\end{flushleft}


\begin{flushleft}
M (2|u| $-$ M )
\end{flushleft}





\begin{flushleft}
$\phi$(u) =
\end{flushleft}





\begin{flushleft}
|u| $<$ a
\end{flushleft}


\begin{flushleft}
|u| $\geq$ a,
\end{flushleft}





\begin{flushleft}
|u| $\leq$ M
\end{flushleft}


\begin{flushleft}
|u| $>$ M,
\end{flushleft}





\begin{flushleft}
with M $>$ 0.
\end{flushleft}


\begin{flushleft}
(d) Log-Chebyshev approximation: minimize maxi=1,...,m | log(aTi x) $-$ log bi |. We assume
\end{flushleft}


\begin{flushleft}
b 0. An equivalent convex form is
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
1/t $\leq$ aTi x/bi $\leq$ t,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
with variables x $\in$ R and t $\in$ R, and domain R × R++ .
\end{flushleft}


\begin{flushleft}
(e) Minimizing the sum of the largest k residuals:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|r|[i]
\end{flushleft}


\begin{flushleft}
r = Ax $-$ b,
\end{flushleft}





\begin{flushleft}
where |r|[1] $\geq$ |r|[2] $\geq$ · · · $\geq$ |r|[m] are the numbers |r1 |, |r2 |, . . . , |rm | sorted in
\end{flushleft}


\begin{flushleft}
decreasing order. (For k = 1, this reduces to $\infty$ -norm approximation; for k = m, it
\end{flushleft}


\begin{flushleft}
reduces to 1 -norm approximation.) Hint. See exercise 5.19.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Deadzone-linear.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T y
\end{flushleft}


\begin{flushleft}
$-$y $-$ a1
\end{flushleft}


\begin{flushleft}
y 0.
\end{flushleft}





\begin{flushleft}
Ax $-$ b
\end{flushleft}





\begin{flushleft}
y + a1
\end{flushleft}





\begin{flushleft}
An LP with variables y $\in$ Rm , x $\in$ Rn .
\end{flushleft}


\begin{flushleft}
(b) Log-barrier penalty. We can express the problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
t2
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
(1 $-$ yi /a)(1
\end{flushleft}





\begin{flushleft}
+ yi /a) $\geq$ t2i , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
$-$1 $\leq$ yi /a $\leq$ 1, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
y = Ax $-$ b,
\end{flushleft}





\begin{flushleft}
with variables t $\in$ Rm , y $\in$ Rm , x $\in$ Rn .
\end{flushleft}


\begin{flushleft}
We can now proceed as in exercise 4.26 (maximizing geometric mean), and reduce
\end{flushleft}


\begin{flushleft}
the problem to an SOCP or an SDP.
\end{flushleft}


\begin{flushleft}
(c) Huber penalty. See exercise 4.5 (c), and also exercise 6.6.
\end{flushleft}


\begin{flushleft}
(d) Log-Chebyshev approximation.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
1/t $\leq$ aTi x/bi $\leq$ t,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
over x $\in$ Rn and t $\in$ R. The left inequalities are hyperbolic constraints
\end{flushleft}


\begin{flushleft}
taTi x $\geq$ bi ,
\end{flushleft}





\begin{flushleft}
t $\geq$ 0,
\end{flushleft}





\begin{flushleft}
aTi x $\geq$ 0
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
that can be formulated as LMI constraints
\end{flushleft}


$\surd$


\begin{flushleft}
bi
\end{flushleft}


\begin{flushleft}
$\surd$t
\end{flushleft}


\begin{flushleft}
bi aTi x
\end{flushleft}


\begin{flushleft}
or SOC constraints
\end{flushleft}





$\surd$


\begin{flushleft}
2 bi
\end{flushleft}


\begin{flushleft}
t $-$ aTi x
\end{flushleft}





2





0,





\begin{flushleft}
$\leq$ t + aTi x.
\end{flushleft}





\begin{flushleft}
(e) Sum of largest residuals.
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
kt + 1T z
\end{flushleft}


\begin{flushleft}
$-$t1 $-$ z Ax $-$ b
\end{flushleft}


\begin{flushleft}
z 0,
\end{flushleft}





\begin{flushleft}
t1 + z
\end{flushleft}





\begin{flushleft}
with variables x $\in$ Rn , t $\in$ R, z $\in$ Rm .
\end{flushleft}


\begin{flushleft}
6.4 A differentiable approximation of 1 -norm approximation. The function $\phi$(u) = (u2 + )1/2 ,
\end{flushleft}


\begin{flushleft}
with parameter $>$ 0, is sometimes used as a differentiable approximation of the absolute
\end{flushleft}


\begin{flushleft}
value function |u|. To approximately solve the 1 -norm approximation problem
\end{flushleft}


\begin{flushleft}
Ax $-$ b 1 ,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





(6.26)





\begin{flushleft}
where A $\in$ Rm×n , we solve instead the problem
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$\phi$(aTi x $-$ bi ),
\end{flushleft}





(6.27)





\begin{flushleft}
where aTi is the ith row of A. We assume rank A = n.
\end{flushleft}


\begin{flushleft}
Let p denote the optimal value of the 1 -norm approximation problem (6.26). Let x
\end{flushleft}


ˆ


\begin{flushleft}
denote the optimal solution of the approximate problem (6.27), and let rˆ denote the
\end{flushleft}


\begin{flushleft}
associated residual, rˆ = Aˆ
\end{flushleft}


\begin{flushleft}
x $-$ b.
\end{flushleft}


\begin{flushleft}
(a) Show that p $\geq$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
rˆi2 /(ˆ
\end{flushleft}


\begin{flushleft}
ri2 + )1/2 .
\end{flushleft}





\begin{flushleft}
(b) Show that
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
Aˆ
\end{flushleft}


\begin{flushleft}
x$-$b
\end{flushleft}





1





\begin{flushleft}
$\leq$p +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





|ˆ


\begin{flushleft}
ri | 1 $-$
\end{flushleft}





|ˆ


\begin{flushleft}
ri |
\end{flushleft}


(ˆ


\begin{flushleft}
ri2 + )1/2
\end{flushleft}





.





\begin{flushleft}
(By evaluating the righthand side after computing x
\end{flushleft}


\begin{flushleft}
ˆ, we obtain a bound on how suboptimal x
\end{flushleft}


\begin{flushleft}
ˆ is for the 1 -norm approximation problem.)
\end{flushleft}


\begin{flushleft}
Solution. One approach is based on duality. The point x
\end{flushleft}


\begin{flushleft}
ˆ minimizes the differentiable
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


$-$


\begin{flushleft}
b
\end{flushleft}


),


\begin{flushleft}
so
\end{flushleft}


\begin{flushleft}
its
\end{flushleft}


\begin{flushleft}
gradient
\end{flushleft}


\begin{flushleft}
vanishes:
\end{flushleft}


\begin{flushleft}
convex function
\end{flushleft}


\begin{flushleft}
$\phi$(a
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
rˆi (ˆ
\end{flushleft}


\begin{flushleft}
ri2 + )$-$1/2 ai = 0.
\end{flushleft}





\begin{flushleft}
$\phi$ (ˆ
\end{flushleft}


\begin{flushleft}
ri )ai =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Now, the dual of the
\end{flushleft}





\begin{flushleft}
1 -norm
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
approximation problem is
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
b$\lambda$
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}


\begin{flushleft}
|$\lambda$i | $\leq$ 1,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
$\lambda$a
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


= 0.





\begin{flushleft}
Thus, we see that the vector
\end{flushleft}


\begin{flushleft}
$\lambda$i = $-$
\end{flushleft}





\begin{flushleft}
rˆi
\end{flushleft}


,


(ˆ


\begin{flushleft}
ri2 + )$-$1/2
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
is dual feasible. It follows that its dual function value,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$bi rˆi
\end{flushleft}


,


(ˆ


\begin{flushleft}
ri2 + )$-$1/2
\end{flushleft}





\begin{flushleft}
$-$bi $\lambda$i =
\end{flushleft}





\begin{flushleft}
provides a lower bound on p . Now we use the fact that
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i ai = 0 to obtain
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





$\geq$





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$bi $\lambda$i
\end{flushleft}


\begin{flushleft}
ˆ $-$ bi )$\lambda$i
\end{flushleft}


\begin{flushleft}
(aTi x
\end{flushleft}


\begin{flushleft}
rˆi $\lambda$i
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





=





(ˆ


\begin{flushleft}
ri2
\end{flushleft}





\begin{flushleft}
rˆi2
\end{flushleft}


.


+ )$-$1/2





\begin{flushleft}
Now we establish part (b). We start with the result above,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
p $\geq$
\end{flushleft}


\begin{flushleft}
and subtract Aˆ
\end{flushleft}


\begin{flushleft}
x$-$b
\end{flushleft}





1





=





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
rˆi2 /(ˆ
\end{flushleft}


\begin{flushleft}
ri2 + )1/2 ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





|ˆ


\begin{flushleft}
ri | from both sides to get
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
p $-$ Aˆ
\end{flushleft}


\begin{flushleft}
x$-$b
\end{flushleft}





1





$\geq$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
rˆi2 /(ˆ
\end{flushleft}


\begin{flushleft}
ri2 + )1/2 $-$ |ˆ
\end{flushleft}


\begin{flushleft}
ri | .
\end{flushleft}





\begin{flushleft}
Re-arranging gives the desired result,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
Aˆ
\end{flushleft}


\begin{flushleft}
x$-$b
\end{flushleft}





1





\begin{flushleft}
$\leq$p +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|ri | 1 $-$
\end{flushleft}





\begin{flushleft}
|ri |
\end{flushleft}


\begin{flushleft}
(ri2 + )1/2
\end{flushleft}





.





\begin{flushleft}
6.5 Minimum length approximation. Consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
length(x)
\end{flushleft}


\begin{flushleft}
Ax $-$ b $\leq$ ,
\end{flushleft}





\begin{flushleft}
where length(x) = min\{k | xi = 0 for i $>$ k\}. The problem variable is x $\in$ Rn ; the
\end{flushleft}


\begin{flushleft}
problem parameters are A $\in$ Rm×n , b $\in$ Rm , and $>$ 0. In a regression context, we are
\end{flushleft}


\begin{flushleft}
asked to find the minimum number of columns of A, taken in order, that can approximate
\end{flushleft}


\begin{flushleft}
the vector b within .
\end{flushleft}


\begin{flushleft}
Show that this is a quasiconvex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. length(x) $\leq$ $\alpha$ if and only if xk = 0 for k $>$ $\alpha$. Thus, the sublevel sets of length
\end{flushleft}


\begin{flushleft}
are convex, so length is quasiconvex.
\end{flushleft}


\begin{flushleft}
6.6 Duals of some penalty function approximation problems. Derive a Lagrange dual for the
\end{flushleft}


\begin{flushleft}
problem
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
$\phi$(ri )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
subject to r = Ax $-$ b,
\end{flushleft}


\begin{flushleft}
for the following penalty functions $\phi$ : R $\rightarrow$ R. The variables are x $\in$ Rn , r $\in$ Rm .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) Deadzone-linear penalty (with deadzone width a = 1),
\end{flushleft}


0


\begin{flushleft}
|u| $-$ 1
\end{flushleft}





\begin{flushleft}
|u| $\leq$ 1
\end{flushleft}


\begin{flushleft}
|u| $>$ 1.
\end{flushleft}





\begin{flushleft}
u2
\end{flushleft}


\begin{flushleft}
2|u| $-$ 1
\end{flushleft}





\begin{flushleft}
|u| $\leq$ 1
\end{flushleft}


\begin{flushleft}
|u| $>$ 1.
\end{flushleft}





\begin{flushleft}
$\phi$(u) =
\end{flushleft}


\begin{flushleft}
(b) Huber penalty (with M = 1),
\end{flushleft}


\begin{flushleft}
$\phi$(u) =
\end{flushleft}


\begin{flushleft}
(c) Log-barrier (with limit a = 1),
\end{flushleft}





\begin{flushleft}
$\phi$(u) = $-$ log(1 $-$ x2 ),
\end{flushleft}





\begin{flushleft}
dom $\phi$ = ($-$1, 1).
\end{flushleft}





\begin{flushleft}
(d) Relative deviation from one,
\end{flushleft}


\begin{flushleft}
u$\geq$1
\end{flushleft}


\begin{flushleft}
u $\leq$ 1,
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}


\begin{flushleft}
1/u
\end{flushleft}





\begin{flushleft}
$\phi$(u) = max\{u, 1/u\} =
\end{flushleft}


\begin{flushleft}
with dom $\phi$ = R++ .
\end{flushleft}





\begin{flushleft}
Solution. We first derive a dual for general penalty function approximation. The Lagrangian is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
L(x, r, $\lambda$) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\phi$(ri ) + $\nu$ T (Ax $-$ b $-$ r).
\end{flushleft}





\begin{flushleft}
The minimum over x is bounded if and only if AT $\nu$ = 0, so we have
\end{flushleft}


\begin{flushleft}
g($\nu$) =
\end{flushleft}


\begin{flushleft}
Using
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$bT $\nu$ +
\end{flushleft}


$-$$\infty$





\begin{flushleft}
AT $\nu$ = 0
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
inf ri ($\phi$(ri ) $-$ $\nu$i ri )
\end{flushleft}





\begin{flushleft}
inf ($\phi$(ri ) $-$ $\nu$i ri ) = $-$ sup($\nu$i ri $-$ $\phi$(ri )) = $-$$\phi$∗ ($\nu$i ),
\end{flushleft}


\begin{flushleft}
ri
\end{flushleft}





\begin{flushleft}
ri
\end{flushleft}





\begin{flushleft}
we can express the general dual as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT $\nu$ $-$
\end{flushleft}


\begin{flushleft}
AT $\nu$ = 0.
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\phi$∗ ($\nu$i )
\end{flushleft}





\begin{flushleft}
Now we'll work out the conjugates of the given penalty functions.
\end{flushleft}


\begin{flushleft}
(a) Deadzone-linear penalty. The conjugate of the deadzone-linear function is
\end{flushleft}


\begin{flushleft}
$\phi$∗ (z) =
\end{flushleft}





\begin{flushleft}
|z|
\end{flushleft}


$\infty$





\begin{flushleft}
|z| $\leq$ 1
\end{flushleft}


\begin{flushleft}
|z| $>$ 1,
\end{flushleft}





\begin{flushleft}
so the dual of the dead-zone linear penalty function approximation problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$bT $\nu$ $-$ $\nu$
\end{flushleft}


\begin{flushleft}
AT $\nu$ = 0,
\end{flushleft}





1





\begin{flushleft}
$\nu$
\end{flushleft}





$\infty$





$\leq$ 1.





\begin{flushleft}
(b) Huber penalty.
\end{flushleft}


\begin{flushleft}
$\phi$∗ (z) =
\end{flushleft}


\begin{flushleft}
so we get the dual problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
z 2 /4
\end{flushleft}


$\infty$





\begin{flushleft}
|z| $\leq$ 2
\end{flushleft}


\begin{flushleft}
otherwise,
\end{flushleft}





\begin{flushleft}
$-$(1/4) $\nu$ 22 $-$ bT $\nu$
\end{flushleft}


\begin{flushleft}
AT $\nu$ = 0
\end{flushleft}


\begin{flushleft}
$\nu$ $\infty$ $\leq$ 2.
\end{flushleft}





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
(c) Log-barrier. The conjugate of $\phi$ is
\end{flushleft}


\begin{flushleft}
$\phi$∗ (z)
\end{flushleft}





\begin{flushleft}
sup xz + log(1 $-$ x2 )
\end{flushleft}





=





\begin{flushleft}
|x|$<$1
\end{flushleft}





\begin{flushleft}
1 + z 2 + log($-$1 +
\end{flushleft}





$-$1 +





=





\begin{flushleft}
1 + z 2 ) $-$ 2 log |z| + log 2.
\end{flushleft}





\begin{flushleft}
(d) Relative deviation from one. Here we have
\end{flushleft}


$\surd$


\begin{flushleft}
$-$2 $-$z
\end{flushleft}


\begin{flushleft}
z$-$1
\end{flushleft}


$-$$\infty$





∗





\begin{flushleft}
$\phi$ (z) = sup(xz $-$ max\{x, 1/x\}) =
\end{flushleft}


\begin{flushleft}
x$>$0
\end{flushleft}





\begin{flushleft}
z $\leq$ $-$1
\end{flushleft}


\begin{flushleft}
$-$1 $\leq$ z $\leq$ 1
\end{flushleft}


\begin{flushleft}
z $>$ 1.
\end{flushleft}





\begin{flushleft}
Plugging this in the dual problem gives
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





$\surd$


\begin{flushleft}
2 $-$$\nu$i
\end{flushleft}


\begin{flushleft}
1 $-$ $\nu$i
\end{flushleft}





\begin{flushleft}
s($\nu$i ) =
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$bT $\nu$ +
\end{flushleft}


\begin{flushleft}
AT $\nu$ = 0,
\end{flushleft}





\begin{flushleft}
$\nu$
\end{flushleft}





\begin{flushleft}
s($\nu$i )
\end{flushleft}


1,





\begin{flushleft}
$\nu$i $\leq$ $-$1
\end{flushleft}


\begin{flushleft}
$\nu$i $\geq$ $-$1.
\end{flushleft}





\begin{flushleft}
Regularization and robust approximation
\end{flushleft}


\begin{flushleft}
6.7 Bi-criterion optimization with Euclidean norms. We consider the bi-criterion optimization
\end{flushleft}


\begin{flushleft}
problem
\end{flushleft}


\begin{flushleft}
minimize (w.r.t. R2+ ) ( Ax $-$ b 22 , x 22 ),
\end{flushleft}


\begin{flushleft}
where A $\in$ Rm×n has rank r, and b $\in$ Rm . Show how to find the solution of each of the
\end{flushleft}


\begin{flushleft}
following problems from the singular value decomposition of A,
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
A = U diag($\sigma$)V T =
\end{flushleft}





\begin{flushleft}
$\sigma$i ui viT
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(see §A.5.4).
\end{flushleft}


\begin{flushleft}
(a) Tikhonov regularization: minimize Ax $-$ b
\end{flushleft}





\begin{flushleft}
(b) Minimize Ax $-$ b
\end{flushleft}





2


2





2


2





\begin{flushleft}
(c) Maximize Ax $-$ b
\end{flushleft}





\begin{flushleft}
subject to x
\end{flushleft}





2


2





\begin{flushleft}
= $\gamma$.
\end{flushleft}





\begin{flushleft}
subject to x
\end{flushleft}





2


2





\begin{flushleft}
= $\gamma$.
\end{flushleft}





2


2





\begin{flushleft}
+ $\delta$ x 22 .
\end{flushleft}





\begin{flushleft}
Here $\delta$ and $\gamma$ are positive parameters.
\end{flushleft}


\begin{flushleft}
Your results provide efficient methods for computing the optimal trade-off curve and the
\end{flushleft}


\begin{flushleft}
set of achievable values of the bi-criterion problem.
\end{flushleft}


\begin{flushleft}
Solution. Define
\end{flushleft}


\begin{flushleft}
˜b = (U T b, U2T b).
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜ = (V T x, V2T x),
\end{flushleft}


\begin{flushleft}
where V2 $\in$ Rn×(n$-$r) satisfies V2T V2 = I, V2T V = 0, and U2 $\in$ Rm×(m$-$r) satisfies
\end{flushleft}


\begin{flushleft}
U2T U2 = I, U2T U = 0. We have
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
Ax $-$ b
\end{flushleft}





2


2





\begin{flushleft}
We will use x
\end{flushleft}


\begin{flushleft}
˜ as variable.
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
($\sigma$i x
\end{flushleft}


\begin{flushleft}
˜i $-$ ˜bi )2 +
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
˜b2i ,
\end{flushleft}


\begin{flushleft}
i=r+1
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





2


2





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜2i .
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) Tikhonov regularization. Setting the gradient (with respect to x
\end{flushleft}


\begin{flushleft}
˜) to zero gives
\end{flushleft}


\begin{flushleft}
($\sigma$i2 + $\delta$)˜
\end{flushleft}


\begin{flushleft}
xi = $\sigma$i˜bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , r,
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i = 0,
\end{flushleft}





\begin{flushleft}
i = r + 1, . . . , n.
\end{flushleft}





\begin{flushleft}
The solution is
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i =
\end{flushleft}





\begin{flushleft}
˜bi $\sigma$i
\end{flushleft}


,


\begin{flushleft}
$\delta$ + $\sigma$i2
\end{flushleft}





\begin{flushleft}
i = 1, . . . , r,
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i = 0,
\end{flushleft}





\begin{flushleft}
i = r + 1, . . . , n.
\end{flushleft}





\begin{flushleft}
In terms of the original variables,
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
x=
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\sigma$i
\end{flushleft}


\begin{flushleft}
(uTi b)vi .
\end{flushleft}


\begin{flushleft}
$\delta$ + $\sigma$i2
\end{flushleft}





\begin{flushleft}
If $\delta$ = 0, this is the least-squares solution
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
x = A\dag{} b = V $\Sigma$$-$1 U T b =
\end{flushleft}





\begin{flushleft}
(1/$\sigma$i )(uTi b)vi .
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
receives a weight $\sigma$i /($\delta$ + $\sigma$i2 ). The function
\end{flushleft}


\begin{flushleft}
If $\delta$ $>$ 0, each component
\end{flushleft}


2


\begin{flushleft}
$\sigma$/($\delta$ + $\sigma$ ) is zero if $\sigma$ = 0, goes through a maximum of 1/(1 + $\delta$) at $\sigma$ = $\delta$, and
\end{flushleft}


\begin{flushleft}
decreases to zero as 1/$\sigma$ for $\sigma$ $\rightarrow$ $\infty$.
\end{flushleft}


\begin{flushleft}
In other words, if $\sigma$i is large ($\sigma$i
\end{flushleft}


\begin{flushleft}
$\delta$), we keep the ith term in the LS solution. For
\end{flushleft}


\begin{flushleft}
small $\sigma$i ($\sigma$i $\approx$ $\delta$ or less), we dampen its weight, replacing 1/$\sigma$i by $\sigma$i /($\delta$ + $\sigma$i2 ).
\end{flushleft}


\begin{flushleft}
(b) After the change of variables, this problem is
\end{flushleft}


\begin{flushleft}
(uTi b)vi
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
($\sigma$i x
\end{flushleft}


\begin{flushleft}
˜i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


2


=


\begin{flushleft}
x
\end{flushleft}


˜


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
˜b2
\end{flushleft}


\begin{flushleft}
i=r+1 i
\end{flushleft}





\begin{flushleft}
$-$ ˜bi )2 +
\end{flushleft}


\begin{flushleft}
$\gamma$.
\end{flushleft}





\begin{flushleft}
Although the problem is not convex, it is clear that a necessary and sufficient condition for a feasible x
\end{flushleft}


\begin{flushleft}
˜ to be optimal is that either the gradient of the objective vanishes
\end{flushleft}


\begin{flushleft}
at x
\end{flushleft}


\begin{flushleft}
˜, or the gradient is normal to the sphere through x
\end{flushleft}


\begin{flushleft}
˜, and pointing toward the
\end{flushleft}


\begin{flushleft}
interior of the sphere. In other words, the optimality conditions are that x
\end{flushleft}


\begin{flushleft}
˜ 22 = $\gamma$
\end{flushleft}


\begin{flushleft}
and there exists a $\nu$ $\geq$ 0, such that
\end{flushleft}


\begin{flushleft}
($\sigma$i2 + $\nu$)˜
\end{flushleft}


\begin{flushleft}
xi = $\sigma$i˜bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , r,
\end{flushleft}





\begin{flushleft}
$\nu$x
\end{flushleft}


\begin{flushleft}
˜i = 0,
\end{flushleft}





\begin{flushleft}
i = r + 1, . . . , n.
\end{flushleft}





\begin{flushleft}
We distinguish two cases.
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
$\bullet$ If
\end{flushleft}


\begin{flushleft}
(˜b /$\sigma$i )2 $\leq$ $\gamma$, then $\nu$ = 0 and
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i = ˜bi $\sigma$i ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , r,
\end{flushleft}





\begin{flushleft}
(i.e., the unconstrained minimum) is optimal. For the other variables we can
\end{flushleft}


\begin{flushleft}
choose any x
\end{flushleft}


\begin{flushleft}
˜i , i = r + 1, . . . , n that gives x
\end{flushleft}


\begin{flushleft}
˜ 22 = $\gamma$.
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}


2


˜


\begin{flushleft}
$\bullet$ If
\end{flushleft}


\begin{flushleft}
(b /$\sigma$i ) $>$ $\gamma$, we must take $\nu$ $>$ 0, and
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i =
\end{flushleft}





\begin{flushleft}
˜bi $\sigma$i
\end{flushleft}


,


\begin{flushleft}
$\sigma$i2 + $\nu$
\end{flushleft}





\begin{flushleft}
i = 1, . . . , r,
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i = 0,
\end{flushleft}





\begin{flushleft}
i = r + 1, . . . , n.
\end{flushleft}





\begin{flushleft}
We determine $\nu$ $>$ 0 by solving the nonlinear equation
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜2i =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
˜bi $\sigma$i
\end{flushleft}


\begin{flushleft}
$\sigma$i2 + $\nu$
\end{flushleft}





2





\begin{flushleft}
= $\gamma$.
\end{flushleft}





\begin{flushleft}
The left hand side is monotonically decreasing with $\nu$, and by assumption it is
\end{flushleft}


\begin{flushleft}
greater than $\gamma$ at $\nu$ = 0, so the equation has a unique positive solution.
\end{flushleft}





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
(c) After the change of variables to x
\end{flushleft}


\begin{flushleft}
˜, this problem reduces to
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
($\sigma$i x
\end{flushleft}


\begin{flushleft}
˜i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


2


\begin{flushleft}
x
\end{flushleft}


˜


=


\begin{flushleft}
i=1 i
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$ ˜bi )2 +
\end{flushleft}


\begin{flushleft}
$\gamma$.
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
˜b2
\end{flushleft}


\begin{flushleft}
i=r+1 i
\end{flushleft}





\begin{flushleft}
Without loss of generality we can replace the equality with an inequality, since a
\end{flushleft}


\begin{flushleft}
convex function reaches its maximum over a compact convex on the boundary. As
\end{flushleft}


\begin{flushleft}
shown in §B.1, strong duality holds for quadratic optimization problems with one
\end{flushleft}


\begin{flushleft}
inequality constraint.
\end{flushleft}


\begin{flushleft}
In this case, however, it is also easy to derive this result directly, without appealing
\end{flushleft}


\begin{flushleft}
to the general result in §B.1. We will first derive and solve the dual, and then show
\end{flushleft}


\begin{flushleft}
strong duality by establishing a feasible x
\end{flushleft}


\begin{flushleft}
˜ with the same primal objective value as
\end{flushleft}


\begin{flushleft}
the dual optimum.
\end{flushleft}


\begin{flushleft}
The Lagrangian of the problem above (after switching the sign of the objective) is
\end{flushleft}


=





$-$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
($\sigma$i x
\end{flushleft}


\begin{flushleft}
˜i $-$ ˜bi )2 $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
˜b2i + $\nu$(
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=r+1
\end{flushleft}





\begin{flushleft}
($\nu$ $-$ $\sigma$i2 )˜
\end{flushleft}


\begin{flushleft}
x2i + 2
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜2i $-$ $\gamma$)
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





=





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
L(˜
\end{flushleft}


\begin{flushleft}
x, $\nu$)
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
˜i $-$
\end{flushleft}


\begin{flushleft}
$\sigma$i˜bi x
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
˜b2i $-$ $\nu$$\gamma$.
\end{flushleft}





\begin{flushleft}
L is bounded below as a function of x
\end{flushleft}


\begin{flushleft}
˜ only if $\nu$ $>$ $\sigma$12 , or if $\nu$ = $\sigma$12 and ˜b1 = 0. The
\end{flushleft}


\begin{flushleft}
infimum is
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
($\sigma$i˜bi )2
\end{flushleft}


\begin{flushleft}
˜b2i $-$ $\nu$$\gamma$,
\end{flushleft}


\begin{flushleft}
inf L(˜
\end{flushleft}


\begin{flushleft}
x, $\nu$) = $-$
\end{flushleft}


$-$


\begin{flushleft}
x
\end{flushleft}


˜


\begin{flushleft}
$\nu$ $-$ $\sigma$i2
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
with domain
\end{flushleft}


\begin{flushleft}
and where for $\nu$ =
\end{flushleft}


\begin{flushleft}
we interpret ˜b21 /($\nu$ $-$ $\sigma$12 ) as $\infty$ if ˜b1 =
\end{flushleft}


\begin{flushleft}
0, and as 0 if ˜b1 = 0. The dual problem is therefore (after switching back to
\end{flushleft}


\begin{flushleft}
maximization)
\end{flushleft}


\begin{flushleft}
[$\sigma$12 , $\infty$),
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\sigma$12
\end{flushleft}





\begin{flushleft}
g($\nu$) =
\end{flushleft}


\begin{flushleft}
$\nu$ $\geq$ $\sigma$12 .
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}


\begin{flushleft}
(˜b $\sigma$ )2 /($\nu$
\end{flushleft}


\begin{flushleft}
i=1 i i
\end{flushleft}





\begin{flushleft}
$-$ $\sigma$i2 ) + $\nu$$\gamma$ +
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
˜b2
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}





\begin{flushleft}
The derivative of g is
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
g ($\nu$) = $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(˜bi $\sigma$i )2
\end{flushleft}


\begin{flushleft}
+ $\gamma$.
\end{flushleft}


\begin{flushleft}
($\nu$ $-$ $\sigma$i2 )2
\end{flushleft}





\begin{flushleft}
We can distinguish three cases. We assume that the first singular value is repeated
\end{flushleft}


\begin{flushleft}
k times where k $\leq$ r.
\end{flushleft}


\begin{flushleft}
$\bullet$ g($\sigma$12 ) = $\infty$. This is the case if at least one of the coefficients ˜b1 , . . . , ˜bk is
\end{flushleft}


\begin{flushleft}
nonzero.
\end{flushleft}


\begin{flushleft}
In this case g first decreases as we increase $\nu$ $>$ $\sigma$12 and then increases as $\nu$ goes
\end{flushleft}


\begin{flushleft}
to infinity. There is therefore a unique $\nu$ $>$ $\sigma$12 where the derivative is zero:
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(˜bi $\sigma$i )2
\end{flushleft}


\begin{flushleft}
= $\gamma$.
\end{flushleft}


\begin{flushleft}
($\nu$ $-$ $\sigma$i2 )2
\end{flushleft}





\begin{flushleft}
From $\nu$ we compute the optimal primal x
\end{flushleft}


\begin{flushleft}
˜ as
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i =
\end{flushleft}





\begin{flushleft}
$-$$\sigma$i˜bi
\end{flushleft}


,


\begin{flushleft}
$\nu$ $-$ $\sigma$i2
\end{flushleft}





\begin{flushleft}
i = 1, . . . , r,
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i = 0,
\end{flushleft}





\begin{flushleft}
i = r + 1, . . . , n.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


2





\begin{flushleft}
This point satisfies x
\end{flushleft}


˜





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
= $\gamma$ and its objective value is
\end{flushleft}





\begin{flushleft}
$\sigma$i2 x
\end{flushleft}


\begin{flushleft}
˜2i $-$ 2
\end{flushleft}





\begin{flushleft}
˜b2i
\end{flushleft}





\begin{flushleft}
˜i +
\end{flushleft}


\begin{flushleft}
$\sigma$i˜bi x
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





=





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
($\sigma$i2 $-$ $\nu$)˜
\end{flushleft}


\begin{flushleft}
x2i $-$ 2
\end{flushleft}


\begin{flushleft}
$\sigma$i2˜b2i
\end{flushleft}


\begin{flushleft}
$\nu$ $-$ $\sigma$i2
\end{flushleft}





\begin{flushleft}
˜b2i + $\nu$$\gamma$
\end{flushleft}





\begin{flushleft}
˜i +
\end{flushleft}


\begin{flushleft}
$\sigma$i˜bi x
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
˜b2i + $\nu$$\gamma$
\end{flushleft}





+


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
g($\nu$).
\end{flushleft}





\begin{flushleft}
By weak duality, this means x
\end{flushleft}


\begin{flushleft}
˜ is optimal.
\end{flushleft}


2


2


\begin{flushleft}
$\bullet$ g($\sigma$1 ) is finite and g ($\sigma$1 ) $<$ 0. This is the case when ˜b1 = · · · = ˜bk = 0 and
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
g ($\sigma$12 ) = $-$
\end{flushleft}





\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
(˜bi $\sigma$i )2
\end{flushleft}


\begin{flushleft}
+ $\gamma$ $<$ 0.
\end{flushleft}


\begin{flushleft}
($\sigma$12 $-$ $\sigma$i2 )2
\end{flushleft}





\begin{flushleft}
$\sigma$12 ,
\end{flushleft}





\begin{flushleft}
the dual objective first decreases, and then increases as
\end{flushleft}


\begin{flushleft}
As we increase $\nu$ $>$
\end{flushleft}


\begin{flushleft}
$\nu$ goes to infinity. The solution is the same as in the previous case: we compute
\end{flushleft}


\begin{flushleft}
$\nu$ by solving g ($\nu$) = 0, and then calculate x
\end{flushleft}


\begin{flushleft}
˜ as above.
\end{flushleft}


\begin{flushleft}
$\bullet$ g($\sigma$12 ) is finite and g ($\sigma$12 ) $\geq$ 0. This is the case when ˜b1 = · · · = ˜bk = 0 and
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
g ($\sigma$12 ) = $-$
\end{flushleft}


\begin{flushleft}
In this case $\nu$ =
\end{flushleft}





\begin{flushleft}
$\sigma$12
\end{flushleft}





\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
(˜bi $\sigma$i )2
\end{flushleft}


\begin{flushleft}
+ $\gamma$ $\geq$ 0.
\end{flushleft}


\begin{flushleft}
($\sigma$12 $-$ $\sigma$i2 )2
\end{flushleft}





\begin{flushleft}
is optimal. A primal optimal solution is
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
g ($\nu$)
\end{flushleft}


0


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜i =
\end{flushleft}


2


2


˜


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
 $-$bi $\sigma$i /($\sigma$1 $-$ $\sigma$i )
\end{flushleft}


0





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
i = 1, . . . , k
\end{flushleft}


\begin{flushleft}
i = k + 1, . . . , r
\end{flushleft}


\begin{flushleft}
i = r + 1, . . . , n.
\end{flushleft}





\begin{flushleft}
(The first k coefficients are arbitrary as long as their squares add up to g ($\nu$).)
\end{flushleft}


\begin{flushleft}
To verify that x
\end{flushleft}


\begin{flushleft}
˜ is optimal, we note that it is feasible, i.e.,
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


˜





2


2





\begin{flushleft}
= g ($\nu$) +
\end{flushleft}


\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
˜b2i $\sigma$i2
\end{flushleft}


\begin{flushleft}
= $\gamma$,
\end{flushleft}


\begin{flushleft}
($\sigma$12 $-$ $\sigma$i2 )2
\end{flushleft}





\begin{flushleft}
and that its objective value equals g($\sigma$12 ):
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
˜i )
\end{flushleft}


\begin{flushleft}
($\sigma$i2 x
\end{flushleft}


\begin{flushleft}
˜2i $-$ 2$\sigma$i˜bi x
\end{flushleft}





=





\begin{flushleft}
$\sigma$12 g ($\sigma$12 ) +
\end{flushleft}


\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
˜i )
\end{flushleft}


\begin{flushleft}
($\sigma$i2 x
\end{flushleft}


\begin{flushleft}
˜2i $-$ 2$\sigma$i˜bi x
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





=





\begin{flushleft}
$\sigma$12
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
˜2i
\end{flushleft}





\begin{flushleft}
g ($\sigma$12 ) +
\end{flushleft}


\begin{flushleft}
i=k+1
\end{flushleft}





+


\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
˜i
\end{flushleft}


\begin{flushleft}
($\sigma$i2 $-$ $\sigma$12 )˜
\end{flushleft}


\begin{flushleft}
x2i $-$ 2$\sigma$i˜bi x
\end{flushleft}





\begin{flushleft}
r
\end{flushleft}





=





\begin{flushleft}
$\sigma$12 $\gamma$ +
\end{flushleft}


\begin{flushleft}
i=k+1
\end{flushleft}


\begin{flushleft}
r
\end{flushleft}





=





\begin{flushleft}
$\sigma$12 $\gamma$ +
\end{flushleft}


\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
˜i
\end{flushleft}


\begin{flushleft}
($\sigma$i2 $-$ $\sigma$12 )˜
\end{flushleft}


\begin{flushleft}
x2i $-$ 2$\sigma$i˜bi x
\end{flushleft}


\begin{flushleft}
(˜bi $\sigma$i )2
\end{flushleft}


\begin{flushleft}
$\sigma$12 $-$ $\sigma$i2
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=





\begin{flushleft}
g($\sigma$12 ) $-$
\end{flushleft}





\begin{flushleft}
˜b2i .
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
6.8 Formulate the following robust approximation problems as LPs, QPs, SOCPs, or SDPs.
\end{flushleft}


\begin{flushleft}
For each subproblem, consider the 1 -, 2 -, and the $\infty$ -norms.
\end{flushleft}


\begin{flushleft}
(a) Stochastic robust approximation with a finite set of parameter values, i.e., the sumof-norms problem
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
p Ai x $-$ b
\end{flushleft}


\begin{flushleft}
i=1 i
\end{flushleft}


\begin{flushleft}
where p 0 and 1T p = 1. (See §6.4.1.)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


$\bullet$





\begin{flushleft}
1 -norm:
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
$-$yi
\end{flushleft}





\begin{flushleft}
p i 1T y i
\end{flushleft}


\begin{flushleft}
Ai x $-$ b
\end{flushleft}





\begin{flushleft}
yi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , k.
\end{flushleft}





\begin{flushleft}
An LP with variables x $\in$ Rn , yi $\in$ Rm , i = 1, . . . , k.
\end{flushleft}


\begin{flushleft}
$\bullet$ 2 -norm:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
pT y
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
Ai x $-$ b 2 $\leq$ yi , i = 1, . . . , k.
\end{flushleft}


\begin{flushleft}
An SOCP with variables x $\in$ Rn , y $\in$ Rk .
\end{flushleft}


\begin{flushleft}
$\bullet$ $\infty$ -norm:
\end{flushleft}


\begin{flushleft}
pT y
\end{flushleft}


\begin{flushleft}
$-$yi 1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
Ai x $-$ b $\leq$ yi 1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , k.
\end{flushleft}





\begin{flushleft}
An LP with variables x $\in$ Rn , y $\in$ Rk .
\end{flushleft}





\begin{flushleft}
(b) Worst-case robust approximation with coefficient bounds:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
supA$\in$A Ax $-$ b
\end{flushleft}





\begin{flushleft}
A = \{A $\in$ Rm×n | lij $\leq$ aij $\leq$ uij , i = 1, . . . , m, j = 1, . . . , n\}.
\end{flushleft}





\begin{flushleft}
Here the uncertainty set is described by giving upper and lower bounds for the
\end{flushleft}


\begin{flushleft}
components of A. We assume lij $<$ uij .
\end{flushleft}


\begin{flushleft}
Solution. We first note that
\end{flushleft}


\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
lij $\leq$aij $\leq$uij
\end{flushleft}





\begin{flushleft}
|aTi x $-$ bi |
\end{flushleft}





=





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
lij $\leq$aij $\leq$uij
\end{flushleft}





=





\begin{flushleft}
max\{
\end{flushleft}





\begin{flushleft}
max\{aTi x $-$ bi , $-$aTi x + bi \}
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
lij $\leq$aij $\leq$uij
\end{flushleft}





\begin{flushleft}
(aTi x $-$ bi ),
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}





\begin{flushleft}
($-$aTi x + bi )\}.
\end{flushleft}





\begin{flushleft}
lij $\leq$aij $\leq$uij
\end{flushleft}





\begin{flushleft}
Now,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}





(





\begin{flushleft}
lij $\leq$aij $\leq$uij
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
aij xj $-$ bi ) = a
\end{flushleft}


\begin{flushleft}
¯Ti x $-$ bi +
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
vij |xj |
\end{flushleft}





\begin{flushleft}
where a
\end{flushleft}


\begin{flushleft}
¯ij = (lij + uij )/2, and vij = (uij $-$ lij )/2, and
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}





($-$





\begin{flushleft}
lij $\leq$aij $\leq$uij
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
aij xj + bi ) = $-$¯
\end{flushleft}


\begin{flushleft}
aTi x + bi +
\end{flushleft}





\begin{flushleft}
Therefore
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
vij |xj |.
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
lij $\leq$aij $\leq$uij
\end{flushleft}





\begin{flushleft}
aTi x $-$ bi | +
\end{flushleft}


\begin{flushleft}
|aTi x $-$ bi | = |¯
\end{flushleft}





\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
vij |xj |.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


$\bullet$





\begin{flushleft}
1 -norm:
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





|¯


\begin{flushleft}
aTi x $-$ bi | +
\end{flushleft}





\begin{flushleft}
vij |xj | .
\end{flushleft}





\begin{flushleft}
This can be expressed as an LP
\end{flushleft}


\begin{flushleft}
1T (y + V w)
\end{flushleft}


\begin{flushleft}
¯ $-$b
\end{flushleft}


\begin{flushleft}
$-$y Ax
\end{flushleft}


\begin{flushleft}
$-$w x w.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
The variables are x $\in$ Rn , y $\in$ Rm , w $\in$ Rn .
\end{flushleft}


\begin{flushleft}
$\bullet$ 2 -norm:
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}





|¯


\begin{flushleft}
aTi x $-$ bi | +
\end{flushleft}





\begin{flushleft}
y
\end{flushleft}





2





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
vij |xj |
\end{flushleft}





.





\begin{flushleft}
This can be expressed as an SOCP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
¯ $-$b y
\end{flushleft}


\begin{flushleft}
$-$y Ax
\end{flushleft}


\begin{flushleft}
$-$w x w
\end{flushleft}


\begin{flushleft}
y + V w 2 $\leq$ t.
\end{flushleft}





\begin{flushleft}
The variables are x $\in$ Rn , y $\in$ Rm , w $\in$ Rn , t $\in$ R.
\end{flushleft}


\begin{flushleft}
$\bullet$ $\infty$ -norm:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
j=1
\end{flushleft}





\begin{flushleft}
aTi x $-$ bi | +
\end{flushleft}


\begin{flushleft}
maxi=1,...,m |¯
\end{flushleft}





\begin{flushleft}
vij |xj | .
\end{flushleft}





\begin{flushleft}
This can be expressed as an LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
¯ $-$b y
\end{flushleft}


\begin{flushleft}
$-$y Ax
\end{flushleft}


\begin{flushleft}
$-$w x w
\end{flushleft}


\begin{flushleft}
$-$t1 y + V w $\leq$ t1.
\end{flushleft}





\begin{flushleft}
The variables are x $\in$ Rn , y $\in$ Rm , w $\in$ Rn , t $\in$ R.
\end{flushleft}





\begin{flushleft}
(c) Worst-case robust approximation with polyhedral uncertainty:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
supA$\in$A Ax $-$ b
\end{flushleft}





\begin{flushleft}
A = \{[a1 · · · am ]T | Ci ai
\end{flushleft}





\begin{flushleft}
di , i = 1, . . . , m\}.
\end{flushleft}





\begin{flushleft}
The uncertainty is described by giving a polyhedron Pi = \{ai | Ci ai di \} of possible
\end{flushleft}


\begin{flushleft}
values for each row. The parameters Ci $\in$ Rpi ×n , di $\in$ Rpi , i = 1, . . . , m, are given.
\end{flushleft}


\begin{flushleft}
We assume that the polyhedra Pi are nonempty and bounded.
\end{flushleft}


\begin{flushleft}
Solution. Pi = \{a | Ci a
\end{flushleft}





\begin{flushleft}
di \}.
\end{flushleft}





\begin{flushleft}
sup |aTi x $-$ bi |
\end{flushleft}





=





\begin{flushleft}
ai $\in$Pi
\end{flushleft}





\begin{flushleft}
sup max\{aTi x $-$ bi , $-$aTi x + bi \}
\end{flushleft}





\begin{flushleft}
ai $\in$Pi
\end{flushleft}





=





\begin{flushleft}
max\{ sup (aTi x) $-$ bi , sup ($-$aTi x) + bi \}.
\end{flushleft}


\begin{flushleft}
ai $\in$Pi
\end{flushleft}





\begin{flushleft}
ai $\in$Pi
\end{flushleft}





\begin{flushleft}
By LP duality,
\end{flushleft}


\begin{flushleft}
sup aTi x
\end{flushleft}





=





\begin{flushleft}
inf\{dTi v | CiT v = x, v
\end{flushleft}





=





\begin{flushleft}
inf\{dTi w | CiT w = $-$x, w
\end{flushleft}





\begin{flushleft}
ai $\in$Pi
\end{flushleft}





\begin{flushleft}
sup ($-$aTi x)
\end{flushleft}


\begin{flushleft}
ai $\in$Pi
\end{flushleft}





0\}


0\}.





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
Therefore, ti $\geq$ supai $\in$Pi |aTi x $-$ bi | if and only if there exist v, w, such that
\end{flushleft}


\begin{flushleft}
v, w
\end{flushleft}





0,





\begin{flushleft}
x = CiT v = $-$CiT w,
\end{flushleft}





\begin{flushleft}
dTi v $\leq$ ti ,
\end{flushleft}





\begin{flushleft}
dTi w $\leq$ ti .
\end{flushleft}





\begin{flushleft}
This allows us to pose the robust approximation problem as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





$\bullet$





$\bullet$





$\bullet$





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
x = CiT vi ,
\end{flushleft}


\begin{flushleft}
dTi vi $\leq$ ti ,
\end{flushleft}


\begin{flushleft}
vi , wi 0,
\end{flushleft}





\begin{flushleft}
1 -norm:
\end{flushleft}





\begin{flushleft}
x = $-$CiT wi ,
\end{flushleft}


\begin{flushleft}
dTi wi $\leq$ ti ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T t
\end{flushleft}


\begin{flushleft}
x = CiT vi ,
\end{flushleft}


\begin{flushleft}
dTi vi $\leq$ ti ,
\end{flushleft}


\begin{flushleft}
vi , wi 0,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}


\begin{flushleft}
x = CiT vi ,
\end{flushleft}


\begin{flushleft}
dTi vi $\leq$ ti ,
\end{flushleft}


\begin{flushleft}
vi , wi 0,
\end{flushleft}


\begin{flushleft}
t 2 $\leq$ u.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
x = CiT vi ,
\end{flushleft}


\begin{flushleft}
dTi vi $\leq$ t,
\end{flushleft}


\begin{flushleft}
vi , wi 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
x = $-$CiT wi ,
\end{flushleft}


\begin{flushleft}
dTi wi $\leq$ ti ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
x = $-$CiT wi ,
\end{flushleft}


\begin{flushleft}
dTi wi $\leq$ ti ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
2 -norm:
\end{flushleft}





\begin{flushleft}
$\infty$ -norm:
\end{flushleft}





\begin{flushleft}
x = $-$CiT wi , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
dTi wi $\leq$ t, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Function fitting and interpolation
\end{flushleft}


\begin{flushleft}
6.9 Minimax rational function fitting. Show that the following problem is quasiconvex:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
max
\end{flushleft}





\begin{flushleft}
i=1,...,k
\end{flushleft}





\begin{flushleft}
p(ti )
\end{flushleft}


\begin{flushleft}
$-$ yi
\end{flushleft}


\begin{flushleft}
q(ti )
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
p(t) = a0 + a1 t + a2 t2 + · · · + am tm ,
\end{flushleft}





\begin{flushleft}
q(t) = 1 + b1 t + · · · + bn tn ,
\end{flushleft}





\begin{flushleft}
and the domain of the objective function is defined as
\end{flushleft}


\begin{flushleft}
D = \{(a, b) $\in$ Rm+1 × Rn | q(t) $>$ 0, $\alpha$ $\leq$ t $\leq$ $\beta$\}.
\end{flushleft}


\begin{flushleft}
In this problem we fit a rational function p(t)/q(t) to given data, while constraining the
\end{flushleft}


\begin{flushleft}
denominator polynomial to be positive on the interval [$\alpha$, $\beta$]. The optimization variables
\end{flushleft}


\begin{flushleft}
are the numerator and denominator coefficients ai , bi . The interpolation points ti $\in$ [$\alpha$, $\beta$],
\end{flushleft}


\begin{flushleft}
and desired function values yi , i = 1, . . . , k, are given.
\end{flushleft}


\begin{flushleft}
Solution. Let's show the objective is quasiconvex. Its domain is convex. Since q(ti ) $>$ 0
\end{flushleft}


\begin{flushleft}
for i = 1, . . . , k, we have
\end{flushleft}


\begin{flushleft}
max |p(ti )/q(ti ) $-$ yi | $\leq$ $\gamma$
\end{flushleft}


\begin{flushleft}
i=1,...,k
\end{flushleft}





\begin{flushleft}
if and only if
\end{flushleft}


\begin{flushleft}
$-$$\gamma$q(ti ) $\leq$ p(ti ) $-$ yi q(ti ) $\leq$ $\gamma$q(ti ),
\end{flushleft}





\begin{flushleft}
which is a pair of linear inequalities.
\end{flushleft}





\begin{flushleft}
i = 1, . . . , k,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
6.10 Fitting data with a concave nonnegative nondecreasing quadratic function. We are given
\end{flushleft}


\begin{flushleft}
the data
\end{flushleft}


\begin{flushleft}
x1 , . . . , x N $\in$ R n ,
\end{flushleft}


\begin{flushleft}
y1 , . . . , yN $\in$ R,
\end{flushleft}


\begin{flushleft}
and wish to fit a quadratic function of the form
\end{flushleft}





\begin{flushleft}
f (x) = (1/2)xT P x + q T x + r,
\end{flushleft}


\begin{flushleft}
where P $\in$ Sn , q $\in$ Rn , and r $\in$ R are the parameters in the model (and, therefore, the
\end{flushleft}


\begin{flushleft}
variables in the fitting problem).
\end{flushleft}


\begin{flushleft}
Our model will be used only on the box B = \{x $\in$ Rn | l x u\}. You can assume that
\end{flushleft}


\begin{flushleft}
l ≺ u, and that the given data points xi are in this box.
\end{flushleft}


\begin{flushleft}
We will use the simple sum of squared errors objective,
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(f (xi ) $-$ yi )2 ,
\end{flushleft}





\begin{flushleft}
as the criterion for the fit. We also impose several constraints on the function f . First,
\end{flushleft}


\begin{flushleft}
it must be concave. Second, it must be nonnegative on B, i.e., f (z) $\geq$ 0 for all z $\in$ B.
\end{flushleft}


\begin{flushleft}
Third, f must be nondecreasing on B, i.e., whenever z, z˜ $\in$ B satisfy z
\end{flushleft}


\begin{flushleft}
z˜, we have
\end{flushleft}


\begin{flushleft}
f (z) $\leq$ f (˜
\end{flushleft}


\begin{flushleft}
z ).
\end{flushleft}


\begin{flushleft}
Show how to formulate this fitting problem as a convex problem. Simplify your formulation as much as you can.
\end{flushleft}


\begin{flushleft}
Solution. The objective function is a convex quadratic function of the function parameters, which are the variables in the fitting problem, so we need only consider the
\end{flushleft}


\begin{flushleft}
constraints. The function f is concave if and only if P
\end{flushleft}


\begin{flushleft}
0, which is a convex constraint,
\end{flushleft}


\begin{flushleft}
in fact, a linear matrix inequality. The nonnegativity constraint states that f (z) $\geq$ 0 for
\end{flushleft}


\begin{flushleft}
each z $\in$ B. For each such z, the constraint is a linear inequality in the variables P, q, r,
\end{flushleft}


\begin{flushleft}
so the constraint is the intersection of an infinite number of linear inequalities (one for
\end{flushleft}


\begin{flushleft}
each z $\in$ B) and therefore convex. But we can derive a much simpler representation for
\end{flushleft}


\begin{flushleft}
this constraint. Since we will impose the condition that f is nondecreasing, it follows that
\end{flushleft}


\begin{flushleft}
the lowest value of f must be attained at the point l. Thus, f is nonnegative on B if and
\end{flushleft}


\begin{flushleft}
only if f (l) $\geq$ 0, which is a single linear inequality.
\end{flushleft}


\begin{flushleft}
Now let's look at the monotonicity constraint. We claim this is equivalent to $\nabla$f (z) 0
\end{flushleft}


\begin{flushleft}
for z $\in$ B. Let's show that first. Suppose f is monotone on B and let z $\in$ int B. Then
\end{flushleft}


\begin{flushleft}
for small positive t $\in$ R, we have f (z + tei ) $\geq$ f (z). Subtracting, and taking the limit as
\end{flushleft}


\begin{flushleft}
t $\rightarrow$ 0 gives the conclusion $\nabla$f (z)i $\geq$ 0. To show the converse, suppose that $\nabla$f (z)
\end{flushleft}


0


\begin{flushleft}
on B, and let z, z˜ $\in$ B, with z z˜. Define g(t) = f (z + t(˜
\end{flushleft}


\begin{flushleft}
z $-$ z)). Then we have
\end{flushleft}


\begin{flushleft}
f (˜
\end{flushleft}


\begin{flushleft}
z ) $-$ f (z)
\end{flushleft}





=





\begin{flushleft}
g(1) $-$ g(0)
\end{flushleft}


1





\begin{flushleft}
g (t) dt
\end{flushleft}





=


0


1





=


0





$\geq$





(˜


\begin{flushleft}
z $-$ z)T $\nabla$f (z + t(˜
\end{flushleft}


\begin{flushleft}
z $-$ z)) dt
\end{flushleft}





0,





\begin{flushleft}
since z˜ $-$ z
\end{flushleft}


\begin{flushleft}
0 and $\nabla$f
\end{flushleft}


\begin{flushleft}
0 on B. (Note that this result doesn't depend on f being
\end{flushleft}


\begin{flushleft}
quadratic.)
\end{flushleft}


\begin{flushleft}
For our function, monotonicity is equivalent to $\nabla$f (z) = P z + q
\end{flushleft}


\begin{flushleft}
0 for z $\in$ B. This
\end{flushleft}


\begin{flushleft}
too is convex, since for each z, it is a set of linear inequalities in the parameters of
\end{flushleft}


\begin{flushleft}
the function. We replace this abstract constraint with 2n constraints, by insisting that
\end{flushleft}


\begin{flushleft}
$\nabla$f (z) = P z+q 0 must hold at the 2n vertices of B (obtained by setting each component
\end{flushleft}


\begin{flushleft}
equal to li or ui ). But there is a far better description of the monotonicity constraint.
\end{flushleft}





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
Let us express P as P = P+ $-$ P$-$ , where P+ and P$-$ are the elementwise positive and
\end{flushleft}


\begin{flushleft}
negative parts of P , respectively:
\end{flushleft}


\begin{flushleft}
(P+ )ij = max\{Pij , 0\},
\end{flushleft}





\begin{flushleft}
(P$-$ )ij = max\{$-$Pij , 0\}.
\end{flushleft}





\begin{flushleft}
Then
\end{flushleft}


\begin{flushleft}
Pz + q
\end{flushleft}





0





\begin{flushleft}
for all l
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
holds if and only if
\end{flushleft}


\begin{flushleft}
P+ l $-$ P $-$ u + q
\end{flushleft}





0.





\begin{flushleft}
Note that in contrast to our set of 2n linear inequalities, this representation involves
\end{flushleft}


\begin{flushleft}
n(n + 1) new variables, and n linear inequality constraints.
\end{flushleft}


\begin{flushleft}
(Another method to get a compact representation of the monotonicity constraint is based
\end{flushleft}


\begin{flushleft}
on deriving the alternative inequality to the condition that P z + q 0 for l z u; this
\end{flushleft}


\begin{flushleft}
results in an equivalent formulation.)
\end{flushleft}


\begin{flushleft}
Finally, we can express the problem as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





2





\begin{flushleft}
(1/2)xTi P xi + q T xi + r $-$ yi
\end{flushleft}





\begin{flushleft}
P
\end{flushleft}


0


\begin{flushleft}
(1/2)lT P l + q T l + r $\geq$ 0
\end{flushleft}


\begin{flushleft}
P = P+ $-$ P$-$ , (P+ )ij $\geq$ 0,
\end{flushleft}


\begin{flushleft}
P+ l $-$ P$-$ u + q 0,
\end{flushleft}





\begin{flushleft}
(P$-$ )ij $\geq$ 0
\end{flushleft}





\begin{flushleft}
with variables P, P+ , P$-$ $\in$ Sn , q $\in$ R, and r $\in$ R. The objective is convex quadratic, there
\end{flushleft}


\begin{flushleft}
is one linear matrix inequality (LMI) constraint, and some linear equality and inequality
\end{flushleft}


\begin{flushleft}
constraints. This problem can be expressed as an SDP.
\end{flushleft}


\begin{flushleft}
We should note one common pitfall. We argue that f is concave, so its gradient must be
\end{flushleft}


\begin{flushleft}
monotone nonincreasing. Therefore, the argument goes, its {`}lowest' value in B is achieved
\end{flushleft}


\begin{flushleft}
at the upper corner u. Therefore, for P u+q 0 is enough to ensure that the monotonicity
\end{flushleft}


\begin{flushleft}
condition holds. One variation on this argument holds that it is enough to impose the
\end{flushleft}


\begin{flushleft}
two inequalities P l + q 0 and P u + q 0.
\end{flushleft}


\begin{flushleft}
This sounds very reasonable, and in fact is true for dimensions n = 1 and n = 2. But
\end{flushleft}


\begin{flushleft}
sadly, it is false in general. Here is a counterexample:
\end{flushleft}


\begin{flushleft}
P =
\end{flushleft}





$-$1


1


$-$1





1


$-$10


0





\begin{flushleft}
It is easily checked that P
\end{flushleft}





$-$1


0


$-$10





,





1


$-$1


0





\begin{flushleft}
l=
\end{flushleft}





\begin{flushleft}
0, P l + q
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
u=
\end{flushleft}





\begin{flushleft}
0, and P u + q
\end{flushleft}





\begin{flushleft}
z=
\end{flushleft}


\begin{flushleft}
which satisfies l
\end{flushleft}





,





1


$-$1


1





1.1


1


1





,





\begin{flushleft}
q=
\end{flushleft}





2.1


20


20





.





\begin{flushleft}
0. However, consider the point
\end{flushleft}





,





\begin{flushleft}
u. For this point we have
\end{flushleft}


\begin{flushleft}
Pz + q =
\end{flushleft}





$-$0.9


31


9





0.





\begin{flushleft}
6.11 Least-squares direction interpolation. Suppose F1 , . . . , Fn : Rk $\rightarrow$ Rp , and we form the
\end{flushleft}


\begin{flushleft}
linear combination F : Rk $\rightarrow$ Rp ,
\end{flushleft}


\begin{flushleft}
F (u) = x1 F1 (u) + · · · + xn Fn (u),
\end{flushleft}


\begin{flushleft}
where x is the variable in the interpolation problem.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
In this problem we require that (F (vj ), qj ) = 0, j = 1, . . . , m, where qj are given vectors
\end{flushleft}


\begin{flushleft}
in Rp , which we assume satisfy qj 2 = 1. In other words, we require the direction of
\end{flushleft}


\begin{flushleft}
F to take on specified values at the points vj . To ensure that F (vj ) is not zero (which
\end{flushleft}


\begin{flushleft}
makes the angle undefined), we impose the minimum length constraints F (vj ) 2 $\geq$ ,
\end{flushleft}


\begin{flushleft}
j = 1, . . . , m, where $>$ 0 is given.
\end{flushleft}


\begin{flushleft}
Show how to find x that minimizes x 2 , and satisfies the direction (and minimum length)
\end{flushleft}


\begin{flushleft}
conditions above, using convex optimization.
\end{flushleft}


\begin{flushleft}
Solution. Introduce variables yi , and constraints
\end{flushleft}


\begin{flushleft}
yj $\geq$ ,
\end{flushleft}





\begin{flushleft}
F (vj ) = yj qj ,
\end{flushleft}


\begin{flushleft}
and minimize x 2 . This is a QP.
\end{flushleft}





\begin{flushleft}
6.12 Interpolation with monotone functions. A function f : Rk $\rightarrow$ R is monotone nondecreasing (with respect to Rk+ ) if f (u) $\geq$ f (v) whenever u v.
\end{flushleft}


\begin{flushleft}
(a) Show that there exists a monotone nondecreasing function f : Rk $\rightarrow$ R, that satisfies
\end{flushleft}


\begin{flushleft}
f (ui ) = yi for i = 1, . . . , m, if and only if
\end{flushleft}


\begin{flushleft}
yi $\geq$ yj whenever ui
\end{flushleft}





\begin{flushleft}
uj ,
\end{flushleft}





\begin{flushleft}
i, j = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
(b) Show that there exists a convex monotone nondecreasing function f : Rk $\rightarrow$ R, with
\end{flushleft}


\begin{flushleft}
dom f = Rk , that satisfies f (ui ) = yi for i = 1, . . . , m, if and only if there exist
\end{flushleft}


\begin{flushleft}
gi $\in$ Rk , i = 1, . . . , m, such that
\end{flushleft}


\begin{flushleft}
gi
\end{flushleft}





0,





\begin{flushleft}
yj $\geq$ yi + giT (uj $-$ ui ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
i, j = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The condition is obviously necessary. It is also sufficient. Define
\end{flushleft}


\begin{flushleft}
f (x) = max yi .
\end{flushleft}


\begin{flushleft}
ui
\end{flushleft}





\begin{flushleft}
This function is monotone, because v
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
w always implies
\end{flushleft}





\begin{flushleft}
f (v) = max yi $\leq$ max yi = f (w).
\end{flushleft}


\begin{flushleft}
ui
\end{flushleft}





\begin{flushleft}
v
\end{flushleft}





\begin{flushleft}
ui
\end{flushleft}





\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
f satisfies the interpolation conditions if
\end{flushleft}


\begin{flushleft}
f (ui ) = max yj = yi ,
\end{flushleft}


\begin{flushleft}
uj
\end{flushleft}





\begin{flushleft}
ui
\end{flushleft}





\begin{flushleft}
which is true if ui uj implies yi $\geq$ yj .
\end{flushleft}


\begin{flushleft}
If we want dom f = Rk , we can define f as
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
mini yi
\end{flushleft}


\begin{flushleft}
maxui
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
yi
\end{flushleft}





\begin{flushleft}
x ui , i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
(b) We first show it is necessary. Suppose f is convex, monotone nondecreasing, with
\end{flushleft}


\begin{flushleft}
dom f = Rk , and satisfies the interpolation conditions. Let gi be a normal vector
\end{flushleft}


\begin{flushleft}
to a supporting hyperplane at ui to f , i.e.,
\end{flushleft}


\begin{flushleft}
f (x) $\geq$ yi + giT (x $-$ ui ),
\end{flushleft}


\begin{flushleft}
for all x. In particular, at x = uj , this inequality reduces to
\end{flushleft}


\begin{flushleft}
yj $\geq$ yi + giT (x $-$ ui ),
\end{flushleft}





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
0: If gik $<$ 0, then choosing x = ui $-$ ek gives
\end{flushleft}





\begin{flushleft}
It also follows that gi
\end{flushleft}





\begin{flushleft}
f (x) $\geq$ yi + giT (x $-$ ui ) = yi $-$ gij $>$ yi ,
\end{flushleft}


\begin{flushleft}
so f is not monotone.
\end{flushleft}


\begin{flushleft}
To show that the conditions are sufficient, consider
\end{flushleft}


\begin{flushleft}
f (x) = max
\end{flushleft}





\begin{flushleft}
i=1,...,m
\end{flushleft}





\begin{flushleft}
yi + giT (x $-$ ui ) .
\end{flushleft}





\begin{flushleft}
f is convex, satisfies the interpolation conditions, and is monotone: if v
\end{flushleft}





\begin{flushleft}
w, then
\end{flushleft}





\begin{flushleft}
yi + giT (v $-$ ui ) $\leq$ yi + giT (w $-$ ui )
\end{flushleft}


\begin{flushleft}
for all i, and hence f (v) $\leq$ f (w).
\end{flushleft}


\begin{flushleft}
6.13 Interpolation with quasiconvex functions. Show that there exists a quasiconvex function
\end{flushleft}


\begin{flushleft}
f : Rk $\rightarrow$ R, that satisfies f (ui ) = yi for i = 1, . . . , m, if and only if there exist gi $\in$ Rk ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m, such that
\end{flushleft}


\begin{flushleft}
giT (uj $-$ ui ) $\leq$ $-$1 whenever yj $<$ yi ,
\end{flushleft}





\begin{flushleft}
i, j = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Solution. We first show that the condition is necessary. For each i = 1, . . . , m, define
\end{flushleft}


\begin{flushleft}
Ji = \{j = 1, . . . , m | yj $<$ yi \}. Suppose the condition does not hold, i.e., for some i, the
\end{flushleft}


\begin{flushleft}
set of inequalities
\end{flushleft}


\begin{flushleft}
giT (uj $-$ ui ) $\leq$ $-$1, j $\in$ Ji
\end{flushleft}


\begin{flushleft}
is infeasible. By a theorem of alternatives, there exists $\lambda$ 0 such that
\end{flushleft}





\begin{flushleft}
j$\in$Ji
\end{flushleft}





\begin{flushleft}
$\lambda$j (uj $-$ ui ) = 0,
\end{flushleft}





\begin{flushleft}
$\lambda$j = 1.
\end{flushleft}


\begin{flushleft}
j$\in$Ji
\end{flushleft}





\begin{flushleft}
This means ui is a convex combination of uj , j $\in$ Ji . On the other hand, yi $>$ yj for
\end{flushleft}


\begin{flushleft}
j $\in$ Ji , so if f (ui ) = yi and f (uj ) = yj , then f cannot be quasiconvex.
\end{flushleft}


\begin{flushleft}
Next we prove the condition is sufficient. Suppose the condition holds. Define f : Rk $\rightarrow$ R
\end{flushleft}


\begin{flushleft}
as
\end{flushleft}


\begin{flushleft}
f (x) = max ymin , max\{yj | gjT (x $-$ uj ) $\geq$ 0\}
\end{flushleft}


\begin{flushleft}
where ymin = mini yi .
\end{flushleft}


\begin{flushleft}
We first verify that f satisfies the interpolation conditions f (ui ) = yi . It is immediate
\end{flushleft}


\begin{flushleft}
from the definition of f that f (ui ) $\geq$ yi . Also, f (ui ) $>$ yi only if gjT (ui $-$ uj ) $\geq$ 0 for some
\end{flushleft}


\begin{flushleft}
j with yj $>$ yi . This contradicts the definition of gj . Therefore f (ui ) = yi .
\end{flushleft}


\begin{flushleft}
Finally, we check that f is quasiconvex. The sublevel sets of f are convex because f (x) $\leq$ $\alpha$
\end{flushleft}


\begin{flushleft}
if and only if
\end{flushleft}


\begin{flushleft}
gjT (x $-$ uj ) $\geq$ 0 =$\Rightarrow$ yj $\leq$ $\alpha$
\end{flushleft}





\begin{flushleft}
or equivalently, gjT (x $-$ uj ) $<$ 0 for all j with yj $>$ $\alpha$.
\end{flushleft}


\begin{flushleft}
6.14 [Nes00] Interpolation with positive-real functions. Suppose z1 , . . . , zn $\in$ C are n distinct
\end{flushleft}


\begin{flushleft}
points with |zi | $>$ 1. We define Knp as the set of vectors y $\in$ Cn for which there exists a
\end{flushleft}


\begin{flushleft}
function f : C $\rightarrow$ C that satisfies the following conditions.
\end{flushleft}


\begin{flushleft}
$\bullet$ f is positive-real, which means it is analytic outside the unit circle (i.e., for |z| $>$ 1),
\end{flushleft}


\begin{flushleft}
and its real part is nonnegative outside the unit circle ( f (z) $\geq$ 0 for |z| $>$ 1).
\end{flushleft}


\begin{flushleft}
$\bullet$ f satisfies the interpolation conditions
\end{flushleft}


\begin{flushleft}
f (z1 ) = y1 ,
\end{flushleft}





\begin{flushleft}
f (z2 ) = y2 ,
\end{flushleft}





...,





\begin{flushleft}
f (zn ) = yn .
\end{flushleft}





\begin{flushleft}
If we denote the set of positive-real functions as F, then we can express Knp as
\end{flushleft}


\begin{flushleft}
Knp = \{y $\in$ Cn | $\exists$f $\in$ F, yk = f (zk ), k = 1, . . . , n\}.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) It can be shown that f is positive-real if and only if there exists a nondecreasing
\end{flushleft}


\begin{flushleft}
function $\rho$ such that for all z with |z| $>$ 1,
\end{flushleft}


\begin{flushleft}
2$\pi$
\end{flushleft}





\begin{flushleft}
ei$\theta$ + z $-$1
\end{flushleft}


\begin{flushleft}
d$\rho$($\theta$),
\end{flushleft}


\begin{flushleft}
ei$\theta$ $-$ z $-$1
\end{flushleft}





\begin{flushleft}
f (z) = i f ($\infty$) +
\end{flushleft}


0





$\surd$





\begin{flushleft}
where i = $-$1 (see [KN77, page 389]). Use this representation to show that K np
\end{flushleft}


\begin{flushleft}
is a closed convex cone.
\end{flushleft}


\begin{flushleft}
Solution. It follows that every element in Knp can be expressed as i$\alpha$1 + v where
\end{flushleft}


\begin{flushleft}
$\alpha$ $\in$ R and v is in the conic hull of the vectors
\end{flushleft}


\begin{flushleft}
v($\theta$) =
\end{flushleft}





\begin{flushleft}
ei$\theta$ + z1$-$1 ei$\theta$ + z2$-$1
\end{flushleft}


\begin{flushleft}
ei$\theta$ + zn$-$1
\end{flushleft}


,


,


.


.


.


,


\begin{flushleft}
ei$\theta$ $-$ z1$-$1 ei$\theta$ $-$ z2$-$1
\end{flushleft}


\begin{flushleft}
ei$\theta$ $-$ zn$-$1
\end{flushleft}





\begin{flushleft}
0 $\leq$ $\theta$ $\leq$ 2$\pi$.
\end{flushleft}





,





\begin{flushleft}
Therefore Knp is the sum of a convex cone and a line, so it is also a convex cone.
\end{flushleft}


\begin{flushleft}
Closedness is less obvious. The set
\end{flushleft}


\begin{flushleft}
C = \{v($\theta$) | 0 $\leq$ $\theta$ $\leq$ 2$\pi$\}
\end{flushleft}


\begin{flushleft}
is compact, because v is continuous on [0, 2$\pi$]. The convex hull of a compact set is
\end{flushleft}


\begin{flushleft}
compact, and the conic hull of a compact set is closed. Therefore Knp is the sum of
\end{flushleft}


\begin{flushleft}
two closed sets (the conic hull of C and the line i$\alpha$R), hence it is closed.
\end{flushleft}


\begin{flushleft}
(b) We will use the inner product (xH y) between vectors x, y $\in$ Cn , where xH denotes
\end{flushleft}


\begin{flushleft}
the complex conjugate transpose of x. Show that the dual cone of Knp is given by
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


∗


\begin{flushleft}
Knp
\end{flushleft}


=





\begin{flushleft}
x $\in$ Cn
\end{flushleft}





\begin{flushleft}
(1T x) = 0,
\end{flushleft}





\begin{flushleft}
xl
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}





\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
e$-$i$\theta$ $-$ z¯l$-$1
\end{flushleft}





\begin{flushleft}
$\geq$ 0 $\forall$$\theta$ $\in$ [0, 2$\pi$]
\end{flushleft}





.





∗


\begin{flushleft}
Solution. x $\in$ Knp
\end{flushleft}


\begin{flushleft}
if
\end{flushleft}





\begin{flushleft}
((i$\alpha$1 + v)H x) = $\alpha$ (1T x) + (v H x) $\geq$ 0
\end{flushleft}


\begin{flushleft}
for all $\alpha$ $\in$ R and all v in the conic hull of the vectors v($\theta$). This condition is
\end{flushleft}


\begin{flushleft}
equivalent to (1T x) = 0 and (v($\theta$)H x) $\geq$ 0 for all $\theta$ $\in$ [0, 2$\pi$].
\end{flushleft}


\begin{flushleft}
(c) Show that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


∗


\begin{flushleft}
Knp
\end{flushleft}


=





\begin{flushleft}
x $\in$ Cn
\end{flushleft}





\begin{flushleft}
$\exists$Q $\in$ Hn
\end{flushleft}


\begin{flushleft}
+ , xl =
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
Qkl
\end{flushleft}


\begin{flushleft}
, l = 1, . . . , n
\end{flushleft}


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}





\begin{flushleft}
Hn
\end{flushleft}


+





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
denotes the set of positive semidefinite Hermitian matrices of size n × n.
\end{flushleft}


\begin{flushleft}
Use the following result (known as Riesz-Fej´er theorem; see [KN77, page 60]). A
\end{flushleft}


\begin{flushleft}
function of the form
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
(yk e$-$ik$\theta$ + y¯k eik$\theta$ )
\end{flushleft}


\begin{flushleft}
k=0
\end{flushleft}





\begin{flushleft}
is nonnegative for all $\theta$ if and only if there exist a0 , . . . , an $\in$ C such that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





2





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
(yk e
\end{flushleft}





\begin{flushleft}
$-$ik$\theta$
\end{flushleft}





\begin{flushleft}
+ y¯k e
\end{flushleft}





\begin{flushleft}
ik$\theta$
\end{flushleft}





)=





\begin{flushleft}
k=0
\end{flushleft}





\begin{flushleft}
ak e
\end{flushleft}





\begin{flushleft}
ik$\theta$
\end{flushleft}





.





\begin{flushleft}
k=0
\end{flushleft}





\begin{flushleft}
Solution. We first show that any x of the form
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
xl =
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
Qkl
\end{flushleft}


,


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}





\begin{flushleft}
l = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
(6.14.A)
\end{flushleft}





\newpage
6





\begin{flushleft}
Approximation and fitting
\end{flushleft}





∗


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
where Q $\in$ Hn
\end{flushleft}


\begin{flushleft}
+ , belongs to Knp . Suppose x satisfies (6.14.A) for some Q $\in$ H+ . We
\end{flushleft}


\begin{flushleft}
have
\end{flushleft}





\begin{flushleft}
2i (1T x)
\end{flushleft}





\begin{flushleft}
1 T x $-$ 1T x
\end{flushleft}


¯





=





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=


\begin{flushleft}
k=1 l=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=


\begin{flushleft}
k=1 l=1
\end{flushleft}





=





\begin{flushleft}
Qkl
\end{flushleft}


$-$


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}


\begin{flushleft}
Qkl
\end{flushleft}


$-$


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
k=1 l=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
l=1 k=1
\end{flushleft}





\begin{flushleft}
Qlk
\end{flushleft}


\begin{flushleft}
1 $-$ z¯k$-$1 zl$-$1
\end{flushleft}


\begin{flushleft}
Qkl
\end{flushleft}


\begin{flushleft}
1 $-$ z¯l$-$1 zk$-$1
\end{flushleft}





0.





\begin{flushleft}
Also,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
xl
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}





\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
e$-$i$\theta$ $-$ z¯l$-$1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=





=





1


2





=





1


2





=





1


2





\begin{flushleft}
k=1 l=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
k=1 l=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
k=1 l=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
k=1 l=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=


\begin{flushleft}
k=1 l=1
\end{flushleft}





$\geq$





\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
Qkl
\end{flushleft}


\begin{flushleft}
$-$1 $-$1 $-$i$\theta$
\end{flushleft}


\begin{flushleft}
1 $-$ zk z¯l e
\end{flushleft}


\begin{flushleft}
$-$ z¯l$-$1
\end{flushleft}


\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
ei$\theta$ + zl$-$1
\end{flushleft}


\begin{flushleft}
Qkl
\end{flushleft}


\begin{flushleft}
Qlk
\end{flushleft}


+


\begin{flushleft}
$-$1 $-$1 $-$i$\theta$
\end{flushleft}


$-$1


\begin{flushleft}
$-$1 $-$1 i$\theta$
\end{flushleft}


\begin{flushleft}
1 $-$ zk z¯l e
\end{flushleft}


\begin{flushleft}
$-$ z¯l
\end{flushleft}


\begin{flushleft}
1 $-$ z¯k zl e $-$ zl$-$1
\end{flushleft}


\begin{flushleft}
ei$\theta$ + zk$-$1
\end{flushleft}


\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


+


$-$1


\begin{flushleft}
e$-$i$\theta$ $-$ z¯l
\end{flushleft}


\begin{flushleft}
ei$\theta$ $-$ zk$-$1
\end{flushleft}





\begin{flushleft}
Qkl
\end{flushleft}


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}





\begin{flushleft}
2(1 $-$ zk$-$1 z¯l$-$1 )
\end{flushleft}


\begin{flushleft}
Qkl
\end{flushleft}


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1 (ei$\theta$ $-$ zk$-$1 )(e$-$i$\theta$ $-$ z¯l$-$1 )
\end{flushleft}





\begin{flushleft}
(ei$\theta$
\end{flushleft}





$-$





\begin{flushleft}
Qkl
\end{flushleft}


$-$1


\begin{flushleft}
zk )(e$-$i$\theta$
\end{flushleft}





\begin{flushleft}
$-$ z¯l$-$1 )
\end{flushleft}





0.





∗


\begin{flushleft}
Therefore x $\in$ Knp
\end{flushleft}


.





∗


\begin{flushleft}
Conversely, suppose x $\in$ Knp
\end{flushleft}


\begin{flushleft}
, i.e.,
\end{flushleft}





\begin{flushleft}
(1T x) = 0 and the function
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
R($\theta$) =
\end{flushleft}





\begin{flushleft}
xl
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}





\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
e$-$i$\theta$ $-$ z¯l$-$1
\end{flushleft}





\begin{flushleft}
is nonnegative. We can write R as
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
R($\theta$)
\end{flushleft}





=





=





=





1


2





\begin{flushleft}
xl
\end{flushleft}





\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
e$-$i$\theta$ $-$ z¯l$-$1
\end{flushleft}





\begin{flushleft}
xl
\end{flushleft}





\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
ei$\theta$ + zl$-$1
\end{flushleft}


\begin{flushleft}
+x
\end{flushleft}


\begin{flushleft}
¯l i$\theta$
\end{flushleft}


$-$1


\begin{flushleft}
$-$i$\theta$
\end{flushleft}


\begin{flushleft}
e
\end{flushleft}


\begin{flushleft}
$-$ z¯l
\end{flushleft}


\begin{flushleft}
e $-$ zl$-$1
\end{flushleft}





\begin{flushleft}
l=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
l=1
\end{flushleft}


\begin{flushleft}
n$-$1
\end{flushleft}


\begin{flushleft}
k=0
\end{flushleft}





\begin{flushleft}
yk e$-$ik$\theta$ + y¯k eik$\theta$
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}





\begin{flushleft}
|ei$\theta$ $-$ zl$-$1 |2
\end{flushleft}





\begin{flushleft}
for some y. The last line follows by bringing all terms in the previous line on the
\end{flushleft}


\begin{flushleft}
same denominator. The absence of a term k = n in the numerator on the last line
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
requires some explanation. The coefficient of the term ein$\theta$ /
\end{flushleft}


\begin{flushleft}
y¯n
\end{flushleft}





=





=


=





1


2





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}





\begin{flushleft}
|ei$\theta$ $-$ zl$-$1 |2 is
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
xl z¯l$-$1
\end{flushleft}


\begin{flushleft}
k=l
\end{flushleft}





\begin{flushleft}
l=1
\end{flushleft}





($-$¯


\begin{flushleft}
zk$-$1 ) $-$ x
\end{flushleft}


\begin{flushleft}
¯l z¯l$-$1
\end{flushleft}





\begin{flushleft}
k=l
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
($-$1)n$-$1
\end{flushleft}


2





($-$¯


\begin{flushleft}
zk$-$1 )
\end{flushleft}





\begin{flushleft}
z¯k$-$1 )
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
(xl $-$ x
\end{flushleft}


\begin{flushleft}
¯l )
\end{flushleft}





0





\begin{flushleft}
because (1T x) = 0.
\end{flushleft}


\begin{flushleft}
Applying the Riesz-Fej´er theorem to the numerator in the last expression for R we
\end{flushleft}


\begin{flushleft}
get
\end{flushleft}


2


\begin{flushleft}
n$-$1
\end{flushleft}


\begin{flushleft}
a eik$\theta$
\end{flushleft}


\begin{flushleft}
k=0 k
\end{flushleft}


\begin{flushleft}
R($\theta$) =
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(ei$\theta$ $-$ zl$-$1 )
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}


\begin{flushleft}
for some set of coefficients ak , and hence
\end{flushleft}





2





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
R($\theta$) =
\end{flushleft}


\begin{flushleft}
l=1
\end{flushleft}





\begin{flushleft}
bl
\end{flushleft}


\begin{flushleft}
ei$\theta$ $-$ zl$-$1
\end{flushleft}





.





\begin{flushleft}
for some b $\in$ Cn . Therefore
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
R($\theta$)
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=





¯





\begin{flushleft}
(ei$\theta$
\end{flushleft}





\begin{flushleft}
l=1 k=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=





1


2





\begin{flushleft}
bk ¯bl
\end{flushleft}


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}





\begin{flushleft}
l=1 k=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





$-$





\begin{flushleft}
bk bl
\end{flushleft}


\begin{flushleft}
zk$-$1 )(e$-$i$\theta$
\end{flushleft}





=


\begin{flushleft}
l=1 k=1
\end{flushleft}





\begin{flushleft}
$-$ z¯l$-$1 )
\end{flushleft}


\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
ei$\theta$ + zk$-$1
\end{flushleft}


\begin{flushleft}
+ i$\theta$
\end{flushleft}


$-$1


\begin{flushleft}
$-$i$\theta$
\end{flushleft}


\begin{flushleft}
e
\end{flushleft}


\begin{flushleft}
$-$ z¯l
\end{flushleft}


\begin{flushleft}
e $-$ zk$-$1
\end{flushleft}





\begin{flushleft}
e$-$i$\theta$ + z¯l$-$1
\end{flushleft}


\begin{flushleft}
bk ¯bl
\end{flushleft}


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1 e$-$i$\theta$ $-$ z¯l$-$1
\end{flushleft}





.





\begin{flushleft}
Since the functions (e$-$i$\theta$ + z¯l$-$1 )/(e$-$i$\theta$ $-$ z¯l$-$1 ) are linearly independent, we conclude
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
bk ¯bl
\end{flushleft}


\begin{flushleft}
xl =
\end{flushleft}


,


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
H
\end{flushleft}





\begin{flushleft}
i.e., we can choose Q = bb .
\end{flushleft}


\begin{flushleft}
(d) Show that Knp = \{y $\in$ Cn | P (y)
\end{flushleft}


\begin{flushleft}
P (y)kl =
\end{flushleft}





\begin{flushleft}
0\} where P (y) $\in$ Hn is defined as
\end{flushleft}





\begin{flushleft}
yk + y l
\end{flushleft}


,


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}





\begin{flushleft}
l, k = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
The matrix P (y) is called the Nevanlinna-Pick matrix associated with the points
\end{flushleft}


\begin{flushleft}
zk , yk .
\end{flushleft}


∗∗


\begin{flushleft}
Hint. As we noted in part (a), Knp is a closed convex cone, so Knp = Knp
\end{flushleft}


.


∗∗


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
Solution. From the result in (c), x $\in$ Knp if and only if for all Q $\in$ H+ ,
\end{flushleft}


\begin{flushleft}
(xH y)
\end{flushleft}





=





\begin{flushleft}
1 H
\end{flushleft}


\begin{flushleft}
(x y + y H x)
\end{flushleft}


2





=





1


2





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
yl
\end{flushleft}


\begin{flushleft}
l=1 k=1
\end{flushleft}





\begin{flushleft}
Qlk
\end{flushleft}


\begin{flushleft}
Qkl
\end{flushleft}


\begin{flushleft}
+ y¯l
\end{flushleft}


\begin{flushleft}
1 $-$ z¯k$-$1 zl$-$1
\end{flushleft}


\begin{flushleft}
1 $-$ zk$-$1 z¯l$-$1
\end{flushleft}





\newpage
6


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=





1


2





=


$\geq$





\begin{flushleft}
tr(QP (y))
\end{flushleft}


0.





\begin{flushleft}
Qlk
\end{flushleft}


\begin{flushleft}
l=1 k=1
\end{flushleft}





\begin{flushleft}
In other words, if and only if P (y)
\end{flushleft}





\begin{flushleft}
Approximation and fitting
\end{flushleft}





\begin{flushleft}
yl + y¯k
\end{flushleft}


\begin{flushleft}
1 $-$ z¯k$-$1 zl$-$1
\end{flushleft}





0.





\begin{flushleft}
(e) As an application, pose the following problem as a convex optimization problem:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
|f (zk ) $-$ wk |2
\end{flushleft}


\begin{flushleft}
f $\in$ F.
\end{flushleft}





\begin{flushleft}
The problem data are n points zk with |zk | $>$ 1 and n complex numbers w1 , . . . ,
\end{flushleft}


\begin{flushleft}
wn . We optimize over all positive-real functions f .
\end{flushleft}


\begin{flushleft}
Solution. We can express this problem as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
|yk $-$ wk |2
\end{flushleft}


\begin{flushleft}
P (y) 0,
\end{flushleft}





\begin{flushleft}
where P (y) is the Nevanlinna-Pick matrix, and the variable is the (complex) vector
\end{flushleft}


\begin{flushleft}
y. Since P is linear in y, the constraint is a (complex) LMI, which can be expressed
\end{flushleft}


\begin{flushleft}
as a real LMI in the real and imaginary parts of y, following exercise 4.42. The
\end{flushleft}


\begin{flushleft}
objective is (convex) quadratic.
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 7
\end{flushleft}





\begin{flushleft}
Statistical estimation
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Estimation
\end{flushleft}


\begin{flushleft}
7.1 Linear measurements with exponentially distributed noise. Show how to solve the ML
\end{flushleft}


\begin{flushleft}
estimation problem (7.2) when the noise is exponentially distributed, with density
\end{flushleft}


\begin{flushleft}
(1/a)e$-$z/a
\end{flushleft}


0





\begin{flushleft}
p(z) =
\end{flushleft}


\begin{flushleft}
where a $>$ 0.
\end{flushleft}


\begin{flushleft}
Solution. Solve the LP
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
z$\geq$0
\end{flushleft}


\begin{flushleft}
z $<$ 0,
\end{flushleft}





\begin{flushleft}
1T (y $-$ Ax)
\end{flushleft}


\begin{flushleft}
Ax y.
\end{flushleft}





\begin{flushleft}
7.2 ML estimation and $\infty$ -norm approximation. We consider the linear measurement model
\end{flushleft}


\begin{flushleft}
y = Ax + v of page 352, with a uniform noise distribution of the form
\end{flushleft}


\begin{flushleft}
|z| $\leq$ $\alpha$
\end{flushleft}


\begin{flushleft}
|z| $>$ $\alpha$.
\end{flushleft}





\begin{flushleft}
1/(2$\alpha$)
\end{flushleft}


0





\begin{flushleft}
p(z) =
\end{flushleft}





\begin{flushleft}
As mentioned in example 7.1, page 352, any x that satisfies Ax $-$ y $\infty$ $\leq$ $\alpha$ is a ML
\end{flushleft}


\begin{flushleft}
estimate.
\end{flushleft}


\begin{flushleft}
Now assume that the parameter $\alpha$ is not known, and we wish to estimate $\alpha$, along with
\end{flushleft}


\begin{flushleft}
the parameters x. Show that the ML estimates of x and $\alpha$ are found by solving the
\end{flushleft}


\begin{flushleft}
$\infty$ -norm approximation problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
Ax $-$ y
\end{flushleft}





$\infty$,





\begin{flushleft}
where aTi are the rows of A.
\end{flushleft}


\begin{flushleft}
Solution. The log-likelihood function is
\end{flushleft}


\begin{flushleft}
Ax $-$ y $\infty$ $\leq$ $\alpha$
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
m log(1/2$\alpha$)
\end{flushleft}


$-$$\infty$





\begin{flushleft}
l(x, $\alpha$) =
\end{flushleft}





\begin{flushleft}
Maximizing over $\alpha$ and y is equivalent to solving the $\infty$ -norm problem.
\end{flushleft}


\begin{flushleft}
7.3 Probit model. Suppose y $\in$ \{0, 1\} is random variable given by
\end{flushleft}


\begin{flushleft}
y=
\end{flushleft}





1


0





\begin{flushleft}
aT u + b + v $\leq$ 0
\end{flushleft}


\begin{flushleft}
aT u + b + v $>$ 0,
\end{flushleft}





\begin{flushleft}
where the vector u $\in$ Rn is a vector of explanatory variables (as in the logistic model
\end{flushleft}


\begin{flushleft}
described on page 354), and v is a zero mean unit variance Gaussian variable.
\end{flushleft}


\begin{flushleft}
Formulate the ML estimation problem of estimating a and b, given data consisting of
\end{flushleft}


\begin{flushleft}
pairs (ui , yi ), i = 1, . . . , N , as a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. We have
\end{flushleft}


\begin{flushleft}
prob(y = 1) = Q(aT u + b),
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
prob(y = 0) = 1 $-$ Q(aT u + b) = P ($-$aT u $-$ b)
\end{flushleft}





1


\begin{flushleft}
Q(z) = $\surd$
\end{flushleft}


\begin{flushleft}
2$\pi$
\end{flushleft}





$\infty$





\begin{flushleft}
et
\end{flushleft}





2





/2





\begin{flushleft}
dt.
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
The log-likelihood function is
\end{flushleft}


\begin{flushleft}
log Q(aT ui + b) +
\end{flushleft}





\begin{flushleft}
l(a, b) =
\end{flushleft}


\begin{flushleft}
yi =1
\end{flushleft}





\begin{flushleft}
which is a concave function of a and b.
\end{flushleft}





\begin{flushleft}
yi =0
\end{flushleft}





\begin{flushleft}
log Q($-$aT ui $-$ b),
\end{flushleft}





\newpage
7





\begin{flushleft}
Statistical estimation
\end{flushleft}





\begin{flushleft}
7.4 Estimation of covariance and mean of a multivariate normal distribution. We consider the
\end{flushleft}


\begin{flushleft}
problem of estimating the covariance matrix R and the mean a of a Gaussian probability
\end{flushleft}


\begin{flushleft}
density function
\end{flushleft}


\begin{flushleft}
pR,a (y) = (2$\pi$)$-$n/2 det(R)$-$1/2 exp($-$(y $-$ a)T R$-$1 (y $-$ a)/2),
\end{flushleft}





\begin{flushleft}
based on N independent samples y1 , y2 , . . . , yN $\in$ Rn .
\end{flushleft}





\begin{flushleft}
(a) We first consider the estimation problem when there are no additional constraints
\end{flushleft}


\begin{flushleft}
on R and a. Let $\mu$ and Y be the sample mean and covariance, defined as
\end{flushleft}


\begin{flushleft}
$\mu$=
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





1


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
yk ,
\end{flushleft}





\begin{flushleft}
Y =
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





1


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
(yk $-$ $\mu$)(yk $-$ $\mu$)T .
\end{flushleft}





\begin{flushleft}
Show that the log-likelihood function
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
l(R, a) = $-$(N n/2) log(2$\pi$) $-$ (N/2) log det R $-$ (1/2)
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
(yk $-$ a)T R$-$1 (yk $-$ a)
\end{flushleft}





\begin{flushleft}
can be expressed as
\end{flushleft}


\begin{flushleft}
l(R, a) =
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
$-$n log(2$\pi$) $-$ log det R $-$ tr(R$-$1 Y ) $-$ (a $-$ $\mu$)T R$-$1 (a $-$ $\mu$) .
\end{flushleft}


2





\begin{flushleft}
Use this expression to show that if Y
\end{flushleft}


\begin{flushleft}
0, the ML estimates of R and a are unique,
\end{flushleft}


\begin{flushleft}
and given by
\end{flushleft}


\begin{flushleft}
aml = $\mu$,
\end{flushleft}


\begin{flushleft}
Rml = Y.
\end{flushleft}


\begin{flushleft}
(b) The log-likelihood function includes a convex term ($-$ log det R), so it is not obviously concave. Show that l is concave, jointly in R and a, in the region defined
\end{flushleft}


\begin{flushleft}
by
\end{flushleft}


\begin{flushleft}
R 2Y.
\end{flushleft}


\begin{flushleft}
This means we can use convex optimization to compute simultaneous ML estimates
\end{flushleft}


\begin{flushleft}
of R and a, subject to convex constraints, as long as the constraints include R 2Y ,
\end{flushleft}


\begin{flushleft}
i.e., the estimate R must not exceed twice the unconstrained ML estimate.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We show that
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
(y
\end{flushleft}


\begin{flushleft}
k=1 k
\end{flushleft}





\begin{flushleft}
$-$ a)(yk $-$ a)T = N (Y $-$ (a $-$ $\mu$)(a $-$ $\mu$)T ):
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
(yk $-$ a)(yk $-$ a)
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





=


\begin{flushleft}
k=1
\end{flushleft}





\begin{flushleft}
yk ykT $-$ N a$\mu$T $-$ N $\mu$aT + N aaT
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





=


\begin{flushleft}
k=1
\end{flushleft}





=





\begin{flushleft}
(yk $-$ $\mu$)(yk $-$ $\mu$)T + N $\mu$$\mu$T $-$ N a$\mu$T $-$ N $\mu$aT + N aaT
\end{flushleft}





\begin{flushleft}
N Y + N (a $-$ $\mu$)(a $-$ $\mu$T ).
\end{flushleft}





\begin{flushleft}
This proves that the two expressions for l are equivalent.
\end{flushleft}


\begin{flushleft}
Now let's maximize l. It is not (in general) a concave function of R, so we have to
\end{flushleft}


\begin{flushleft}
be careful. We do know that at the global optimizer, the gradient vanishes (but not
\end{flushleft}


\begin{flushleft}
conversely). Setting the gradient with respect to R and $\mu$ to zero gives
\end{flushleft}


\begin{flushleft}
$-$R$-$1 + R$-$1 (Y + (a $-$ $\mu$)(a $-$ $\mu$)T )R$-$1 = 0,
\end{flushleft}





\begin{flushleft}
$-$2R$-$1 (a $-$ $\mu$) = 0,
\end{flushleft}





\begin{flushleft}
which has only one solution,
\end{flushleft}


\begin{flushleft}
Y + (a $-$ $\mu$)(a $-$ $\mu$)T = R,
\end{flushleft}





\begin{flushleft}
a = $\mu$.
\end{flushleft}





\begin{flushleft}
It must be the global maximizer of l, since l is not unbounded above.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) We show that the function
\end{flushleft}


\begin{flushleft}
f (R) = $-$ log det R $-$ tr(R$-$1 Y )
\end{flushleft}


\begin{flushleft}
is concave in R for 0 ≺ R 2Y . This will establish concavity of the log-likelihood
\end{flushleft}


\begin{flushleft}
function because the remaining term of l is concave in a and R.
\end{flushleft}


\begin{flushleft}
The gradient and Hessian of f are given by
\end{flushleft}


\begin{flushleft}
$\nabla$f (R)
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (R)[V ]
\end{flushleft}





\begin{flushleft}
$-$R$-$1 + R$-$1 Y R$-$1
\end{flushleft}





=





\begin{flushleft}
R$-$1 V R$-$1 $-$ R$-$1 V R$-$1 Y R$-$1 $-$ R$-$1 Y R$-$1 V R$-$1
\end{flushleft}





=





\begin{flushleft}
where by $\nabla$2 f (R)[V ] we mean
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (R)[V ] =
\end{flushleft}





\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
$\nabla$f (R + tV )
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
We show that
\end{flushleft}


\begin{flushleft}
tr(V $\nabla$2 f (R)[V ]) =
\end{flushleft}





.


\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
d2
\end{flushleft}


\begin{flushleft}
f (R + tV )
\end{flushleft}


\begin{flushleft}
dt2
\end{flushleft}





\begin{flushleft}
t=0
\end{flushleft}





$\leq$0





\begin{flushleft}
for all V . We have
\end{flushleft}


\begin{flushleft}
tr(V $\nabla$2 f (R)[V ])
\end{flushleft}





\begin{flushleft}
for all V if
\end{flushleft}


\begin{flushleft}
i.e., R
\end{flushleft}





=


=


$\leq$





\begin{flushleft}
tr(V R$-$1 V R$-$1 ) $-$ 2 tr(V R$-$1 V R$-$1 Y R$-$1 )
\end{flushleft}


\begin{flushleft}
tr (R$-$1/2 V R$-$1/2 )2 (I $-$ 2R$-$1/2 Y R$-$1/2 )
\end{flushleft}


0





\begin{flushleft}
2R$-$1/2 Y R$-$1/2
\end{flushleft}





\begin{flushleft}
I,
\end{flushleft}





\begin{flushleft}
2Y .
\end{flushleft}





\begin{flushleft}
7.5 Markov chain estimation. Consider a Markov chain with n states, and transition probability matrix P $\in$ Rn×n defined as
\end{flushleft}


\begin{flushleft}
Pij = prob(y(t + 1) = i | y(t) = j).
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
The transition probabilities must satisfy Pij $\geq$ 0 and
\end{flushleft}


\begin{flushleft}
P = 1, j = 1, . . . , n. We
\end{flushleft}


\begin{flushleft}
i=1 ij
\end{flushleft}


\begin{flushleft}
consider the problem of estimating the transition probabilities, given an observed sample
\end{flushleft}


\begin{flushleft}
sequence y(1) = k1 , y(2) = k2 , . . . , y(N ) = kn .
\end{flushleft}


\begin{flushleft}
(a) Show that if there are no other prior constraints on Pij , then the ML estimates are
\end{flushleft}


\begin{flushleft}
the empirical transition frequencies: Pˆij is the ratio of the number of times the state
\end{flushleft}


\begin{flushleft}
transitioned from j into i, divided by the number of times it was j, in the observed
\end{flushleft}


\begin{flushleft}
sample.
\end{flushleft}


\begin{flushleft}
(b) Suppose that an equilibrium distribution p of the Markov chain is known, i.e., a
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
vector q $\in$ Rn
\end{flushleft}


\begin{flushleft}
+ satisfying 1 q = 1 and P q = q. Show that the problem of computing
\end{flushleft}


\begin{flushleft}
the ML estimate of P , given the observed sequence and knowledge of q, can be
\end{flushleft}


\begin{flushleft}
expressed as a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The probability of the sequence y(2), . . . , y(N ), given that we start in y(1) is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
Pk2 ,k1 Pk3 ,k2 · · · Pkn ,kn$-$1 =
\end{flushleft}





\begin{flushleft}
c
\end{flushleft}





\begin{flushleft}
Pijij
\end{flushleft}


\begin{flushleft}
i,k=1
\end{flushleft}





\begin{flushleft}
where cij is the number of times the state transitioned from j to i. The ML estimation problem is therefore
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
cij log Pij
\end{flushleft}


\begin{flushleft}
1 T P = 1T .
\end{flushleft}





\newpage
7





\begin{flushleft}
Statistical estimation
\end{flushleft}





\begin{flushleft}
The problem is separable, and can be solved column by column. Let pj = (P1j , . . . , Pnj )
\end{flushleft}


\begin{flushleft}
be column j of P . It is the solution of
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





1





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
c log pij
\end{flushleft}


\begin{flushleft}
i=1 ij
\end{flushleft}


\begin{flushleft}
pj = 1.
\end{flushleft}





\begin{flushleft}
Using Lagrange multipliers we find that
\end{flushleft}


\begin{flushleft}
cij
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Pij =
\end{flushleft}





\begin{flushleft}
cij
\end{flushleft}





.





\begin{flushleft}
(b) The ML estimation problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
cij log Pij
\end{flushleft}


\begin{flushleft}
1 P = 1T
\end{flushleft}


\begin{flushleft}
P q = q.
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
7.6 Estimation of mean and variance. Consider a random variable x $\in$ R with density p,
\end{flushleft}


\begin{flushleft}
which is normalized, i.e., has zero mean and unit variance. Consider a random variable
\end{flushleft}


\begin{flushleft}
y = (x+b)/a obtained by an affine transformation of x, where a $>$ 0. The random variable
\end{flushleft}


\begin{flushleft}
y has mean b and variance 1/a2 . As a and b vary over R+ and R, respectively, we generate
\end{flushleft}


\begin{flushleft}
a family of densities obtained from p by scaling and shifting, uniquely parametrized by
\end{flushleft}


\begin{flushleft}
mean and variance.
\end{flushleft}


\begin{flushleft}
Show that if p is log-concave, then finding the ML estimate of a and b, given samples
\end{flushleft}


\begin{flushleft}
y1 , . . . , yn of y, is a convex problem.
\end{flushleft}


\begin{flushleft}
As an example, work out an analytical solution for the ML estimates of a and b, assuming
\end{flushleft}


\begin{flushleft}
p is a normalized Laplacian density, p(x) = e$-$2|x| .
\end{flushleft}


\begin{flushleft}
Solution. The density of y is given by
\end{flushleft}


\begin{flushleft}
py (u) = ap(au $-$ b).
\end{flushleft}


\begin{flushleft}
The log-likelihood function is given by
\end{flushleft}


\begin{flushleft}
log py (u) = log a + log p(au $-$ b).
\end{flushleft}


\begin{flushleft}
If p is log-concave, then this log-likelihood function is a concave function of a and b. This
\end{flushleft}


\begin{flushleft}
allows us to compute ML estimates of the mean and variance of a random variable with
\end{flushleft}


\begin{flushleft}
a normalized density that is log-concave.
\end{flushleft}


\begin{flushleft}
Suppose that n samples y1 , . . . , yn are drawn from the distribution of y, which has a
\end{flushleft}


\begin{flushleft}
log-concave normalized density. To find the ML estimate of the parameters a and b, we
\end{flushleft}


\begin{flushleft}
maximize the concave function
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
py (yi ) = n log a +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log p(ayi $-$ b).
\end{flushleft}





\begin{flushleft}
For the Laplace distribution, you get
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
py (yi ) = n log a $-$ 2
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|ayi $-$ b|,
\end{flushleft}





\begin{flushleft}
so the ML estimates solve
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$-$n log a + 2
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|ayi $-$ b|.
\end{flushleft}





\begin{flushleft}
We can define c = b/a, and solve
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
$-$n log a + 2a
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
|yi $-$ c|.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
The solution c is the median of yi . a can be found by setting the derivative equal to zero:
\end{flushleft}


\begin{flushleft}
a=
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


.


\begin{flushleft}
|yi $-$ c|
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





2





\begin{flushleft}
7.7 ML estimation of Poisson distributions. Suppose xi , i = 1, . . . , n, are independent random
\end{flushleft}


\begin{flushleft}
variables with Poisson distributions
\end{flushleft}


\begin{flushleft}
prob(xi = k) =
\end{flushleft}





\begin{flushleft}
e$-$$\mu$i $\mu$ki
\end{flushleft}


,


\begin{flushleft}
k!
\end{flushleft}





\begin{flushleft}
with unknown means $\mu$i . The variables xi represent the number of times that one of n
\end{flushleft}


\begin{flushleft}
possible independent events occurs during a certain period. In emission tomography, for
\end{flushleft}


\begin{flushleft}
example, they might represent the number of photons emitted by n sources.
\end{flushleft}


\begin{flushleft}
We consider an experiment designed to determine the means $\mu$i . The experiment involves
\end{flushleft}


\begin{flushleft}
m detectors. If event i occurs, it is detected by detector j with probability pji . We assume
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
the probabilities pji are given (with pji $\geq$ 0,
\end{flushleft}


\begin{flushleft}
p $\leq$ 1). The total number of events
\end{flushleft}


\begin{flushleft}
j=1 ji
\end{flushleft}


\begin{flushleft}
recorded by detector j is denoted yj ,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
yji ,
\end{flushleft}





\begin{flushleft}
yj =
\end{flushleft}





\begin{flushleft}
j = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Formulate the ML estimation problem of estimating the means $\mu$i , based on observed
\end{flushleft}


\begin{flushleft}
values of yj , j = 1, . . . , m, as a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Hint. The variables yji have Poisson distributions with means pji $\mu$i , i.e.,
\end{flushleft}


\begin{flushleft}
prob(yji = k) =
\end{flushleft}





\begin{flushleft}
e$-$pji $\mu$i (pji $\mu$i )k
\end{flushleft}


.


\begin{flushleft}
k!
\end{flushleft}





\begin{flushleft}
The sum of n independent Poisson variables with means $\lambda$1 , . . . , $\lambda$n has a Poisson distribution with mean $\lambda$1 + · · · + $\lambda$n .
\end{flushleft}


\begin{flushleft}
Solution. It follows from the two hints that yj has a Poisson distribution with mean
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
pji $\mu$i = pTj $\mu$.
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Therefore,
\end{flushleft}


\begin{flushleft}
log(prob(yj = k)) = $-$pTj $\mu$ + k log(pTj $\mu$) $-$ log k!.
\end{flushleft}





\begin{flushleft}
Suppose the observed values of yj are kj , j = 1, . . . , n. Then the ML estimation problem
\end{flushleft}


\begin{flushleft}
is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
maximize $-$ j=1 pTj $\mu$ + j=1 kj log(pTj $\mu$)
\end{flushleft}


\begin{flushleft}
subject to $\mu$ 0,
\end{flushleft}


\begin{flushleft}
which is convex in $\mu$.
\end{flushleft}


\begin{flushleft}
For completeness we also prove the two hints. Suppose x is a Poisson random variable
\end{flushleft}


\begin{flushleft}
with mean $\mu$ (number of times that an event occurs). It is well known that the Poisson
\end{flushleft}


\begin{flushleft}
distribution is the limit of a binomial distribution
\end{flushleft}


\begin{flushleft}
prob(x = k) =
\end{flushleft}





\begin{flushleft}
e$-$$\mu$ $\mu$k
\end{flushleft}


=


\begin{flushleft}
lim
\end{flushleft}


\begin{flushleft}
n$\rightarrow$$\infty$, nq$\rightarrow$$\mu$
\end{flushleft}


\begin{flushleft}
k!
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
q k (1 $-$ q)n$-$k ,
\end{flushleft}





\begin{flushleft}
i.e., we can think of x is the total number of positives in n Bernoulli trials with q = $\mu$/n.
\end{flushleft}





\newpage
7





\begin{flushleft}
Statistical estimation
\end{flushleft}





\begin{flushleft}
Now suppose y is the total number of positives that is detected, where the probability of
\end{flushleft}


\begin{flushleft}
detection is p. In the binomial formula, we simply replace q with pq, and in the limit
\end{flushleft}


\begin{flushleft}
prob(y = k)
\end{flushleft}





=


=


=





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
lim
\end{flushleft}





\begin{flushleft}
n$\rightarrow$$\infty$, nq$\rightarrow$$\mu$
\end{flushleft}





\begin{flushleft}
(pq)k (1 $-$ (pq))n$-$k
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
lim
\end{flushleft}





\begin{flushleft}
n$\rightarrow$$\infty$, nq$\rightarrow$p$\mu$
\end{flushleft}





\begin{flushleft}
q k (1 $-$ q)n$-$k
\end{flushleft}





\begin{flushleft}
e$-$p$\mu$ (p$\mu$)k
\end{flushleft}


.


\begin{flushleft}
k!
\end{flushleft}





\begin{flushleft}
Assume x and y are independent Poisson variables with means $\mu$ and $\lambda$. Then
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
prob(x + y = k)
\end{flushleft}





=


\begin{flushleft}
i=0
\end{flushleft}





\begin{flushleft}
prob(x = i) prob(y = k $-$ i)
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





=





\begin{flushleft}
e$-$$\mu$$-$$\lambda$
\end{flushleft}


\begin{flushleft}
i=0
\end{flushleft}





=


=





\begin{flushleft}
e
\end{flushleft}


\begin{flushleft}
e
\end{flushleft}





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
$-$$\mu$$-$$\lambda$
\end{flushleft}





\begin{flushleft}
k!
\end{flushleft}





\begin{flushleft}
k!
\end{flushleft}


\begin{flushleft}
$\mu$i $\lambda$k$-$i
\end{flushleft}


\begin{flushleft}
i!(k $-$ i)!
\end{flushleft}





\begin{flushleft}
i=0
\end{flushleft}





\begin{flushleft}
$-$$\mu$$-$$\lambda$
\end{flushleft}





\begin{flushleft}
k!
\end{flushleft}





\begin{flushleft}
$\mu$i $\lambda$k$-$i
\end{flushleft}


\begin{flushleft}
i!(k $-$ i)!
\end{flushleft}





\begin{flushleft}
($\lambda$ + $\mu$)k .
\end{flushleft}





\begin{flushleft}
7.8 Estimation using sign measurements. We consider the measurement setup
\end{flushleft}


\begin{flushleft}
yi = sign(aTi x + bi + vi ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
where x $\in$ R is the vector to be estimated, and yi $\in$ \{$-$1, 1\} are the measurements. The
\end{flushleft}


\begin{flushleft}
vectors ai $\in$ Rn and scalars bi $\in$ R are known, and vi are IID noises with a log-concave
\end{flushleft}


\begin{flushleft}
probability density. (You can assume that aTi x + bi + vi = 0 does not occur.) Show that
\end{flushleft}


\begin{flushleft}
maximum likelihood estimation of x is a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. We re-order the observations so that yi = 1 for i = 1, . . . , k and yi = 0 for
\end{flushleft}


\begin{flushleft}
i = k + 1, . . . , m. The probability of this event is
\end{flushleft}





=





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
prob(aTi x + bi + vi $>$ 0) · i=k+1 prob(aTi x +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
F ($-$ai x $-$ bi ) · i=k+1 (1 $-$ F ($-$aTi x $-$ bi )),
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
bi + vi $<$ 0)
\end{flushleft}





\begin{flushleft}
where F is the cumulative distribution of the noise density. The integral of a log-concave
\end{flushleft}


\begin{flushleft}
function is log-concave, so F is log-concave, and so is 1 $-$ F . The log-likelihood function
\end{flushleft}


\begin{flushleft}
is
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
l(x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
log F ($-$aTi x $-$ bi ) +
\end{flushleft}





\begin{flushleft}
i=k+1
\end{flushleft}





\begin{flushleft}
log(1 $-$ F ($-$aTi x $-$ bi )),
\end{flushleft}





\begin{flushleft}
which is concave. Therefore, maximizing it is a convex problem.
\end{flushleft}


\begin{flushleft}
7.9 Estimation with unknown sensor nonlinearity. We consider the measurement setup
\end{flushleft}


\begin{flushleft}
yi = f (aTi x + bi + vi ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
where x $\in$ Rn is the vector to be estimated, yi $\in$ R are the measurements, ai $\in$ Rn ,
\end{flushleft}


\begin{flushleft}
bi $\in$ R are known, and vi are IID noises with log-concave probability density. The function
\end{flushleft}


\begin{flushleft}
f : R $\rightarrow$ R, which represents a measurement nonlinearity, is not known. However, it is
\end{flushleft}


\begin{flushleft}
known that f (t) $\in$ [l, u] for all t, where 0 $<$ l $<$ u are given.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Explain how to use convex optimization to find a maximum likelihood estimate of x, as
\end{flushleft}


\begin{flushleft}
well as the function f . (This is an infinite-dimensional ML estimation problem, but you
\end{flushleft}


\begin{flushleft}
can be informal in your approach and explanation.)
\end{flushleft}


\begin{flushleft}
Solution. For fixed function f and vector x, we observe y1 , . . . , ym if and only if
\end{flushleft}


\begin{flushleft}
f $-$1 (yi ) $-$ aTi x $-$ bi = vi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
(Note that the assumption 0 $<$ l $<$ u implies f is invertible.) It follows that the probability
\end{flushleft}


\begin{flushleft}
of observing y1 , . . . , ym is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
pv f $-$1 (yi ) $-$ aTi x $-$ bi .
\end{flushleft}





\begin{flushleft}
The log of this expression, regarded as a function of x and the function f , is the loglikelihood function:
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
l(x, f ) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log pv zi $-$ aTi x $-$ bi ,
\end{flushleft}





$-$1





\begin{flushleft}
where zi = f (yi ). This is a concave function of z and x.
\end{flushleft}


\begin{flushleft}
The function f only affects the log-likelihood function through the numbers zi . The
\end{flushleft}


\begin{flushleft}
constraints can be expressed in terms of the inverse as
\end{flushleft}


\begin{flushleft}
(d/dt)f $-$1 (t) $\in$ [1/u, 1/l],
\end{flushleft}


\begin{flushleft}
so we conclude that
\end{flushleft}


\begin{flushleft}
(1/u)|yi $-$ yj | $\leq$ |zi $-$ zj | $\leq$ (1/l)|yi $-$ yj |,
\end{flushleft}


\begin{flushleft}
for all i, j. Conversely, if these inequalities hold, then there is a function f that satisfies
\end{flushleft}


\begin{flushleft}
the inequality, with f $-$1 (yi ) = zi . (Actually, this is true only in the limit, but we're being
\end{flushleft}


\begin{flushleft}
informal here.)
\end{flushleft}


\begin{flushleft}
Therefore, to find the ML estimate, we maximize the concave function of x and z above,
\end{flushleft}


\begin{flushleft}
subject to the linear inequalities on z.
\end{flushleft}


\begin{flushleft}
7.10 Nonparametric distributions on Rk . We consider a random variable x $\in$ Rk with values
\end{flushleft}


\begin{flushleft}
in a finite set \{$\alpha$1 , . . . , $\alpha$n \}, and with distribution
\end{flushleft}


\begin{flushleft}
pi = prob(x = $\alpha$i ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
Show that a lower bound on the covariance of X,
\end{flushleft}


\begin{flushleft}
S
\end{flushleft}





\begin{flushleft}
E(X $-$ E X)(X $-$ E X)T ,
\end{flushleft}





\begin{flushleft}
is a convex constraint in p.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
E(X $-$ E X)(X $-$ E X)T =
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
p i $\alpha$i
\end{flushleft}





\begin{flushleft}
p i $\alpha$i
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
if and only if
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
p $\alpha$ $\alpha$T $-$
\end{flushleft}


\begin{flushleft}
i=1 i i i
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
( i=1 pi $\alpha$i )T
\end{flushleft}





\begin{flushleft}
S
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





1





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
pi $\alpha$i $\alpha$iT $-$
\end{flushleft}





\begin{flushleft}
p i $\alpha$i
\end{flushleft}





0.





\begin{flushleft}
S
\end{flushleft}





\newpage
7





\begin{flushleft}
Statistical estimation
\end{flushleft}





\begin{flushleft}
Optimal detector design
\end{flushleft}


\begin{flushleft}
7.11 Randomized detectors. Show that every randomized detector can be expressed as a convex
\end{flushleft}


\begin{flushleft}
combination of a set of deterministic detectors: If
\end{flushleft}


\begin{flushleft}
t1
\end{flushleft}





\begin{flushleft}
T =
\end{flushleft}





···





\begin{flushleft}
t2
\end{flushleft}





\begin{flushleft}
$\in$ Rm×n
\end{flushleft}





\begin{flushleft}
tn
\end{flushleft}





\begin{flushleft}
0 and 1T tk = 1, then T can be expressed as
\end{flushleft}





\begin{flushleft}
satisfies tk
\end{flushleft}





\begin{flushleft}
T = $\theta$ 1 T1 + · · · + $\theta$ N TN ,
\end{flushleft}


\begin{flushleft}
where Ti is a zero-one matrix with exactly one element equal to one per column, and
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
$\theta$i $\geq$ 0, i=1 $\theta$i = 1. What is the maximum number of deterministic detectors N we may
\end{flushleft}


\begin{flushleft}
need?
\end{flushleft}


\begin{flushleft}
We can interpret this convex decomposition as follows. The randomized detector can be
\end{flushleft}


\begin{flushleft}
realized as a bank of N deterministic detectors. When we observe X = k, the estimator
\end{flushleft}


\begin{flushleft}
chooses a random index from the set \{1, . . . , N \}, with probability prob(j = i) = $\theta$i , and
\end{flushleft}


\begin{flushleft}
then uses deterministic detector Tj .
\end{flushleft}


\begin{flushleft}
Solution. The detector T can be expressed as a convex combination of deterministic
\end{flushleft}


\begin{flushleft}
detectors as follows:
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
T =
\end{flushleft}


\begin{flushleft}
i1 =1 i2 =1
\end{flushleft}





···





\begin{flushleft}
$\theta$i1 ,i2 ,...,im
\end{flushleft}





\begin{flushleft}
e i2
\end{flushleft}





\begin{flushleft}
e i1
\end{flushleft}





\begin{flushleft}
in =1
\end{flushleft}





···





\begin{flushleft}
e in
\end{flushleft}





.





\begin{flushleft}
e i2
\end{flushleft}





···





\begin{flushleft}
e in
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
$\theta$i1 ,i2 ,...,im = ti1 ,1 ti2 ,2 · · · tin ,n .
\end{flushleft}





\begin{flushleft}
To see this, note that
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i1 =1 i2 =1
\end{flushleft}





···





\begin{flushleft}
$\theta$i1 ,i2 ,...,im
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





=


\begin{flushleft}
in =1
\end{flushleft}





···





\begin{flushleft}
in =1
\end{flushleft}





\begin{flushleft}
in =1
\end{flushleft}





\begin{flushleft}
i2 =1
\end{flushleft}





\begin{flushleft}
in =1
\end{flushleft}





\begin{flushleft}
(tin ,n · · · ti2 ,2 )
\end{flushleft}





\begin{flushleft}
ti1 ,1
\end{flushleft}





\begin{flushleft}
e i1
\end{flushleft}





\begin{flushleft}
i1 =1
\end{flushleft}





\begin{flushleft}
(tin ,n · · · ti2 ,2 )
\end{flushleft}





\begin{flushleft}
t1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





···





\begin{flushleft}
(tin ,n · · · ti3 ,3 )
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





···





\begin{flushleft}
e i2
\end{flushleft}





···





\begin{flushleft}
e in
\end{flushleft}





\begin{flushleft}
ti2 ,2
\end{flushleft}





\begin{flushleft}
t1
\end{flushleft}





\begin{flushleft}
e i2
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i3 =1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
e in
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





···





\begin{flushleft}
m
\end{flushleft}





=





···





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i2 =1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
e i2
\end{flushleft}





\begin{flushleft}
e i1
\end{flushleft}





\begin{flushleft}
in =1
\end{flushleft}





\begin{flushleft}
i2 =1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i3 =1
\end{flushleft}





\begin{flushleft}
(tin ,n · · · ti3 ,3 )
\end{flushleft}





\begin{flushleft}
t1
\end{flushleft}





\begin{flushleft}
t2
\end{flushleft}





\begin{flushleft}
i2 =1
\end{flushleft}





..


.


\begin{flushleft}
m
\end{flushleft}





=





\begin{flushleft}
tin ,n
\end{flushleft}





\begin{flushleft}
t1
\end{flushleft}





\begin{flushleft}
t2
\end{flushleft}





···





\begin{flushleft}
t2
\end{flushleft}





\begin{flushleft}
in =1
\end{flushleft}





=





\begin{flushleft}
t1
\end{flushleft}





···





\begin{flushleft}
tn$-$1
\end{flushleft}





···





\begin{flushleft}
tn$-$1
\end{flushleft}





\begin{flushleft}
tn
\end{flushleft}





\begin{flushleft}
e in
\end{flushleft}





.





\begin{flushleft}
It is also clear that
\end{flushleft}


\begin{flushleft}
$\theta$i1 ,i2 ,...,im = 1.
\end{flushleft}


\begin{flushleft}
i1 ,i2 ,...,im
\end{flushleft}





···





\begin{flushleft}
e in
\end{flushleft}





\begin{flushleft}
e in
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
The following general argument (familiar from linear programming) shows that every detector can be expressed as a convex combination of no more than n(m$-$1)+1 deterministic
\end{flushleft}


\begin{flushleft}
detectors.
\end{flushleft}


\begin{flushleft}
Suppose v1 , . . . , vN are affinely dependent points in Rp , which means that
\end{flushleft}


\begin{flushleft}
rank
\end{flushleft}





\begin{flushleft}
v1
\end{flushleft}


1





\begin{flushleft}
v2
\end{flushleft}


1





···


···





\begin{flushleft}
vN
\end{flushleft}


1





\begin{flushleft}
$<$ N,
\end{flushleft}





\begin{flushleft}
and suppose x is a strict convex combination of the points vk :
\end{flushleft}


\begin{flushleft}
x = $\theta$ 1 v1 + · · · + $\theta$ N vN ,
\end{flushleft}





\begin{flushleft}
1 = $\theta$1 + · · · + $\theta$N ,
\end{flushleft}





\begin{flushleft}
$\theta$
\end{flushleft}





0,





\begin{flushleft}
Then x is a convex combination of a subset of the points vi . To see this note that the
\end{flushleft}


\begin{flushleft}
rank condition implies that there exists a $\lambda$ = 0 such that
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
$\lambda$i = 0.
\end{flushleft}





\begin{flushleft}
$\lambda$i vi = 0,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Therefore,
\end{flushleft}


\begin{flushleft}
x = ($\theta$1 + t$\lambda$1 )v1 + · · · + ($\theta$N + t$\lambda$N )vN ,
\end{flushleft}





\begin{flushleft}
1 = ($\theta$1 + t$\lambda$1 )v1 + · · · + ($\theta$N + t$\lambda$N )vN ,
\end{flushleft}





\begin{flushleft}
for all t. Since $\lambda$ has at least one negative component and $\theta$
\end{flushleft}


\begin{flushleft}
tmax = sup\{t | $\theta$ + t$\lambda$
\end{flushleft}





\begin{flushleft}
0, the number
\end{flushleft}





0\}





\begin{flushleft}
is finite and positive. Define $\theta$ˆ = $\theta$ + tmax $\lambda$. We have
\end{flushleft}


\begin{flushleft}
x = $\theta$ˆ1 v1 + · · · + $\theta$ˆN vN ,
\end{flushleft}





\begin{flushleft}
1 = $\theta$ˆ1 + · · · + $\theta$ˆN ,
\end{flushleft}





\begin{flushleft}
$\theta$ˆ
\end{flushleft}





0,





\begin{flushleft}
and at least one of the coefficients of $\theta$ is zero. We have expressed x as strict convex
\end{flushleft}


\begin{flushleft}
combination of a subset of the vectors vi . Repeating this argument, we can express x as
\end{flushleft}


\begin{flushleft}
a strict convex combination of an affinely independent subset of \{v1 , . . . , vN \}.
\end{flushleft}


\begin{flushleft}
Applied to the detector problem, this means that every randomized detector can be
\end{flushleft}


\begin{flushleft}
expressed as a convex combination of affinely independent deterministic detectors. Since
\end{flushleft}


\begin{flushleft}
the affine hull of the set of all detectors has dimension n(m $-$ 1), it is impossible to find
\end{flushleft}


\begin{flushleft}
more than n(m $-$ 1) + 1 affinely independent deterministic detectors.
\end{flushleft}





\begin{flushleft}
7.12 Optimal action. In detector design, we are given a matrix P $\in$ Rn×m (whose columns
\end{flushleft}


\begin{flushleft}
are probability distributions), and then design a matrix T $\in$ Rm×n (whose columns are
\end{flushleft}


\begin{flushleft}
probability distributions), so that D = T P has large diagonal elements (and small offdiagonal elements). In this problem we study the dual problem: Given P , find a matrix
\end{flushleft}


\begin{flushleft}
˜ = P S $\in$ Rn×n has
\end{flushleft}


\begin{flushleft}
S $\in$ Rm×n (whose columns are probability distributions), so that D
\end{flushleft}


\begin{flushleft}
large diagonal elements (and small off-diagonal elements). To make the problem specific,
\end{flushleft}


\begin{flushleft}
˜ on the diagonal.
\end{flushleft}


\begin{flushleft}
we take the objective to be maximizing the minimum element of D
\end{flushleft}


\begin{flushleft}
We can interpret this problem as follows. There are n outcomes, which depend (stochastically) on which of m inputs or actions we take: Pij is the probability that outcome i
\end{flushleft}


\begin{flushleft}
occurs, given action j. Our goal is find a (randomized) strategy that, to the extent possible, causes any specified outcome to occur. The strategy is given by the matrix S: S ji
\end{flushleft}


\begin{flushleft}
is the probability that we take action j, when we want outcome i to occur. The matrix
\end{flushleft}


\begin{flushleft}
˜ gives the action error probability matrix: D
\end{flushleft}


\begin{flushleft}
˜ ij is the probability that outcome i occurs,
\end{flushleft}


\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
˜ ii is the probability that outcome i
\end{flushleft}


\begin{flushleft}
when we want outcome j to occur. In particular, D
\end{flushleft}


\begin{flushleft}
occurs, when we want it to occur.
\end{flushleft}


\begin{flushleft}
Show that this problem has a simple analytical solution. Show that (unlike the corresponding detector problem) there is always an optimal solution that is deterministic.
\end{flushleft}


\begin{flushleft}
Hint. Show that the problem is separable in the columns of S.
\end{flushleft}





\newpage
7





\begin{flushleft}
Statistical estimation
\end{flushleft}





\begin{flushleft}
Solution. Let p˜Tk be kth row of P . The problem is then
\end{flushleft}


\begin{flushleft}
mink p˜Tk sk
\end{flushleft}


\begin{flushleft}
sk 0, k = 1, . . . , m
\end{flushleft}


\begin{flushleft}
1T sk = 1, k = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
This problem is separable (when put in epigraph form): we can just as well choose each
\end{flushleft}


\begin{flushleft}
sk to maximize p˜Tk sk subject to sk 0, 1T sk = 1. But this is easy: we choose an index l
\end{flushleft}


\begin{flushleft}
of p˜k which has maximum entry, and take sk = el .
\end{flushleft}


\begin{flushleft}
In other words, the optimal strategy is very simple: when the outcome i is desired, simply
\end{flushleft}


\begin{flushleft}
choose (deterministically) an input that maximizes the probability of the outcome k.
\end{flushleft}





\begin{flushleft}
Chebyshev and Chernoff bounds
\end{flushleft}


\begin{flushleft}
7.13 Chebyshev-type inequalities on a finite set. Assume X is a random variable taking values
\end{flushleft}


\begin{flushleft}
in the set \{$\alpha$1 , $\alpha$2 , . . . , $\alpha$m \}, and let S be a subset of \{$\alpha$1 , . . . , $\alpha$m \}. The distribution of X
\end{flushleft}


\begin{flushleft}
is unknown, but we are given the expected values of n functions fi :
\end{flushleft}


\begin{flushleft}
E fi (X) = bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





(7.32)





\begin{flushleft}
Show that the optimal value of the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x0 +
\end{flushleft}


\begin{flushleft}
x0 +
\end{flushleft}


\begin{flushleft}
x0 +
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
b i xi
\end{flushleft}


\begin{flushleft}
fi ($\alpha$)xi $\geq$ 1,
\end{flushleft}


\begin{flushleft}
fi ($\alpha$)xi $\geq$ 0,
\end{flushleft}





\begin{flushleft}
$\alpha$$\in$S
\end{flushleft}


\begin{flushleft}
$\alpha$ $\in$ S,
\end{flushleft}





\begin{flushleft}
with variables x0 , . . . , xn , is an upper bound on prob(X $\in$ S), valid for all distributions
\end{flushleft}


\begin{flushleft}
that satisfy (7.32). Show that there always exists a distribution that achieves the upper
\end{flushleft}


\begin{flushleft}
bound.
\end{flushleft}


\begin{flushleft}
Solution. The best upper bound on prob(x $\in$ S) is the optimal value of
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
p $\alpha$
\end{flushleft}


\begin{flushleft}
$\alpha$$\in$S k
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
pk = 1
\end{flushleft}


\begin{flushleft}
k=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
p f ($\alpha$k )
\end{flushleft}


\begin{flushleft}
k=1 k i
\end{flushleft}





\begin{flushleft}
= bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n
\end{flushleft}





0.





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x0 +
\end{flushleft}


\begin{flushleft}
x0 +
\end{flushleft}


\begin{flushleft}
x0 +
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
xi b i
\end{flushleft}


\begin{flushleft}
xi fi ($\alpha$) $\geq$ 1,
\end{flushleft}


\begin{flushleft}
xi fi ($\alpha$) $\geq$ 0,
\end{flushleft}





\begin{flushleft}
$\alpha$$\in$S
\end{flushleft}


\begin{flushleft}
$\alpha$ $\in$ S,
\end{flushleft}





\begin{flushleft}
The dual problem is feasible, so strong duality holds. Furthermore, the dual problem
\end{flushleft}


\begin{flushleft}
is bounded below, so the optimal value is finite, and hence there is a primal optimal
\end{flushleft}


\begin{flushleft}
solution.
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 8
\end{flushleft}





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Projection on a set
\end{flushleft}


\begin{flushleft}
8.1 Uniqueness of projection. Show that if C $\subseteq$ Rn is nonempty, closed and convex, and the
\end{flushleft}


\begin{flushleft}
norm · is strictly convex, then for every x0 there is exactly one x $\in$ C closest to x0 . In
\end{flushleft}


\begin{flushleft}
other words the projection of x0 on C is unique.
\end{flushleft}


\begin{flushleft}
Solution. There is at least one projection (this is true for any norm): Suppose x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ C,
\end{flushleft}


\begin{flushleft}
then the projection is found by minimizing the continuous function x $-$ x0 over a closed
\end{flushleft}


\begin{flushleft}
bounded set C $\cap$ \{x | x $-$ x0 $\leq$ x
\end{flushleft}


\begin{flushleft}
ˆ $-$ x0 \}, so the minimum is attained.
\end{flushleft}


\begin{flushleft}
To show that it is unique if the norm is strictly convex, suppose u, v $\in$ C with u = v and
\end{flushleft}


\begin{flushleft}
u $-$ x0 = v $-$ x0 = D. Then (1/2)(u + v) $\in$ C and
\end{flushleft}


\begin{flushleft}
(1/2)(u + v) $-$ x0
\end{flushleft}





=


$<$


=





\begin{flushleft}
(1/2)(u $-$ x0 ) + (1/2)(v $-$ x0 )
\end{flushleft}


\begin{flushleft}
(1/2) u $-$ x0 + (1/2) v $-$ x0
\end{flushleft}


\begin{flushleft}
D,
\end{flushleft}





\begin{flushleft}
so u and v are not the projection of x0 on C.
\end{flushleft}


\begin{flushleft}
8.2 [Web94, Val64] Chebyshev characterization of convexity. A set C $\in$ R n is called a Chebyshev set if for every x0 $\in$ Rn , there is a unique point in C closest (in Euclidean norm)
\end{flushleft}


\begin{flushleft}
to x0 . From the result in exercise 8.1, every nonempty, closed, convex set is a Chebyshev
\end{flushleft}


\begin{flushleft}
set. In this problem we show the converse, which is known as Motzkin's theorem.
\end{flushleft}


\begin{flushleft}
Let C $\in$ Rn be a Chebyshev set.
\end{flushleft}





\begin{flushleft}
(a) Show that C is nonempty and closed.
\end{flushleft}


\begin{flushleft}
(b) Show that PC , the Euclidean projection on C, is continuous.
\end{flushleft}


\begin{flushleft}
(c) Suppose x0 $\in$ C. Show that PC (x) = PC (x0 ) for all x = $\theta$x0 + (1 $-$ $\theta$)PC (x0 ) with
\end{flushleft}


\begin{flushleft}
0 $\leq$ $\theta$ $\leq$ 1.
\end{flushleft}


\begin{flushleft}
(d) Suppose x0 $\in$ C. Show that PC (x) = PC (x0 ) for all x = $\theta$x0 + (1 $-$ $\theta$)PC (x0 ) with
\end{flushleft}


\begin{flushleft}
$\theta$ $\geq$ 1.
\end{flushleft}


\begin{flushleft}
(e) Combining parts (c) and (d), we can conclude that all points on the ray with base
\end{flushleft}


\begin{flushleft}
PC (x0 ) and direction x0 $-$ PC (x0 ) have projection PC (x0 ). Show that this implies
\end{flushleft}


\begin{flushleft}
that C is convex.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) C is nonempty, because it contains the projection of an arbitrary point x0 $\in$ Rn .
\end{flushleft}


\begin{flushleft}
To show that C is closed, let xk , k = 1, 2, . . . be a sequence of points in C with limit
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
¯. We have
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
¯ $-$ PC (¯
\end{flushleft}


\begin{flushleft}
x) 2 $\leq$ x
\end{flushleft}


\begin{flushleft}
¯ $-$ xk 2
\end{flushleft}


\begin{flushleft}
for all k (by definition of PC (¯
\end{flushleft}


\begin{flushleft}
x)). Taking the limit of the righthand side for k $\rightarrow$ $\infty$
\end{flushleft}


\begin{flushleft}
gives x
\end{flushleft}


\begin{flushleft}
¯ $-$ PC (¯
\end{flushleft}


\begin{flushleft}
x) 2 = 0. Therefore x
\end{flushleft}


\begin{flushleft}
¯ = PC (¯
\end{flushleft}


\begin{flushleft}
x) $\in$ C.
\end{flushleft}


\begin{flushleft}
(b) Let xk , k = 1, 2, . . ., be a sequence of points converging to x
\end{flushleft}


\begin{flushleft}
¯. We have
\end{flushleft}


\begin{flushleft}
xk $-$ PC (xk )
\end{flushleft}





2





\begin{flushleft}
$\leq$ xk $-$ PC (¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}





2





\begin{flushleft}
$\leq$ xk $-$ x
\end{flushleft}


¯





2





\begin{flushleft}
+ x
\end{flushleft}


\begin{flushleft}
¯ $-$ PC (¯
\end{flushleft}


\begin{flushleft}
x) 2 .
\end{flushleft}





\begin{flushleft}
Taking limits on both sides, we see that
\end{flushleft}


\begin{flushleft}
lim
\end{flushleft}





\begin{flushleft}
k$\rightarrow$$\infty$
\end{flushleft}





\begin{flushleft}
xk $-$ PC (xk )
\end{flushleft}





2





\begin{flushleft}
= lim
\end{flushleft}





\begin{flushleft}
k$\rightarrow$$\infty$
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
¯ $-$ PC (xk )
\end{flushleft}





2





\begin{flushleft}
$\leq$ x
\end{flushleft}


\begin{flushleft}
¯ $-$ PC (¯
\end{flushleft}


\begin{flushleft}
x) 2 .
\end{flushleft}





\begin{flushleft}
Now x
\end{flushleft}


\begin{flushleft}
¯ has a unique projection, and therefore PC (¯
\end{flushleft}


\begin{flushleft}
x) is the only element of C in the
\end{flushleft}


\begin{flushleft}
ball \{x | x $-$ x
\end{flushleft}


\begin{flushleft}
¯ 2 $\leq$ dist(¯
\end{flushleft}


\begin{flushleft}
x, C)\}. Moreover C is a closed set. Therefore
\end{flushleft}


\begin{flushleft}
lim
\end{flushleft}





\begin{flushleft}
k$\rightarrow$$\infty$
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
¯ $-$ PC (xk )
\end{flushleft}





2





\begin{flushleft}
$\leq$ x
\end{flushleft}


\begin{flushleft}
¯ $-$ PC (¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}





\begin{flushleft}
is only possible if PC (xk ) converges to PC (¯
\end{flushleft}


\begin{flushleft}
x).
\end{flushleft}





2





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
(c) Suppose x = $\theta$x0 + (1 $-$ $\theta$)PC (x0 ) with 0 $\leq$ $\theta$ $<$ 1. We have
\end{flushleft}


\begin{flushleft}
x0 $-$ PC (x)
\end{flushleft}





2





$\leq$


$\leq$


=


=





\begin{flushleft}
x0 $-$ x 2 + x $-$ PC (x) 2
\end{flushleft}


\begin{flushleft}
x0 $-$ x 2 + x $-$ PC (x0 ) 2
\end{flushleft}


\begin{flushleft}
(1 $-$ $\theta$)(x0 $-$ PC (x0 )) 2 + $\theta$(x0 $-$ PC (x0 ))
\end{flushleft}


\begin{flushleft}
x0 $-$ PC (x0 ) 2 .
\end{flushleft}





2





\begin{flushleft}
(The first inequality is the triangle inequality. The second inequality follows from
\end{flushleft}


\begin{flushleft}
the definition of PC (x).) Since C is a Chebyshev set, PC (x) = PC (x0 ).
\end{flushleft}


\begin{flushleft}
(d) We will use the following fact (which follows from Brouwer's fixed point theorem):
\end{flushleft}


\begin{flushleft}
If g : Rn $\rightarrow$ Rn is continuous and g(x) = 0 for x 2 = 1, then there exists an x with
\end{flushleft}


\begin{flushleft}
x 2 = 1 and g(x)/ g(x) 2 = x.
\end{flushleft}


\begin{flushleft}
Let x = $\theta$x0 + (1 $-$ $\theta$)PC (x0 ) with $\theta$ $>$ 1. To simplify the notation we assume that
\end{flushleft}


\begin{flushleft}
x0 = 0 and
\end{flushleft}


\begin{flushleft}
x $-$ x0 2 = ($\theta$ $-$ 1) PC (x0 ) 2 = 1.
\end{flushleft}


\begin{flushleft}
The function g(x) = $-$PC (x) is continuous (see part (b)). g(x) = 0 for x = 0 because
\end{flushleft}


\begin{flushleft}
x0 = 0 $\in$ C. Using the fixed point theorem, we conclude that there exists a y with
\end{flushleft}


\begin{flushleft}
y 2 = 1 such that
\end{flushleft}


\begin{flushleft}
PC (y)
\end{flushleft}


\begin{flushleft}
y=$-$
\end{flushleft}


.


\begin{flushleft}
PC (y) 2
\end{flushleft}





\begin{flushleft}
This means that x0 = 0 lies on the line segment between PC (y) and y. Hence, from
\end{flushleft}


\begin{flushleft}
(c), PC (x0 ) = PC (y), and
\end{flushleft}


\begin{flushleft}
y=$-$
\end{flushleft}





\begin{flushleft}
PC (x0 )
\end{flushleft}


\begin{flushleft}
= (1 $-$ $\theta$)PC (x0 ) = x.
\end{flushleft}


\begin{flushleft}
PC (x0 ) 2
\end{flushleft}





\begin{flushleft}
We conclude that PC (x) = PC (x0 ).
\end{flushleft}


\begin{flushleft}
(e) It is sufficient to show that C is midpoint convex. Suppose it is not, i.e., there
\end{flushleft}


\begin{flushleft}
exist x1 , x2 $\in$ C with x0 = (1/2)(x1 + x2 ) $\in$ C. For simplicity we assume that
\end{flushleft}


\begin{flushleft}
x1 $-$ x2 2 = 2, so x0 $-$ x2 2 = x0 $-$ x1 2 = 1.
\end{flushleft}


\begin{flushleft}
Define D = x0 $-$ PC (x0 ) 2 . We must have 0 $<$ D $<$ 1. (D $>$ 0 because x0 $\in$ C
\end{flushleft}


\begin{flushleft}
and C is closed; D $<$ 1 because otherwise x0 would have two projections, x1 and x2 ,
\end{flushleft}


\begin{flushleft}
contradicting the fact that C is a Chebyshev set.)
\end{flushleft}


\begin{flushleft}
By the result in (c) and (d), all points x($\theta$) = PC (x0 ) + $\theta$(x0 $-$ PC (x0 )) are projected
\end{flushleft}


\begin{flushleft}
on PC (x0 ), i.e.,
\end{flushleft}


\begin{flushleft}
dist(x($\theta$), C) = PC (x0 ) + $\theta$(x0 $-$ PC (x0 )) $-$ PC (x0 )
\end{flushleft}





2





\begin{flushleft}
= $\theta$ x0 $-$ PC (x0 )
\end{flushleft}





2





\begin{flushleft}
= $\theta$D.
\end{flushleft}





\begin{flushleft}
Without loss of generality, assume that
\end{flushleft}


\begin{flushleft}
(x0 $-$ PC (x0 ))T (x1 $-$ x0 ) $\leq$ 0.
\end{flushleft}


\begin{flushleft}
(Otherwise, switch the roles of x1 and x2 ). We have for $\theta$ $\geq$ 1,
\end{flushleft}


\begin{flushleft}
$\theta$2 D2
\end{flushleft}





=


$<$


=


=


=


$\leq$





\begin{flushleft}
dist(x($\theta$), C)2
\end{flushleft}


2


2


2


\begin{flushleft}
$-$ x0 2
\end{flushleft}


2 2





\begin{flushleft}
x($\theta$) $-$ x1
\end{flushleft}





\begin{flushleft}
x($\theta$)
\end{flushleft}





\begin{flushleft}
+ x 0 $-$ x1
\end{flushleft}





2


2





\begin{flushleft}
+ 2(x($\theta$) $-$ x0 )T (x0 $-$ x1 )
\end{flushleft}





\begin{flushleft}
($\theta$ $-$ 1) D + 1 + 2(x($\theta$) $-$ x0 )T (x0 $-$ x1 )
\end{flushleft}





\begin{flushleft}
($\theta$ $-$ 1)2 D2 + 1 + 2($\theta$ $-$ 1)(x0 $-$ PC (x0 ))T (x0 $-$ x1 )
\end{flushleft}





\begin{flushleft}
($\theta$ $-$ 1)2 D2 + 1.
\end{flushleft}





\begin{flushleft}
(The first inequality follows from the fact that PC (x0 ) = x1 .) Therefore 0 $<$ (1 $-$
\end{flushleft}


\begin{flushleft}
2$\theta$)D 2 + 1, which is false for $\theta$ $\geq$ (1/2)(1 + 1/D 2 ).
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
8.3 Euclidean projection on proper cones.
\end{flushleft}


\begin{flushleft}
(a) Nonnegative orthant. Show that Euclidean projection onto the nonnegative orthant
\end{flushleft}


\begin{flushleft}
is given by the expression on page 399.
\end{flushleft}


\begin{flushleft}
Solution. The inner product of two nonnegative vectors is zero if and only the
\end{flushleft}


\begin{flushleft}
componentwise product is zero. We can therefore solve the equations
\end{flushleft}


\begin{flushleft}
x0,i = x+,i $-$ x$-$,i ,
\end{flushleft}





\begin{flushleft}
x+,i $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x$-$,i $\geq$ 0,
\end{flushleft}





\begin{flushleft}
x+,i x$-$,i = 0,
\end{flushleft}





\begin{flushleft}
for i = 1, . . . , n. If x0,i $>$ 0 the solution is x+,i = x0,i , x$-$,i = 0. If x0,i $<$ 0 the
\end{flushleft}


\begin{flushleft}
solution is x+,i = 0, x$-$,i = $-$x0,i . If x0,i = 0 the solution is x+,i = x$-$,i = 0.
\end{flushleft}





\begin{flushleft}
(b) Positive semidefinite cone. Show that Euclidean projection onto the positive semidefinite cone is given by the expression on page 399.
\end{flushleft}


\begin{flushleft}
˜ + = V T X+ V , X
\end{flushleft}


\begin{flushleft}
˜ $-$ = V T X$-$ V . These matrices must satisfy
\end{flushleft}


\begin{flushleft}
Solution. Define X
\end{flushleft}


\begin{flushleft}
˜+ $-$ X
\end{flushleft}


˜$-$,


\begin{flushleft}
$\Lambda$=X
\end{flushleft}





˜+


\begin{flushleft}
X
\end{flushleft}





˜$-$


\begin{flushleft}
X
\end{flushleft}





0,





\begin{flushleft}
˜+X
\end{flushleft}


˜ $-$ ) = 0.


\begin{flushleft}
tr(X
\end{flushleft}





0,





\begin{flushleft}
˜ + )ij = (X
\end{flushleft}


\begin{flushleft}
˜ $-$ )ij
\end{flushleft}


\begin{flushleft}
The first condition implies that the off-diagonal elements are equal: (X
\end{flushleft}


\begin{flushleft}
if i = j. The third equation implies
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
˜ + )ij (X
\end{flushleft}


\begin{flushleft}
˜ $-$ )ij = 0
\end{flushleft}


\begin{flushleft}
(X
\end{flushleft}





\begin{flushleft}
˜ + )ii (X
\end{flushleft}


\begin{flushleft}
˜ $-$ )ii +
\end{flushleft}


\begin{flushleft}
(X
\end{flushleft}





\begin{flushleft}
˜ + X$-$ ) =
\end{flushleft}


\begin{flushleft}
tr(X
\end{flushleft}





\begin{flushleft}
i=1 j=i
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
which is only possible if
\end{flushleft}


\begin{flushleft}
˜ + )ij = (X
\end{flushleft}


\begin{flushleft}
˜ $-$ )ij = 0,
\end{flushleft}


\begin{flushleft}
(X
\end{flushleft}





\begin{flushleft}
i=j
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
˜ + )ii (X
\end{flushleft}


\begin{flushleft}
˜ $-$ )ii = 0, i = 1, . . . , n.
\end{flushleft}


\begin{flushleft}
(X
\end{flushleft}


\begin{flushleft}
˜ + and X
\end{flushleft}


\begin{flushleft}
˜ $-$ are diagonal, with a complementary zero-nonzero patIn other words, X
\end{flushleft}


\begin{flushleft}
tern on the diagonal, i.e.,
\end{flushleft}


\begin{flushleft}
˜ + )ii = max\{$\lambda$i , 0\},
\end{flushleft}


\begin{flushleft}
(X
\end{flushleft}





\begin{flushleft}
˜ 0 )ii = max\{$-$$\lambda$i , 0\}.
\end{flushleft}


\begin{flushleft}
(X
\end{flushleft}





\begin{flushleft}
(c) Second-order cone. Show that the Euclidean projection of (x0 , t0 ) on the secondorder cone
\end{flushleft}


\begin{flushleft}
K = \{(x, t) $\in$ Rn+1 | x 2 $\leq$ t\}
\end{flushleft}


\begin{flushleft}
is given by
\end{flushleft}





\begin{flushleft}
PK (x0 , t0 ) =
\end{flushleft}





0


\begin{flushleft}
(x0 , t0 )
\end{flushleft}


\begin{flushleft}
(1/2)(1 + t0 / x0
\end{flushleft}





\begin{flushleft}
2 )(x0 ,
\end{flushleft}





\begin{flushleft}
x0
\end{flushleft}





2)





\begin{flushleft}
x0
\end{flushleft}


\begin{flushleft}
x0
\end{flushleft}


\begin{flushleft}
x0
\end{flushleft}





2


2


2





\begin{flushleft}
$\leq$ $-$t0
\end{flushleft}


\begin{flushleft}
$\leq$ t0
\end{flushleft}


\begin{flushleft}
$\geq$ |t0 |.
\end{flushleft}





\begin{flushleft}
Solution. The second-order cone is self-dual, so the conditions are
\end{flushleft}


\begin{flushleft}
x0 = u $-$ v,
\end{flushleft}





\begin{flushleft}
t0 = $\mu$ $-$ $\tau$,
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





2





\begin{flushleft}
$\leq$ $\mu$,
\end{flushleft}





\begin{flushleft}
v
\end{flushleft}





2





\begin{flushleft}
$\leq$ $\tau$,
\end{flushleft}





\begin{flushleft}
uT v + $\mu$$\tau$ = 0.
\end{flushleft}





\begin{flushleft}
It follows from the Cauchy-Schwarz inequality that the last three conditions are
\end{flushleft}


\begin{flushleft}
satisfied if one of the following three cases holds.
\end{flushleft}


\begin{flushleft}
$\bullet$ $\mu$ = 0, u = 0, v 2 $\leq$ $\tau$ . The first two conditions give v = $-$x0 , t0 = $-$$\tau$ . The
\end{flushleft}


\begin{flushleft}
fourth condition implies t0 $\leq$ 0, and $-$ x0 2 $\leq$ $-$t0 .
\end{flushleft}


\begin{flushleft}
In this case (x0 , t0 ) is in the negative second-order cone, and its projection is
\end{flushleft}


\begin{flushleft}
the origin.
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
$\bullet$ $\tau$ = 0, v = 0, u 2 $\leq$ $\mu$. The first two conditions give u = x0 , $\mu$ = t0 . The third
\end{flushleft}


\begin{flushleft}
condition implies x0 2 $\leq$ t0 .
\end{flushleft}


\begin{flushleft}
In this case (x0 , t0 ) is in the second-order cone, so it is its own projection.
\end{flushleft}


\begin{flushleft}
$\bullet$ u 2 = $\mu$ $>$ 0, v 2 = $\tau$ $>$ 0, $\tau$ u = $-$$\mu$v. We can express v as v = $-$($\tau$ /$\mu$)u.
\end{flushleft}


\begin{flushleft}
From x0 = u $-$ v,
\end{flushleft}


\begin{flushleft}
x0 = (1 + $\tau$ /$\mu$)u,
\end{flushleft}


\begin{flushleft}
$\mu$ = u 2,
\end{flushleft}


\begin{flushleft}
and therefore $\mu$ + $\tau$ = x0 2 . Also, t0 = $\mu$ $-$ $\tau$ . Solving for $\mu$ and $\tau$ gives
\end{flushleft}


\begin{flushleft}
$\mu$ = (1/2)(t0 + x0
\end{flushleft}


\begin{flushleft}
$\tau$ is only positive if t0 $<$ x0
\end{flushleft}


\begin{flushleft}
u=
\end{flushleft}





\begin{flushleft}
t0 + x 0 2
\end{flushleft}


\begin{flushleft}
x0 ,
\end{flushleft}


\begin{flushleft}
2 x0 2
\end{flushleft}





\begin{flushleft}
$\mu$=
\end{flushleft}





\begin{flushleft}
x0
\end{flushleft}





2.


2





2 ),





\begin{flushleft}
$\tau$ = (1/2)($-$t0 + x0
\end{flushleft}





2 ).





\begin{flushleft}
We obtain
\end{flushleft}


\begin{flushleft}
+ t0
\end{flushleft}





2





,





\begin{flushleft}
v=
\end{flushleft}





\begin{flushleft}
t0 $-$ x 0 2
\end{flushleft}


\begin{flushleft}
x0 ,
\end{flushleft}


\begin{flushleft}
2 x0 2
\end{flushleft}





\begin{flushleft}
$\tau$ =
\end{flushleft}





\begin{flushleft}
x0
\end{flushleft}





2





2





\begin{flushleft}
$-$ t0
\end{flushleft}





.





\begin{flushleft}
8.4 The Euclidean projection of a point on a convex set yields a simple separating hyperplane
\end{flushleft}


\begin{flushleft}
(PC (x0 ) $-$ x0 )T (x $-$ (1/2)(x0 + PC (x0 ))) = 0.
\end{flushleft}


\begin{flushleft}
Find a counterexample that shows that this construction does not work for general norms.
\end{flushleft}


\begin{flushleft}
Solution. We use the 1 -norm, with
\end{flushleft}


\begin{flushleft}
C = \{x $\in$ R2 | x1 + x2 /2 $\leq$ 1\},
\end{flushleft}





\begin{flushleft}
x0 = (1, 1).
\end{flushleft}





\begin{flushleft}
The projection is PC (x0 ) = (1/2, 1), so the hyperplane as above,
\end{flushleft}


\begin{flushleft}
(PC (x0 ) $-$ x0 )T (x $-$ (1/2)(x0 + PC (x0 ))) = 0,
\end{flushleft}


\begin{flushleft}
simplifies to x1 = 3/4. This does not separate (1, 1) from C.
\end{flushleft}


\begin{flushleft}
8.5 [HUL93, volume 1, page 154] Depth function and signed distance to boundary. Let C $\subseteq$ R n
\end{flushleft}


\begin{flushleft}
be a nonempty convex set, and let dist(x, C) be the distance of x to C in some norm.
\end{flushleft}


\begin{flushleft}
We already know that dist(x, C) is a convex function of x.
\end{flushleft}


\begin{flushleft}
(a) Show that the depth function,
\end{flushleft}


\begin{flushleft}
depth(x, C) = dist(x, Rn \ensuremath{\backslash} C),
\end{flushleft}


\begin{flushleft}
is concave for x $\in$ C.
\end{flushleft}


\begin{flushleft}
Solution. We will show that the depth function can be expressed as
\end{flushleft}


\begin{flushleft}
depth(x, C) =
\end{flushleft}





\begin{flushleft}
inf (SC (y) $-$ y T x),
\end{flushleft}





\begin{flushleft}
y ∗ =1
\end{flushleft}





\begin{flushleft}
where SC is the support function of C. This proves that the depth function is
\end{flushleft}


\begin{flushleft}
concave because it is the infimum of a family of affine functions of x.
\end{flushleft}


\begin{flushleft}
We first prove the following result. Suppose a = 0. The distance of a point x0 , in
\end{flushleft}


\begin{flushleft}
the norm · , to the hyperplane defined by aT x = b, is given by |aT x $-$ b|/ a ∗ .
\end{flushleft}


\begin{flushleft}
We can show this by applying Lagrange duality for the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x $-$ x0
\end{flushleft}


\begin{flushleft}
aT x = b.
\end{flushleft}





\begin{flushleft}
The dual function is
\end{flushleft}


\begin{flushleft}
g($\nu$)
\end{flushleft}





=





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
x $-$ x0 + $\nu$(aT x $-$ b)
\end{flushleft}





=





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
x $-$ x0 + $\nu$aT (x $-$ x0 ) + $\nu$(aT x0 $-$ b)
\end{flushleft}





=





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
$\nu$(aT x0 $-$ b)
\end{flushleft}


$-$$\infty$





\begin{flushleft}
$\nu$a ∗ $\leq$ 1
\end{flushleft}


\begin{flushleft}
otherwise
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
so we obtain the dual problem
\end{flushleft}


\begin{flushleft}
$\nu$(aT x0 $-$ b)
\end{flushleft}


\begin{flushleft}
|$\nu$| $\leq$ 1/ a ∗ .
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
If aT x0 $\geq$ b, the solution is $\nu$ = 1/ a ∗ . If aT x0 $\leq$ b, the solution is $\nu$ = $-$1/ a ∗ .
\end{flushleft}


\begin{flushleft}
In both cases the optimal value is |aT x0 $-$ b|/ a ∗ .
\end{flushleft}


\begin{flushleft}
We now give a geometric interpretation and proof of the expression for the depth
\end{flushleft}


\begin{flushleft}
function. Let H be the set of all halfspaces defined by supporting hyperplanes of C,
\end{flushleft}


\begin{flushleft}
and containing C. We can describe any H $\in$ H by a linear inequality xT y $\leq$ SC (y)
\end{flushleft}


\begin{flushleft}
where y is a nonzero vector in dom SC (y).
\end{flushleft}


\begin{flushleft}
Let H $\in$ H. The function dist(x, Rn \ensuremath{\backslash} H) is affine for all x $\in$ C:
\end{flushleft}


\begin{flushleft}
dist(x, Rn \ensuremath{\backslash} H) =
\end{flushleft}





\begin{flushleft}
SC (y) $-$ xT y
\end{flushleft}


.


\begin{flushleft}
y ∗
\end{flushleft}





\begin{flushleft}
The intersection of all H in H is equal to cl C and therefore
\end{flushleft}


\begin{flushleft}
depth(x, C)
\end{flushleft}





=


=


=





\begin{flushleft}
inf dist(x, Rn \ensuremath{\backslash} H)
\end{flushleft}





\begin{flushleft}
H$\in$H
\end{flushleft}





\begin{flushleft}
inf (SC (y) $-$ xT y)/ y
\end{flushleft}





\begin{flushleft}
y=0
\end{flushleft}





∗





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
inf (SC (y) $-$ x y).
\end{flushleft}





\begin{flushleft}
y ∗ =1
\end{flushleft}





\begin{flushleft}
(b) The signed distance to the boundary of C is defined as
\end{flushleft}


\begin{flushleft}
s(x) =
\end{flushleft}





\begin{flushleft}
dist(x, C)
\end{flushleft}


\begin{flushleft}
$-$ depth(x, C)
\end{flushleft}





\begin{flushleft}
x$\in$C
\end{flushleft}


\begin{flushleft}
x $\in$ C.
\end{flushleft}





\begin{flushleft}
Thus, s(x) is positive outside C, zero on its boundary, and negative on its interior.
\end{flushleft}


\begin{flushleft}
Show that s is a convex function.
\end{flushleft}


\begin{flushleft}
Solution. We will show that if we extend the expression in part (a) to points x $\in$ C,
\end{flushleft}


\begin{flushleft}
we obtain the signed distance:
\end{flushleft}


\begin{flushleft}
s(x) = sup (y T x $-$ SC (y)).
\end{flushleft}


\begin{flushleft}
y ∗ =1
\end{flushleft}





\begin{flushleft}
In part (a) we have shown that this is true for x $\in$ C.
\end{flushleft}





\begin{flushleft}
If x $\in$ bd C, then y T x $\leq$ SC (y) for all unit norm y, with equality if y is the
\end{flushleft}


\begin{flushleft}
normalized normal vector to a supporting hyperplane at x, so the expression for s
\end{flushleft}


\begin{flushleft}
holds.
\end{flushleft}


\begin{flushleft}
If x $\in$ cl C, then for all y with y ∗ = 1, y T x $-$ SC (y) is the distance of x to a
\end{flushleft}


\begin{flushleft}
hyperplane supporting C (as proved in part (a)), and therefore
\end{flushleft}


\begin{flushleft}
y T x $-$ SC (y) $\leq$ dist(x, C).
\end{flushleft}


\begin{flushleft}
Equality holds if we take y equal to the optimal solution of
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
y T x $-$ SC (y)
\end{flushleft}


\begin{flushleft}
y ∗$\leq$1
\end{flushleft}





\begin{flushleft}
with variable y. As we have seen in §8.1.3 the optimal value of this problem is equal
\end{flushleft}


\begin{flushleft}
to dist(x, C).
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
The geometric interpretation is as follows. As in part (a), we let H be the set of all
\end{flushleft}


\begin{flushleft}
halfspaces defined by supporting hyperplanes of C, and containing C. From part (a),
\end{flushleft}


\begin{flushleft}
we already know that for H $\in$ H
\end{flushleft}


\begin{flushleft}
$-$ depth(x, C) = max s(x, H),
\end{flushleft}


\begin{flushleft}
H$\in$H
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
where s(x, R \ensuremath{\backslash} H) is the signed distance from x to H. We now have to show that
\end{flushleft}


\begin{flushleft}
for x outside of C
\end{flushleft}


\begin{flushleft}
dist(x, C) = sup s(x, H).
\end{flushleft}


\begin{flushleft}
H$\in$H
\end{flushleft}





\begin{flushleft}
By construction, we know that for all G $\in$ H, we must have dist(x, C) $\geq$ s(x, G).
\end{flushleft}


\begin{flushleft}
Now, let B be a ball of radius dist(x, C) centered at x. Because both B and C
\end{flushleft}


\begin{flushleft}
are convex with B closed, there is a separating hyperplane H such that H $\in$ H and
\end{flushleft}


\begin{flushleft}
s(x, H) = dist(x, C), hence
\end{flushleft}


\begin{flushleft}
dist(x, C) $\leq$ sup s(x, H),
\end{flushleft}


\begin{flushleft}
H$\in$H
\end{flushleft}





\begin{flushleft}
and the desired result.
\end{flushleft}





\begin{flushleft}
Distance between sets
\end{flushleft}


\begin{flushleft}
8.6 Let C, D be convex sets.
\end{flushleft}


\begin{flushleft}
(a) Show that dist(C, x + D) is a convex function of x.
\end{flushleft}


\begin{flushleft}
(b) Show that dist(tC, x + tD) is a convex function of (x, t) for t $>$ 0.
\end{flushleft}


\begin{flushleft}
Solution. To prove the first, we note that
\end{flushleft}


\begin{flushleft}
dist(C, x + D) = inf (IC (u) + IC (x + v) + u $-$ (x + v) ) .
\end{flushleft}


\begin{flushleft}
u,v
\end{flushleft}





\begin{flushleft}
The righthand side is convex in (u, v, x). Therefore dist(C, x + D) is convex by the
\end{flushleft}


\begin{flushleft}
minimization rule. To prove the second, we note that
\end{flushleft}


\begin{flushleft}
dist(tC, x + tD) = t dist(C, x/t + D).
\end{flushleft}


\begin{flushleft}
The righthand side is the perspective of the convex function from part (a).
\end{flushleft}


\begin{flushleft}
8.7 Separation of ellipsoids. Let E1 and E2 be two ellipsoids defined as
\end{flushleft}


\begin{flushleft}
E2 = \{x | (x $-$ x2 )T P2$-$1 (x $-$ x2 ) $\leq$ 1\},
\end{flushleft}





\begin{flushleft}
E1 = \{x | (x $-$ x1 )T P1$-$1 (x $-$ x1 ) $\leq$ 1\},
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
where P1 , P2 $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ . Show that E1 $\cap$ E2 = $\emptyset$ if and only if there exists an a $\in$ R with
\end{flushleft}


1/2





\begin{flushleft}
P2
\end{flushleft}





\begin{flushleft}
a
\end{flushleft}





2





1/2





\begin{flushleft}
+ P1
\end{flushleft}





\begin{flushleft}
a
\end{flushleft}





2





\begin{flushleft}
$<$ aT (x1 $-$ x2 ).
\end{flushleft}





\begin{flushleft}
Solution. The two sets are closed and bounded, so the intersection is nonempty if and
\end{flushleft}


\begin{flushleft}
only if there is an a = 0 satisfying
\end{flushleft}


\begin{flushleft}
inf aT x $>$ sup aT x.
\end{flushleft}





\begin{flushleft}
x$\in$E1
\end{flushleft}





\begin{flushleft}
x$\in$E2
\end{flushleft}





\begin{flushleft}
The infimum is giving by the optimal value of
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


$-$1/2





\begin{flushleft}
A change of variables y = P1
\end{flushleft}





\begin{flushleft}
aT x
\end{flushleft}


\begin{flushleft}
(x $-$ x1 )T P1$-$1 (x $-$ x1 ) $\leq$ 1.
\end{flushleft}





\begin{flushleft}
(x $-$ x1 ) yields
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
aT x1 + aT P 1/2 y
\end{flushleft}


\begin{flushleft}
y T y $\leq$ 1,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
which has optimal value aT x1 $-$ P 1/2 a 2 .
\end{flushleft}


\begin{flushleft}
Similarly,
\end{flushleft}


\begin{flushleft}
sup aT x = aT x2 + P 1/2 a 2 .
\end{flushleft}


\begin{flushleft}
x$\in$E2
\end{flushleft}





\begin{flushleft}
The condition therefore reduces to
\end{flushleft}


\begin{flushleft}
aT x1 $-$ P 1/2 a
\end{flushleft}





2





\begin{flushleft}
$>$ aT x2 + P 1/2 a 2 .
\end{flushleft}





\begin{flushleft}
We can also derive this result directly from duality, without using the separating hyperplane theorem. The distance between the two sets is the optimal value of the problem
\end{flushleft}


\begin{flushleft}
x$-$y 2
\end{flushleft}


$-$1/2


\begin{flushleft}
P1
\end{flushleft}


\begin{flushleft}
(x $-$ x1 )
\end{flushleft}


$-$1/2


\begin{flushleft}
(y $-$ x2 )
\end{flushleft}


\begin{flushleft}
P2
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





2


2





$\leq$1


$\leq$ 1,





\begin{flushleft}
with variables x and y. The optimal value is positive if and only if the intersection of the
\end{flushleft}


\begin{flushleft}
ellipsoids is empty, and zero otherwise.
\end{flushleft}


\begin{flushleft}
To derive a dual, we first reformulate the problem as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
u 2
\end{flushleft}


\begin{flushleft}
v 2 $\leq$ 1,
\end{flushleft}


\begin{flushleft}
w 2$\leq$1
\end{flushleft}


1/2


\begin{flushleft}
P1 v = x $-$ x 1
\end{flushleft}


1/2


\begin{flushleft}
P2 w = y $-$ x 2
\end{flushleft}


\begin{flushleft}
u = x $-$ y,
\end{flushleft}





\begin{flushleft}
with new variables u, v, w. The Lagrangian is
\end{flushleft}


\begin{flushleft}
L(x, y, u, v, w, $\lambda$1 , $\lambda$2 , z1 , z2 , z)
\end{flushleft}


=





\begin{flushleft}
u
\end{flushleft}


+





=





2





\begin{flushleft}
+ $\lambda$1 ( v
\end{flushleft}





1/2


\begin{flushleft}
z2T (P2 w
\end{flushleft}





2





\begin{flushleft}
$-$ 1) + $\lambda$2 ( w
\end{flushleft}





1/2





\begin{flushleft}
$-$ 1) + z1T (P1
\end{flushleft}





2





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
$-$ y + x2 ) + z (u $-$ x + y)
\end{flushleft}





\begin{flushleft}
v $-$ x + x1 )
\end{flushleft}





\begin{flushleft}
$-$$\lambda$1 $-$ $\lambda$2 + z1T x1 + z2T x2 $-$ (z + z1 )T x + (z $-$ z2 )T y
\end{flushleft}


\begin{flushleft}
+ u
\end{flushleft}





2





\begin{flushleft}
+ z T u + $\lambda$1 v
\end{flushleft}





2





1/2





\begin{flushleft}
+ z1T P1
\end{flushleft}





\begin{flushleft}
v + $\lambda$2 w
\end{flushleft}





2





1/2





\begin{flushleft}
+ z2T P2
\end{flushleft}





\begin{flushleft}
w.
\end{flushleft}





\begin{flushleft}
The minimum over x is unbounded below unless z1 = $-$z. The minimum over y is
\end{flushleft}


\begin{flushleft}
unbounded below unless z2 = z. Eliminating z1 and z2 we can therefore write the dual
\end{flushleft}


\begin{flushleft}
function as
\end{flushleft}


\begin{flushleft}
g($\lambda$1 , $\lambda$2 , z)
\end{flushleft}





=





\begin{flushleft}
$-$$\lambda$1 $-$ $\lambda$2 + z T (x2 $-$ x1 ) + inf ( u
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
+ inf ($\lambda$1 v
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}





2





1/2





\begin{flushleft}
$-$ z T P1
\end{flushleft}





2





\begin{flushleft}
+ z T u)
\end{flushleft}





\begin{flushleft}
v) + + inf ($\lambda$2 w
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





2





1/2





\begin{flushleft}
+ z T P2
\end{flushleft}





\begin{flushleft}
w).
\end{flushleft}





\begin{flushleft}
We have
\end{flushleft}


\begin{flushleft}
inf ( u
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





2





\begin{flushleft}
+ z T u) =
\end{flushleft}





0


$-$$\infty$





\begin{flushleft}
z 2$\leq$1
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
This follows from the Cauchy-Schwarz inequality: if z 2 $\leq$ 1, then z T u $\geq$ $-$ z 2 u 2 $\geq$
\end{flushleft}


\begin{flushleft}
$-$ u 2 , with equality if u = 0. If z 2 $>$ 1, we can take u = $-$tz with t $\rightarrow$ $\infty$ to show
\end{flushleft}


\begin{flushleft}
that u 2 + z T u = t z 1 (1 $-$ z 2 )) is unbounded below.
\end{flushleft}


\begin{flushleft}
We also have
\end{flushleft}


1/2


0


\begin{flushleft}
P 1 z 2 $\leq$ $\lambda$1
\end{flushleft}


1/2


\begin{flushleft}
inf ($\lambda$1 v 2 $-$ z T P1 v) =
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}


\begin{flushleft}
$-$$\infty$ otherwise.
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
This can be shown by distinguishing two cases: if $\lambda$1 = 0 then the infimum is zero if
\end{flushleft}


1/2


\begin{flushleft}
P1 z = 0 and $-$$\infty$ otherwise. If $\lambda$1 $<$ 0 the minimum is $-$$\infty$. If $\lambda$1 $>$ 0, we have
\end{flushleft}


\begin{flushleft}
inf ($\lambda$1 v
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}





2





1/2





\begin{flushleft}
$-$ z T P1
\end{flushleft}





\begin{flushleft}
v)
\end{flushleft}





=





\begin{flushleft}
$\lambda$1 inf ( v
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}





=





1/2





\begin{flushleft}
$-$ (1/$\lambda$1 )z T P1
\end{flushleft}





2





\begin{flushleft}
v)
\end{flushleft}





1/2





0


$-$$\infty$





\begin{flushleft}
P 1 z 2 $\leq$ $\lambda$1
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





0


$-$$\infty$





\begin{flushleft}
P 2 z 2 $\leq$ $\lambda$2
\end{flushleft}


\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
Similarly,
\end{flushleft}


\begin{flushleft}
inf ($\lambda$2 w
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





2





1/2





\begin{flushleft}
+ z T P2
\end{flushleft}





\begin{flushleft}
w) =
\end{flushleft}





1/2





\begin{flushleft}
Putting this all together, we obtain the dual problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$$\lambda$1 $-$ $\lambda$2 + z T (x2 $-$ x1 )
\end{flushleft}


1/2


\begin{flushleft}
z 2 $\leq$ 1,
\end{flushleft}


\begin{flushleft}
P 1 z 2 $\leq$ $\lambda$1 ,
\end{flushleft}





1/2





\begin{flushleft}
P2
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





2





\begin{flushleft}
$\leq$ $\lambda$2 ,
\end{flushleft}





\begin{flushleft}
which is equivalent to
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





1/2





1/2





\begin{flushleft}
$-$ P 1 z 2 $-$ P2
\end{flushleft}


\begin{flushleft}
z 2 $\leq$ 1.
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





2





\begin{flushleft}
+ z T (x2 $-$ x1 )
\end{flushleft}





\begin{flushleft}
The intersection of the ellipsoids is empty if and only if the optimal value is positive, i.e.,
\end{flushleft}


\begin{flushleft}
there exists a z with
\end{flushleft}


1/2





\begin{flushleft}
$-$ P1
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





2





1/2





\begin{flushleft}
$-$ P2
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





2





\begin{flushleft}
+ z T (x2 $-$ x1 ) $>$ 0.
\end{flushleft}





\begin{flushleft}
Setting a = $-$z gives the desired inequality.
\end{flushleft}





\begin{flushleft}
8.8 Intersection and containment of polyhedra. Let P1 and P2 be two polyhedra defined as
\end{flushleft}


\begin{flushleft}
P1 = \{x | Ax
\end{flushleft}


\begin{flushleft}
m×n
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
P2 = \{x | F x
\end{flushleft}





\begin{flushleft}
b\},
\end{flushleft}





\begin{flushleft}
p×n
\end{flushleft}





\begin{flushleft}
g\},
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
with A $\in$ R
\end{flushleft}


\begin{flushleft}
,b$\in$R ,F $\in$R
\end{flushleft}


\begin{flushleft}
, g $\in$ R . Formulate each of the following problems
\end{flushleft}


\begin{flushleft}
as an LP feasibility problem, or a set of LP feasibility problems.
\end{flushleft}


\begin{flushleft}
(a) Find a point in the intersection P1 $\cap$ P2 .
\end{flushleft}





\begin{flushleft}
(b) Determine whether P1 $\subseteq$ P2 .
\end{flushleft}





\begin{flushleft}
For each problem, derive a set of linear inequalities and equalities that forms a strong
\end{flushleft}


\begin{flushleft}
alternative, and give a geometric interpretation of the alternative.
\end{flushleft}


\begin{flushleft}
Repeat the question for two polyhedra defined as
\end{flushleft}


\begin{flushleft}
P1 = conv\{v1 , . . . , vK \},
\end{flushleft}





\begin{flushleft}
P2 = conv\{w1 , . . . , wL \}.
\end{flushleft}





\begin{flushleft}
Solution
\end{flushleft}


\begin{flushleft}
Inequality description.
\end{flushleft}


\begin{flushleft}
(a) Solve
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
b,
\end{flushleft}





\begin{flushleft}
Fx
\end{flushleft}





\begin{flushleft}
g.
\end{flushleft}





\begin{flushleft}
The alternative is
\end{flushleft}


\begin{flushleft}
AT u + F T v = 0,
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





0,





\begin{flushleft}
v
\end{flushleft}





0,





\begin{flushleft}
bT u + g T v $<$ 0.
\end{flushleft}





\begin{flushleft}
Interpretation: if the sets do not intersect, then they can be separated by a hyperplane with normal vector a = AT u = $-$F T v. If Ax b and F y g,
\end{flushleft}


\begin{flushleft}
aT x = uT Ax $\leq$ uT b $<$ $-$v T g $\leq$ $-$v T F x $\leq$ aT y.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) P1 $\subseteq$ P2 if and only if
\end{flushleft}





\begin{flushleft}
sup fiT x $\leq$ gi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p.
\end{flushleft}





\begin{flushleft}
Ax b
\end{flushleft}





\begin{flushleft}
We can solve p LPs, and compare the optimal values with gi . Using LP duality we
\end{flushleft}


\begin{flushleft}
can write the same conditions as
\end{flushleft}


\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
AT z=fi , z
\end{flushleft}





0





\begin{flushleft}
bT z $\leq$ g i ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
which is equivalent to p (decoupled) LP feasibility problems
\end{flushleft}


\begin{flushleft}
AT zi = f i ,
\end{flushleft}





\begin{flushleft}
zi
\end{flushleft}





\begin{flushleft}
b T zi $\leq$ g i
\end{flushleft}





0,





\begin{flushleft}
with variables zi . The alternative for this system is
\end{flushleft}


\begin{flushleft}
Ax
\end{flushleft}





\begin{flushleft}
fiT x $>$ $\lambda$gi ,
\end{flushleft}





\begin{flushleft}
$\lambda$b,
\end{flushleft}





\begin{flushleft}
$\lambda$ $\geq$ 0.
\end{flushleft}





\begin{flushleft}
If $\lambda$ $>$ 0, this means that (1/$\lambda$)x $\in$ P1 , (1/$\lambda$)x $\in$ P2 .
\end{flushleft}


\begin{flushleft}
If $\lambda$ = 0, it means that if x
\end{flushleft}


\begin{flushleft}
¯ $\in$ P1, then x
\end{flushleft}


\begin{flushleft}
¯ + tx $\in$ P2 for t sufficiently large.
\end{flushleft}


\begin{flushleft}
Vertex description.
\end{flushleft}


\begin{flushleft}
(a) P1 $\cap$ P2 = $\emptyset$? Solve
\end{flushleft}


\begin{flushleft}
$\lambda$
\end{flushleft}





\begin{flushleft}
1T $\lambda$ = 1,
\end{flushleft}





0,





\begin{flushleft}
$\mu$
\end{flushleft}





\begin{flushleft}
1T $\mu$ = 1,
\end{flushleft}





0,





\begin{flushleft}
V $\lambda$ = W $\mu$,
\end{flushleft}





\begin{flushleft}
where V has columns vi and W has columns wi .
\end{flushleft}


\begin{flushleft}
From Farkas' lemma the alternative is
\end{flushleft}


\begin{flushleft}
V T z + t1
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
i.e., V z
\end{flushleft}





0,





\begin{flushleft}
$-$W T z + u1
\end{flushleft}





\begin{flushleft}
t $<$ 0,
\end{flushleft}





0,





\begin{flushleft}
u $<$ 0,
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
0, W z ≺ 0. Therefore z defines a separating hyperplane.
\end{flushleft}





\begin{flushleft}
(b) P1 $\subseteq$ P2 ? For i = 1, . . . , K,
\end{flushleft}





\begin{flushleft}
wi = V $\mu$ i ,
\end{flushleft}





\begin{flushleft}
$\mu$i
\end{flushleft}





0,





\begin{flushleft}
1T $\mu$i = 1.
\end{flushleft}





\begin{flushleft}
The alternative (from Farkas lemma) is
\end{flushleft}


\begin{flushleft}
V T zi + t i 1
\end{flushleft}





\begin{flushleft}
wiT zi + ti $<$ 0,
\end{flushleft}





0,





\begin{flushleft}
i.e., wiT zi 1 $<$ V T zi . Thus, zi defines a hyperplane separating wi from P2 .
\end{flushleft}





\begin{flushleft}
Euclidean distance and angle problems
\end{flushleft}


\begin{flushleft}
8.9 Closest Euclidean distance matrix to given data. We are given data dˆij , for i, j = 1, . . . , n,
\end{flushleft}


\begin{flushleft}
which are corrupted measurements of the Euclidean distances between vectors in R k :
\end{flushleft}


\begin{flushleft}
dˆij = xi $-$ xj
\end{flushleft}





2





\begin{flushleft}
+ vij ,
\end{flushleft}





\begin{flushleft}
i, j = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
where vij is some noise or error. These data satisfy dˆij $\geq$ 0 and dˆij = dˆji , for all i, j. The
\end{flushleft}


\begin{flushleft}
dimension k is not specified.
\end{flushleft}


\begin{flushleft}
Show how to solve the following problem using convex optimization. Find a dimension
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
k and x1 , . . . , xn $\in$ Rk so that
\end{flushleft}


\begin{flushleft}
(d $-$ dˆij )2 is minimized, where dij = xi $-$ xj 2 ,
\end{flushleft}


\begin{flushleft}
i,j=1 ij
\end{flushleft}


\begin{flushleft}
i, j = 1, . . . , n. In other words, given some data that are approximate Euclidean distances,
\end{flushleft}


\begin{flushleft}
you are to find the closest set of actual Euclidean distances, in the least-squares sense.
\end{flushleft}


\begin{flushleft}
Solution. The condition that dij are actual Euclidean distances can be expressed in
\end{flushleft}


\begin{flushleft}
terms of the associated Euclidean distance matrix, Dij = d2ij :
\end{flushleft}


\begin{flushleft}
Dii = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
Dij $\geq$ 0,
\end{flushleft}





\begin{flushleft}
i, j = 1, . . . , n
\end{flushleft}





\newpage
8


\begin{flushleft}
(I $-$ (1/n)11T )D(I $-$ (1/n)11T )
\end{flushleft}





\begin{flushleft}
Geometric problems
\end{flushleft}





0,





\begin{flushleft}
which is a set of convex conditions on D.
\end{flushleft}


\begin{flushleft}
The objective can be expressed in terms of D as
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
(dij $-$ dˆij )2
\end{flushleft}





=


\begin{flushleft}
i,j=1
\end{flushleft}





1/2


\begin{flushleft}
(Dij $-$ dˆij )2
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=


\begin{flushleft}
i,j=1
\end{flushleft}





1/2


\begin{flushleft}
Dij $-$ 2Dij dˆij + dˆ2ij ,
\end{flushleft}





1/2


\begin{flushleft}
which is a convex function of D (since Dij dˆij is concave). Thus we minimize this
\end{flushleft}


\begin{flushleft}
function, subject to the constraints above. We reconstruct xi as described in the text,
\end{flushleft}


\begin{flushleft}
using Cholesky factorization.
\end{flushleft}





\begin{flushleft}
8.10 Minimax angle fitting. Suppose that y1 , . . . , ym $\in$ Rk are affine functions of a variable
\end{flushleft}


\begin{flushleft}
x $\in$ Rn :
\end{flushleft}


\begin{flushleft}
yi = Ai x + bi , i = 1, . . . , m,
\end{flushleft}


\begin{flushleft}
and z1 , . . . , zm $\in$ Rk are given nonzero vectors. We want to choose the variable x, subject
\end{flushleft}


\begin{flushleft}
to some convex constraints, (e.g., linear inequalities) to minimize the maximum angle
\end{flushleft}


\begin{flushleft}
between yi and zi ,
\end{flushleft}


\begin{flushleft}
max\{ (y1 , z1 ), . . . , (ym , zm )\}.
\end{flushleft}


\begin{flushleft}
The angle between nonzero vectors is defined as usual:
\end{flushleft}


\begin{flushleft}
(u, v) = cos$-$1
\end{flushleft}





\begin{flushleft}
uT v
\end{flushleft}


\begin{flushleft}
u 2 v
\end{flushleft}





,


2





\begin{flushleft}
where we take cos$-$1 (a) $\in$ [0, $\pi$]. We are only interested in the case when the optimal
\end{flushleft}


\begin{flushleft}
objective value does not exceed $\pi$/2.
\end{flushleft}


\begin{flushleft}
Formulate this problem as a convex or quasiconvex optimization problem. When the
\end{flushleft}


\begin{flushleft}
constraints on x are linear inequalities, what kind of problem (or problems) do you have
\end{flushleft}


\begin{flushleft}
to solve?
\end{flushleft}


\begin{flushleft}
Solution. This is a quasiconvex optimization problem. To see this, we note that
\end{flushleft}


\begin{flushleft}
(u, v) = cos$-$1
\end{flushleft}





\begin{flushleft}
uT v
\end{flushleft}


\begin{flushleft}
u 2 v
\end{flushleft}





2





\begin{flushleft}
$\leq$$\theta$
\end{flushleft}





$\Leftarrow$$\Rightarrow$





\begin{flushleft}
uT v
\end{flushleft}


\begin{flushleft}
u 2 v
\end{flushleft}





$\Leftarrow$$\Rightarrow$





\begin{flushleft}
cos($\theta$) u
\end{flushleft}





2





\begin{flushleft}
$\geq$ cos($\theta$)
\end{flushleft}


2





\begin{flushleft}
v
\end{flushleft}





2





\begin{flushleft}
$\leq$ uT v,
\end{flushleft}





\begin{flushleft}
where in the first line we use the fact that cos$-$1 is monotone decreasing. Now suppose
\end{flushleft}


\begin{flushleft}
that v is fixed, and u is a variable. For $\theta$ $\leq$ $\pi$/2, the sublevel set of (u, v) (in u) is a
\end{flushleft}


\begin{flushleft}
convex set, in fact, a simple second-order cone constraint. Thus, (u, v) is a quasiconvex
\end{flushleft}


\begin{flushleft}
function of u, for fixed v, as long as uT v $\geq$ 0. It follows that the objective in the angle
\end{flushleft}


\begin{flushleft}
fitting problem,
\end{flushleft}


\begin{flushleft}
max\{ (y1 , z1 ), . . . , (ym , zm )\},
\end{flushleft}


\begin{flushleft}
is quasiconvex in x, provided it does not exceed $\pi$/2.
\end{flushleft}


\begin{flushleft}
To formulate the angle fitting problem, we first check whether the optimal objective value
\end{flushleft}


\begin{flushleft}
does not exceed $\pi$/2. To do this we solve the inequality system
\end{flushleft}


\begin{flushleft}
(Ai x + bi )T zi $\geq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
together with inequalities on x, say, F x
\end{flushleft}


\begin{flushleft}
g. This can be done via LP. If this set
\end{flushleft}


\begin{flushleft}
of inequalities is not feasible, then the optimal objective for the angle fitting problem
\end{flushleft}


\begin{flushleft}
exceeds $\pi$/2, and we quit. If it is feasible, we solve the SOC inequality system
\end{flushleft}


\begin{flushleft}
Fx
\end{flushleft}





\begin{flushleft}
g,
\end{flushleft}





\begin{flushleft}
(Ai x + bi )T zi $\geq$ cos($\theta$) Ai x + bi
\end{flushleft}





2





\begin{flushleft}
zi
\end{flushleft}





2,





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
to check if the optimal objective is more or less than $\theta$. We can then bisect on $\theta$ to find
\end{flushleft}


\begin{flushleft}
the smallest value for which this system is feasible. Thus, we need to solve a sequence of
\end{flushleft}


\begin{flushleft}
SOCPs to solve the minimax angle fitting problem.
\end{flushleft}


\begin{flushleft}
8.11 Smallest Euclidean cone containing given points. In Rn , we define a Euclidean cone, with
\end{flushleft}


\begin{flushleft}
center direction c = 0, and angular radius $\theta$, with 0 $\leq$ $\theta$ $\leq$ $\pi$/2, as the set
\end{flushleft}


\begin{flushleft}
\{x $\in$ Rn | (c, x) $\leq$ $\theta$\}.
\end{flushleft}


\begin{flushleft}
(A Euclidean cone is a second-order cone, i.e., it can be represented as the image of the
\end{flushleft}


\begin{flushleft}
second-order cone under a nonsingular linear mapping.)
\end{flushleft}


\begin{flushleft}
Let a1 , . . . , am $\in$ R. How would you find the Euclidean cone, of smallest angular radius,
\end{flushleft}


\begin{flushleft}
that contains a1 , . . . , am ? (In particular, you should explain how to solve the feasibility
\end{flushleft}


\begin{flushleft}
problem, i.e., how to determine whether there is a Euclidean cone which contains the
\end{flushleft}


\begin{flushleft}
points.)
\end{flushleft}


\begin{flushleft}
Solution. First of all, we can assume that each ai is nonzero, since the points that are
\end{flushleft}


\begin{flushleft}
zero lie in all cones, and can be ignored. The points lie in some Euclidean cone if and only
\end{flushleft}


\begin{flushleft}
if they lie in some halfspace, which is the {`}largest' Euclidean cone, with angular radius
\end{flushleft}


\begin{flushleft}
$\pi$/2. This can be checked by solving a set of linear inequalities:
\end{flushleft}


\begin{flushleft}
aTi x $\geq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Now, on to finding the smallest possible Euclidean cone. The points lie in a cone of
\end{flushleft}


\begin{flushleft}
angular radius $\theta$ if and only if there is a (nonzero) vector x $\in$ Rn such that
\end{flushleft}


\begin{flushleft}
aTi x
\end{flushleft}


\begin{flushleft}
ai 2 x
\end{flushleft}





2





\begin{flushleft}
$\geq$ cos $\theta$,
\end{flushleft}





2





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Since $\theta$ $\leq$ $\pi$/2, this is the same as
\end{flushleft}


\begin{flushleft}
aTi x $\geq$ ai
\end{flushleft}





2





\begin{flushleft}
cos $\theta$,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
which is a set of second-order cone constraints. Thus, we can find the smallest cone by
\end{flushleft}


\begin{flushleft}
bisecting $\theta$, and solving a sequence of SOCP feasibility problems.
\end{flushleft}





\begin{flushleft}
Extremal volume ellipsoids
\end{flushleft}


\begin{flushleft}
8.12 Show that the maximum volume ellipsoid enclosed in a set is unique. Show that the
\end{flushleft}


\begin{flushleft}
L¨
\end{flushleft}


\begin{flushleft}
owner-John ellipsoid of a set is unique.
\end{flushleft}


\begin{flushleft}
Solution. Follows from strict convexity of f (A) = log det A$-$1 .
\end{flushleft}


\begin{flushleft}
8.13 L¨
\end{flushleft}


\begin{flushleft}
owner-John ellipsoid of a simplex. In this exercise we show that the L¨
\end{flushleft}


\begin{flushleft}
owner-John ellipsoid of a simplex in Rn must be shrunk by a factor n to fit inside the simplex. Since
\end{flushleft}


\begin{flushleft}
the L¨
\end{flushleft}


\begin{flushleft}
owner-John ellipsoid is affinely invariant, it is sufficient to show the result for one
\end{flushleft}


\begin{flushleft}
particular simplex.
\end{flushleft}


\begin{flushleft}
Derive the L¨
\end{flushleft}


\begin{flushleft}
owner-John ellipsoid Elj for the simplex C = conv\{0, e1 , . . . , en \}. Show that
\end{flushleft}


\begin{flushleft}
Elj must be shrunk by a factor 1/n to fit inside the simplex.
\end{flushleft}


\begin{flushleft}
Solution. By symmetry, the center of the LJ ellipsoid must lie in the direction 1, and
\end{flushleft}


\begin{flushleft}
its intersection with any hyperplane orthogonal to 1 should be a ball. This means we can
\end{flushleft}


\begin{flushleft}
describe the ellipsoid by a quadratic inequality
\end{flushleft}


\begin{flushleft}
(x $-$ $\alpha$1)T (I + $\beta$11T )(x $-$ $\alpha$1) $\leq$ $\gamma$,
\end{flushleft}


\begin{flushleft}
parameterized by three parameters $\alpha$, $\beta$, $\gamma$.
\end{flushleft}


\begin{flushleft}
The extreme points must be in the boundary of the ellipsoid. For x = 0, this gives the
\end{flushleft}


\begin{flushleft}
condition
\end{flushleft}


\begin{flushleft}
$\gamma$ = $\alpha$2 n(1 + n$\beta$).
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
For x = ei , we get the condition
\end{flushleft}


\begin{flushleft}
$\alpha$=
\end{flushleft}





\begin{flushleft}
1+$\beta$
\end{flushleft}


.


\begin{flushleft}
2(1 + n$\beta$)
\end{flushleft}





\begin{flushleft}
The volume of the ellipsoid is proportional to
\end{flushleft}


\begin{flushleft}
$\gamma$ n det(I + $\beta$11T )$-$1 =
\end{flushleft}





\begin{flushleft}
$\gamma$n
\end{flushleft}


,


\begin{flushleft}
1 + $\beta$n
\end{flushleft}





\begin{flushleft}
and its logarithm is
\end{flushleft}


\begin{flushleft}
n log $\gamma$ $-$ log(1 + $\beta$n)
\end{flushleft}





=





\begin{flushleft}
n log($\alpha$2 n(1 + n$\beta$)) $-$ log(1 + $\beta$n)
\end{flushleft}





=





\begin{flushleft}
n log
\end{flushleft}





=





\begin{flushleft}
n log(n/4) + 2n log(1 + $\beta$) $-$ (n + 1) log(1 + n$\beta$).
\end{flushleft}





\begin{flushleft}
(1 + $\beta$)2
\end{flushleft}


\begin{flushleft}
4(1 + $\beta$)
\end{flushleft}





\begin{flushleft}
$-$ log(1 + $\beta$n)
\end{flushleft}





\begin{flushleft}
Setting the derivative equal to zero gives $\beta$ = 1, and hence
\end{flushleft}


\begin{flushleft}
$\alpha$=
\end{flushleft}





1


,


\begin{flushleft}
n+1
\end{flushleft}





\begin{flushleft}
$\beta$ = 1,
\end{flushleft}





\begin{flushleft}
$\gamma$=
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


.


\begin{flushleft}
1+n
\end{flushleft}





\begin{flushleft}
We conclude that Elj is the solution set of the quadratic inequality
\end{flushleft}


1


1


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
1)T (I + 11T )(x $-$
\end{flushleft}


1) $\leq$


,


\begin{flushleft}
n+1
\end{flushleft}


\begin{flushleft}
n+1
\end{flushleft}


\begin{flushleft}
1+n
\end{flushleft}





\begin{flushleft}
(x $-$
\end{flushleft}





\begin{flushleft}
which simplifies to xT x + (1 $-$ 1T x)2 $\leq$ 1. The shrunk ellipsoid is the solution set of the
\end{flushleft}


\begin{flushleft}
quadratic inequality
\end{flushleft}


\begin{flushleft}
(x $-$
\end{flushleft}





1


1


1


\begin{flushleft}
1)T (I + 11T )(x $-$
\end{flushleft}


1) $\leq$


,


\begin{flushleft}
n+1
\end{flushleft}


\begin{flushleft}
n+1
\end{flushleft}


\begin{flushleft}
n(1 + n)
\end{flushleft}





\begin{flushleft}
which simplifies to
\end{flushleft}





1


.


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
We verify that the shrunk ellipsoid lies in C by maximizing the linear functions 1T x, $-$xi ,
\end{flushleft}


\begin{flushleft}
i = 1, . . . , n subject to the quadratic inequality. The solution of
\end{flushleft}


\begin{flushleft}
xT x + (1 $-$ 1T x)2 $\leq$
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
1T x
\end{flushleft}


\begin{flushleft}
xT x + (1 $-$ 1T x)2 $\leq$ 1/n
\end{flushleft}





\begin{flushleft}
is the point (1/n)1. The solution of
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
xi
\end{flushleft}


\begin{flushleft}
xT x + (1 $-$ 1T x)2 $\leq$ 1/n
\end{flushleft}





\begin{flushleft}
is the point (1/n)(1 $-$ ei ).
\end{flushleft}





\begin{flushleft}
8.14 Efficiency of ellipsoidal inner approximation. Let C be a polyhedron in R n described as
\end{flushleft}


\begin{flushleft}
C = \{x | Ax b\}, and suppose that \{x | Ax ≺ b\} is nonempty.
\end{flushleft}


\begin{flushleft}
(a) Show that the maximum volume ellipsoid enclosed in C, expanded by a factor n
\end{flushleft}


\begin{flushleft}
about its center, is an ellipsoid that contains C.
\end{flushleft}


\begin{flushleft}
(b) Show that if C is symmetric about the origin, i.e., of the form C = \{x | $-$1$\surd$ Ax
\end{flushleft}


\begin{flushleft}
1\}, then expanding the maximum volume inscribed ellipsoid by a factor n gives
\end{flushleft}


\begin{flushleft}
an ellipsoid that contains C.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) The ellipsoid E = \{Bu + d | u
\end{flushleft}


\begin{flushleft}
if B and d solve
\end{flushleft}





2





\begin{flushleft}
$\leq$ 1\} is the maximum volume inscribed ellipsoid,
\end{flushleft}





\begin{flushleft}
log det B $-$1
\end{flushleft}


\begin{flushleft}
Bai 2 $\leq$ bi $-$ aTi d,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
or in generalized inequality notation
\end{flushleft}


\begin{flushleft}
log det B $-$1
\end{flushleft}


\begin{flushleft}
(Bai , bi $-$ aTi d)
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
K
\end{flushleft}





0,





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
where K is the second-order cone. The Lagrangian is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
L(B, d, u, v) = log det B $-$1 $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
uTi Bai $-$ v T (b $-$ Ad).
\end{flushleft}





\begin{flushleft}
Minimizing over B and d gives
\end{flushleft}


\begin{flushleft}
B $-$1 = $-$
\end{flushleft}





1


2





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
(ai uTi + ui aTi ),
\end{flushleft}





\begin{flushleft}
AT v = 0.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
The dual problem is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
log det($-$(1/2) i=1 (ai uTi + ui aTi )) $-$ bT v + n
\end{flushleft}


\begin{flushleft}
AT v = 0
\end{flushleft}


\begin{flushleft}
ui 2 $\leq$ vi , i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
The optimality conditions are: primal and dual feasibility and
\end{flushleft}


\begin{flushleft}
B $-$1 = $-$
\end{flushleft}





1


2





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
(ai uTi + ui aTi ),
\end{flushleft}





\begin{flushleft}
uTi Bai + vi (bi $-$ aTi d) = 0,
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
To simplify the notation we will assume that B = I, d = 0, so the optimality
\end{flushleft}


\begin{flushleft}
conditions reduce to
\end{flushleft}


\begin{flushleft}
ai
\end{flushleft}





2





\begin{flushleft}
$\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
AT v = 0,
\end{flushleft}





\begin{flushleft}
ui
\end{flushleft}





2





\begin{flushleft}
$\leq$ vi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
I=$-$
\end{flushleft}





1


2





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
(ai uTi + ui aTi ),
\end{flushleft}





\begin{flushleft}
uTi ai + vi bi = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
(8.14.A)
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
From the Cauchy-Schwarz inequality the last inequality, combined with ai
\end{flushleft}


\begin{flushleft}
and ui 2 $\leq$ vi , implies that and ui = 0, vi = 0 if ai 2 $<$ bi , and
\end{flushleft}


\begin{flushleft}
ui = $-$( ui
\end{flushleft}


\begin{flushleft}
if ai 2 = bi .
\end{flushleft}


\begin{flushleft}
We need to show that x
\end{flushleft}





2





\begin{flushleft}
2 /bi )ai ,
\end{flushleft}





\begin{flushleft}
$\leq$ n if Ax
\end{flushleft}





\begin{flushleft}
vi = u i
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
b. The optimality conditions (8.14.A) give
\end{flushleft}


\begin{flushleft}
aTi u = bT v
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
xT x = $-$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
(uTi x)(aTi x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\leq$ bi
\end{flushleft}





2





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
n=$-$
\end{flushleft}





2





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ui
\end{flushleft}


\begin{flushleft}
ai
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


2


2





\begin{flushleft}
(aTi x)2 $\leq$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ui
\end{flushleft}


\begin{flushleft}
ai
\end{flushleft}





2 2


\begin{flushleft}
bi .
\end{flushleft}





2





\newpage
8


\begin{flushleft}
Since ui = 0, vi = 0 if ai
\end{flushleft}





2





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
$<$ bi , the last sum further simplifies and we obtain
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
xT x $\leq$
\end{flushleft}





\begin{flushleft}
ui
\end{flushleft}





\begin{flushleft}
2 bi
\end{flushleft}





\begin{flushleft}
= bT v = n.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(b) Let E = \{x | xT Q$-$1 x $\leq$ 1\} be the maximum volume ellipsoid with center at the
\end{flushleft}


\begin{flushleft}
origin inscribed in C, where Q $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ . We are asked to show that the ellipsoid
\end{flushleft}


$\surd$


\begin{flushleft}
nE = \{x | xT Q$-$1 x $\leq$ n\}
\end{flushleft}





\begin{flushleft}
contains C.
\end{flushleft}


\begin{flushleft}
We first formulate this problem as a convex optimization problem. x $\in$ E if x =
\end{flushleft}


\begin{flushleft}
Q1/2 y for some y with y 2 $\leq$ 1, so we have E $\subseteq$ C if and only if for i = 1, . . . , p,
\end{flushleft}


\begin{flushleft}
sup aTi Q1/2 y = Q1/2 ai
\end{flushleft}





2





\begin{flushleft}
y 2 $\leq$1
\end{flushleft}





$\leq$ 1,





\begin{flushleft}
or in other words aTi Qai = Q1/2 ai
\end{flushleft}


\begin{flushleft}
ellipsoid by solving
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





2


2





\begin{flushleft}
inf
\end{flushleft}





\begin{flushleft}
y 2 $\leq$1
\end{flushleft}





\begin{flushleft}
aTi Q1/2 y = $-$ Q1/2 ai
\end{flushleft}





2





$\geq$ $-$1,





\begin{flushleft}
$\leq$ 1. We find the maximum volume inscribed
\end{flushleft}





\begin{flushleft}
log det Q$-$1
\end{flushleft}


\begin{flushleft}
aTi Qai $\leq$ 1,
\end{flushleft}





\begin{flushleft}
(8.14.B)
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p.
\end{flushleft}





\begin{flushleft}
The variable is the matrix Q $\in$ Sn .
\end{flushleft}


\begin{flushleft}
The dual function is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
g($\lambda$) = inf L(Q, $\lambda$) = inf
\end{flushleft}


\begin{flushleft}
Q 0
\end{flushleft}





\begin{flushleft}
Q 0
\end{flushleft}





\begin{flushleft}
log det Q$-$1 +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i (aTi Qai $-$ 1)
\end{flushleft}





.





\begin{flushleft}
Minimizing over Q gives
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
Q$-$1 =
\end{flushleft}





\begin{flushleft}
$\lambda$i ai aTi ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
and hence
\end{flushleft}


\begin{flushleft}
g($\lambda$) =
\end{flushleft}





\begin{flushleft}
log det
\end{flushleft}


$-$$\infty$





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i ai aTi $-$
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
($\lambda$i ai aTi )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i + n
\end{flushleft}





0





\begin{flushleft}
otherwise.
\end{flushleft}





\begin{flushleft}
The resulting dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
log det
\end{flushleft}


\begin{flushleft}
$\lambda$ 0.
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i ai aTi $-$
\end{flushleft}





\begin{flushleft}
The KKT conditions are primal and dual feasibility (Q
\end{flushleft}


\begin{flushleft}
plus
\end{flushleft}





\begin{flushleft}
$\lambda$i + n
\end{flushleft}





\begin{flushleft}
0, aTi Qai $\leq$ 1, $\lambda$
\end{flushleft}





0),





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
Q$-$1 =
\end{flushleft}





\begin{flushleft}
$\lambda$i ai aTi ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i (1 $-$ aTi Qai ) = 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p.
\end{flushleft}





\begin{flushleft}
(8.14.C)
\end{flushleft}





\begin{flushleft}
The third condition (the complementary slackness condition) implies that aTi Qai = 1
\end{flushleft}


\begin{flushleft}
if $\lambda$i $>$ 0. Note that Slater's condition for (8.14.B) holds (aTi Qai $<$ 1 for Q = I
\end{flushleft}


\begin{flushleft}
and $>$ 0 small enough), so we have strong duality, and the KKT conditions are
\end{flushleft}


\begin{flushleft}
necessary and sufficient for optimality.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Now suppose Q and $\lambda$ are primal and dual optimal. If we multiply (8.14.C) with Q
\end{flushleft}


\begin{flushleft}
on the left and take the trace, we have
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
$\lambda$i aTi Qai =
\end{flushleft}





\begin{flushleft}
$\lambda$i tr(Qai aTi ) =
\end{flushleft}





\begin{flushleft}
n = tr(QQ$-$1 ) =
\end{flushleft}





\begin{flushleft}
$\lambda$i .
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
The last inequality follows from the fact that aTi Qai = 1 when $\lambda$i = 0. This proves
\end{flushleft}


\begin{flushleft}
1T $\lambda$ = n. Finally, we note that (8.14.C) implies that if x $\in$ C,
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
xT Q$-$1 x =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i (aTi x)2 $\leq$
\end{flushleft}





\begin{flushleft}
$\lambda$i = n.
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
8.15 Minimum volume ellipsoid covering union of ellipsoids. Formulate the following problem
\end{flushleft}


\begin{flushleft}
as a convex optimization problem. Find the minimum volume ellipsoid E = \{x | (x $-$
\end{flushleft}


\begin{flushleft}
x0 )T A$-$1 (x $-$ x0 ) $\leq$ 1\} that contains K given ellipsoids
\end{flushleft}


\begin{flushleft}
Ei = \{x | xT Ai x + 2bTi x + ci $\leq$ 0\},
\end{flushleft}





\begin{flushleft}
i = 1, . . . , K.
\end{flushleft}





\begin{flushleft}
Hint. See appendix B.
\end{flushleft}


\begin{flushleft}
Solution. E contains Ei if
\end{flushleft}


\begin{flushleft}
sup (x $-$ x0 )T A$-$1 (x $-$ x0 ) $\leq$ 1,
\end{flushleft}





\begin{flushleft}
x$\in$Ei
\end{flushleft}





\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
xT Ai x + 2bTi x + ci $\leq$ 0
\end{flushleft}





\begin{flushleft}
xT A$-$1 x $-$ 2xT0 A$-$1 x + xT0 A$-$1 x0 $-$ 1 $\leq$ 0.
\end{flushleft}





=$\Rightarrow$





\begin{flushleft}
From the S-procedure in appendix B, this is true if and only if there exists a $\lambda$i $\geq$ 0 such
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


\begin{flushleft}
A$-$1
\end{flushleft}


\begin{flushleft}
$-$A$-$1 x0
\end{flushleft}


\begin{flushleft}
A i bi
\end{flushleft}


.


\begin{flushleft}
$\lambda$i
\end{flushleft}


$-$1


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T $-$1
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
$-$(A x0 )
\end{flushleft}


\begin{flushleft}
x0 A x0 $-$ 1
\end{flushleft}


\begin{flushleft}
bi ci
\end{flushleft}


\begin{flushleft}
In other words,
\end{flushleft}


\begin{flushleft}
$\lambda$i A i
\end{flushleft}


\begin{flushleft}
$\lambda$i bTi
\end{flushleft}


\begin{flushleft}
i.e., the LMI
\end{flushleft}





\begin{flushleft}
$\lambda$i b i
\end{flushleft}


\begin{flushleft}
1 + $\lambda$ i ci
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
A
\end{flushleft}


\begin{flushleft}
 I
\end{flushleft}


\begin{flushleft}
$-$x0
\end{flushleft}





$-$





\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
$-$xT0
\end{flushleft}





\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
$\lambda$i Ai
\end{flushleft}


\begin{flushleft}
$\lambda$i bTi
\end{flushleft}





\begin{flushleft}
A$-$1
\end{flushleft}





\begin{flushleft}
$-$x0
\end{flushleft}





\begin{flushleft}
I
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
$-$xT0
\end{flushleft}


\begin{flushleft}
$\lambda$i b i 
\end{flushleft}


\begin{flushleft}
1 + $\lambda$ i ci
\end{flushleft}





0,





0





\begin{flushleft}
holds. We therefore obtain the SDP formulation
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





$-$1


\begin{flushleft}
log
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
 det A
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}


\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
$-$xT0
\end{flushleft}


\begin{flushleft}
 I
\end{flushleft}


\begin{flushleft}
$\lambda$i Ai
\end{flushleft}


\begin{flushleft}
$\lambda$i b i 
\end{flushleft}


\begin{flushleft}
$-$x0 $\lambda$i bTi 1 + $\lambda$i ci
\end{flushleft}


\begin{flushleft}
$\lambda$i $\geq$ 0, i = 1, . . . , K.
\end{flushleft}





\begin{flushleft}
The variables are A $\in$ Sn , x0 $\in$ Rn , and $\lambda$i , i = 1, . . . , K.
\end{flushleft}





0,





\begin{flushleft}
i = 1, . . . , K
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
8.16 Maximum volume rectangle inside a polyhedron. Formulate the following problem as a
\end{flushleft}


\begin{flushleft}
convex optimization problem. Find the rectangle
\end{flushleft}


\begin{flushleft}
R = \{x $\in$ Rn | l
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
u\}
\end{flushleft}





\begin{flushleft}
of maximum volume, enclosed in a polyhedron P = \{x | Ax
\end{flushleft}


\begin{flushleft}
b\}. The variables are
\end{flushleft}


\begin{flushleft}
l, u $\in$ Rn . Your formulation should not involve an exponential number of constraints.
\end{flushleft}


\begin{flushleft}
Solution. A straightforward, but very inefficient, way to express the constraint R $\subseteq$ P
\end{flushleft}


\begin{flushleft}
is to use the set of m2n inequalities Av i b, where v i are the (2n ) corners of R. (If the
\end{flushleft}


\begin{flushleft}
corners of a box lie inside a polyhedron, then the box does.) Fortunately it is possible to
\end{flushleft}


\begin{flushleft}
express the constraint in a far more efficient way. Define
\end{flushleft}


\begin{flushleft}
a+
\end{flushleft}


\begin{flushleft}
ij = max\{aij , 0\},
\end{flushleft}





\begin{flushleft}
a$-$
\end{flushleft}


\begin{flushleft}
ij = max\{$-$aij , 0\}.
\end{flushleft}





\begin{flushleft}
Then we have R $\subseteq$ P if and only if
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





$-$


\begin{flushleft}
(a+
\end{flushleft}


\begin{flushleft}
ij uj $-$ aij lj ) $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
The maximum volume rectangle is the solution of
\end{flushleft}


\begin{flushleft}
1/n
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
(ui $-$ li )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


+


$-$


\begin{flushleft}
(aij uj $-$ aij lj )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
with implicit constraint u
\end{flushleft}


\begin{flushleft}
objective, which yields
\end{flushleft}





\begin{flushleft}
$\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
l. Another formulation can be found by taking the log of the
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
log(ui $-$ li )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


+


$-$


\begin{flushleft}
(a
\end{flushleft}


\begin{flushleft}
ij uj $-$ aij lj )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Centering
\end{flushleft}


\begin{flushleft}
8.17 Affine invariance of analytic center. Show that the analytic center of a set of inequalities is
\end{flushleft}


\begin{flushleft}
affine invariant. Show that it is invariant with respect to positive scaling of the inequalities.
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
Solution. If xac is the minimizer of $-$ i=1 log($-$fi (x)) then yac = T xac + x0 is the
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
minimizer of $-$ i=1 log($-$fi (T x + x0 )).
\end{flushleft}


\begin{flushleft}
Positive scaling of the inequalities adds a constant to the logarithmic barrier function.
\end{flushleft}


\begin{flushleft}
8.18 Analytic center and redundant inequalities. Two sets of linear inequalities that describe
\end{flushleft}


\begin{flushleft}
the same polyhedron can have different analytic centers. Show that by adding redundant
\end{flushleft}


\begin{flushleft}
inequalities, we can make any interior point x0 of a polyhedron
\end{flushleft}


\begin{flushleft}
P = \{x $\in$ Rn | Ax
\end{flushleft}





\begin{flushleft}
b\}
\end{flushleft}





\begin{flushleft}
the analytic center. More specifically, suppose A $\in$ Rm×n and Ax0 ≺ b. Show that there
\end{flushleft}


\begin{flushleft}
exist c $\in$ Rn , $\gamma$ $\in$ R, and a positive integer q, such that P is the solution set of the m + q
\end{flushleft}


\begin{flushleft}
inequalities
\end{flushleft}


\begin{flushleft}
Ax b,
\end{flushleft}


\begin{flushleft}
cT x $\leq$ $\gamma$,
\end{flushleft}


\begin{flushleft}
cT x $\leq$ $\gamma$, . . . , cT x $\leq$ $\gamma$
\end{flushleft}


(8.36)





\begin{flushleft}
(where the inequality cT x $\leq$ $\gamma$ is added q times), and x0 is the analytic center of (8.36).
\end{flushleft}


\begin{flushleft}
Solution. The optimality conditions are
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





1


\begin{flushleft}
q
\end{flushleft}


\begin{flushleft}
c=0
\end{flushleft}


\begin{flushleft}
ai +
\end{flushleft}


\begin{flushleft}
$\gamma$ $-$ cT x
\end{flushleft}


\begin{flushleft}
bi $-$ aTi x
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
so we have to choose
\end{flushleft}


\begin{flushleft}
c=$-$
\end{flushleft}





\begin{flushleft}
$\gamma$ $-$ cT x T
\end{flushleft}


\begin{flushleft}
A d
\end{flushleft}


\begin{flushleft}
q
\end{flushleft}





\begin{flushleft}
where di = 1/(bi $-$ aTi x ). We can choose c = $-$AT d, and for q any integer satisfying
\end{flushleft}


\begin{flushleft}
q $\geq$ max\{cT x|Ax $\leq$ b\} $-$ cT x ,
\end{flushleft}


\begin{flushleft}
and $\gamma$ = q + cT x .
\end{flushleft}


\begin{flushleft}
8.19 Let xac be the analytic center of a set of linear inequalities
\end{flushleft}


\begin{flushleft}
aTi x $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
and define H as the Hessian of the logarithmic barrier function at xac :
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
H=
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





1


\begin{flushleft}
ai aTi .
\end{flushleft}


\begin{flushleft}
(bi $-$ aTi xac )2
\end{flushleft}





\begin{flushleft}
Show that the kth inequality is redundant (i.e., it can be deleted without changing the
\end{flushleft}


\begin{flushleft}
feasible set) if
\end{flushleft}


\begin{flushleft}
bk $-$ aTk xac $\geq$ m(aTk H $-$1 ak )1/2 .
\end{flushleft}


\begin{flushleft}
Solution. We have an enclosing ellipsoid defined by
\end{flushleft}


\begin{flushleft}
(x $-$ xac )T H(x $-$ xac ) $\leq$ m(m $-$ 1).
\end{flushleft}


\begin{flushleft}
The maximum of aTk x over the enclosing ellipsoid is
\end{flushleft}


\begin{flushleft}
aTk xac +
\end{flushleft}


\begin{flushleft}
so if
\end{flushleft}





\begin{flushleft}
aTk xac +
\end{flushleft}





\begin{flushleft}
m(m $-$ 1)
\end{flushleft}


\begin{flushleft}
m(m $-$ 1)
\end{flushleft}





\begin{flushleft}
the inequality is redundant.
\end{flushleft}





\begin{flushleft}
aTk H $-$1 ak
\end{flushleft}





\begin{flushleft}
aTk H $-$1 ak $\leq$ bk ,
\end{flushleft}





\begin{flushleft}
8.20 Ellipsoidal approximation from analytic center of linear matrix inequality. Let C be the
\end{flushleft}


\begin{flushleft}
solution set of the LMI
\end{flushleft}


\begin{flushleft}
x1 A1 + x2 A2 + · · · + xn An B,
\end{flushleft}


\begin{flushleft}
where Ai , B $\in$ Sm , and let xac be its analytic center. Show that
\end{flushleft}


\begin{flushleft}
Einner $\subseteq$ C $\subseteq$ Eouter ,
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
Einner
\end{flushleft}





\begin{flushleft}
Eouter
\end{flushleft}





=


=





\begin{flushleft}
\{x | (x $-$ xac )T H(x $-$ xac ) $\leq$ 1\},
\end{flushleft}





\begin{flushleft}
\{x | (x $-$ xac )T H(x $-$ xac ) $\leq$ m(m $-$ 1)\},
\end{flushleft}





\begin{flushleft}
and H is the Hessian of the logarithmic barrier function
\end{flushleft}


\begin{flushleft}
$-$ log det(B $-$ x1 A1 $-$ x2 A2 $-$ · · · $-$ xn An )
\end{flushleft}


\begin{flushleft}
evaluated at xac .
\end{flushleft}


\begin{flushleft}
Solution. Define F (x) = B $-$
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
xi Ai . and Fac = F (xac ) The Hessian is given by
\end{flushleft}





$-$1


$-$1


\begin{flushleft}
Hij = tr(Fac
\end{flushleft}


\begin{flushleft}
Ai Fac
\end{flushleft}


\begin{flushleft}
Aj ),
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
so we have
\end{flushleft}


\begin{flushleft}
(x $-$ xac )T H(x $-$ xac )
\end{flushleft}





=


\begin{flushleft}
i,j
\end{flushleft}





$-$1


$-$1


\begin{flushleft}
(xi $-$ xac,i )(xj $-$ xac,j ) tr(Fac
\end{flushleft}


\begin{flushleft}
Ai Fac
\end{flushleft}


\begin{flushleft}
Aj )
\end{flushleft}





=





$-$1


$-$1


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac )Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac )
\end{flushleft}





=





$-$1/2


$-$1/2


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac )Fac
\end{flushleft}





2





.





\begin{flushleft}
We first consider the inner ellipsoid. Suppose x $\in$ Einner , i.e.,
\end{flushleft}


$-$1/2


$-$1/2


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac )Fac
\end{flushleft}





2





$-$1/2


$-$1/2


\begin{flushleft}
= Fac
\end{flushleft}


\begin{flushleft}
F (x)Fac
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





2


\begin{flushleft}
F
\end{flushleft}





$\leq$ 1.





\begin{flushleft}
This implies that
\end{flushleft}


$-$1/2


$-$1/2


\begin{flushleft}
$-$1 $\leq$ $\lambda$i (Fac
\end{flushleft}


\begin{flushleft}
F (x)Fac
\end{flushleft}


) $-$ 1 $\leq$ 1,





\begin{flushleft}
i.e.,
\end{flushleft}





$-$1/2


$-$1/2


\begin{flushleft}
0 $\leq$ $\lambda$i (Fac
\end{flushleft}


\begin{flushleft}
F (x)Fac
\end{flushleft}


)$\leq$2





\begin{flushleft}
for i = 1, . . . , m. In particular, F (x) 0, i.e., x $\in$ C.
\end{flushleft}


\begin{flushleft}
To prove that C $\subseteq$ Eouter , we first note that the gradient of the logarithmic barrier function
\end{flushleft}


\begin{flushleft}
vanishes at xac , and therefore,
\end{flushleft}


$-$1


\begin{flushleft}
tr(Fac
\end{flushleft}


\begin{flushleft}
Ai ) = 0,
\end{flushleft}





\begin{flushleft}
and therefore
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n,
\end{flushleft}





$-$1


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac ) = 0,
\end{flushleft}





$-$1


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
F (x) = m.
\end{flushleft}





\begin{flushleft}
Now assume x $\in$ C. Then
\end{flushleft}





\begin{flushleft}
(x $-$ xac )T H(x $-$ ac)
\end{flushleft}


=


=


=





$-$1/2


$-$1/2


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac )Fac
\end{flushleft}





2





$-$1


$-$1


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac )Fac
\end{flushleft}


\begin{flushleft}
(F (x) $-$ Fac )
\end{flushleft}





$-$1


$-$1


$-$1


$-$1


$-$1


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
F (x)Fac
\end{flushleft}


\begin{flushleft}
F (x) $-$ 2 tr Fac
\end{flushleft}


\begin{flushleft}
F (x) + tr Fac
\end{flushleft}


\begin{flushleft}
Fac Fac
\end{flushleft}


\begin{flushleft}
Fac
\end{flushleft}





=





$-$1


$-$1


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
F (x)Fac
\end{flushleft}


\begin{flushleft}
F (x) $-$ 2m + m
\end{flushleft}





=





$-$1/2


$-$1/2


\begin{flushleft}
tr Fac
\end{flushleft}


\begin{flushleft}
F (x)Fac
\end{flushleft}





$\leq$


=





2





\begin{flushleft}
$-$m
\end{flushleft}





$-$1/2


$-$1/2 2


\begin{flushleft}
tr(Fac
\end{flushleft}


\begin{flushleft}
F (x)Fac
\end{flushleft}


)


2





\begin{flushleft}
$-$m
\end{flushleft}





\begin{flushleft}
m $-$ m.
\end{flushleft}





\begin{flushleft}
The inequality follows by applying the inequality
\end{flushleft}


$-$1/2


$-$1/2


\begin{flushleft}
eigenvalues of Fac F (x)Fac .
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\lambda$2i $\leq$ (
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\lambda$i )2 for $\lambda$
\end{flushleft}





\begin{flushleft}
0 to the
\end{flushleft}





\begin{flushleft}
8.21 [BYT99] Maximum likelihood interpretation of analytic center. We use the linear measurement model of page 352,
\end{flushleft}


\begin{flushleft}
y = Ax + v,
\end{flushleft}


\begin{flushleft}
where A $\in$ Rm×n . We assume the noise components vi are IID with support [$-$1, 1]. The
\end{flushleft}


\begin{flushleft}
set of parameters x consistent with the measurements y $\in$ Rm is the polyhedron defined
\end{flushleft}


\begin{flushleft}
by the linear inequalities
\end{flushleft}


\begin{flushleft}
$-$1 + y Ax 1 + y.
\end{flushleft}


(8.37)





\begin{flushleft}
Suppose the probability density function of vi has the form
\end{flushleft}


\begin{flushleft}
p(v) =
\end{flushleft}





\begin{flushleft}
$\alpha$r (1 $-$ v 2 )r
\end{flushleft}


0





\begin{flushleft}
$-$1 $\leq$ v $\leq$ 1
\end{flushleft}


\begin{flushleft}
otherwise,
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
where r $\geq$ 1 and $\alpha$r $>$ 0. Show that the maximum likelihood estimate of x is the analytic
\end{flushleft}


\begin{flushleft}
center of (8.37).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
log(1 + yi $-$ aTi x) + log(1 $-$ yi + aTi x) .
\end{flushleft}





\begin{flushleft}
L = m log $\alpha$r + r
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
8.22 Center of gravity. The center of gravity of a set C $\subseteq$ Rn with nonempty interior is defined
\end{flushleft}


\begin{flushleft}
as
\end{flushleft}


\begin{flushleft}
u du
\end{flushleft}


.


\begin{flushleft}
xcg = C
\end{flushleft}


\begin{flushleft}
1 du
\end{flushleft}


\begin{flushleft}
C
\end{flushleft}


\begin{flushleft}
The center of gravity is affine invariant, and (clearly) a function of the set C, and not
\end{flushleft}


\begin{flushleft}
its particular description. Unlike the centers described in the chapter, however, it is very
\end{flushleft}


\begin{flushleft}
difficult to compute the center of gravity, except in simple cases (e.g., ellipsoids, balls,
\end{flushleft}


\begin{flushleft}
simplexes).
\end{flushleft}


\begin{flushleft}
Show that the center of gravity xcg is the minimizer of the convex function
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
C
\end{flushleft}





\begin{flushleft}
u$-$x
\end{flushleft}





2


2





\begin{flushleft}
du.
\end{flushleft}





\begin{flushleft}
Solution. Setting the gradient equal to zero gives
\end{flushleft}





\begin{flushleft}
C
\end{flushleft}





\begin{flushleft}
2(u $-$ x) du = 0
\end{flushleft}





\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
u du =
\end{flushleft}


\begin{flushleft}
C
\end{flushleft}





\begin{flushleft}
1 du x.
\end{flushleft}


\begin{flushleft}
C
\end{flushleft}





\begin{flushleft}
Classification
\end{flushleft}


\begin{flushleft}
8.23 Robust linear discrimination. Consider the robust linear discrimination problem given
\end{flushleft}


\begin{flushleft}
in (8.23).
\end{flushleft}


\begin{flushleft}
(a) Show that the optimal value t is positive if and only if the two sets of points can
\end{flushleft}


\begin{flushleft}
be linearly separated. When the two sets of points can be linearly separated, show
\end{flushleft}


\begin{flushleft}
that the inequality a 2 $\leq$ 1 is tight, i.e., we have a 2 = 1, for the optimal a .
\end{flushleft}


\begin{flushleft}
(b) Using the change of variables a
\end{flushleft}


\begin{flushleft}
˜ = a/t, ˜b = b/t, prove that the problem (8.23) is
\end{flushleft}


\begin{flushleft}
equivalent to the QP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
a
\end{flushleft}


˜ 2


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
˜T xi $-$ ˜b $\geq$ 1, i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}


\begin{flushleft}
˜T yi $-$ ˜b $\leq$ $-$1, i = 1, . . . , M.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) If t $>$ 0, then
\end{flushleft}


\begin{flushleft}
a
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
xi $\geq$ t + b $>$ b $>$ b $-$ t $\geq$ a
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
yi ,
\end{flushleft}





\begin{flushleft}
so a , b define a separating hyperplane.
\end{flushleft}


\begin{flushleft}
Conversely if a, b define a separating hyperplane, then there is a positive t satisfying
\end{flushleft}


\begin{flushleft}
the constraints.
\end{flushleft}


\begin{flushleft}
The constraint is tight because the other constraints are homogeneous.
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
(b) Suppose a, b, t are feasible in problem (8.23), with t $>$ 0. Then a
\end{flushleft}


\begin{flushleft}
˜, ˜b are feasible in
\end{flushleft}


\begin{flushleft}
the QP, with objective value a
\end{flushleft}


\begin{flushleft}
˜ 2 = a 2 /t $\leq$ 1/t.
\end{flushleft}


\begin{flushleft}
Conversely, if a
\end{flushleft}


\begin{flushleft}
˜, ˜b are feasible in the QP, then t = 1/ a
\end{flushleft}


\begin{flushleft}
˜ 2, a = a
\end{flushleft}


\begin{flushleft}
˜/ a
\end{flushleft}


\begin{flushleft}
˜ 2 , b = ˜b/ a
\end{flushleft}


˜ 2,


\begin{flushleft}
are feasible in problem (8.23), with objective value t = 1/ a
\end{flushleft}


˜ 2.


\begin{flushleft}
8.24 Linear discrimination maximally robust to weight errors. Suppose we are given two sets of
\end{flushleft}


\begin{flushleft}
points \{x1 , . . . , xN \} and and \{y1 , . . . , yM \} in Rn that can be linearly separated. In §8.6.1
\end{flushleft}


\begin{flushleft}
we showed how to find the affine function that discriminates the sets, and gives the largest
\end{flushleft}


\begin{flushleft}
gap in function values. We can also consider robustness with respect to changes in the
\end{flushleft}


\begin{flushleft}
vector a, which is sometimes called the weight vector. For a given a and b for which
\end{flushleft}


\begin{flushleft}
f (x) = aT x $-$ b separates the two sets, we define the weight error margin as the norm of
\end{flushleft}


\begin{flushleft}
the smallest u $\in$ Rn such that the affine function (a + u)T x $-$ b no longer separates the
\end{flushleft}


\begin{flushleft}
two sets of points. In other words, the weight error margin is the maximum $\rho$ such that
\end{flushleft}


\begin{flushleft}
(a + u)T xi $\geq$ b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , N,
\end{flushleft}





\begin{flushleft}
(a + u)T yj $\leq$ b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , M,
\end{flushleft}





\begin{flushleft}
holds for all u with u 2 $\leq$ $\rho$.
\end{flushleft}


\begin{flushleft}
Show how to find a and b that maximize the weight error margin, subject to the normalization constraint a 2 $\leq$ 1.
\end{flushleft}


\begin{flushleft}
Solution. The weight error margin is the maximum $\rho$ such that
\end{flushleft}


\begin{flushleft}
(a + u)T xi $\geq$ b,
\end{flushleft}


\begin{flushleft}
for all u with u
\end{flushleft}





2





\begin{flushleft}
i = 1, . . . , N,
\end{flushleft}





\begin{flushleft}
(a + u)T yj $\leq$ b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , M,
\end{flushleft}





\begin{flushleft}
$\leq$ $\rho$, i.e.,
\end{flushleft}


\begin{flushleft}
aT x i $-$ $\rho$ x i
\end{flushleft}





2





\begin{flushleft}
$\geq$ bi ,
\end{flushleft}





\begin{flushleft}
aT y i + $\rho$ y i
\end{flushleft}





2





\begin{flushleft}
$\leq$ bi .
\end{flushleft}





\begin{flushleft}
This shows that the weight error margin is given by
\end{flushleft}


\begin{flushleft}
min
\end{flushleft}


\begin{flushleft}
i=1,...,N
\end{flushleft}


\begin{flushleft}
j=1,...,M
\end{flushleft}





\begin{flushleft}
aT x i $-$ b b $-$ a T y i
\end{flushleft}


,


\begin{flushleft}
xi 2
\end{flushleft}


\begin{flushleft}
yi 2
\end{flushleft}





.





\begin{flushleft}
We can maximize the weight error margin by solving the problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
a T xi $-$ b $\geq$ t x i 2 ,
\end{flushleft}


\begin{flushleft}
b $-$ a T yi $\geq$ t y i 2 ,
\end{flushleft}


\begin{flushleft}
a 2$\leq$1
\end{flushleft}





\begin{flushleft}
i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
j = 1, . . . , M
\end{flushleft}





\begin{flushleft}
with variables a, b, t.
\end{flushleft}


\begin{flushleft}
8.25 Most spherical separating ellipsoid. We are given two sets of vectors x 1 , . . . , xN $\in$ Rn , and
\end{flushleft}


\begin{flushleft}
y1 , . . . , yM $\in$ Rn , and wish to find the ellipsoid with minimum eccentricity (i.e., minimum
\end{flushleft}


\begin{flushleft}
condition number of the defining matrix) that contains the points x1 , . . . , xN , but not the
\end{flushleft}


\begin{flushleft}
points y1 , . . . , yM . Formulate this as a convex optimization problem.
\end{flushleft}


\begin{flushleft}
Solution. This can be solved as the SDP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$\gamma$
\end{flushleft}


\begin{flushleft}
xTi P xi + q T xi + r $\geq$ 0,
\end{flushleft}


\begin{flushleft}
yiT P yi + q T yi + r $\leq$ 0,
\end{flushleft}


\begin{flushleft}
I P
\end{flushleft}


\begin{flushleft}
$\gamma$I,
\end{flushleft}





\begin{flushleft}
with variables P $\in$ Sn , q $\in$ Rn , and r, $\gamma$ $\in$ R.
\end{flushleft}





\begin{flushleft}
i = 1, . . . , N
\end{flushleft}


\begin{flushleft}
i = 1, . . . , M
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Placement and floor planning
\end{flushleft}


\begin{flushleft}
8.26 Quadratic placement. We consider a placement problem in R2 , defined by an undirected
\end{flushleft}


\begin{flushleft}
graph A with N nodes, and with quadratic costs:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
(i,j)$\in$A
\end{flushleft}





\begin{flushleft}
xi $-$ xj
\end{flushleft}





2


2.





\begin{flushleft}
The variables are the positions xi $\in$ R2 , i = 1, . . . , M . The positions xi , i = M + 1, . . . , N
\end{flushleft}


\begin{flushleft}
are given. We define two vectors u, v $\in$ RM by
\end{flushleft}


\begin{flushleft}
u = (x11 , x21 , . . . , xM 1 ),
\end{flushleft}





\begin{flushleft}
v = (x12 , x22 , . . . , xM 2 ),
\end{flushleft}





\begin{flushleft}
containing the first and second components, respectively, of the free nodes.
\end{flushleft}


\begin{flushleft}
Show that u and v can be found by solving two sets of linear equations,
\end{flushleft}


\begin{flushleft}
Cu = d1 ,
\end{flushleft}





\begin{flushleft}
Cv = d2 ,
\end{flushleft}





\begin{flushleft}
M
\end{flushleft}





\begin{flushleft}
where C $\in$ S . Give a simple expression for the coefficients of C in terms of the graph A.
\end{flushleft}


\begin{flushleft}
Solution. The objective function is
\end{flushleft}





\begin{flushleft}
(i,j)$\in$A
\end{flushleft}





\begin{flushleft}
(ui $-$ uj )2 +
\end{flushleft}





\begin{flushleft}
(i,j)$\in$A
\end{flushleft}





\begin{flushleft}
(vj $-$ vj )2 .
\end{flushleft}





\begin{flushleft}
Setting the gradients with respect to u and v equal to zero gives equations Cu = d 1 and
\end{flushleft}


\begin{flushleft}
Cv = d2 with
\end{flushleft}


\begin{flushleft}
degree of node i
\end{flushleft}


\begin{flushleft}
$-$(number of arcs between i and j)
\end{flushleft}





\begin{flushleft}
Cij =
\end{flushleft}





\begin{flushleft}
i=j
\end{flushleft}


\begin{flushleft}
i = j,
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
d1i =
\end{flushleft}





\begin{flushleft}
xj1 ,
\end{flushleft}





\begin{flushleft}
d2i =
\end{flushleft}





\begin{flushleft}
j$>$M, (i,j)$\in$A
\end{flushleft}





\begin{flushleft}
xj2 .
\end{flushleft}


\begin{flushleft}
j$>$M, (i,j)$\in$A
\end{flushleft}





\begin{flushleft}
8.27 Problems with minimum distance constraints. We consider a problem with variables
\end{flushleft}


\begin{flushleft}
x1 , . . . , xN $\in$ Rk . The objective, f0 (x1 , . . . , xN ), is convex, and the constraints
\end{flushleft}


\begin{flushleft}
fi (x1 , . . . , xN ) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
are convex (i.e., the functions fi : RN k $\rightarrow$ R are convex). In addition, we have the
\end{flushleft}


\begin{flushleft}
minimum distance constraints
\end{flushleft}


\begin{flushleft}
xi $-$ xj
\end{flushleft}





2





\begin{flushleft}
$\geq$ Dmin ,
\end{flushleft}





\begin{flushleft}
i = j, i, j = 1, . . . , N.
\end{flushleft}





\begin{flushleft}
In general, this is a hard nonconvex problem.
\end{flushleft}


\begin{flushleft}
Following the approach taken in floorplanning, we can form a convex restriction of the
\end{flushleft}


\begin{flushleft}
problem, i.e., a problem which is convex, but has a smaller feasible set. (Solving the
\end{flushleft}


\begin{flushleft}
restricted problem is therefore easy, and any solution is guaranteed to be feasible for the
\end{flushleft}


\begin{flushleft}
nonconvex problem.) Let aij $\in$ Rk , for i $<$ j, i, j = 1, . . . , N , satisfy aij 2 = 1.
\end{flushleft}


\begin{flushleft}
Show that the restricted problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x1 , . . . , xN )
\end{flushleft}


\begin{flushleft}
fi (x1 , . . . , xN ) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
aTij (xi $-$ xj ) $\geq$ Dmin , i $<$ j, i, j = 1, . . . , N,
\end{flushleft}





\begin{flushleft}
is convex, and that every feasible point satisfies the minimum distance constraint.
\end{flushleft}


\begin{flushleft}
Remark. There are many good heuristics for choosing the directions aij . One simple
\end{flushleft}


\begin{flushleft}
one starts with an approximate solution x
\end{flushleft}


\begin{flushleft}
ˆ1 , . . . , x
\end{flushleft}


\begin{flushleft}
ˆN (that need not satisfy the minimum
\end{flushleft}


\begin{flushleft}
distance constraints). We then set aij = (ˆ
\end{flushleft}


\begin{flushleft}
xi $-$ x
\end{flushleft}


\begin{flushleft}
ˆj )/ x
\end{flushleft}


\begin{flushleft}
ˆi $-$ x
\end{flushleft}


\begin{flushleft}
ˆj 2 .
\end{flushleft}


\begin{flushleft}
Solution. Follows immediately from the Cauchy-Schwarz inequality:
\end{flushleft}


\begin{flushleft}
1 $\leq$ aT (u $-$ v) $\leq$ a
\end{flushleft}





2





\begin{flushleft}
u$-$v
\end{flushleft}





2





\begin{flushleft}
= u$-$v
\end{flushleft}





2.





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
Miscellaneous problems
\end{flushleft}


\begin{flushleft}
8.28 Let P1 and P2 be two polyhedra described as
\end{flushleft}


\begin{flushleft}
P1 = \{x | Ax
\end{flushleft}





\begin{flushleft}
b\} ,
\end{flushleft}





\begin{flushleft}
P2 = \{x | $-$1
\end{flushleft}





\begin{flushleft}
Cx
\end{flushleft}





1\} ,





\begin{flushleft}
where A $\in$ Rm×n , C $\in$ Rp×n , and b $\in$ Rm . The polyhedron P2 is symmetric about the
\end{flushleft}


\begin{flushleft}
origin. For t $\geq$ 0 and xc $\in$ Rn , we use the notation tP2 + xc to denote the polyhedron
\end{flushleft}


\begin{flushleft}
tP2 + xc = \{tx + xc | x $\in$ P2 \},
\end{flushleft}


\begin{flushleft}
which is obtained by first scaling P2 by a factor t about the origin, and then translating
\end{flushleft}


\begin{flushleft}
its center to xc .
\end{flushleft}


\begin{flushleft}
Show how to solve the following two problems, via an LP, or a set of LPs.
\end{flushleft}


\begin{flushleft}
(a) Find the largest polyhedron tP2 + xc enclosed in P1 , i.e.,
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
tP2 + xc $\subseteq$ P1
\end{flushleft}


\begin{flushleft}
t $\geq$ 0.
\end{flushleft}





\begin{flushleft}
(b) Find the smallest polyhedron tP2 + xc containing P1 , i.e.,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
P1 $\subseteq$ tP2 + xc
\end{flushleft}


\begin{flushleft}
t $\geq$ 0.
\end{flushleft}





\begin{flushleft}
In both problems the variables are t $\in$ R and xc $\in$ Rn .
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We can write the problem as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
or
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
If we define
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
supx$\in$tP2 +xc aTi x $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
aTi xc + sup$-$t1$\leq$Cv$\leq$t1 aTi v $\leq$ bi ,
\end{flushleft}


\begin{flushleft}
p(ai ) =
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
aTi v,
\end{flushleft}





\begin{flushleft}
(8.28.A)
\end{flushleft}


\begin{flushleft}
(8.28.B)
\end{flushleft}





\begin{flushleft}
$-$1$\leq$Cv$\leq$1
\end{flushleft}





\begin{flushleft}
we can write (8.28.A) as
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
aTi xc + tp(ai ) $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
(8.28.C)
\end{flushleft}





\begin{flushleft}
which is an LP in xc and t. Note that p(ai ) can be evaluated by solving the LP in
\end{flushleft}


\begin{flushleft}
the definition (8.28.B).
\end{flushleft}


\begin{flushleft}
In summary we can solve the problem by first determining p(ai ) for i = 1, . . . , m,
\end{flushleft}


\begin{flushleft}
by solving m LPs, and then solving the LP (8.28.C) for t and xc .
\end{flushleft}


\begin{flushleft}
(b) We first note that x $\in$ tP2 + xc if and only
\end{flushleft}


\begin{flushleft}
$-$t1 $\leq$ C(x $-$ xc ) $\leq$ t1.
\end{flushleft}


\begin{flushleft}
The problem is therefore equivalent to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
supx$\in$P1 cTi x $-$ cTi xc $\leq$ t,
\end{flushleft}


\begin{flushleft}
inf x$\in$P1 cTi x
\end{flushleft}





$-$





\begin{flushleft}
cTi xc
\end{flushleft}





\begin{flushleft}
$\geq$ $-$t,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , l
\end{flushleft}


\begin{flushleft}
i = 1, . . . , l
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
or
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
$-$t + supAx$\leq$b cTi x $\leq$ cTi xc $\leq$ t + inf Ax$\leq$b cTi x,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , l.
\end{flushleft}





\begin{flushleft}
If we define p(ci ) and q(ci ) as
\end{flushleft}


\begin{flushleft}
p(ci ) = sup cTi x,
\end{flushleft}





\begin{flushleft}
q(ci ) = inf cTi x
\end{flushleft}


\begin{flushleft}
Ax$\leq$b
\end{flushleft}





\begin{flushleft}
Ax$\leq$b
\end{flushleft}





\begin{flushleft}
(8.28.D)
\end{flushleft}





\begin{flushleft}
then the problem simplifies to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
$-$t + p(ci ) $\leq$ cTi xc $\leq$ t + q(ci ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , l,
\end{flushleft}





\begin{flushleft}
(8.28.E)
\end{flushleft}





\begin{flushleft}
which is an LP in xc and t.
\end{flushleft}


\begin{flushleft}
In conclusion, we can solve the problem by first determining p(ci ) and q(ci ), i =
\end{flushleft}


\begin{flushleft}
1, . . . , p from the 2l LPs in the definition (8.28.D), and then solving the LP (8.28.E).
\end{flushleft}


\begin{flushleft}
8.29 Outer polyhedral approximations. Let P = \{x $\in$ Rn | Ax
\end{flushleft}


\begin{flushleft}
b\} be a polyhedron, and
\end{flushleft}


\begin{flushleft}
C $\subseteq$ Rn a given set (not necessarily convex). Use the support function SC to formulate
\end{flushleft}


\begin{flushleft}
the following problem as an LP:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
C $\subseteq$ tP + x
\end{flushleft}


\begin{flushleft}
t $\geq$ 0.
\end{flushleft}





\begin{flushleft}
Here tP + x = \{tu + x | u $\in$ P\}, the polyhedron P scaled by a factor of t about the origin,
\end{flushleft}


\begin{flushleft}
and translated by x. The variables are t $\in$ R and x $\in$ Rn .
\end{flushleft}


\begin{flushleft}
Solution. We have C $\subseteq$ tP + x if and only if (1/t)(C $-$ x) $\subseteq$ P, i.e.,
\end{flushleft}


\begin{flushleft}
S(1/t)(C$-$x) (ai ) $\leq$ bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
Noting that for t $\geq$ 0,
\end{flushleft}


\begin{flushleft}
S(1/t)(C$-$x) (a) = sup aT ((1/t)(u $-$ x)) = (1/t)(SC (a) $-$ aT x),
\end{flushleft}


\begin{flushleft}
u$\in$C
\end{flushleft}





\begin{flushleft}
we can express the problem as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
SC (ai ) $-$ aTi x $\leq$ tbi ,
\end{flushleft}


\begin{flushleft}
t $\geq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
which is an LP in the variables x, t.
\end{flushleft}


\begin{flushleft}
8.30 Interpolation with piecewise-arc curve. A sequence of points a1 , . . . , an $\in$ R2 is given. We
\end{flushleft}


\begin{flushleft}
construct a curve that passes through these points, in order, and is an arc (i.e., part of a
\end{flushleft}


\begin{flushleft}
circle) or line segment (which we think of as an arc of infinite radius) between consecutive
\end{flushleft}


\begin{flushleft}
points. Many arcs connect ai and ai+1 ; we parameterize these arcs by giving the angle
\end{flushleft}


\begin{flushleft}
$\theta$i $\in$ ($-$$\pi$, $\pi$) between its tangent at ai and the line segment [ai , ai+1 ]. Thus, $\theta$i = 0 means
\end{flushleft}


\begin{flushleft}
the arc between ai and ai+1 is in fact the line segment [ai , ai+1 ]; $\theta$i = $\pi$/2 means the arc
\end{flushleft}


\begin{flushleft}
between ai and ai+1 is a half-circle (above the linear segment [a1 , a2 ]); $\theta$i = $-$$\pi$/2 means
\end{flushleft}


\begin{flushleft}
the arc between ai and ai+1 is a half-circle (below the linear segment [a1 , a2 ]). This is
\end{flushleft}


\begin{flushleft}
illustrated below.
\end{flushleft}





\newpage
8





\begin{flushleft}
Geometric problems
\end{flushleft}





\begin{flushleft}
$\theta$i = 3$\pi$/4
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}


\begin{flushleft}
$\theta$i = $\pi$/2
\end{flushleft}


\begin{flushleft}
$\theta$i = $\pi$/4
\end{flushleft}


\begin{flushleft}
$\theta$i = 0
\end{flushleft}


\begin{flushleft}
ai
\end{flushleft}


\begin{flushleft}
ai+1
\end{flushleft}


\begin{flushleft}
Our curve is completely specified by the angles $\theta$1 , . . . , $\theta$n , which can be chosen in the
\end{flushleft}


\begin{flushleft}
interval ($-$$\pi$, $\pi$). The choice of $\theta$i affects several properties of the curve, for example, its
\end{flushleft}


\begin{flushleft}
total arc length L, or the joint angle discontinuities, which can be described as follows.
\end{flushleft}


\begin{flushleft}
At each point ai , i = 2, . . . , n $-$ 1, two arcs meet, one coming from the previous point and
\end{flushleft}


\begin{flushleft}
one going to the next point. If the tangents to these arcs exactly oppose each other, so the
\end{flushleft}


\begin{flushleft}
curve is differentiable at ai , we say there is no joint angle discontinuity at ai . In general,
\end{flushleft}


\begin{flushleft}
we define the joint angle discontinuity at ai as |$\theta$i$-$1 +$\theta$i +$\psi$i |, where $\psi$i is the angle between
\end{flushleft}


\begin{flushleft}
the line segment [ai , ai+1 ] and the line segment [ai$-$1 , ai ], i.e., $\psi$i = (ai $-$ ai+1 , ai$-$1 $-$ ai ).
\end{flushleft}


\begin{flushleft}
This is shown below. Note that the angles $\psi$i are known (since the ai are known).
\end{flushleft}


\begin{flushleft}
PSfrag replacements
\end{flushleft}


\begin{flushleft}
$\psi$i
\end{flushleft}





\begin{flushleft}
ai
\end{flushleft}





\begin{flushleft}
$\theta$i$-$1
\end{flushleft}





\begin{flushleft}
ai+1
\end{flushleft}


\begin{flushleft}
$\theta$i
\end{flushleft}





\begin{flushleft}
ai$-$1
\end{flushleft}


\begin{flushleft}
We define the total joint angle discontinuity as
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
D=
\end{flushleft}


\begin{flushleft}
i=2
\end{flushleft}





\begin{flushleft}
|$\theta$i$-$1 + $\theta$i + $\psi$i |.
\end{flushleft}





\begin{flushleft}
Formulate the problem of minimizing total arc length length L, and total joint angle
\end{flushleft}


\begin{flushleft}
discontinuity D, as a bi-criterion convex optimization problem. Explain how you would
\end{flushleft}


\begin{flushleft}
find the extreme points on the optimal trade-off curve.
\end{flushleft}


\begin{flushleft}
Solution. The total joint angle discontinuity is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
D=
\end{flushleft}


\begin{flushleft}
i=2
\end{flushleft}





\begin{flushleft}
|$\theta$i$-$1 + $\theta$i + $\psi$i |,
\end{flushleft}





\begin{flushleft}
which is evidently convex in $\theta$.
\end{flushleft}


\begin{flushleft}
The other objective is the total arc length, which turns out to be
\end{flushleft}


\begin{flushleft}
n$-$1
\end{flushleft}





\begin{flushleft}
L=
\end{flushleft}





\begin{flushleft}
li
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\theta$i
\end{flushleft}


,


\begin{flushleft}
sin $\theta$i
\end{flushleft}





\begin{flushleft}
where li = ai $-$ ai+1 2 . We will show that L is a convex function of $\theta$. Of course we
\end{flushleft}


\begin{flushleft}
need only show that the function f (x) = x/ sin x is convex over the interval |x| $<$ $\pi$. In
\end{flushleft}


\begin{flushleft}
fact f is log-convex. With g = log(x/ sin x), we have
\end{flushleft}


\begin{flushleft}
g =$-$
\end{flushleft}





1


1


+


.


\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
sin2 x
\end{flushleft}





\begin{flushleft}
Now since | sin x| $\leq$ |x| for (all) x, we have 1/x2 $\leq$ 1/ sin2 x for all x, and hence g $\geq$ 0.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Therefore we find that both objectives D and L are convex. To find the optimal trade-off
\end{flushleft}


\begin{flushleft}
curve, we minimize various (nonnegative) weighted combinations of D and L, i.e., D +$\lambda$L,
\end{flushleft}


\begin{flushleft}
for various values of $\lambda$ $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Now let's consider the extreme points of the trade-off curve. Obviously L is minimized by
\end{flushleft}


\begin{flushleft}
taking $\theta$i = 0, i.e., with the curve consisting of the line segments connecting the points.
\end{flushleft}


\begin{flushleft}
So $\theta$ = 0 is one end of the optimal trade-off curve.
\end{flushleft}


\begin{flushleft}
We can also say something about the other extreme point, which we claim occurs when
\end{flushleft}


\begin{flushleft}
the total joint angle discontinuity is zero (which means that the curve is differentiable).
\end{flushleft}


\begin{flushleft}
This occurs when the recursion
\end{flushleft}


\begin{flushleft}
$\theta$i = $-$$\theta$i$-$1 $-$ $\psi$i ,
\end{flushleft}





\begin{flushleft}
i = 2, . . . , n,
\end{flushleft}





\begin{flushleft}
holds. This shows that once the first angle $\theta$1 is fixed, the whole curve is fixed. Thus,
\end{flushleft}


\begin{flushleft}
there is a one-parameter family of piecewise-arc curves that pass through the points,
\end{flushleft}


\begin{flushleft}
parametrized by $\theta$1 . To find the other extreme point of the optimal trade-off curve, we
\end{flushleft}


\begin{flushleft}
need to find the curve in this family that has minimum length. This can be found by
\end{flushleft}


\begin{flushleft}
solving the one-dimensional problem of minimizing L, over $\theta$1 , using the recursion above.
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 9
\end{flushleft}





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Unconstrained minimization
\end{flushleft}


\begin{flushleft}
9.1 Minimizing a quadratic function.
\end{flushleft}


\begin{flushleft}
Consider the problem of minimizing a quadratic
\end{flushleft}


\begin{flushleft}
function:
\end{flushleft}


\begin{flushleft}
minimize f (x) = (1/2)xT P x + q T x + r,
\end{flushleft}


\begin{flushleft}
where P $\in$ Sn (but we do not assume P
\end{flushleft}





0).





\begin{flushleft}
(a) Show that if P
\end{flushleft}


\begin{flushleft}
0, i.e., the objective function f is not convex, then the problem is
\end{flushleft}


\begin{flushleft}
unbounded below.
\end{flushleft}


\begin{flushleft}
(b) Now suppose that P
\end{flushleft}


\begin{flushleft}
0 (so the objective function is convex), but the optimality
\end{flushleft}


\begin{flushleft}
condition P x = $-$q does not have a solution. Show that the problem is unbounded
\end{flushleft}


\begin{flushleft}
below.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) If P
\end{flushleft}





\begin{flushleft}
0, we can find v such that v T P v $<$ 0. With x = tv we have
\end{flushleft}


\begin{flushleft}
f (x) = t2 (v T P v/2) + t(q T v) + r,
\end{flushleft}





\begin{flushleft}
which converges to $-$$\infty$ as t becomes large.
\end{flushleft}





\begin{flushleft}
(b) This means q $\in$ R(P ). Express q as q = q˜ + v, where q˜ is the Euclidean projection
\end{flushleft}


\begin{flushleft}
of q onto R(P ), and take v = q $-$ q˜. This vector is nonzero and orthogonal to R(P ),
\end{flushleft}


\begin{flushleft}
i.e., v T P v = 0. It follows that for x = tv, we have
\end{flushleft}


\begin{flushleft}
f (x) = tq T v + r = t(˜
\end{flushleft}


\begin{flushleft}
q + v)T v + r = t(v T v) + r,
\end{flushleft}


\begin{flushleft}
which is unbounded below.
\end{flushleft}


\begin{flushleft}
9.2 Minimizing a quadratic-over-linear fractional function. Consider the problem of minimizing the function f : Rn $\rightarrow$ R, defined as
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
Ax $-$ b 22
\end{flushleft}


,


\begin{flushleft}
cT x + d
\end{flushleft}





\begin{flushleft}
dom f = \{x | cT x + d $>$ 0\}.
\end{flushleft}





\begin{flushleft}
We assume rank A = n and b $\in$ R(A).
\end{flushleft}


\begin{flushleft}
(a) Show that f is closed.
\end{flushleft}


\begin{flushleft}
(b) Show that the minimizer x of f is given by
\end{flushleft}


\begin{flushleft}
x = x1 + tx2
\end{flushleft}


\begin{flushleft}
where x1 = (AT A)$-$1 AT b, x2 = (AT A)$-$1 c, and t $\in$ R can be calculated by solving
\end{flushleft}


\begin{flushleft}
a quadratic equation.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Since b $\in$ R(A), the numerator is bounded below by a positive number ( Axls $-$b 22 ).
\end{flushleft}


\begin{flushleft}
Therefore f (x) $\rightarrow$ $\infty$ as x approaches the boundary of dom f .
\end{flushleft}





\begin{flushleft}
(b) The optimality conditions are
\end{flushleft}


\begin{flushleft}
$\nabla$f (x)
\end{flushleft}





=


=


=





\begin{flushleft}
Ax $-$ b 22
\end{flushleft}


2


\begin{flushleft}
c
\end{flushleft}


\begin{flushleft}
AT (Ax $-$ b) $-$ T
\end{flushleft}


\begin{flushleft}
$-$d
\end{flushleft}


\begin{flushleft}
(c x $-$ d)2
\end{flushleft}





\begin{flushleft}
cT x
\end{flushleft}





\begin{flushleft}
Ax $-$ b 22
\end{flushleft}


2


\begin{flushleft}
(x
\end{flushleft}


$-$


\begin{flushleft}
x
\end{flushleft}


)


$-$


\begin{flushleft}
x2
\end{flushleft}


1


\begin{flushleft}
cT x $-$ d
\end{flushleft}


\begin{flushleft}
(cT x $-$ d)2
\end{flushleft}


0,





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
i.e., x = x1 + tx2 where
\end{flushleft}


\begin{flushleft}
Ax1 + tAx2 $-$ b 22
\end{flushleft}


\begin{flushleft}
Ax $-$ b 22
\end{flushleft}


=


.


\begin{flushleft}
2(cT x $-$ d)
\end{flushleft}


\begin{flushleft}
2(cT x1 + tcT x2 $-$ d)
\end{flushleft}





\begin{flushleft}
t=
\end{flushleft}





\begin{flushleft}
In other words t must satisfy
\end{flushleft}


\begin{flushleft}
2t2 cT x2 + 2t(cT x1 $-$ d)
\end{flushleft}





=


=





\begin{flushleft}
t2 Ax2
\end{flushleft}


\begin{flushleft}
2 T
\end{flushleft}





2


2





\begin{flushleft}
+ 2t(Ax1 $-$ b)T Ax2 + Ax1 $-$ b
\end{flushleft}





\begin{flushleft}
t c x2 + Ax1 $-$ b
\end{flushleft}





2


2





2


2,





\begin{flushleft}
which reduces to a quadratic equation
\end{flushleft}


\begin{flushleft}
t2 cT x2 + 2t(cT x1 $-$ d) $-$ Ax1 $-$ b
\end{flushleft}





2


2





= 0.





\begin{flushleft}
We have to pick the root
\end{flushleft}


\begin{flushleft}
t=
\end{flushleft}





\begin{flushleft}
$-$(cT x1 $-$ d) $\pm$
\end{flushleft}





\begin{flushleft}
(cT x1 $-$ d)2 + (cT x2 ) Ax1 $-$ b
\end{flushleft}


\begin{flushleft}
c T x2
\end{flushleft}





2


2





,





\begin{flushleft}
so that
\end{flushleft}


\begin{flushleft}
cT (x1 + tx2 ) $-$ d
\end{flushleft}





=


=


$>$





\begin{flushleft}
cT x1 $-$ d $-$ (cT x1 $-$ d) +
\end{flushleft}


0.





\begin{flushleft}
(cT x1 $-$ d)2 + (cT x2 ) Ax1 $-$ b
\end{flushleft}





\begin{flushleft}
(cT x1 $-$ d)2 + (cT x2 ) Ax1 $-$ b
\end{flushleft}





2


2





2


2





\begin{flushleft}
9.3 Initial point and sublevel set condition. Consider the function f (x) = x 21 + x22 with domain
\end{flushleft}


\begin{flushleft}
dom f = \{(x1 , x2 ) | x1 $>$ 1\}.
\end{flushleft}


\begin{flushleft}
(a) What is p ?
\end{flushleft}





\begin{flushleft}
(b) Draw the sublevel set S = \{x | f (x) $\leq$ f (x(0) )\} for x(0) = (2, 2). Is the sublevel set
\end{flushleft}


\begin{flushleft}
S closed? Is f strongly convex on S?
\end{flushleft}


\begin{flushleft}
(c) What happens if we apply the gradient method with backtracking line search, starting at x(0) ? Does f (x(k) ) converge to p ?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) p = limx$\rightarrow$(1,0) f (x1 .x2 ) = 1.
\end{flushleft}


\begin{flushleft}
(b) No, the sublevel set is not closed. The points (1 + 1/k, 1) are in the sublevel set for
\end{flushleft}


\begin{flushleft}
k = 1, 2, . . ., but the limit, (1, 1), is not.
\end{flushleft}


\begin{flushleft}
(c) The algorithm gets stuck at (1, 1).
\end{flushleft}


\begin{flushleft}
9.4 Do you agree with the following argument? The
\end{flushleft}


\begin{flushleft}
expressed as
\end{flushleft}





\begin{flushleft}
1 -norm
\end{flushleft}





\begin{flushleft}
of a vector x $\in$ Rm can be
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
Therefore the
\end{flushleft}





\begin{flushleft}
1 -norm
\end{flushleft}





1





\begin{flushleft}
x2i /yi + 1T y
\end{flushleft}





\begin{flushleft}
= (1/2) inf
\end{flushleft}





\begin{flushleft}
y 0
\end{flushleft}





.





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
approximation problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
Ax $-$ b
\end{flushleft}





1





\begin{flushleft}
is equivalent to the minimization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
f (x, y) =
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
(aTi x
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$ bi )2 /yi + 1T y,
\end{flushleft}





(9.62)





\begin{flushleft}
with dom f = \{(x, y) $\in$ Rn × Rm | y 0\}, where aTi is the ith row of A. Since f is twice
\end{flushleft}


\begin{flushleft}
differentiable and convex, we can solve the 1 -norm approximation problem by applying
\end{flushleft}


\begin{flushleft}
Newton's method to (9.62).
\end{flushleft}


\begin{flushleft}
Solution. The reformulation is valid. The hitch is that the objective function f is not
\end{flushleft}


\begin{flushleft}
closed.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
9.5 Backtracking line search. Suppose f is strongly convex with mI
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
M I. Let
\end{flushleft}


\begin{flushleft}
∆x be a descent direction at x. Show that the backtracking stopping condition holds for
\end{flushleft}


\begin{flushleft}
0$<$t$\leq$$-$
\end{flushleft}





\begin{flushleft}
$\nabla$f (x)T ∆x
\end{flushleft}


.


\begin{flushleft}
M ∆x 22
\end{flushleft}





\begin{flushleft}
Use this to give an upper bound on the number of backtracking iterations.
\end{flushleft}


\begin{flushleft}
Solution. The upper bound $\nabla$2 f (x) M I implies
\end{flushleft}


\begin{flushleft}
f (x + t∆x) $\leq$ f (x) + t$\nabla$f (x)T ∆x + (M/2)t2 ∆xT ∆x
\end{flushleft}


\begin{flushleft}
hence f (x + t∆x) $\leq$ f (x) + $\alpha$t$\nabla$f (x)T ∆x if
\end{flushleft}


\begin{flushleft}
t(1 $-$ $\alpha$)$\nabla$f (x)T ∆x + (M/2)t2 ∆xT ∆x $\leq$ 0
\end{flushleft}


\begin{flushleft}
i.e., the exit condition certainly holds if 0 $\leq$ t $\leq$ t0 with
\end{flushleft}


\begin{flushleft}
t0 = $-$2(1 $-$ $\alpha$)
\end{flushleft}





\begin{flushleft}
$\nabla$f (x)T ∆x
\end{flushleft}


\begin{flushleft}
$\nabla$f (x)T ∆x
\end{flushleft}


$\geq$$-$


.


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
M ∆x ∆x
\end{flushleft}


\begin{flushleft}
M ∆xT ∆x
\end{flushleft}





\begin{flushleft}
Assume t0 $\leq$ 1. Then $\beta$ k t $\leq$ t0 for k $\geq$ log(1/t0 )/ log(1/$\beta$).
\end{flushleft}





\begin{flushleft}
Gradient and steepest descent methods
\end{flushleft}


\begin{flushleft}
9.6 Quadratic problem in R2 . Verify the expressions for the iterates x(k) in the first example
\end{flushleft}


\begin{flushleft}
of §9.3.2.
\end{flushleft}


\begin{flushleft}
Solution. For k = 0, we get the starting point x(0) = ($\gamma$, 1).
\end{flushleft}


\begin{flushleft}
(k)
\end{flushleft}





\begin{flushleft}
(k)
\end{flushleft}





\begin{flushleft}
The gradient at x(k) is (x1 , $\gamma$x2 ), so we get
\end{flushleft}


\begin{flushleft}
x(k) $-$ t$\nabla$f (x(k) ) =
\end{flushleft}





\begin{flushleft}
(k)
\end{flushleft}





\begin{flushleft}
(1 $-$ t)x1
\end{flushleft}


\begin{flushleft}
(k)
\end{flushleft}


\begin{flushleft}
(1 $-$ $\gamma$t)x2
\end{flushleft}





\begin{flushleft}
$\gamma$$-$1
\end{flushleft}


\begin{flushleft}
$\gamma$+1
\end{flushleft}





=





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
(1 $-$ t)$\gamma$
\end{flushleft}


\begin{flushleft}
(1 $-$ $\gamma$t)($-$1)k
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
$\gamma$$-$1
\end{flushleft}


\begin{flushleft}
$\gamma$+1
\end{flushleft}





\begin{flushleft}
f (x(k) $-$ t$\nabla$f (x(k) )) = ($\gamma$ 2 (1 $-$ t)2 + $\gamma$(1 $-$ $\gamma$t)2 )
\end{flushleft}





\begin{flushleft}
2k
\end{flushleft}





.





\begin{flushleft}
This is minimized by t = 2/(1 + $\gamma$), so we have
\end{flushleft}


\begin{flushleft}
x(k+1)
\end{flushleft}





=





\begin{flushleft}
x(k) $-$ t$\nabla$f (x(k) )
\end{flushleft}


\begin{flushleft}
(k)
\end{flushleft}





=





\begin{flushleft}
(1 $-$ t)x1
\end{flushleft}


\begin{flushleft}
(k)
\end{flushleft}


\begin{flushleft}
(1 $-$ $\gamma$t)$\gamma$x2
\end{flushleft}





=





\begin{flushleft}
$\gamma$$-$1
\end{flushleft}


\begin{flushleft}
$\gamma$+1
\end{flushleft}





=





\begin{flushleft}
$\gamma$$-$1
\end{flushleft}


\begin{flushleft}
$\gamma$+1
\end{flushleft}





\begin{flushleft}
(k)
\end{flushleft}





\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
(k)
\end{flushleft}


\begin{flushleft}
$-$x2
\end{flushleft}





\begin{flushleft}
k+1
\end{flushleft}





\begin{flushleft}
$\gamma$
\end{flushleft}


\begin{flushleft}
($-$1)k
\end{flushleft}





.





\begin{flushleft}
9.7 Let ∆xsd and ∆xsd be the normalized and unnormalized steepest descent directions at x,
\end{flushleft}


\begin{flushleft}
for the norm · . Prove the following identities.
\end{flushleft}


\begin{flushleft}
(a) $\nabla$f (x)T ∆xnsd = $-$ $\nabla$f (x)
\end{flushleft}





\begin{flushleft}
(b) $\nabla$f (x)T ∆xsd = $-$ $\nabla$f (x)
\end{flushleft}





∗.


2


∗.





\begin{flushleft}
(c) ∆xsd = argminv ($\nabla$f (x)T v + (1/2) v
\end{flushleft}





2





).





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) By definition of dual norm.
\end{flushleft}


\begin{flushleft}
(b) By (a) and the definition of ∆xsd .
\end{flushleft}


\begin{flushleft}
(c) Suppose v = tw with w = 1 and w fixed. We optimize over t and w separately.
\end{flushleft}


\begin{flushleft}
We have
\end{flushleft}


\begin{flushleft}
$\nabla$f (x)T v + (1/2) v 2 = t$\nabla$f (x)T w + t2 /2.
\end{flushleft}


\begin{flushleft}
Minimizing over t $\geq$ 0 gives the optimum tˆ = $-$$\nabla$f (x)T w if $\nabla$f (x)T w $\leq$ 0, and
\end{flushleft}


\begin{flushleft}
tˆ = 0 otherwise. This shows that we should choose w such that $\nabla$f (x)T w $\leq$ 0.
\end{flushleft}


\begin{flushleft}
Substituting tˆ = $-$$\nabla$f (x)T w gives
\end{flushleft}


\begin{flushleft}
tˆ$\nabla$f (x)T w + tˆ2 /2 = $-$($\nabla$f (x)T w)2 /2.
\end{flushleft}


\begin{flushleft}
We now minimize over w, i.e., solve
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$($\nabla$f (x)T w)2 /2
\end{flushleft}


\begin{flushleft}
w = 1.
\end{flushleft}





\begin{flushleft}
The solution is w = ∆xnsd by definition. This gives
\end{flushleft}


\begin{flushleft}
tˆ = $-$∆xTnsd $\nabla$f (x) = $\nabla$f (x)
\end{flushleft}





∗,





\begin{flushleft}
and v = tˆw = ∆xsd .
\end{flushleft}


\begin{flushleft}
9.8 Steepest descent method in $\infty$ -norm. Explain how to find a steepest descent direction in
\end{flushleft}


\begin{flushleft}
the $\infty$ -norm, and give a simple interpretation.
\end{flushleft}


\begin{flushleft}
Solution. The normalized steepest descent direction is given by
\end{flushleft}


\begin{flushleft}
∆xnsd = $-$ sign($\nabla$f (x)),
\end{flushleft}


\begin{flushleft}
where the sign is taken componentwise. Interpretation: If the partial derivative with
\end{flushleft}


\begin{flushleft}
respect to xk is positive we take a step that reduces xk ; if it is positive, we take a step
\end{flushleft}


\begin{flushleft}
that increases xk .
\end{flushleft}


\begin{flushleft}
The unnormalized steepest descent direction is given by
\end{flushleft}


\begin{flushleft}
∆xsd = $-$ $\nabla$f (x)
\end{flushleft}





1





\begin{flushleft}
sign($\nabla$f (x)).
\end{flushleft}





\begin{flushleft}
Newton's method
\end{flushleft}


\begin{flushleft}
9.9 Newton decrement. Show that the Newton decrement $\lambda$(x) satisfies
\end{flushleft}


\begin{flushleft}
$\lambda$(x) =
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
v T $\nabla$2 f (x)v=1
\end{flushleft}





\begin{flushleft}
($-$v T $\nabla$f (x)) = sup
\end{flushleft}


\begin{flushleft}
v=0
\end{flushleft}





\begin{flushleft}
$-$v T $\nabla$f (x)
\end{flushleft}





\begin{flushleft}
(v T $\nabla$2 f (x)v)1/2
\end{flushleft}





.





\begin{flushleft}
Solution. The first expression follows from a a change of variables
\end{flushleft}


\begin{flushleft}
w = $\nabla$2 f (x)1/2 v,
\end{flushleft}


\begin{flushleft}
and from
\end{flushleft}





\begin{flushleft}
v = $\nabla$2 f (x)$-$1/2 w
\end{flushleft}





\begin{flushleft}
sup $-$wT $\nabla$2 f (x)$-$1/2 $\nabla$f (x) = $\nabla$f (x)$-$1/2 $\nabla$f (x)
\end{flushleft}





2





\begin{flushleft}
= $\lambda$(x).
\end{flushleft}





\begin{flushleft}
w 2 =1
\end{flushleft}





\begin{flushleft}
The second expression follows immediately from the first.
\end{flushleft}


\begin{flushleft}
9.10 The pure Newton method. Newton's method with fixed step size t = 1 can diverge if the
\end{flushleft}


\begin{flushleft}
initial point is not close to x . In this problem we consider two examples.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) f (x) = log(ex + e$-$x ) has a unique minimizer x = 0. Run Newton's method with
\end{flushleft}


\begin{flushleft}
fixed step size t = 1, starting at x(0) = 1 and at x(0) = 1.1.
\end{flushleft}


\begin{flushleft}
(b) f (x) = $-$ log x + x has a unique minimizer x = 1. Run Newton's method with fixed
\end{flushleft}


\begin{flushleft}
step size t = 1, starting at x(0) = 3.
\end{flushleft}


\begin{flushleft}
Plot f and f , and show the first few iterates.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
$\bullet$ f (x) = log(ex + e$-$x ) is a smooth convex function, with a unique minimum at
\end{flushleft}


\begin{flushleft}
the origin. The pure Newton method started at x(0) = 1 produces the following
\end{flushleft}


\begin{flushleft}
sequence.
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


1


2


3


4


5





\begin{flushleft}
x(k)
\end{flushleft}


$-$8.134 · 10$-$01


4.094 · 10$-$01


$-$4.730 · 10$-$02


7.060 · 10$-$05


$-$2.346 · 10$-$13





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}


4.338 · 10$-$1


2.997 · 10$-$1


8.156 · 10$-$2


1.118 · 10$-$3


2.492 · 10$-$9





\begin{flushleft}
Started at x(0) = 1.1, the method diverges.
\end{flushleft}


\begin{flushleft}
k
\end{flushleft}


1


2


3


4


5





\begin{flushleft}
x(k)
\end{flushleft}


$-$1.129 · 100


1.234 · 100


$-$1.695 · 100


5.715 · 100


$-$2.302 · 104





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}


5.120 · 10$-$1


5.349 · 10$-$1


6.223 · 10$-$1


1.035 · 100


2.302 · 104





\begin{flushleft}
$\bullet$ f (x) = $-$ log x + x is smooth and convex on dom f = \{x | x $>$ 0\}, with a unique
\end{flushleft}


\begin{flushleft}
minimizer at x = 1. The pure Newton method started at x(0) = 3 gives as first
\end{flushleft}


\begin{flushleft}
iterate
\end{flushleft}


\begin{flushleft}
x(1) = 3 $-$ f (3)/f (3) = $-$3
\end{flushleft}


\begin{flushleft}
which lies outside dom f .
\end{flushleft}





\begin{flushleft}
9.11 Gradient and Newton methods for composition functions. Suppose $\phi$ : R $\rightarrow$ R is increasing
\end{flushleft}


\begin{flushleft}
and convex, and f : Rn $\rightarrow$ R is convex, so g(x) = $\phi$(f (x)) is convex. (We assume that
\end{flushleft}


\begin{flushleft}
f and g are twice differentiable.) The problems of minimizing f and minimizing g are
\end{flushleft}


\begin{flushleft}
clearly equivalent.
\end{flushleft}


\begin{flushleft}
Compare the gradient method and Newton's method, applied to f and g. How are the
\end{flushleft}


\begin{flushleft}
search directions related? How are the methods related if an exact line search is used?
\end{flushleft}


\begin{flushleft}
Hint. Use the matrix inversion lemma (see §C.4.3).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Gradient method. The gradients are positive multiples
\end{flushleft}


\begin{flushleft}
$\nabla$g(x) = $\phi$ (f (x))$\nabla$f (x),
\end{flushleft}


\begin{flushleft}
so with exact line search the iterates are identical for f and g. With backtracking
\end{flushleft}


\begin{flushleft}
there can be big differences.
\end{flushleft}


\begin{flushleft}
(b) Newton method. The Hessian of g is
\end{flushleft}


\begin{flushleft}
$\phi$ (f (x))$\nabla$f (x)$\nabla$f (x)T + $\phi$ (f (x))$\nabla$2 f (x),
\end{flushleft}


\begin{flushleft}
so the Newton direction for g is
\end{flushleft}


\begin{flushleft}
$-$ $\phi$ (f (x))$\nabla$f (x)$\nabla$f (x)T + $\phi$ (f (x))$\nabla$2 f (x)
\end{flushleft}





$-$1





\begin{flushleft}
$\nabla$f (x).
\end{flushleft}





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
From the matrix inversion lemma, we see that this is some positive multiple of the
\end{flushleft}


\begin{flushleft}
Newton direction for f . Hence with exact line search, the iterates are identical.
\end{flushleft}


\begin{flushleft}
Without exact line search, e.g., with Newton step one, there can be big differences.
\end{flushleft}


\begin{flushleft}
Take e.g., f (x) = x2 and $\phi$(x) = x2 for x $\geq$ 0.
\end{flushleft}





\begin{flushleft}
9.12 Trust region Newton method. If $\nabla$2 f (x) is singular (or very ill-conditioned), the Newton
\end{flushleft}


\begin{flushleft}
step ∆xnt = $-$$\nabla$2 f (x)$-$1 $\nabla$f (x) is not well defined. Instead we can define a search direction
\end{flushleft}


\begin{flushleft}
∆xtr as the solution of
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
(1/2)v T Hv + g T v
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
v 2 $\leq$ $\gamma$,
\end{flushleft}





\begin{flushleft}
where H = $\nabla$2 f (x), g = $\nabla$f (x), and $\gamma$ is a positive constant. The point x+∆xtr minimizes
\end{flushleft}


\begin{flushleft}
the second-order approximation of f at x, subject to the constraint that (x+∆x tr )$-$x 2 $\leq$
\end{flushleft}


\begin{flushleft}
$\gamma$. The set \{v | v 2 $\leq$ $\gamma$\} is called the trust region. The parameter $\gamma$, the size of the trust
\end{flushleft}


\begin{flushleft}
region, reflects our confidence in the second-order model.
\end{flushleft}


\begin{flushleft}
Show that ∆xtr minimizes
\end{flushleft}


\begin{flushleft}
(1/2)v T Hv + g T v + $\beta$ˆ v
\end{flushleft}





2


2,





\begin{flushleft}
ˆ This quadratic function can be interpreted as a regularized quadratic model
\end{flushleft}


\begin{flushleft}
for some $\beta$.
\end{flushleft}


\begin{flushleft}
for f around x.
\end{flushleft}


\begin{flushleft}
Solution. This follows from duality. If we associate a multiplier $\beta$ with the constraint,
\end{flushleft}


\begin{flushleft}
then the optimal v must be a minimizer of the Lagrangian
\end{flushleft}


\begin{flushleft}
(1/2)v T Hv + g T v + $\beta$( v
\end{flushleft}





2


2





\begin{flushleft}
$-$ $\gamma$).
\end{flushleft}





\begin{flushleft}
The value of $\beta$ˆ can be determined as follows. The optimality conditions are
\end{flushleft}


\begin{flushleft}
Hv + g + $\beta$v = 0,
\end{flushleft}





\begin{flushleft}
v T v $\leq$ $\gamma$,
\end{flushleft}





\begin{flushleft}
$\beta$ $\geq$ 0,
\end{flushleft}





\begin{flushleft}
$\beta$($\gamma$ $-$ v T v) = 0.
\end{flushleft}





\begin{flushleft}
$\bullet$ If H
\end{flushleft}


\begin{flushleft}
0, then H + $\beta$I is invertible for all $\beta$ $\geq$ 0, so from the first equation,
\end{flushleft}


\begin{flushleft}
v = $-$(H + $\beta$I)$-$1 g. The norm of v is a decreasing function of $\beta$. If H $-$1 g 2 $\leq$ $\gamma$,
\end{flushleft}


\begin{flushleft}
then the optimal solution is
\end{flushleft}


\begin{flushleft}
v = $-$H $-$1 g,
\end{flushleft}





\begin{flushleft}
$\beta$ = 0.
\end{flushleft}





\begin{flushleft}
If H $-$1 g 2 $>$ $\gamma$, then $\beta$ is the unique positive solution of the equation
\end{flushleft}


\begin{flushleft}
$\beta$I)$-$1 g 2 = $\gamma$.
\end{flushleft}





\begin{flushleft}
(H +
\end{flushleft}





\begin{flushleft}
$\bullet$ If H is singular, then we have $\beta$ = 0 only if g $\in$ R(H) and H \dag{} g 2 $\leq$ $\gamma$.
\end{flushleft}


\begin{flushleft}
Otherwise, $\beta$ is the unique solution positive solution of the equation (H+$\beta$I)$-$1 g
\end{flushleft}


\begin{flushleft}
$\gamma$.
\end{flushleft}





2





=





\begin{flushleft}
Self-concordance
\end{flushleft}


\begin{flushleft}
9.13 Self-concordance and the inverse barrier.
\end{flushleft}


\begin{flushleft}
(a) Show that f (x) = 1/x with domain (0, 8/9) is self-concordant.
\end{flushleft}


\begin{flushleft}
(b) Show that the function
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
f (x) = $\alpha$
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





1


\begin{flushleft}
bi $-$ aTi x
\end{flushleft}





\begin{flushleft}
aTi x
\end{flushleft}





\begin{flushleft}
$<$ bi , i = 1, . . . , m\}, is self-concordant if dom f is
\end{flushleft}


\begin{flushleft}
with dom f = \{x $\in$ R |
\end{flushleft}


\begin{flushleft}
bounded and
\end{flushleft}


\begin{flushleft}
$\alpha$ $>$ (9/8) max
\end{flushleft}


\begin{flushleft}
sup (bi $-$ aTi x).
\end{flushleft}


\begin{flushleft}
i=1,...,m x$\in$dom f
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(a) The derivatives are
\end{flushleft}


\begin{flushleft}
f (x) = $-$1/x2 ,
\end{flushleft}





\begin{flushleft}
f (x) = 2/x3 ,
\end{flushleft}





\begin{flushleft}
so the self-concordance condition is
\end{flushleft}


2


6


$\leq$2


\begin{flushleft}
x4
\end{flushleft}


\begin{flushleft}
x3
\end{flushleft}


\begin{flushleft}
which holds if
\end{flushleft}





$\surd$





$\surd$


\begin{flushleft}
x $\leq$ 4 2/6 =
\end{flushleft}





3/2





=





\begin{flushleft}
f (x) = $-$6/x4 ,
\end{flushleft}


$\surd$


4 2


$\surd$ .


\begin{flushleft}
x4 x
\end{flushleft}





8/9.





\begin{flushleft}
(b) If we make an affine change of variables yi = 8(bi $-$ aTi x)/(9$\alpha$), then yi $<$ 8/9 for all
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
x $\in$ dom f . The function f reduces to
\end{flushleft}


\begin{flushleft}
(1/yi ), which is self-concordant by the
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
result in (a).
\end{flushleft}


\begin{flushleft}
9.14 Composition with logarithm. Let g : R $\rightarrow$ R be a convex function with dom g = R ++ ,
\end{flushleft}


\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
g (x)
\end{flushleft}


\begin{flushleft}
|g (x)| $\leq$ 3
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
for all x. Prove that f (x) = $-$ log($-$g(x)) $-$ log x is self-concordant on \{x | x $>$ 0, g(x) $<$
\end{flushleft}


\begin{flushleft}
0\}. Hint. Use the inequality
\end{flushleft}


3


3 2


\begin{flushleft}
rp + q 3 + p2 q + r 3 $\leq$ 1
\end{flushleft}


2


2


\begin{flushleft}
which holds for p, q, r $\in$ R+ with p2 + q 2 + r2 = 1.
\end{flushleft}


\begin{flushleft}
Solution. The derivatives of f are
\end{flushleft}


$-$





\begin{flushleft}
g (x)
\end{flushleft}


1


$-$


\begin{flushleft}
g(x)
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
f (x)
\end{flushleft}





=





\begin{flushleft}
f (x)
\end{flushleft}





=





\begin{flushleft}
f (x)
\end{flushleft}





=





$-$





\begin{flushleft}
|f (x)|
\end{flushleft}





$\leq$





\begin{flushleft}
|g (x)|
\end{flushleft}


+2


\begin{flushleft}
$-$g(x)
\end{flushleft}





\begin{flushleft}
|g (x)|
\end{flushleft}


\begin{flushleft}
$-$g(x)
\end{flushleft}





$\leq$





\begin{flushleft}
3g (x)
\end{flushleft}


+2


\begin{flushleft}
$-$xg(x)
\end{flushleft}





\begin{flushleft}
|g (x)|
\end{flushleft}


\begin{flushleft}
$-$g(x)
\end{flushleft}





\begin{flushleft}
g (x)
\end{flushleft}


\begin{flushleft}
g(x)
\end{flushleft}





2





$-$





\begin{flushleft}
g (x)
\end{flushleft}


$-$2


\begin{flushleft}
g(x)
\end{flushleft}





\begin{flushleft}
g (x)
\end{flushleft}


1


+ 2


\begin{flushleft}
g(x)
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
g (x)
\end{flushleft}


\begin{flushleft}
g(x)
\end{flushleft}





3





+





\begin{flushleft}
3g (x)g (x)
\end{flushleft}


2


$-$ 3.


\begin{flushleft}
g(x)2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





+





\begin{flushleft}
3g (x)|g (x)|
\end{flushleft}


2


+ 3


\begin{flushleft}
g(x)2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





+





\begin{flushleft}
3g (x)|g (x)|
\end{flushleft}


2


+ 3.


\begin{flushleft}
g(x)2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
We have
\end{flushleft}


3





3





\begin{flushleft}
We will show that
\end{flushleft}


\begin{flushleft}
3g (x)
\end{flushleft}


+2


\begin{flushleft}
$-$xg(x)
\end{flushleft}





\begin{flushleft}
|g (x)|
\end{flushleft}


\begin{flushleft}
$-$g(x)
\end{flushleft}





3





\begin{flushleft}
g (x)
\end{flushleft}


\begin{flushleft}
g(x)
\end{flushleft}





\begin{flushleft}
3g (x)|g (x)|
\end{flushleft}


2


+


+ 3 $\leq$2


\begin{flushleft}
g(x)2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





2





3/2





\begin{flushleft}
g (x)
\end{flushleft}


1


$-$


+ 2


\begin{flushleft}
g(x)
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
To simplify the formulas we define
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





=





\begin{flushleft}
q
\end{flushleft}





=





\begin{flushleft}
r
\end{flushleft}





=





\begin{flushleft}
($-$g (x)/g(x))
\end{flushleft}





1/2





\begin{flushleft}
($-$g (x)/g(x) + g (x)2 /g(x)2 + 1/x2 )1/2
\end{flushleft}


\begin{flushleft}
$-$|g (x)|/g(x)
\end{flushleft}


\begin{flushleft}
($-$g (x)/g(x) + g (x)2 /g(x)2 + 1/x2 )1/2
\end{flushleft}


\begin{flushleft}
1/x
\end{flushleft}


\begin{flushleft}
($-$g (x)/g(x) + g (x)2 /g(x)2 + 1/x2 )1/2
\end{flushleft}





.





.





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
Note that p $\geq$ 0, q $\geq$ 0, r $\geq$ 0, and p2 + q 2 + r2 = 1. With these substitutions, the
\end{flushleft}


\begin{flushleft}
inequality reduces to the inequality
\end{flushleft}


3


3 2


\begin{flushleft}
rp + q 3 + p2 q + r 3 $\leq$ 1
\end{flushleft}


2


2


\begin{flushleft}
in the hint.
\end{flushleft}


\begin{flushleft}
For completeness we also derive the inequality:
\end{flushleft}


3 2


3


\begin{flushleft}
rp + q 3 + p2 q + r 3
\end{flushleft}


2


2





=


=


=


$\leq$





3


\begin{flushleft}
(r + q)( p2 + q 2 + r2 $-$ qr)
\end{flushleft}


2


1


3


\begin{flushleft}
(r + q)( (p2 + q 2 + r2 ) $-$ (r + q)2 )
\end{flushleft}


2


2


1


2


\begin{flushleft}
(r + q)(3 $-$ (r + q) )
\end{flushleft}


2


1.





\begin{flushleft}
On the last line we use the inequality (1/2)x(3 $-$ x2 ) $\leq$ 1 for 0 $\leq$ x $\leq$ 1, which is easily
\end{flushleft}


\begin{flushleft}
verified.
\end{flushleft}


\begin{flushleft}
9.15 Prove that the following functions are self-concordant. In your proof, restrict the function
\end{flushleft}


\begin{flushleft}
to a line, and apply the composition with logarithm rule.
\end{flushleft}


\begin{flushleft}
(a) f (x, y) = $-$ log(y 2 $-$ xT x) on \{(x, y) | x
\end{flushleft}


\begin{flushleft}
(b) f (x, y) = $-$2 log y $-$ log(y
\end{flushleft}





\begin{flushleft}
2/p
\end{flushleft}





2





\begin{flushleft}
$<$ y\}.
\end{flushleft}





2





\begin{flushleft}
$-$ x ), with p $\geq$ 1, on \{(x, y) $\in$ R2 | |x|p $<$ y\}.
\end{flushleft}





\begin{flushleft}
(c) f (x, y) = $-$ log y $-$ log(log y $-$ x) on \{(x, y) | ex $<$ y\}.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) To prove this, we write f as f (x, y) = $-$ log y $-$ log(y $-$ xT x/y) and restrict the
\end{flushleft}


\begin{flushleft}
function to a line x = x
\end{flushleft}


\begin{flushleft}
ˆ + tv, y = yˆ + tw,
\end{flushleft}


\begin{flushleft}
f (ˆ
\end{flushleft}


\begin{flushleft}
x + tv, yˆ + tw) = $-$ log
\end{flushleft}





\begin{flushleft}
yˆ + tw $-$
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
ˆT x
\end{flushleft}


ˆ


\begin{flushleft}
2tˆ
\end{flushleft}


\begin{flushleft}
xT v
\end{flushleft}


\begin{flushleft}
t2 v T v
\end{flushleft}


$-$


$-$


\begin{flushleft}
yˆ + tw
\end{flushleft}


\begin{flushleft}
yˆ + tw
\end{flushleft}


\begin{flushleft}
yˆ + tw
\end{flushleft}





\begin{flushleft}
$-$ log(ˆ
\end{flushleft}


\begin{flushleft}
y + tw).
\end{flushleft}





\begin{flushleft}
If w = 0, the argument of the log reduces to a quadratic function of t, which is the
\end{flushleft}


\begin{flushleft}
case considered in example 9.6.
\end{flushleft}


\begin{flushleft}
Otherwise, we can use y instead of t as variable (i.e., make a change of variables
\end{flushleft}


\begin{flushleft}
t = (y $-$ yˆ)/w). We obtain
\end{flushleft}


\begin{flushleft}
f (ˆ
\end{flushleft}


\begin{flushleft}
x + tv, yˆ + tw) = $-$ log($\alpha$ + $\beta$y $-$ $\gamma$/y) $-$ log y
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
$\alpha$=2
\end{flushleft}





\begin{flushleft}
yˆv T v
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
ˆT v
\end{flushleft}


$-$


2


,


\begin{flushleft}
w2
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
$\beta$ =1$-$
\end{flushleft}





\begin{flushleft}
vT v
\end{flushleft}


,


\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
$\gamma$=x
\end{flushleft}


\begin{flushleft}
ˆT x
\end{flushleft}


ˆ$-$2





\begin{flushleft}
yˆx
\end{flushleft}


\begin{flushleft}
ˆT v
\end{flushleft}


\begin{flushleft}
yˆ2 v T v
\end{flushleft}


+


.


\begin{flushleft}
w
\end{flushleft}


\begin{flushleft}
w2
\end{flushleft}





\begin{flushleft}
Defining g(y) = $-$$\alpha$ $-$ $\beta$y + $\gamma$/y, we have
\end{flushleft}


\begin{flushleft}
f (ˆ
\end{flushleft}


\begin{flushleft}
x + tv, yˆ + tw) = $-$ log($-$g(y)) $-$ log y
\end{flushleft}


\begin{flushleft}
The function g is convex (since $\gamma$ $>$ 0) and satisfies (9.43) because
\end{flushleft}


\begin{flushleft}
g (y) = $-$6$\gamma$/y 4 ,
\end{flushleft}





\begin{flushleft}
g (y) = 2$\gamma$/y 3 .
\end{flushleft}





\begin{flushleft}
(b) We can write f as a sum of two functions
\end{flushleft}


\begin{flushleft}
f1 (x, y) = $-$ log y $-$ log(y 1/p $-$ x),
\end{flushleft}





\begin{flushleft}
f2 (x, y) = $-$ log y $-$ log(y 1/p + x).
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We restrict the functions to a line x = x
\end{flushleft}


\begin{flushleft}
ˆ + tv, y = yˆ + tw. If w = 0, both functions
\end{flushleft}


\begin{flushleft}
reduce to logs of affine functions, so they are self-concordant. If w = 0, we can use
\end{flushleft}


\begin{flushleft}
y as variable (i.e., make a change of variables t = (y $-$ yˆ)/w), and reduce the proof
\end{flushleft}


\begin{flushleft}
to showing that the function
\end{flushleft}


\begin{flushleft}
$-$ log y $-$ log(y 1/p + ay + b)
\end{flushleft}


\begin{flushleft}
is self-concordant. This is true because g(x) = $-$ax $-$ b $-$ x1/p is convex, with
\end{flushleft}


\begin{flushleft}
derivatives
\end{flushleft}


\begin{flushleft}
g (x) = $-$
\end{flushleft}





\begin{flushleft}
(1 $-$ p)(1 $-$ 2p) 1/p$-$3
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


,


\begin{flushleft}
p3
\end{flushleft}





\begin{flushleft}
g (x) =
\end{flushleft}





\begin{flushleft}
p $-$ 1 1/p$-$2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


,


\begin{flushleft}
p2
\end{flushleft}





\begin{flushleft}
so the inequality (9.43) reduces
\end{flushleft}


\begin{flushleft}
(p $-$ 1)(2p $-$ 1)
\end{flushleft}


\begin{flushleft}
p$-$1
\end{flushleft}


$\leq$3 2 ,


\begin{flushleft}
p3
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i.e., p $\geq$ $-$1.
\end{flushleft}





\begin{flushleft}
(c) We restrict the function to a line x = x
\end{flushleft}


\begin{flushleft}
ˆ + tv, y = yˆ + tw:
\end{flushleft}


\begin{flushleft}
f (ˆ
\end{flushleft}


\begin{flushleft}
x + tv, yˆ + tw) = $-$ log(ˆ
\end{flushleft}


\begin{flushleft}
y + tw) $-$ log(log(ˆ
\end{flushleft}


\begin{flushleft}
y + tw) $-$ x
\end{flushleft}


\begin{flushleft}
ˆ $-$ tw).
\end{flushleft}


\begin{flushleft}
If w = 0 the function is obviously self-concordant. If w = 0, we use y as variable
\end{flushleft}


\begin{flushleft}
(i.e., use a change of variables t = (y $-$ yˆ)/w), and the function reduces to
\end{flushleft}


\begin{flushleft}
$-$ log y $-$ log(log y $-$ a $-$ by),
\end{flushleft}


\begin{flushleft}
so we need to show that g(y) = a + by $-$ log y satisfies the inequality (9.43). We have
\end{flushleft}


\begin{flushleft}
g (y) = $-$
\end{flushleft}





2


,


\begin{flushleft}
y3
\end{flushleft}





\begin{flushleft}
g (y) =
\end{flushleft}





1


,


\begin{flushleft}
y2
\end{flushleft}





\begin{flushleft}
so (9.43) becomes
\end{flushleft}


2


3


$\leq$ 3.


\begin{flushleft}
y3
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}


\begin{flushleft}
9.16 Let f : R $\rightarrow$ R be a self-concordant function.
\end{flushleft}


\begin{flushleft}
(a) Suppose f (x) = 0. Show that the self-concordance condition (9.41) can be expressed as
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
f (x)$-$1/2 $\leq$ 1.
\end{flushleft}


\begin{flushleft}
dx
\end{flushleft}


\begin{flushleft}
Find the {`}extreme' self-concordant functions of one variable, i.e., the functions f
\end{flushleft}


\begin{flushleft}
and f˜ that satisfy
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
f (x)$-$1/2 = 1,
\end{flushleft}


\begin{flushleft}
dx
\end{flushleft}





\begin{flushleft}
d ˜
\end{flushleft}


\begin{flushleft}
f (x)$-$1/2 = $-$1,
\end{flushleft}


\begin{flushleft}
dx
\end{flushleft}





\begin{flushleft}
respectively.
\end{flushleft}


\begin{flushleft}
(b) Show that either f (x) = 0 for all x $\in$ dom f , or f (x) $>$ 0 for all x $\in$ dom f .
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We have
\end{flushleft}


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


.


\begin{flushleft}
f (x)$-$1/2 = ($-$1/2)
\end{flushleft}


\begin{flushleft}
dx
\end{flushleft}


\begin{flushleft}
f (x)3/2
\end{flushleft}


\begin{flushleft}
Integrating
\end{flushleft}





\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
f (x)$-$1/2 = 1
\end{flushleft}


\begin{flushleft}
dx
\end{flushleft}





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
gives f (x) = $-$ log(x + c0 ) + c1 x + c2 . Integrating
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
f (x)$-$1/2 = $-$1
\end{flushleft}


\begin{flushleft}
dx
\end{flushleft}


\begin{flushleft}
gives
\end{flushleft}


\begin{flushleft}
f (x) = $-$ log($-$x + c0 ) + c1 x + c2 .
\end{flushleft}


\begin{flushleft}
(b) Suppose f (0) $>$ 0, f (¯
\end{flushleft}


\begin{flushleft}
x) = 0 for x
\end{flushleft}


\begin{flushleft}
¯ $>$ 0, and f (x) $>$ 0 on the interval between 0
\end{flushleft}


\begin{flushleft}
and x
\end{flushleft}


\begin{flushleft}
¯. The inequality
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
f (x)$-$1/2 $\leq$ 1
\end{flushleft}


$-$1 $\leq$


\begin{flushleft}
dx
\end{flushleft}


\begin{flushleft}
holds for x between 0 and x
\end{flushleft}


\begin{flushleft}
¯. Integrating gives
\end{flushleft}


\begin{flushleft}
f (¯
\end{flushleft}


\begin{flushleft}
x)$-$1/2 $-$ f (0)$-$1/2 $\leq$ x
\end{flushleft}


¯


\begin{flushleft}
which contradicts f (¯
\end{flushleft}


\begin{flushleft}
x) = 0.
\end{flushleft}


\begin{flushleft}
9.17 Upper and lower bounds on the Hessian of a self-concordant function.
\end{flushleft}


\begin{flushleft}
(a) Let f : R2 $\rightarrow$ R be a self-concordant function. Show that
\end{flushleft}


\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$ 3 xi
\end{flushleft}





$\leq$





2





\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x2i $\partial$xj
\end{flushleft}





$\leq$





2





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x2i
\end{flushleft}





3/2





,





\begin{flushleft}
i = 1, 2,
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x2j
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x2i
\end{flushleft}





1/2





,





\begin{flushleft}
i=j
\end{flushleft}





\begin{flushleft}
for all x $\in$ dom f .
\end{flushleft}


\begin{flushleft}
Hint. If h : R2 × R2 × R2 $\rightarrow$ R is a symmetric trilinear form, i.e.,
\end{flushleft}


\begin{flushleft}
h(u, v, w)
\end{flushleft}





=





\begin{flushleft}
a1 u1 v1 w1 + a2 (u1 v1 w2 + u1 v2 w1 + u2 v1 w1 )
\end{flushleft}


\begin{flushleft}
+ a3 (u1 v2 w2 + u2 v1 w1 + u2 v2 w1 ) + a4 u2 v2 w2 ,
\end{flushleft}





\begin{flushleft}
then
\end{flushleft}


\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
u,v,w=0
\end{flushleft}





\begin{flushleft}
h(u, v, w)
\end{flushleft}


\begin{flushleft}
u 2 v 2 w
\end{flushleft}





\begin{flushleft}
= sup
\end{flushleft}


2





\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
h(u, u, u)
\end{flushleft}


.


\begin{flushleft}
u 32
\end{flushleft}





\begin{flushleft}
Solution. We first note the following generalization of the result in the hint. Suppose A $\in$ S2++ , and h is symmetric and trilinear. Then h(A$-$1/2 u, A$-$1/2 v, A$-$1/2 w)
\end{flushleft}


\begin{flushleft}
is a symmetric trilinear function, so
\end{flushleft}


\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
u,v,w=0
\end{flushleft}





\begin{flushleft}
h(A$-$1/2 u, A$-$1/2 v, A$-$1/2 w)
\end{flushleft}


\begin{flushleft}
h(A$-$1/2 u, A$-$1/2 u, A$-$1/2 u)
\end{flushleft}


,


\begin{flushleft}
= sup
\end{flushleft}


\begin{flushleft}
u 2 v 2 w 2
\end{flushleft}


\begin{flushleft}
u 32
\end{flushleft}


\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
u,v,w=0
\end{flushleft}





\begin{flushleft}
h(u, u, u)
\end{flushleft}


\begin{flushleft}
h(u, v, w)
\end{flushleft}


\begin{flushleft}
= sup T
\end{flushleft}


.


\begin{flushleft}
(uT Au)1/2 (v T Av)1/2 (wT Aw)1/2
\end{flushleft}


\begin{flushleft}
(u
\end{flushleft}


\begin{flushleft}
Au)3/2
\end{flushleft}


\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
By definition, f : Rn $\rightarrow$ R is self-concordant if and only if
\end{flushleft}


\begin{flushleft}
uT
\end{flushleft}





\begin{flushleft}
d 2
\end{flushleft}


\begin{flushleft}
$\nabla$ f (ˆ
\end{flushleft}


\begin{flushleft}
x + tu)
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
u $\leq$ 2(uT $\nabla$2 f (ˆ
\end{flushleft}


\begin{flushleft}
x)u)3/2 .
\end{flushleft}





\begin{flushleft}
for all u and all x
\end{flushleft}


\begin{flushleft}
ˆ $\in$ dom f . If n = 2 this means that
\end{flushleft}


\begin{flushleft}
|h(u, u, u)| $\leq$ (uT Au)3/2
\end{flushleft}





\begin{flushleft}
(9.17.A)
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
for all u, where
\end{flushleft}


\begin{flushleft}
h(u, v, w)
\end{flushleft}





=


=





\begin{flushleft}
d 2
\end{flushleft}


\begin{flushleft}
$\nabla$ f (ˆ
\end{flushleft}


\begin{flushleft}
x + tv)
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}


\begin{flushleft}
t=0
\end{flushleft}


\begin{flushleft}
$\partial$ 3 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
$\partial$ 3 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
+ (u1 v1 w2 + u1 v2 w1 + u2 v1 w1 ) 2
\end{flushleft}


\begin{flushleft}
u 1 v1 w1
\end{flushleft}


3


\begin{flushleft}
$\partial$x1
\end{flushleft}


\begin{flushleft}
$\partial$x1 $\partial$x2
\end{flushleft}





\begin{flushleft}
uT
\end{flushleft}





\begin{flushleft}
+ (u1 v2 w2 + u2 v1 w2 + u2 v2 w1 )
\end{flushleft}


\begin{flushleft}
uT Au
\end{flushleft}





=





\begin{flushleft}
u21
\end{flushleft}





\begin{flushleft}
$\partial$ 3 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
$\partial$ 3 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
+ u 2 v2 w2
\end{flushleft}


2


\begin{flushleft}
$\partial$x1 $\partial$x2
\end{flushleft}


\begin{flushleft}
$\partial$x32
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
$\partial$ 2 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
$\partial$ 2 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
+ 2u1 u2
\end{flushleft}


,


\begin{flushleft}
+ u22
\end{flushleft}


2


\begin{flushleft}
$\partial$x1 $\partial$x2
\end{flushleft}


\begin{flushleft}
$\partial$x1
\end{flushleft}


\begin{flushleft}
$\partial$x22
\end{flushleft}





\begin{flushleft}
i.e., A = $\nabla$2 f (ˆ
\end{flushleft}


\begin{flushleft}
x). In other words,
\end{flushleft}


\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
h(u, u, u)
\end{flushleft}


$\leq$ 2,


\begin{flushleft}
(uT Au)3/2
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}


\begin{flushleft}
u=0
\end{flushleft}





\begin{flushleft}
$-$h(u, u, u)
\end{flushleft}


$\leq$ 2.


\begin{flushleft}
(uT Au)3/2
\end{flushleft}





\begin{flushleft}
Applying (9.17.A) (to h and $-$h), we also have
\end{flushleft}


\begin{flushleft}
|h(u, v, u)| $\leq$ 2(uT Au)(v T Av)1/2
\end{flushleft}





\begin{flushleft}
(9.17.B)
\end{flushleft}





\begin{flushleft}
for all u and v. The inequalities
\end{flushleft}


\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


$\leq$2


\begin{flushleft}
$\partial$ 3 x1
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x21
\end{flushleft}





3/2





,





\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


$\leq$2


\begin{flushleft}
$\partial$ 3 x2
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x22
\end{flushleft}





3/2





,





\begin{flushleft}
follow from (9.17.B) by choosing u = v = (1, 0) and u = v = (0, 1), respectively.
\end{flushleft}


\begin{flushleft}
The inequalities
\end{flushleft}


\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


$\leq$2


2


\begin{flushleft}
$\partial$x1 $\partial$x2
\end{flushleft}


\begin{flushleft}
$\partial$x21
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x22
\end{flushleft}





1/2





,





\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


$\leq$2


\begin{flushleft}
$\partial$x1 $\partial$x22
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x21
\end{flushleft}





1/2





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


,


\begin{flushleft}
$\partial$x21
\end{flushleft}





\begin{flushleft}
follow by choosing v = (1, 0), w = (0, 1), and v = (0, 1), w = (1, 0), respectively.
\end{flushleft}


\begin{flushleft}
To complete the proof we relax the assumption that $\nabla$2 f (ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
0. Note that if f
\end{flushleft}


\begin{flushleft}
is self-concordant then f (x) + xT x is self-concordant for all $\geq$ 0. Applying the
\end{flushleft}


\begin{flushleft}
inequalities to f (x) + xT x gives
\end{flushleft}


\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


$\leq$2


\begin{flushleft}
$\partial$ 3 xi
\end{flushleft}


\begin{flushleft}
for all
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x2i
\end{flushleft}





3/2





+ ,





\begin{flushleft}
$\partial$ 3 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


$\leq$2


2


\begin{flushleft}
$\partial$xi $\partial$xj
\end{flushleft}


\begin{flushleft}
$\partial$x2i
\end{flushleft}





\begin{flushleft}
$>$ 0. This is only possible if the inequalities hold for
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f (x)
\end{flushleft}


\begin{flushleft}
$\partial$x2j
\end{flushleft}





1/2





+





= 0.





\begin{flushleft}
(b) Let f : Rn $\rightarrow$ R be a self-concordant function. Show that the nullspace of $\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
is independent of x. Show that if f is strictly convex, then $\nabla$2 f (x) is nonsingular
\end{flushleft}


\begin{flushleft}
for all x $\in$ dom f .
\end{flushleft}


\begin{flushleft}
Hint. Prove that if w T $\nabla$2 f (x)w = 0 for some x $\in$ dom f , then w T $\nabla$2 f (y)w = 0 for
\end{flushleft}


\begin{flushleft}
all y $\in$ dom f . To show this, apply the result in (a) to the self-concordant function
\end{flushleft}


\begin{flushleft}
f˜(t, s) = f (x + t(y $-$ x) + sw).
\end{flushleft}


\begin{flushleft}
Solution. Suppose w T $\nabla$2 f (x)w = 0. We show that w T $\nabla$2 f (y)w = 0 for all y $\in$
\end{flushleft}


\begin{flushleft}
dom f .
\end{flushleft}


\begin{flushleft}
Define v = y $-$ x and let f˜ be the restriction of f to the plane through x and defined
\end{flushleft}


\begin{flushleft}
by w, v:
\end{flushleft}


\begin{flushleft}
f˜(s, t) = f (x + sw + tv).
\end{flushleft}





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
Also define
\end{flushleft}


\begin{flushleft}
g(t) = w T $\nabla$2 f (x + tv)w =
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f˜(0, t)
\end{flushleft}


.


\begin{flushleft}
$\partial$s2
\end{flushleft}





\begin{flushleft}
f˜ is a self-concordant function of two variables, so from (a),
\end{flushleft}


\begin{flushleft}
|g (t)| =
\end{flushleft}





\begin{flushleft}
$\partial$ 3 f˜(0, t)
\end{flushleft}


$\leq$2


\begin{flushleft}
$\partial$t$\partial$s2
\end{flushleft}





1/2





\begin{flushleft}
$\partial$ 2 f˜(0, t)
\end{flushleft}


\begin{flushleft}
$\partial$t2
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f˜(0, t)
\end{flushleft}


\begin{flushleft}
$\partial$s2
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f˜(0, t)
\end{flushleft}


=2


\begin{flushleft}
$\partial$s2
\end{flushleft}





1/2





\begin{flushleft}
g(t),
\end{flushleft}





\begin{flushleft}
i.e., if g(t) = 0, then
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
log g(t) $\geq$ $-$2
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f˜(0, t)
\end{flushleft}


\begin{flushleft}
$\partial$s2
\end{flushleft}





1/2





.





\begin{flushleft}
By assumption, g(0) $>$ 0 and g(t) = 0 for t = 1. Assume that g($\tau$ ) $>$ 0 for 0 $\leq$ $\tau$ $<$ t.
\end{flushleft}


\begin{flushleft}
(If not, replace t with the smallest positive t for which g(t) = 0.) Integrating the
\end{flushleft}


\begin{flushleft}
inequality above, we have
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
log(g(t)/g(0))
\end{flushleft}





$\geq$





$-$2





$\geq$





\begin{flushleft}
exp
\end{flushleft}





0





\begin{flushleft}
$\partial$ 2 f˜(0, $\tau$ )
\end{flushleft}


\begin{flushleft}
$\partial$s2
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
g(t)/g(0)
\end{flushleft}





$-$2





0





1/2





\begin{flushleft}
d$\tau$
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f˜(0, $\tau$ )
\end{flushleft}


\begin{flushleft}
$\partial$s2
\end{flushleft}





1/2





\begin{flushleft}
d$\tau$
\end{flushleft}





,





\begin{flushleft}
which contradicts the assumption g(t) = 0. We conclude that either g(t) = 0 for
\end{flushleft}


\begin{flushleft}
all t, or g(t) $>$ 0 for all t. This is true for arbitrary x and v, so a vector w either
\end{flushleft}


\begin{flushleft}
satisfies w T $\nabla$2 f (x)w = 0 for all x, or w T $\nabla$2 f (x)w $>$ 0 for all x.
\end{flushleft}


\begin{flushleft}
Finally, suppose f is strictly convex but satisfies v T $\nabla$2 f (x)v = 0 for some x and
\end{flushleft}


\begin{flushleft}
v = 0. By the previous result, v T $\nabla$2 f (x + tv)v = 0 for all t, i.e., f is affine on the
\end{flushleft}


\begin{flushleft}
line x + tv, and not strictly convex.
\end{flushleft}


\begin{flushleft}
(c) Let f : Rn $\rightarrow$ R be a self-concordant function. Suppose x $\in$ dom f , v $\in$ Rn . Show
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


1


\begin{flushleft}
(1 $-$ t$\alpha$)2 $\nabla$2 f (x) $\nabla$2 f (x + tv)
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\alpha$)2
\end{flushleft}


\begin{flushleft}
for x + tv $\in$ dom f , 0 $\leq$ t $<$ $\alpha$, where $\alpha$ = (v T $\nabla$2 f (x)v)1/2 .
\end{flushleft}


\begin{flushleft}
Solution. As in part (b), we can prove that
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
log g(t) $\leq$ 2
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
$\partial$ 2 f˜(0, t)
\end{flushleft}


\begin{flushleft}
$\partial$s2
\end{flushleft}





1/2





\begin{flushleft}
where g(t) = w T $\nabla$2 f (x + tv)w and f˜(s, t) = f (x + sw + tv). Applying the upper
\end{flushleft}


\begin{flushleft}
bound in (9.46) to the self-concordant function f˜(0, t) = f (x + tv) of one variable,
\end{flushleft}


\begin{flushleft}
t, we obtain
\end{flushleft}


\begin{flushleft}
$\partial$ 2 f˜(0, t)
\end{flushleft}


\begin{flushleft}
$\alpha$2
\end{flushleft}


$\leq$


,


\begin{flushleft}
$\partial$s2
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\alpha$)2
\end{flushleft}


\begin{flushleft}
so
\end{flushleft}


\begin{flushleft}
$-$2$\alpha$
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
2$\alpha$
\end{flushleft}


$\leq$


\begin{flushleft}
log g(t) $\leq$
\end{flushleft}


.


\begin{flushleft}
(1 $-$ t$\alpha$)
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\alpha$)
\end{flushleft}


\begin{flushleft}
Integrating gives
\end{flushleft}





\begin{flushleft}
2 log(1 $-$ t$\alpha$) $\leq$ log(g(t)/g(0)) $\leq$ $-$2 log(1 $-$ t$\alpha$)
\end{flushleft}


\begin{flushleft}
g(0)(1 $-$ t$\alpha$)2 $\leq$ g(t) $\leq$
\end{flushleft}





\begin{flushleft}
g(0)
\end{flushleft}


.


\begin{flushleft}
(1 $-$ t$\alpha$)2
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Finally, observing that g(0) = $\alpha$2 gives the inequalities
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\alpha$)2 wT $\nabla$2 f (x)w $\leq$ w T $\nabla$2 f (x + tv)w $\leq$
\end{flushleft}





\begin{flushleft}
wT $\nabla$2 f (x)w
\end{flushleft}


.


\begin{flushleft}
(1 $-$ t$\alpha$)2
\end{flushleft}





\begin{flushleft}
This holds for all w, and hence
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\alpha$)2 $\nabla$2 f (x)
\end{flushleft}





1


\begin{flushleft}
$\nabla$2 f (x).
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\alpha$)2
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (x + tv)
\end{flushleft}





\begin{flushleft}
9.18 Quadratic convergence. Let f : Rn $\rightarrow$ R be a strictly convex self-concordant function.
\end{flushleft}


\begin{flushleft}
Suppose $\lambda$(x) $<$ 1, and define x+ = x $-$ $\nabla$2 f (x)$-$1 $\nabla$f (x). Prove that $\lambda$(x+ ) $\leq$ $\lambda$(x)2 /(1 $-$
\end{flushleft}


\begin{flushleft}
$\lambda$(x))2 . Hint. Use the inequalities in exercise 9.17, part (c).
\end{flushleft}


\begin{flushleft}
Solution. Let v = $-$$\nabla$2 f (x)$-$1 $\nabla$f (x). From exercise 9.17, part (c),
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\lambda$(x))2 $\nabla$2 f (x)
\end{flushleft}





1


\begin{flushleft}
$\nabla$2 f (x).
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\lambda$(x))2
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (x + tv)
\end{flushleft}





\begin{flushleft}
We can assume without loss of generality that $\nabla$2 f (x) = I (hence, v = $-$$\nabla$f (x)), and
\end{flushleft}


\begin{flushleft}
(1 $-$ $\lambda$(x))2 I
\end{flushleft}





1


\begin{flushleft}
I.
\end{flushleft}


\begin{flushleft}
(1 $-$ $\lambda$(x))2
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (x+ )
\end{flushleft}





\begin{flushleft}
We can write $\lambda$(x+ ) as
\end{flushleft}


\begin{flushleft}
$\lambda$(x+ )
\end{flushleft}





=





\begin{flushleft}
$\nabla$2 f (x+ )$-$1 $\nabla$f (x+ )
\end{flushleft}





2





$\leq$





\begin{flushleft}
(1 $-$ $\lambda$(x))$-$1 $\nabla$f (x+ )
\end{flushleft}





=





\begin{flushleft}
(1 $-$ $\lambda$(x))$-$1
\end{flushleft}





2





1





\begin{flushleft}
$\nabla$2 f (x + tv)v dt + $\nabla$f (x)
\end{flushleft}





0





2





1





=





\begin{flushleft}
(1 $-$ $\lambda$(x))$-$1
\end{flushleft}





\begin{flushleft}
($\nabla$2 f (x + tv) $-$ I) dt v
\end{flushleft}





0


1





$\leq$





\begin{flushleft}
(1 $-$ $\lambda$(x))$-$1
\end{flushleft}





(


0





1


\begin{flushleft}
$-$ 1) dt v
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\lambda$(x))2
\end{flushleft}





1





$\leq$


=





\begin{flushleft}
v
\end{flushleft}





2 (1





\begin{flushleft}
$-$ $\lambda$(x))
\end{flushleft}





\begin{flushleft}
$\lambda$(x)2
\end{flushleft}


.


\begin{flushleft}
(1 $-$ $\lambda$(x))2
\end{flushleft}





$-$1


0





2





2





1


(


\begin{flushleft}
$-$ 1) dt
\end{flushleft}


\begin{flushleft}
(1 $-$ t$\lambda$(x))2
\end{flushleft}





\begin{flushleft}
9.19 Bound on the distance from the optimum. Let f : Rn $\rightarrow$ R be a strictly convex selfconcordant function.
\end{flushleft}


\begin{flushleft}
(a) Suppose $\lambda$(¯
\end{flushleft}


\begin{flushleft}
x) $<$ 1 and the sublevel set \{x | f (x) $\leq$ f (¯
\end{flushleft}


\begin{flushleft}
x)\} is closed. Show that the
\end{flushleft}


\begin{flushleft}
minimum of f is attained and
\end{flushleft}


(¯


\begin{flushleft}
x $-$ x )T $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)(¯
\end{flushleft}


\begin{flushleft}
x$-$x )
\end{flushleft}





1/2





$\leq$





\begin{flushleft}
$\lambda$(¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


.


\begin{flushleft}
1 $-$ $\lambda$(¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}





\begin{flushleft}
(b) Show that if f has a closed sublevel set, and is bounded below, then its minimum is
\end{flushleft}


\begin{flushleft}
attained.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
(a) As in the derivation of (9.47) we consider the function f˜(t) = f (ˆ
\end{flushleft}


\begin{flushleft}
x + tv) for an
\end{flushleft}


\begin{flushleft}
arbitrary descent direction v. Note from (9.44) that
\end{flushleft}


1+





\begin{flushleft}
f˜ (0)
\end{flushleft}


$>$0


\begin{flushleft}
f˜ (0)1/2
\end{flushleft}





\begin{flushleft}
if $\lambda$(¯
\end{flushleft}


\begin{flushleft}
x) $<$ 1.
\end{flushleft}


\begin{flushleft}
We first argue that f˜(t) reaches its minimum for some positive (finite) t . Let
\end{flushleft}


\begin{flushleft}
t0 = sup\{t $\geq$ 0 | x
\end{flushleft}


\begin{flushleft}
ˆ + tv $\in$ dom f \}. If t0 = $\infty$ (i.e., x
\end{flushleft}


\begin{flushleft}
ˆ + tv $\in$ dom f for all t $\geq$ 0),
\end{flushleft}


\begin{flushleft}
then, from (9.47), f˜ (t) $>$ 0 for
\end{flushleft}


\begin{flushleft}
t $>$ t¯ =
\end{flushleft}





\begin{flushleft}
$-$f˜ (0)
\end{flushleft}


,


\begin{flushleft}
f˜ (0) + f˜ (0)1/2 f˜ (0)
\end{flushleft}





\begin{flushleft}
so f˜ must reach a minimum in the interval (0, t¯).
\end{flushleft}


\begin{flushleft}
If t0 is finite, then we must have
\end{flushleft}


\begin{flushleft}
lim f˜(t) $>$ f˜(0).
\end{flushleft}





\begin{flushleft}
t$\rightarrow$t0
\end{flushleft}





\begin{flushleft}
since the sublevel set \{t | f˜(t) $\leq$ f˜(0)\} is closed. Therefore f˜ reaches a minimum in
\end{flushleft}


\begin{flushleft}
the interval (0, t0 ).
\end{flushleft}


\begin{flushleft}
In both cases,
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





$\leq$





\begin{flushleft}
f˜ (0)t
\end{flushleft}





$\leq$


$\leq$





\begin{flushleft}
$-$f˜ (0)
\end{flushleft}


\begin{flushleft}
f˜ (0) + f˜ (0)1/2 f˜ (0)
\end{flushleft}


\begin{flushleft}
$-$f˜ (0)/ f˜ (0)
\end{flushleft}


\begin{flushleft}
1 + f˜ (0)/ f˜ (0)
\end{flushleft}


\begin{flushleft}
$\lambda$(x)
\end{flushleft}


\begin{flushleft}
1 $-$ $\lambda$(x)
\end{flushleft}





\begin{flushleft}
where again we used (9.44). This bound on t holds for any descent vector v. In
\end{flushleft}


\begin{flushleft}
particular, in the direction v = x $-$ x, we have t = 1, so we obtain
\end{flushleft}


(¯


\begin{flushleft}
x $-$ x )T $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)(¯
\end{flushleft}


\begin{flushleft}
x$-$x )
\end{flushleft}





1/2





$\leq$





\begin{flushleft}
$\lambda$(¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


.


\begin{flushleft}
1 $-$ $\lambda$(¯
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}





\begin{flushleft}
(b) If f is strictly convex, and self-concordant, with a closed sublevel set, then our
\end{flushleft}


\begin{flushleft}
convergence analysis of Newton's method applies. In other words, after a finite
\end{flushleft}


\begin{flushleft}
number of iterations, $\lambda$(x) becomes less than one, and from the previous result this
\end{flushleft}


\begin{flushleft}
means that the minimum is attained.
\end{flushleft}


\begin{flushleft}
9.20 Conjugate of a self-concordant function. Suppose f : Rn $\rightarrow$ R is closed, strictly convex,
\end{flushleft}


\begin{flushleft}
and self-concordant. We show that its conjugate (or Legendre transform) f ∗ is selfconcordant.
\end{flushleft}


\begin{flushleft}
(a) Show that for each y $\in$ dom f ∗ , there is a unique x $\in$ dom f that satisfies y =
\end{flushleft}


\begin{flushleft}
$\nabla$f (x). Hint. Refer to the result of exercise 9.19.
\end{flushleft}


\begin{flushleft}
(b) Suppose y¯ = $\nabla$f (¯
\end{flushleft}


\begin{flushleft}
x). Define
\end{flushleft}


\begin{flushleft}
g(t) = f (¯
\end{flushleft}


\begin{flushleft}
x + tv),
\end{flushleft}





\begin{flushleft}
h(t) = f ∗ (¯
\end{flushleft}


\begin{flushleft}
y + tw)
\end{flushleft}





\begin{flushleft}
where v $\in$ Rn and w = $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)v. Show that
\end{flushleft}


\begin{flushleft}
g (0) = h (0),
\end{flushleft}


∗





\begin{flushleft}
g (0) = $-$h (0).
\end{flushleft}





\begin{flushleft}
Use these identities to show that f is self-concordant.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) y $\in$ dom f ∗ means that f (x) $-$ y T x is bounded below as a function of f . From
\end{flushleft}


\begin{flushleft}
exercise 9.19, part (a), the minimum is attained. The minimizer satisfies $\nabla$f (x) = y,
\end{flushleft}


\begin{flushleft}
and is unique because f (x) $-$ y T x is strictly convex.
\end{flushleft}





\begin{flushleft}
(b) Let F be the inverse mapping of $\nabla$f , i.e., x = F (y) if and only if y = $\nabla$f (x). We
\end{flushleft}


\begin{flushleft}
have x
\end{flushleft}


\begin{flushleft}
¯ = F (¯
\end{flushleft}


\begin{flushleft}
y ), and also (from exercise 3.40),
\end{flushleft}


\begin{flushleft}
$\nabla$f ∗ (y) = F (y),
\end{flushleft}





\begin{flushleft}
$\nabla$2 f ∗ (y) = $\nabla$2 f (F (y))$-$1
\end{flushleft}





\begin{flushleft}
for all y $\in$ dom f ∗ .
\end{flushleft}


\begin{flushleft}
The first equality follows from $\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y ) = $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)$-$1 :
\end{flushleft}


\begin{flushleft}
g (0) = v T $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)v = w T $\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y )w = h (0).
\end{flushleft}


\begin{flushleft}
In order to prove the second equality we define
\end{flushleft}


\begin{flushleft}
G=
\end{flushleft}





\begin{flushleft}
d 2
\end{flushleft}


\begin{flushleft}
$\nabla$ f (¯
\end{flushleft}


\begin{flushleft}
x + tv)
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





,


\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
H=
\end{flushleft}





\begin{flushleft}
d 2 ∗
\end{flushleft}


\begin{flushleft}
$\nabla$ f (¯
\end{flushleft}


\begin{flushleft}
y + tw)
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





,


\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
i.e., we have
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x + tv) $\approx$ $\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x) + tG,
\end{flushleft}





\begin{flushleft}
$\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y + tw) $\approx$ $\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y ) + tH
\end{flushleft}





\begin{flushleft}
for small t, and
\end{flushleft}


\begin{flushleft}
$\nabla$2 f ∗ ($\nabla$f (¯
\end{flushleft}


\begin{flushleft}
x + tv))
\end{flushleft}





$\approx$





\begin{flushleft}
$\nabla$2 f ∗ ($\nabla$f (¯
\end{flushleft}


\begin{flushleft}
x) + t$\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x)v)
\end{flushleft}





$\approx$





\begin{flushleft}
$\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y ) + tH.
\end{flushleft}





=





\begin{flushleft}
y + tw)
\end{flushleft}


\begin{flushleft}
$\nabla$2 f ∗ (¯
\end{flushleft}





\begin{flushleft}
Linearizing both sides of the equation
\end{flushleft}


\begin{flushleft}
$\nabla$2 f ∗ ($\nabla$f (¯
\end{flushleft}


\begin{flushleft}
x + tv))$\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x + tv) = I
\end{flushleft}


\begin{flushleft}
gives
\end{flushleft}


2





2





\begin{flushleft}
H$\nabla$2 f (¯
\end{flushleft}


\begin{flushleft}
x) + $\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y )G = 0,
\end{flushleft}





\begin{flushleft}
i.e., G = $-$$\nabla$ f (¯
\end{flushleft}


\begin{flushleft}
x)H$\nabla$ f (¯
\end{flushleft}


\begin{flushleft}
x). Therefore
\end{flushleft}


\begin{flushleft}
g (0)
\end{flushleft}





=


=


=


=


=





\begin{flushleft}
It follows that
\end{flushleft}


∗





\begin{flushleft}
d T 2
\end{flushleft}


\begin{flushleft}
v $\nabla$ f (¯
\end{flushleft}


\begin{flushleft}
x + tv)v
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
v T Gv
\end{flushleft}


\begin{flushleft}
$-$wT Hw
\end{flushleft}


\begin{flushleft}
d
\end{flushleft}


\begin{flushleft}
$-$ wT $\nabla$2 f ∗ (¯
\end{flushleft}


\begin{flushleft}
y + tw)w
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}


\begin{flushleft}
$-$h (0).
\end{flushleft}





\begin{flushleft}
t=0
\end{flushleft}





\begin{flushleft}
|h (0)| $\leq$ 2h (0)3/2 ,
\end{flushleft}





\begin{flushleft}
for any y¯ $\in$ dom f and all w, so f ∗ is self-concordant.
\end{flushleft}


\begin{flushleft}
9.21 Optimal line search parameters. Consider the upper bound (9.56) on the number of
\end{flushleft}


\begin{flushleft}
Newton iterations required to minimize a strictly convex self-concordant functions. What
\end{flushleft}


\begin{flushleft}
is the minimum value of the upper bound, if we minimize over $\alpha$ and $\beta$?
\end{flushleft}


\begin{flushleft}
Solution. Clearly, we should take $\beta$ near one.
\end{flushleft}


\begin{flushleft}
The function
\end{flushleft}


\begin{flushleft}
20 $-$ 8$\alpha$
\end{flushleft}


\begin{flushleft}
$\alpha$(1 $-$ 2$\alpha$)2
\end{flushleft}





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
reaches its minimum at $\alpha$ = 0.1748, with a minimum value of about 252, so the lowest
\end{flushleft}


\begin{flushleft}
upper bound is
\end{flushleft}


\begin{flushleft}
252(f (x(0) ) $-$ p ) + log2 log2 (1/ ).
\end{flushleft}


\begin{flushleft}
9.22 Suppose that f is strictly convex and satisfies (9.42). Give a bound on the number of
\end{flushleft}


\begin{flushleft}
Newton steps required to compute p within , starting at x(0) .
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
f˜(x(0) ) $-$ p˜
\end{flushleft}


\begin{flushleft}
+ log2 log2 (4 /k 2 )
\end{flushleft}


\begin{flushleft}
$\gamma$
\end{flushleft}


\begin{flushleft}
where f˜ = (k 2 /4)f . In other words
\end{flushleft}


\begin{flushleft}
(k 2 /4)
\end{flushleft}





\begin{flushleft}
f (x(0) ) $-$ p
\end{flushleft}


\begin{flushleft}
+ log2 log2 (4 /k 2 ).
\end{flushleft}


\begin{flushleft}
$\gamma$
\end{flushleft}





\begin{flushleft}
Implementation
\end{flushleft}


\begin{flushleft}
9.23 Pre-computation for line searches. For each of the following functions, explain how the
\end{flushleft}


\begin{flushleft}
computational cost of a line search can be reduced by a pre-computation. Give the cost
\end{flushleft}


\begin{flushleft}
of the pre-computation, and the cost of evaluating g(t) = f (x + t∆x) and g (t) with and
\end{flushleft}


\begin{flushleft}
without the pre-computation.
\end{flushleft}


\begin{flushleft}
(a) f (x) = $-$
\end{flushleft}


\begin{flushleft}
(b) f (x) = log
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
log(bi $-$ aTi x).
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
exp(aTi x + bi )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





.





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
(c) f (x) = (Ax $-$ b) (P0 + x1 P1 + · · · + xn Pn )$-$1 (Ax $-$ b), where Pi $\in$ Sm , A $\in$ Rm×n ,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
b $\in$ Rm and dom f = \{x | P0 + i=1 xi Pi 0\}.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Without pre-computation the cost is order mn.
\end{flushleft}


\begin{flushleft}
We can write g as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g(t) = $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(bi $-$ aTi x) $-$
\end{flushleft}





\begin{flushleft}
so if we pre-compute wi =
\end{flushleft}





\begin{flushleft}
aTi ∆x/(bi
\end{flushleft}





$-$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(1 $-$ taTi ∆x/(bi $-$ aTi x)),
\end{flushleft}





\begin{flushleft}
aTi x),
\end{flushleft}





\begin{flushleft}
we can express g as
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g(t) = g(0) $-$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(1 $-$ twi ),
\end{flushleft}





\begin{flushleft}
g (t) = $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
wi
\end{flushleft}


.


\begin{flushleft}
1 $-$ twi
\end{flushleft}





\begin{flushleft}
The cost of the pre-computation is 2mn + m (if we assume b $-$ Ax is already computed). After the pre-computation the cost of evaluating g and g is linear in m.
\end{flushleft}


\begin{flushleft}
(b) Without pre-computation the cost is order mn. We can write g as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g(t)
\end{flushleft}





=





=





\begin{flushleft}
exp(aTi x + bi + taTi ∆x)
\end{flushleft}





\begin{flushleft}
log
\end{flushleft}





\begin{flushleft}
log
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
t$\alpha$i +$\beta$i
\end{flushleft}





\begin{flushleft}
e
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where $\alpha$i = aTi ∆x and $\beta$i = aTi x + bi . If we pre-compute $\alpha$i and $\beta$i (at a cost that
\end{flushleft}


\begin{flushleft}
is order mn), we can reduce the cost of computing g and g to order m.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(c) Without pre-computation the cost is 2mn (for computing Ax $-$ b), plus 2nm2 (for
\end{flushleft}


\begin{flushleft}
computing P (x)), followed by (1/3)m3 (for computing P (x)$-$1 (Ax $-$ b), followed by
\end{flushleft}


\begin{flushleft}
2m for the inner product. The total cost 2nm2 + (1/3)m3 .
\end{flushleft}


\begin{flushleft}
The following pre-computation steps reduce the complexity:
\end{flushleft}


\begin{flushleft}
$\bullet$ Compute the Cholesky factorization P (x) = LLT
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
$\bullet$ Compute the eigenvalue decomposition L$-$1 ( i=1 ∆xi Pi )L$-$T = Q$\Lambda$QT .
\end{flushleft}


\begin{flushleft}
$\bullet$ Compute y = QT L$-$1 Ax, and v = QT L$-$1 A∆x.
\end{flushleft}


\begin{flushleft}
The pre-computation involves steps that are order m3 (Cholesky factorization, eigenvalue decomposition), 2nm2 (computing P (x) and
\end{flushleft}


\begin{flushleft}
∆xi Pi ), and lower order
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
terms.
\end{flushleft}


\begin{flushleft}
After the pre-computation we can express g as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g(x + t∆x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(yi + tvi )2
\end{flushleft}


,


\begin{flushleft}
1 + t$\lambda$i
\end{flushleft}





\begin{flushleft}
which can be evaluated and differentiated in order m operations.
\end{flushleft}


\begin{flushleft}
9.24 Exploiting block diagonal structure in the Newton system. Suppose the Hessian $\nabla$ 2 f (x) of
\end{flushleft}


\begin{flushleft}
a convex function f is block diagonal. How do we exploit this structure when computing
\end{flushleft}


\begin{flushleft}
the Newton step? What does it mean about f ?
\end{flushleft}


\begin{flushleft}
Solution. If the Hessian is block diagonal, then the objective function is separable, i.e.,
\end{flushleft}


\begin{flushleft}
a sum of functions of disjoint sets of variables. This means we might as well solve each of
\end{flushleft}


\begin{flushleft}
the problems separately.
\end{flushleft}


\begin{flushleft}
9.25 Smoothed fit to given data. Consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}





\begin{flushleft}
$\psi$(xi $-$ yi ) + $\lambda$
\end{flushleft}





\begin{flushleft}
n$-$1
\end{flushleft}


\begin{flushleft}
(xi+1
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$ x i )2
\end{flushleft}





\begin{flushleft}
where $\lambda$ $>$ 0 is smoothing parameter, $\psi$ is a convex penalty function, and x $\in$ Rn is the
\end{flushleft}


\begin{flushleft}
variable. We can interpret x as a smoothed fit to the vector y.
\end{flushleft}


\begin{flushleft}
(a) What is the structure in the Hessian of f ?
\end{flushleft}


\begin{flushleft}
(b) Extend to the problem of making a smooth fit to two-dimensional data, i.e., minimizing the function
\end{flushleft}


\begin{flushleft}
n$-$1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i,j=1
\end{flushleft}





\begin{flushleft}
$\psi$(xij $-$ yij ) + $\lambda$
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1 j=1
\end{flushleft}





\begin{flushleft}
(xi+1,j $-$ xij )2 +
\end{flushleft}





\begin{flushleft}
n$-$1
\end{flushleft}





\begin{flushleft}
i=1 j=1
\end{flushleft}





\begin{flushleft}
(xi,j+1 $-$ xij )2
\end{flushleft}





,





\begin{flushleft}
with variable X $\in$ Rn×n , where Y $\in$ Rn×n and $\lambda$ $>$ 0 are given.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Tridiagonal.
\end{flushleft}


\begin{flushleft}
(b) Block-tridiagonal if we store the elements of X columnwise. The blocks have size
\end{flushleft}


\begin{flushleft}
n × n. The diagonal blocks are tridiagonal. The blocks on the first sub-diagonal are
\end{flushleft}


\begin{flushleft}
diagonal.
\end{flushleft}


\begin{flushleft}
9.26 Newton equations with linear structure. Consider the problem of minimizing a function
\end{flushleft}


\begin{flushleft}
of the form
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
$\psi$i (Ai x + bi )
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}





(9.63)





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where Ai $\in$ Rmi ×n , bi $\in$ Rmi , and the functions $\psi$i : Rmi $\rightarrow$ R are twice differentiable
\end{flushleft}


\begin{flushleft}
and convex. The Hessian H and gradient g of f at x are given by
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
ATi Hi Ai ,
\end{flushleft}





\begin{flushleft}
H=
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
ATi gi .
\end{flushleft}





\begin{flushleft}
g=
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





(9.64)





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
where Hi = $\nabla$2 $\psi$i (Ai x + bi ) and gi = $\nabla$$\psi$i (Ai x + bi ).
\end{flushleft}


\begin{flushleft}
Describe how you would implement Newton's method for minimizing f . Assume that
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
mi , the matrices Ai are very sparse, but the Hessian H is dense.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
In many applications, for example, when n is small compared to the dimensions mi , the
\end{flushleft}


\begin{flushleft}
simplest and most efficient way to calculate the Newton direction is to evaluate H and g
\end{flushleft}


\begin{flushleft}
using (9.64), and solve the Newton system with a dense Cholesky factorization.
\end{flushleft}


\begin{flushleft}
It is possible, however, that the matrices Ai are very sparse, while H itself is dense. In that
\end{flushleft}


\begin{flushleft}
case the straightforward method, which involves solving a dense set of linear equations of
\end{flushleft}


\begin{flushleft}
size n, may not be the most efficient method, since it does not take advantage of sparsity.
\end{flushleft}


\begin{flushleft}
Specifically, assume that n
\end{flushleft}


\begin{flushleft}
mi , rank Ai = mi , and Hi 0, so the Hessian is a sum of
\end{flushleft}


\begin{flushleft}
N matrices of rank mi . We can introduce new variables yi = ATi v, and write the Newton
\end{flushleft}


\begin{flushleft}
system as
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
This is an indefinite system of n +
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
$-$H1$-$1
\end{flushleft}


0


..


.


0


\begin{flushleft}
AT1
\end{flushleft}





\begin{flushleft}
yi = Hi ATi v,
\end{flushleft}





\begin{flushleft}
ATi yi = $-$g,
\end{flushleft}





0


\begin{flushleft}
$-$H2$-$1
\end{flushleft}


..


.


0


\begin{flushleft}
AT2
\end{flushleft}





···


···


..


.


···


···





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
i = 1, . . . , N.
\end{flushleft}





\begin{flushleft}
mi linear equations in n +
\end{flushleft}





0


0


..


.


$-$1


\begin{flushleft}
$-$HN
\end{flushleft}


\begin{flushleft}
ATN
\end{flushleft}





\begin{flushleft}
A1
\end{flushleft}


\begin{flushleft}
A2
\end{flushleft}


..


.


\begin{flushleft}
AN
\end{flushleft}


0





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
y1
\end{flushleft}


\begin{flushleft}
y2
\end{flushleft}


..


.


\begin{flushleft}
yN
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
 
\end{flushleft}


\begin{flushleft}
 
\end{flushleft}


\begin{flushleft}
=
\end{flushleft}


\begin{flushleft}
 
\end{flushleft}


\begin{flushleft}
 
\end{flushleft}





0


0


..


.


0


\begin{flushleft}
$-$g
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
mi variables:
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
.
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
(9.26.A)
\end{flushleft}





\begin{flushleft}
This system is larger than the Newton system, but if n
\end{flushleft}


\begin{flushleft}
mi , and the matrices Ai are
\end{flushleft}


\begin{flushleft}
sparse, it may be easier to solve (9.26.A) using a sparse solver than to solve the Newton
\end{flushleft}


\begin{flushleft}
system directly.
\end{flushleft}


\begin{flushleft}
9.27 Analytic center of linear inequalities with variable bounds. Give the most efficient method
\end{flushleft}


\begin{flushleft}
for computing the Newton step of the function
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
f (x) = $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log(xi + 1) $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
log(1 $-$ xi ) $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(bi $-$ aTi x),
\end{flushleft}





\begin{flushleft}
with dom f = \{x $\in$ Rn | $-$1 ≺ x ≺ 1, Ax ≺ b\}, where aTi is the ith row of A. Assume A
\end{flushleft}


\begin{flushleft}
is dense, and distinguish two cases: m $\geq$ n and m $\leq$ n. (See also exercise 9.30.)
\end{flushleft}


\begin{flushleft}
Solution. Note that f has the form (9.60) with k = n, p = m, g = b, F = $-$A, and
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\psi$0 (y) = $-$
\end{flushleft}





\begin{flushleft}
log yi ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\psi$i (xi ) = $-$ log(1 $-$ x2i ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
The Hessian f at x is given by
\end{flushleft}


ˆ


\begin{flushleft}
H = D + AT DA
\end{flushleft}





\begin{flushleft}
(9.27.A)
\end{flushleft}





\begin{flushleft}
ˆ ii = 1/(bi $-$ aTi x)2 .
\end{flushleft}


\begin{flushleft}
where Dii = 1/(1 $-$ xi )2 + 1/(xi + 1)2 , and D
\end{flushleft}


\begin{flushleft}
The first possibility is to form H as given by (9.27.A), and to solve the Newton system
\end{flushleft}


ˆ


\begin{flushleft}
using a dense Cholesky factorization. The cost is mn2 operations (to form AT DA)
\end{flushleft}


\begin{flushleft}
plus
\end{flushleft}


\begin{flushleft}
(1/3)n3 for the Cholesky factorization.
\end{flushleft}


ˆ


\begin{flushleft}
A second possibility is to introduce a new variable y = DAv,
\end{flushleft}


\begin{flushleft}
and to write the Newton
\end{flushleft}


\begin{flushleft}
system as
\end{flushleft}


\begin{flushleft}
ˆ $-$1 y = A∆xnt .
\end{flushleft}


\begin{flushleft}
D∆xnt + AT y = $-$g,
\end{flushleft}


\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
(9.27.B)
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
From the first equation, ∆xnt = D$-$1 ($-$g $-$ AT y), and substituting this in the second
\end{flushleft}


\begin{flushleft}
equation, we obtain
\end{flushleft}


\begin{flushleft}
ˆ $-$1 + AD $-$1 AT )y = $-$AD $-$1 g.
\end{flushleft}


\begin{flushleft}
(D
\end{flushleft}


\begin{flushleft}
(9.27.C)
\end{flushleft}


\begin{flushleft}
This is a positive definite set of m linear equations in the variable y $\in$ Rm . Given y, we find
\end{flushleft}


\begin{flushleft}
∆xnt by evaluating ∆xnt = $-$D $-$1 (g + AT y). The cost of forming and solving (9.27.C) is
\end{flushleft}


\begin{flushleft}
mn2 +(1/3)m3 operations (assuming A is dense). Therefore if m $<$ n, this second method
\end{flushleft}


\begin{flushleft}
is faster than directly solving the Newton system H∆xnt = $-$g.
\end{flushleft}


\begin{flushleft}
A third possibility is to solve (9.27.B) as an indefinite set of m + n linear equations
\end{flushleft}


\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


ˆ $-$1


\begin{flushleft}
$-$D
\end{flushleft}





\begin{flushleft}
∆xnt
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}





=





\begin{flushleft}
$-$g
\end{flushleft}


0





.





\begin{flushleft}
(9.27.D)
\end{flushleft}





\begin{flushleft}
ˆ and
\end{flushleft}


\begin{flushleft}
This method is interesting when A is sparse, and the two matrices D + AT DA
\end{flushleft}


\begin{flushleft}
ˆ $-$1 + AD $-$1 AT are not. In that case, solving (9.27.D) using a sparse solver may be
\end{flushleft}


\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
faster than the two methods above.
\end{flushleft}


\begin{flushleft}
9.28 Analytic center of quadratic inequalities. Describe an efficient method for computing the
\end{flushleft}


\begin{flushleft}
Newton step of the function
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
f (x) = $-$
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log($-$xT Ai x $-$ bTi x $-$ ci ),
\end{flushleft}





\begin{flushleft}
bTi x
\end{flushleft}





\begin{flushleft}
with dom f = \{x | x Ai x +
\end{flushleft}


\begin{flushleft}
+ ci $<$ 0, i = 1, . . . , m\}. Assume that the matrices
\end{flushleft}


\begin{flushleft}
Ai $\in$ Sn
\end{flushleft}


\begin{flushleft}
n.
\end{flushleft}


\begin{flushleft}
++ are large and sparse, and m
\end{flushleft}


\begin{flushleft}
Hint. The Hessian and gradient of f at x are given by
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
(2$\alpha$i Ai + $\alpha$i2 (2Ai x + bi )(2Ai x + bi )T ),
\end{flushleft}





\begin{flushleft}
H=
\end{flushleft}





\begin{flushleft}
g=
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\alpha$i (2Ai x + bi ),
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where $\alpha$i = 1/($-$xT Ai $-$ bTi x $-$ ci ).
\end{flushleft}


\begin{flushleft}
Solution. We can write H as H = Q + F F T , where
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\alpha$i A i ,
\end{flushleft}





\begin{flushleft}
Q=2
\end{flushleft}





\begin{flushleft}
F =
\end{flushleft}





\begin{flushleft}
$\alpha$1 (2A1 x + b1 )
\end{flushleft}





\begin{flushleft}
$\alpha$2 (2A2 x + b2 )
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





···





\begin{flushleft}
$\alpha$m (2Am x + bm )
\end{flushleft}





.





\begin{flushleft}
In general the Hessian will be dense, even when the matrices Ai are sparse, because of
\end{flushleft}


\begin{flushleft}
the dense rank-one terms. Finding the Newton direction by building and solving the
\end{flushleft}


\begin{flushleft}
Newton system Hv = g, therefore costs at least (1/3)n3 operations, since we need a dense
\end{flushleft}


\begin{flushleft}
Cholesky factorization.
\end{flushleft}


\begin{flushleft}
An alternative that may be faster when n
\end{flushleft}


\begin{flushleft}
m is as follows. We introduce a new variable
\end{flushleft}


\begin{flushleft}
y $\in$ Rm , and write the Newton system as
\end{flushleft}


\begin{flushleft}
Substituting v = $-$Q
\end{flushleft}





$-$1





\begin{flushleft}
Qv + F y = $-$g,
\end{flushleft}





\begin{flushleft}
y = F T v.
\end{flushleft}





\begin{flushleft}
(g + F y) in the second equation yields
\end{flushleft}


\begin{flushleft}
(I + F T Q$-$1 F )y = $-$F T Q$-$1 g,
\end{flushleft}





\begin{flushleft}
(9.28.A)
\end{flushleft}





\begin{flushleft}
which is a set of m linear equations.
\end{flushleft}


\begin{flushleft}
We can therefore also compute the Newton direction as follows. We factor Q using a
\end{flushleft}


\begin{flushleft}
sparse Cholesky factorization. Then we calculate the matrix V = Q$-$1 F by solving the
\end{flushleft}


\begin{flushleft}
matrix equation QV = F column by column, using the Cholesky factors of Q. For each
\end{flushleft}


\begin{flushleft}
colum this involves a sparse forward and backward substitution. We then form the matrix
\end{flushleft}


\begin{flushleft}
I + F T V (m2 n flops), factor it using a dense Cholesky factorization ((1/3)m3 flops), and
\end{flushleft}


\begin{flushleft}
solve for y. Finally we compute v by solving Qv = $-$g $-$ F y. The cost of this procedure
\end{flushleft}


\begin{flushleft}
is (1/3)m3 + m2 n operations plus the cost of the sparse Cholesky factorization of Q, and
\end{flushleft}


\begin{flushleft}
the m sparse forward and backward substitutions. If n
\end{flushleft}


\begin{flushleft}
m and Q is sparse, the overall
\end{flushleft}


\begin{flushleft}
cost can be much smaller than solving Hv = $-$g by a dense method.
\end{flushleft}





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
9.29 Exploiting structure in two-stage optimization. This exercise continues exercise 4.64, which
\end{flushleft}


\begin{flushleft}
describes optimization with recourse, or two-stage optimization. Using the notation and
\end{flushleft}


\begin{flushleft}
assumptions in exercise 4.64, we assume in addition that the cost function f is a twice
\end{flushleft}


\begin{flushleft}
differentiable function of (x, z), for each scenario i = 1, . . . , S.
\end{flushleft}


\begin{flushleft}
Explain how to efficiently compute the Newton step for the problem of finding the optimal
\end{flushleft}


\begin{flushleft}
policy. How does the approximate flop count for your method compare to that of a generic
\end{flushleft}


\begin{flushleft}
method (which exploits no structure), as a function of S, the number of scenarios?
\end{flushleft}


\begin{flushleft}
Solution. The problem to be solved is just
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
F (x) =
\end{flushleft}





\begin{flushleft}
S
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\pi$i f (x, zi , i),
\end{flushleft}





\begin{flushleft}
which is convex since for each i, f (x, z, i) is convex in (x, zi ), and $\pi$i $\geq$ 0.
\end{flushleft}


\begin{flushleft}
Now let's see how to compute the Newton step efficiently. The Hessian of F has the
\end{flushleft}


\begin{flushleft}
block-arrow form
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
$\nabla$ F =
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


2





\begin{flushleft}
$\nabla$2x,x F
\end{flushleft}


\begin{flushleft}
$\nabla$2x,z1 F T
\end{flushleft}


\begin{flushleft}
$\nabla$2x,z2 F T
\end{flushleft}


..


.


\begin{flushleft}
$\nabla$2x,zS F T
\end{flushleft}





\begin{flushleft}
$\nabla$2x,z1 F
\end{flushleft}


\begin{flushleft}
$\nabla$2z1 ,z1 F
\end{flushleft}


0


..


.


0





\begin{flushleft}
$\nabla$2x,z2 F
\end{flushleft}


0


\begin{flushleft}
$\nabla$2z2 ,z2 F
\end{flushleft}


..


.


0





···


···


···


..


.


···





\begin{flushleft}
$\nabla$2zS ,x F
\end{flushleft}


0


0


..


.


\begin{flushleft}
$\nabla$2zS ,zS F
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
which we can exploit to compute the Newton step efficiently. First, let's see what happens
\end{flushleft}


\begin{flushleft}
if we don't exploit this structure. We need to solve the set of n + Sq (symmetric, positive
\end{flushleft}


\begin{flushleft}
definite) linear equations $\nabla$2 F ∆nt = $-$$\nabla$F , so the cost is around (1/3)(n + Sq)3 flops. As
\end{flushleft}


\begin{flushleft}
a function of the number of scenarios, this grows like S 3 .
\end{flushleft}


\begin{flushleft}
Now let's exploit the structure to compute ∆nt . We do this by using elimination, eliminating the bottom right block of size Sq × Sq. This block is block diagonal, with S blocks
\end{flushleft}


\begin{flushleft}
of size q × q, This situation is described on page 677 of the text. The overall complexity
\end{flushleft}


\begin{flushleft}
is
\end{flushleft}


\begin{flushleft}
(2/3)Sq 3 + 2nSq 2 + 2n2 Sq + 2n2 Sq + (2/3)n3
\end{flushleft}


=





\begin{flushleft}
(2/3)q 3 + 2nq 2 + 2n2 q + 2n2 q S + (2/3)n3 ,
\end{flushleft}





\begin{flushleft}
which grows linearly in S.
\end{flushleft}


\begin{flushleft}
Here are the explicit details of how we can exploit structure to solve a block arrow, positive
\end{flushleft}


\begin{flushleft}
definite symmetric, system of equations:
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
A11
\end{flushleft}


\begin{flushleft}
AT12
\end{flushleft}


\begin{flushleft}
AT13
\end{flushleft}


..


.


\begin{flushleft}
AT1N
\end{flushleft}





\begin{flushleft}
A12
\end{flushleft}


\begin{flushleft}
A22
\end{flushleft}


0


..


.


0





\begin{flushleft}
A13
\end{flushleft}


0


\begin{flushleft}
A33
\end{flushleft}


..


.


0





···


···


···


..


.


···





\begin{flushleft}
A1N
\end{flushleft}


0


0


..


.


\begin{flushleft}
AN N
\end{flushleft}





\begin{flushleft}
We eliminate xj , for j = 2, . . . , N , to obtain
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
xj = A$-$1
\end{flushleft}


\begin{flushleft}
jj (bj $-$ A1j x1 ),
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
x1
\end{flushleft}


\begin{flushleft}
b1
\end{flushleft}


\begin{flushleft}
x2   b 2 
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
.. 
\end{flushleft}


\begin{flushleft}
 =  ...  .
\end{flushleft}


.


\begin{flushleft}
xN
\end{flushleft}


\begin{flushleft}
bN
\end{flushleft}





\begin{flushleft}
j = 2, . . . , N.
\end{flushleft}





\begin{flushleft}
The first block equation becomes
\end{flushleft}


\begin{flushleft}
N
\end{flushleft}





\begin{flushleft}
A11 $-$
\end{flushleft}





\begin{flushleft}
N
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
A1j A$-$1
\end{flushleft}


\begin{flushleft}
jj A1j
\end{flushleft}





\begin{flushleft}
j=2
\end{flushleft}





\begin{flushleft}
x1 = b 1 $-$
\end{flushleft}





\begin{flushleft}
A1j A$-$1
\end{flushleft}


\begin{flushleft}
jj bj .
\end{flushleft}


\begin{flushleft}
j=2
\end{flushleft}





\begin{flushleft}
We'll solve this equation to find x1 , and then use the equations above to find x2 , . . . , xN .
\end{flushleft}


\begin{flushleft}
To do this we first carry out a Cholesky factorization of A22 , . . . , AN N , and then compute
\end{flushleft}


$-$1


$-$1


$-$1


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
A$-$1
\end{flushleft}


\begin{flushleft}
22 A12 , . . . , AN N A1N , and A22 b2 , . . . , AN N bN , by back substitution. We then form the
\end{flushleft}


\begin{flushleft}
righthand side of the equations above, and the lefthand matrix, which is the Schur complement. We then solve these equations via Cholesky factorization and back substitution.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Numerical experiments
\end{flushleft}


\begin{flushleft}
9.30 Gradient and Newton methods. Consider the unconstrained problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
f (x) = $-$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(1 $-$ aTi x) $-$
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(1 $-$ x2i ),
\end{flushleft}





\begin{flushleft}
with variable x $\in$ Rn , and dom f = \{x | aTi x $<$ 1, i = 1, . . . , m, |xi | $<$ 1, i = 1, . . . , n\}.
\end{flushleft}


\begin{flushleft}
This is the problem of computing the analytic center of the set of linear inequalities
\end{flushleft}


\begin{flushleft}
aTi x $\leq$ 1,
\end{flushleft}





\begin{flushleft}
|xi | $\leq$ 1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
Note that we can choose x(0) = 0 as our initial point. You can generate instances of this
\end{flushleft}


\begin{flushleft}
problem by choosing ai from some distribution on Rn .
\end{flushleft}


\begin{flushleft}
(a) Use the gradient method to solve the problem, using reasonable choices for the backtracking parameters, and a stopping criterion of the form $\nabla$f (x) 2 $\leq$ $\eta$. Plot the
\end{flushleft}


\begin{flushleft}
objective function and step length versus iteration number. (Once you have determined p to high accuracy, you can also plot f $-$ p versus iteration.) Experiment
\end{flushleft}


\begin{flushleft}
with the backtracking parameters $\alpha$ and $\beta$ to see their effect on the total number of
\end{flushleft}


\begin{flushleft}
iterations required. Carry these experiments out for several instances of the problem,
\end{flushleft}


\begin{flushleft}
of different sizes.
\end{flushleft}


\begin{flushleft}
(b) Repeat using Newton's method, with stopping criterion based on the Newton decrement $\lambda$2 . Look for quadratic convergence. You do not have to use an efficient method
\end{flushleft}


\begin{flushleft}
to compute the Newton step, as in exercise 9.27; you can use a general purpose dense
\end{flushleft}


\begin{flushleft}
solver, although it is better to use one that is based on a Cholesky factorization.
\end{flushleft}


\begin{flushleft}
Hint. Use the chain rule to find expressions for $\nabla$f (x) and $\nabla$2 f (x).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Gradient method. The figures show the function values and step lengths versus
\end{flushleft}


\begin{flushleft}
iteration number for an example with m = 200, n = 100. We used $\alpha$ = 0.01,
\end{flushleft}


\begin{flushleft}
$\beta$ = 0.5, and exit condition $\nabla$f (x(k) ) 2 $\leq$ 10$-$3 .
\end{flushleft}


4





10





0.016


0.014





2





10





0.012





0





0.01





$-$2





\begin{flushleft}
t(k)
\end{flushleft}





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}





10


10





0.008


0.006





$-$4





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





0.004





$-$6





10





$-$8





10





0





\begin{flushleft}
PSfrag replacements
\end{flushleft}


100





200





300





400





500





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
The following is a Matlab implementation.
\end{flushleft}





0.002


0


0





100





200





300





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
MAXITERS = 1000;
\end{flushleft}


\begin{flushleft}
GRADTOL = 1e-3;
\end{flushleft}


\begin{flushleft}
x = zeros(n,1);
\end{flushleft}


\begin{flushleft}
for iter = 1:MAXITERS
\end{flushleft}


\begin{flushleft}
val = -sum(log(1-A*x)) - sum(log(1+x)) - sum(log(1-x));
\end{flushleft}


\begin{flushleft}
grad = A'*(1./(1-A*x)) - 1./(1+x) + 1./(1-x);
\end{flushleft}


\begin{flushleft}
if norm(grad) $<$ GRADTOL, break; end;
\end{flushleft}





400





500





600





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
v = -grad;
\end{flushleft}


\begin{flushleft}
fprime = grad'*v;
\end{flushleft}


\begin{flushleft}
t = 1; while ((max(A*(x+t*v)) $>$= 1) | (max(abs(x+t*v)) $>$= 1)),
\end{flushleft}


\begin{flushleft}
t = BETA*t;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
while ( -sum(log(1-A*(x+t*v))) - sum(log(1-(x+t*v).\^{}2)) $>$ ...
\end{flushleft}


\begin{flushleft}
val + ALPHA*t*fprime )
\end{flushleft}


\begin{flushleft}
t = BETA*t;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
x = x+t*v;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
(b) Newton method. The figures show the function values and step lengths versus iteration number for the same example. We used $\alpha$ = 0.01, $\beta$ = 0.5, and exit condition
\end{flushleft}


\begin{flushleft}
$\lambda$(x(k) )2 $\leq$ 10$-$8 .
\end{flushleft}


5





10





1


0.8





0





0.6





\begin{flushleft}
t(k)
\end{flushleft}





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}





10





0.4





$-$5





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}


$-$10





10





0





1





2





3





4





5





6





7





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
The following is a Matlab implementation.
\end{flushleft}





0.2


0


0





2





4





6





8





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
MAXITERS = 1000;
\end{flushleft}


\begin{flushleft}
NTTOL = 1e-8;
\end{flushleft}


\begin{flushleft}
x = zeros(n,1);
\end{flushleft}


\begin{flushleft}
for iter = 1:MAXITERS
\end{flushleft}


\begin{flushleft}
val = -sum(log(1-A*x)) - sum(log(1+x)) - sum(log(1-x));
\end{flushleft}


\begin{flushleft}
d = 1./(1-A*x);
\end{flushleft}


\begin{flushleft}
grad = A'*d - 1./(1+x) + 1./(1-x);
\end{flushleft}


\begin{flushleft}
hess = A'*diag(d.\^{}2)*A + diag(1./(1+x).\^{}2 + 1./(1-x).\^{}2);
\end{flushleft}


\begin{flushleft}
v = -hess\ensuremath{\backslash}grad;
\end{flushleft}


\begin{flushleft}
fprime = grad'*v;
\end{flushleft}


\begin{flushleft}
if abs(fprime) $<$ NTTOL, break; end;
\end{flushleft}


\begin{flushleft}
t = 1; while ((max(A*(x+t*v)) $>$= 1) | (max(abs(x+t*v)) $>$= 1)),
\end{flushleft}


\begin{flushleft}
t = BETA*t;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
while ( -sum(log(1-A*(x+t*v))) - sum(log(1-(x+t*v).\^{}2)) $>$ ...
\end{flushleft}


\begin{flushleft}
val + ALPHA*t*fprime )
\end{flushleft}


\begin{flushleft}
t = BETA*t;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
x = x+t*v;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
9.31 Some approximate Newton methods. The cost of Newton's method is dominated by the
\end{flushleft}


\begin{flushleft}
cost of evaluating the Hessian $\nabla$2 f (x) and the cost of solving the Newton system. For large
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
problems, it is sometimes useful to replace the Hessian by a positive definite approximation
\end{flushleft}


\begin{flushleft}
that makes it easier to form and solve for the search step. In this problem we explore
\end{flushleft}


\begin{flushleft}
some common examples of this idea.
\end{flushleft}


\begin{flushleft}
For each of the approximate Newton methods described below, test the method on some
\end{flushleft}


\begin{flushleft}
instances of the analytic centering problem described in exercise 9.30, and compare the
\end{flushleft}


\begin{flushleft}
results to those obtained using the Newton method and gradient method.
\end{flushleft}


\begin{flushleft}
(a) Re-using the Hessian. We evaluate and factor the Hessian only every N iterations,
\end{flushleft}


\begin{flushleft}
where N $>$ 1, and use the search step ∆x = $-$H $-$1 $\nabla$f (x), where H is the last Hessian
\end{flushleft}


\begin{flushleft}
evaluated. (We need to evaluate and factor the Hessian once every N steps; for the
\end{flushleft}


\begin{flushleft}
other steps, we compute the search direction using back and forward substitution.)
\end{flushleft}


\begin{flushleft}
(b) Diagonal approximation. We replace the Hessian by its diagonal, so we only have
\end{flushleft}


\begin{flushleft}
to evaluate the n second derivatives $\partial$ 2 f (x)/$\partial$x2i , and computing the search step is
\end{flushleft}


\begin{flushleft}
very easy.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The figure shows the function value versus iteration number (for the same example
\end{flushleft}


\begin{flushleft}
as in the solution of exercise 9.30), for N = 1 (i.e., Newton's method), N = 2, and
\end{flushleft}


\begin{flushleft}
N = 5.
\end{flushleft}


5





10





0





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





$-$5





\begin{flushleft}
N =2
\end{flushleft}





10





\begin{flushleft}
N =5
\end{flushleft}





\begin{flushleft}
Newton
\end{flushleft}


$-$10





10





0





5





10





15





20





25





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
We see that the speed of convergence deteriorates rapidly as N increases.
\end{flushleft}


\begin{flushleft}
(b) The figure shows the function value versus iteration number (for the same example
\end{flushleft}


\begin{flushleft}
as in the solution of exercise 9.30), for a diagonal approximation of the Hessian. The
\end{flushleft}


\begin{flushleft}
experiment shows that the algorithm converges very much like the gradient method.
\end{flushleft}


4





10





2





10





0





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}





10





$-$2





10





$-$4





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





$-$6





10





$-$8





10





0





200





400





600





800





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
9.32 Gauss-Newton method for convex nonlinear least-squares problems. We consider a (nonlinear) least-squares problem, in which we minimize a function of the form
\end{flushleft}


\begin{flushleft}
f (x) =
\end{flushleft}





1


2





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
fi (x)2 ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\newpage
9





\begin{flushleft}
Unconstrained minimization
\end{flushleft}





\begin{flushleft}
where fi are twice differentiable functions. The gradient and Hessian of f at x are given
\end{flushleft}


\begin{flushleft}
by
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\nabla$f (x) =
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
fi (x)$\nabla$fi (x),
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (x) =
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\nabla$fi (x)$\nabla$fi (x)T + fi (x)$\nabla$2 fi (x) .
\end{flushleft}





\begin{flushleft}
We consider the case when f is convex. This occurs, for example, if each fi is either
\end{flushleft}


\begin{flushleft}
nonnegative and convex, or nonpositive and concave, or affine.
\end{flushleft}


\begin{flushleft}
The Gauss-Newton method uses the search direction
\end{flushleft}


$-$1





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
∆xgn = $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\nabla$fi (x)$\nabla$fi (x)T
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
fi (x)$\nabla$fi (x)
\end{flushleft}





.





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(We assume here that the inverse exists, i.e., the vectors $\nabla$f1 (x), . . . , $\nabla$fm (x) span Rn .)
\end{flushleft}


\begin{flushleft}
This search direction can be considered an approximate Newton direction (see exercise 9.31), obtained by dropping the second derivative terms from the Hessian of f .
\end{flushleft}


\begin{flushleft}
We can give another simple interpretation of the Gauss-Newton search direction ∆x gn .
\end{flushleft}


\begin{flushleft}
Using the first-order approximation fi (x + v) $\approx$ fi (x) + $\nabla$fi (x)T v we obtain the approximation
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


1


\begin{flushleft}
(fi (x) + $\nabla$fi (x)T v)2 .
\end{flushleft}


\begin{flushleft}
f (x + v) $\approx$
\end{flushleft}


2


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
The Gauss-Newton search step ∆xgn is precisely the value of v that minimizes this approximation of f . (Moreover, we conclude that ∆xgn can be computed by solving a linear
\end{flushleft}


\begin{flushleft}
least-squares problem.)
\end{flushleft}


\begin{flushleft}
Test the Gauss-Newton method on some problem instances of the form
\end{flushleft}


\begin{flushleft}
fi (x) = (1/2)xT Ai x + bTi x + 1,
\end{flushleft}


\begin{flushleft}
T $-$1
\end{flushleft}


\begin{flushleft}
with Ai $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ and bi Ai bi $\leq$ 2 (which ensures that f is convex).
\end{flushleft}


\begin{flushleft}
Solution. We generate random Ai $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ , random bi , and scale Ai and bi so that
\end{flushleft}


\begin{flushleft}
b
\end{flushleft}


=


2.


\begin{flushleft}
We
\end{flushleft}


\begin{flushleft}
take
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


=


50,


\begin{flushleft}
m
\end{flushleft}


=


100.


\begin{flushleft}
The
\end{flushleft}


\begin{flushleft}
figure shows a typical convergence plot.
\end{flushleft}


\begin{flushleft}
bTi A$-$1
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


0





10





$-$2





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}





10





$-$4





10





$-$6





10





$-$8





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}


$-$10





10





0





50





100





150





200





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
We note that the Gauss-Newton method converges linearly, and much more slowly than
\end{flushleft}


\begin{flushleft}
Newton's method (which for this example converged in 2 iterations).
\end{flushleft}


\begin{flushleft}
This was to be expected. From the interpretation of the Gauss-Newton method as an
\end{flushleft}


\begin{flushleft}
approximate Newton method, we expect that it works well if the second term in the
\end{flushleft}


\begin{flushleft}
expression for the Hessian is small compared to the first term, i.e., if either $\nabla$2 fi is small
\end{flushleft}


\begin{flushleft}
(fi is nearly linear), or fi is small. For this test example neither of these conditions was
\end{flushleft}


\begin{flushleft}
satisfied.
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 10
\end{flushleft}





\begin{flushleft}
Equality constrained
\end{flushleft}


\begin{flushleft}
minimization
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
Equality constrained minimization
\end{flushleft}


\begin{flushleft}
10.1 Nonsingularity of the KKT matrix. Consider the KKT matrix
\end{flushleft}


\begin{flushleft}
P
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





,





\begin{flushleft}
p×n
\end{flushleft}


\begin{flushleft}
where P $\in$ Sn
\end{flushleft}


\begin{flushleft}
, and rank A = p $<$ n.
\end{flushleft}


\begin{flushleft}
+, A $\in$ R
\end{flushleft}





\begin{flushleft}
(a) Show that each of the following statements is equivalent to nonsingularity of the
\end{flushleft}


\begin{flushleft}
KKT matrix.
\end{flushleft}


$\bullet$


$\bullet$


$\bullet$


$\bullet$





\begin{flushleft}
N (P ) $\cap$ N (A) = \{0\}.
\end{flushleft}


\begin{flushleft}
Ax = 0, x = 0 =$\Rightarrow$ xT P x $>$ 0.
\end{flushleft}


\begin{flushleft}
FTPF
\end{flushleft}


\begin{flushleft}
0, where F $\in$ Rn×(n$-$p) is a matrix for which R(F ) = N (A).
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
P + A QA 0 for some Q 0.
\end{flushleft}





\begin{flushleft}
(b) Show that if the KKT matrix is nonsingular, then it has exactly n positive and p
\end{flushleft}


\begin{flushleft}
negative eigenvalues.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The second and third are clearly equivalent. To see this, if Ax = 0, x = 0, then x
\end{flushleft}


\begin{flushleft}
must have the form x = F z, where z = 0. Then we have xT P x = z T F T P F z.
\end{flushleft}


\begin{flushleft}
Similarly, the first and second are equivalent. To see this, if x $\in$ N (A)$\cap$N (P ), x = 0,
\end{flushleft}


\begin{flushleft}
then Ax = 0, x = 0, but xT P x = 0, contradicting the second statement. Conversely,
\end{flushleft}


\begin{flushleft}
suppose the second statement fails to hold, i.e., there is an x with Ax = 0, x = 0,
\end{flushleft}


\begin{flushleft}
but xT P x = 0. Since P
\end{flushleft}


\begin{flushleft}
0, we conclude P x = 0, i.e., x $\in$ N (P ), which contradicts
\end{flushleft}


\begin{flushleft}
the first statement.
\end{flushleft}


\begin{flushleft}
Finally, the second and fourth statements are equivalent. If the second holds then
\end{flushleft}


\begin{flushleft}
the last statement holds with Q = I. If the last statement holds for some Q
\end{flushleft}


0


\begin{flushleft}
then it holds for all Q 0, and therefore the second statement holds.
\end{flushleft}


\begin{flushleft}
Now let's show that the four statements are equivalent to nonsingularity of the KKT
\end{flushleft}


\begin{flushleft}
matrix. First suppose that x satisfies Ax = 0, P x = 0, and x = 0. Then
\end{flushleft}


\begin{flushleft}
P
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
x
\end{flushleft}


0





= 0,





\begin{flushleft}
which shows that the KKT matrix is singular.
\end{flushleft}


\begin{flushleft}
Now suppose the KKT matrix is singular, i.e., there are x, z, not both zero, such
\end{flushleft}


\begin{flushleft}
that
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
P AT
\end{flushleft}


= 0.


\begin{flushleft}
z
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}


0


\begin{flushleft}
This means that P x + AT z = 0 and Ax = 0, so multiplying the first equation on the
\end{flushleft}


\begin{flushleft}
left by xT , we find xT P x + xT AT z = 0. Using Ax = 0, this reduces to xT P x = 0,
\end{flushleft}


\begin{flushleft}
so we have P x = 0 (using P
\end{flushleft}


\begin{flushleft}
0). This contradicts (a), unless x = 0. In this case,
\end{flushleft}


\begin{flushleft}
we must have z = 0. But then AT z = 0 contradicts rank A = p.
\end{flushleft}


\begin{flushleft}
(b) From part (a), P +AT A
\end{flushleft}


\begin{flushleft}
such that
\end{flushleft}





\begin{flushleft}
0. Therefore there exists a nonsingular matrix R $\in$ Rn×n
\end{flushleft}


\begin{flushleft}
RT (P + AT A)R = I.
\end{flushleft}





\begin{flushleft}
Let AR = U $\Sigma$V1T be the singular value decomposition of AR, with U $\in$ Rp×p ,
\end{flushleft}


\begin{flushleft}
$\Sigma$ = diag($\sigma$1 , . . . , $\sigma$p ) $\in$ Rp×p and V1 $\in$ Rn×p . Let V2 $\in$ Rn×(n$-$p) be such that
\end{flushleft}


\begin{flushleft}
V =
\end{flushleft}





\begin{flushleft}
V1
\end{flushleft}





\begin{flushleft}
V2
\end{flushleft}





\newpage
10


\begin{flushleft}
is orthogonal, and define
\end{flushleft}


\begin{flushleft}
S=
\end{flushleft}





\begin{flushleft}
$\Sigma$
\end{flushleft}





0





\begin{flushleft}
We have AR = U SV T , so
\end{flushleft}





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
$\in$ Rp×n .
\end{flushleft}





\begin{flushleft}
V T RT (P + AT A)RV = V T RT P RV + S T S = I.
\end{flushleft}


\begin{flushleft}
Therefore V T RT P RV = I $-$ S T S is diagonal. We denote this matrix by $\Lambda$:
\end{flushleft}


\begin{flushleft}
$\Lambda$ = V T RT P RV = diag(1 $-$ $\sigma$12 , . . . , 1 $-$ $\sigma$p2 , 1, . . . , 1).
\end{flushleft}


\begin{flushleft}
Applying a congruence transformation to the KKT matrix gives
\end{flushleft}


\begin{flushleft}
V T RT
\end{flushleft}


0





0


\begin{flushleft}
UT
\end{flushleft}





\begin{flushleft}
P
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
RV
\end{flushleft}


0





0


\begin{flushleft}
U
\end{flushleft}





\begin{flushleft}
$\Lambda$
\end{flushleft}


\begin{flushleft}
S
\end{flushleft}





=





\begin{flushleft}
ST
\end{flushleft}


0





,





\begin{flushleft}
and the inertia of the KKT matrix is equal to the inertia of the matrix on the right.
\end{flushleft}


\begin{flushleft}
Applying a permutation to the matrix on the right gives a block diagonal matrix
\end{flushleft}


\begin{flushleft}
with n diagonal blocks
\end{flushleft}


\begin{flushleft}
$\lambda$i
\end{flushleft}


\begin{flushleft}
$\sigma$i
\end{flushleft}





\begin{flushleft}
$\sigma$i
\end{flushleft}


0





,





\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
$\lambda$i = 1,
\end{flushleft}





\begin{flushleft}
i = p + 1, . . . , n.
\end{flushleft}





\begin{flushleft}
The eigenvalues of the 2 × 2-blocks are
\end{flushleft}


\begin{flushleft}
$\lambda$2i + 4$\sigma$i2
\end{flushleft}


,


2





\begin{flushleft}
$\lambda$i $\pm$
\end{flushleft}





\begin{flushleft}
i.e., one eigenvalue is positive and one is negative. We conclude that there are
\end{flushleft}


\begin{flushleft}
p + (n $-$ p) = n positive eigenvalues and p negative eigenvalues.
\end{flushleft}


\begin{flushleft}
10.2 Projected gradient method. In this problem we explore an extension of the gradient method
\end{flushleft}


\begin{flushleft}
to equality constrained minimization problems. Suppose f is convex and differentiable,
\end{flushleft}


\begin{flushleft}
and x $\in$ dom f satisfies Ax = b, where A $\in$ Rp×n with rank A = p $<$ n. The Euclidean
\end{flushleft}


\begin{flushleft}
projection of the negative gradient $-$$\nabla$f (x) on N (A) is given by
\end{flushleft}


\begin{flushleft}
∆xpg = argmin $-$$\nabla$f (x) $-$ u 2 .
\end{flushleft}


\begin{flushleft}
Au=0
\end{flushleft}





\begin{flushleft}
(a) Let (v, w) be the unique solution of
\end{flushleft}


\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
v
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





=





\begin{flushleft}
$-$$\nabla$f (x)
\end{flushleft}


0





\begin{flushleft}
Show that v = ∆xpg and w = argminy $\nabla$f (x) + AT y
\end{flushleft}





.





2.





\begin{flushleft}
(b) What is the relation between the projected negative gradient ∆xpg and the negative
\end{flushleft}


\begin{flushleft}
gradient of the reduced problem (10.5), assuming F T F = I?
\end{flushleft}


\begin{flushleft}
(c) The projected gradient method for solving an equality constrained minimization
\end{flushleft}


\begin{flushleft}
problem uses the step ∆xpg , and a backtracking line search on f . Use the results of part (b) to give some conditions under which the projected gradient method
\end{flushleft}


\begin{flushleft}
converges to the optimal solution, when started from a point x(0) $\in$ dom f with
\end{flushleft}


\begin{flushleft}
Ax(0) = b.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) These are the optimality conditions for the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$$\nabla$f (x) $-$ u
\end{flushleft}


\begin{flushleft}
Au = 0.
\end{flushleft}





2


2





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) If F T F = I, then ∆xpg = $-$F $\nabla$f˜(F z + x
\end{flushleft}


\begin{flushleft}
ˆ) where x = F z + x
\end{flushleft}


ˆ.





\begin{flushleft}
(c) By part (b), running the projected gradient from x(0) is the same as running the
\end{flushleft}


\begin{flushleft}
gradient method on the reduced problem, assuming F T F = I. This means that
\end{flushleft}


\begin{flushleft}
the projected gradient method converges if the initial sublevel set \{x | f (x) $\leq$
\end{flushleft}


\begin{flushleft}
f (x(0) ), Ax = b\} is closed and the objective function of the reduced or eliminated
\end{flushleft}


\begin{flushleft}
problem, f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ) is strongly convex.
\end{flushleft}





\begin{flushleft}
Newton's method with equality constraints
\end{flushleft}


\begin{flushleft}
10.3 Dual Newton method. In this problem we explore Newton's method for solving the dual
\end{flushleft}


\begin{flushleft}
of the equality constrained minimization problem (10.1). We assume that f is twice
\end{flushleft}


\begin{flushleft}
differentiable, $\nabla$2 f (x) 0 for all x $\in$ dom f , and that for each $\nu$ $\in$ Rp , the Lagrangian
\end{flushleft}


\begin{flushleft}
L(x, $\nu$) = f (x) + $\nu$ T (Ax $-$ b) has a unique minimizer, which we denote x($\nu$).
\end{flushleft}


\begin{flushleft}
(a) Show that the dual function g is twice differentiable. Find an expression for the
\end{flushleft}


\begin{flushleft}
Newton step for the dual function g, evaluated at $\nu$, in terms of f , $\nabla$f , and $\nabla$2 f ,
\end{flushleft}


\begin{flushleft}
evaluated at x = x($\nu$). You can use the results of exercise 3.40.
\end{flushleft}


\begin{flushleft}
(b) Suppose there exists a K such that
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





$-$1





\begin{flushleft}
$\leq$K
\end{flushleft}


2





\begin{flushleft}
for all x $\in$ dom f . Show that g is strongly concave, with $\nabla$2 g($\nu$)
\end{flushleft}





\begin{flushleft}
$-$(1/K)I.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) By the results of exercise 3.40, g is twice differentiable, with
\end{flushleft}


\begin{flushleft}
$\nabla$g($\nu$)
\end{flushleft}





\begin{flushleft}
$\nabla$2 g($\nu$)
\end{flushleft}





=





\begin{flushleft}
A$\nabla$f ∗ ($-$AT $\nu$) = Ax($\nu$)
\end{flushleft}





=





\begin{flushleft}
$-$A$\nabla$2 f ∗ ($-$AT $\nu$)AT = $-$A$\nabla$2 f (x($\nu$))$-$1 AT .
\end{flushleft}





\begin{flushleft}
Therefore the Newton step for g at $\nu$ is given by
\end{flushleft}


\begin{flushleft}
∆$\nu$nt = (A$\nabla$2 f (x($\nu$))$-$1 AT )$-$1 Ax($\nu$).
\end{flushleft}


\begin{flushleft}
(b) Now suppose
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





$-$1





\begin{flushleft}
$\leq$K
\end{flushleft}


2





\begin{flushleft}
for all x $\in$ x(S) = \{x($\nu$) | $\nu$ $\in$ S\}. Using the expression
\end{flushleft}


\begin{flushleft}
H
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





$-$1





\begin{flushleft}
AT
\end{flushleft}


0





=





\begin{flushleft}
H $-$1
\end{flushleft}


0





0


0





$-$





\begin{flushleft}
H $-$1 AT
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





\begin{flushleft}
(AH $-$1 AT )$-$1
\end{flushleft}





\begin{flushleft}
AH $-$1
\end{flushleft}





\begin{flushleft}
(with H = $\nabla$2 f (x)), we see that
\end{flushleft}


\begin{flushleft}
H
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





$-$1





$\geq$


2





=





\begin{flushleft}
u 2 =1
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}





=





\begin{flushleft}
sup
\end{flushleft}





\begin{flushleft}
(AH
\end{flushleft}





$-$1





0


\begin{flushleft}
u
\end{flushleft}





$-$1





\begin{flushleft}
T $-$1
\end{flushleft}





\begin{flushleft}
A )
\end{flushleft}





\begin{flushleft}
(AH $-$1 AT )$-$1 u
\end{flushleft}





\begin{flushleft}
T $-$1
\end{flushleft}





\begin{flushleft}
A )
\end{flushleft}





\begin{flushleft}
u 2 =1
\end{flushleft}





\begin{flushleft}
(AH
\end{flushleft}





$-$1





2





\begin{flushleft}
H $-$1 AT
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





\begin{flushleft}
u 2 =1
\end{flushleft}





$\geq$





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
H
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
sup
\end{flushleft}





2





2





\begin{flushleft}
u
\end{flushleft}





2





\begin{flushleft}
$-$I
\end{flushleft}





\newpage
10





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
for all x $\in$ x(S), which implies that
\end{flushleft}


\begin{flushleft}
$\nabla$2 g($\nu$) = $-$A$\nabla$2 f (x($\nu$))$-$1 AT
\end{flushleft}





\begin{flushleft}
$-$(1/K)I
\end{flushleft}





\begin{flushleft}
for all $\nu$ $\in$ S.
\end{flushleft}


\begin{flushleft}
10.4 Strong convexity and Lipschitz constant of the reduced problem. Suppose f satisfies the
\end{flushleft}


\begin{flushleft}
assumptions given on page 529. Show that the reduced objective function f˜(z) = f (F z+ˆ
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
is strongly convex, and that its Hessian is Lipschitz continuous (on the associated sublevel
\end{flushleft}


\begin{flushleft}
˜ Express the strong convexity and Lipschitz constants of f˜ in terms of K, M , L,
\end{flushleft}


\begin{flushleft}
set S).
\end{flushleft}


\begin{flushleft}
and the maximum and minimum singular values of F .
\end{flushleft}


\begin{flushleft}
Solution. In the text it was shown that $\nabla$2 f˜(z) mI, for m = $\sigma$min (F )2 /(K 2 M ). Here
\end{flushleft}


\begin{flushleft}
we establish the other properties of f˜. We have
\end{flushleft}


\begin{flushleft}
$\nabla$2 f˜(z)
\end{flushleft}





2





\begin{flushleft}
= F T $\nabla$2 f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ)F
\end{flushleft}





2





\begin{flushleft}
$\leq$ F
\end{flushleft}





2


\begin{flushleft}
2 M,
\end{flushleft}





\begin{flushleft}
˜ I, with M
\end{flushleft}


\begin{flushleft}
˜ = F
\end{flushleft}


\begin{flushleft}
using $\nabla$f 2 (x) 2 $\leq$ M . Therefore we have $\nabla$2 f˜(z) M
\end{flushleft}


2 ˜


\begin{flushleft}
Now we establish that $\nabla$ f (z) satisfies a Lipschitz condition:
\end{flushleft}


\begin{flushleft}
$\nabla$2 f˜(z) $-$ $\nabla$2 f˜(w)
\end{flushleft}





2





2


\begin{flushleft}
2M .
\end{flushleft}





\begin{flushleft}
F T ($\nabla$2 f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ) $-$ $\nabla$2 f (F w + x
\end{flushleft}


\begin{flushleft}
ˆ))F
\end{flushleft}





=


$\leq$





\begin{flushleft}
F
\end{flushleft}





$\leq$





2


2





\begin{flushleft}
L F
\end{flushleft}





$\leq$





\begin{flushleft}
L F
\end{flushleft}





2





2


2


3


2





2





\begin{flushleft}
$\nabla$ f (F z + x
\end{flushleft}


\begin{flushleft}
ˆ) $-$ $\nabla$ f (F w + x
\end{flushleft}


ˆ)


\begin{flushleft}
F (z $-$ w)
\end{flushleft}





\begin{flushleft}
z$-$w
\end{flushleft}





2


2





2





2.





\begin{flushleft}
˜ = L F 32 .
\end{flushleft}


\begin{flushleft}
Thus, $\nabla$2 f˜(z) satisfies a Lipschitz condition with constant L
\end{flushleft}


\begin{flushleft}
10.5 Adding a quadratic term to the objective. Suppose Q 0. The problem
\end{flushleft}


\begin{flushleft}
f (x) + (Ax $-$ b)T Q(Ax $-$ b)
\end{flushleft}


\begin{flushleft}
Ax = b
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
is equivalent to the original equality constrained optimization problem (10.1). Is the
\end{flushleft}


\begin{flushleft}
Newton step for this problem the same as the Newton step for the original problem?
\end{flushleft}


\begin{flushleft}
Solution. The Newton step of the new problem satisfies
\end{flushleft}


\begin{flushleft}
H + AT QA
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
∆x
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





=





\begin{flushleft}
$-$g $-$ 2AT QAx + 2AT Qb
\end{flushleft}


0





.





\begin{flushleft}
From the second equation, A∆x = 0. Therefore,
\end{flushleft}


\begin{flushleft}
H
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
∆x
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





\begin{flushleft}
and
\end{flushleft}


\begin{flushleft}
H
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





=





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
$-$g $-$ 2AT QAx + 2AT Qb
\end{flushleft}


0


\begin{flushleft}
∆x
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}


˜





=





\begin{flushleft}
$-$g
\end{flushleft}


0





,





,





\begin{flushleft}
where w
\end{flushleft}


\begin{flushleft}
ˆ = w + 2QAx $-$ 2Qb. We conclude that the Newton steps are equal. Note the
\end{flushleft}


\begin{flushleft}
connection to the last statement in exercise 10.1.
\end{flushleft}


\begin{flushleft}
10.6 The Newton decrement. Show that (10.13) holds, i.e.,
\end{flushleft}


\begin{flushleft}
f (x) $-$ inf\{f (x + v) | A(x + v) = b\} = $\lambda$(x)2 /2.
\end{flushleft}


\begin{flushleft}
Solution. The Newton step is defined by
\end{flushleft}


\begin{flushleft}
H
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





\begin{flushleft}
∆x
\end{flushleft}


\begin{flushleft}
w
\end{flushleft}





=





\begin{flushleft}
$-$g
\end{flushleft}


0





.





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We first note that this implies that ∆xT H∆x = $-$g T ∆x. Therefore
\end{flushleft}


\begin{flushleft}
fˆ(x + ∆x)
\end{flushleft}





=





\begin{flushleft}
f (x) + g T v + (1/2)v T Hv
\end{flushleft}





=





\begin{flushleft}
f (x) + (1/2)g T v
\end{flushleft}





=





\begin{flushleft}
f (x) $-$ (1/2)$\lambda$(x)2 .
\end{flushleft}





\begin{flushleft}
Infeasible start Newton method
\end{flushleft}


\begin{flushleft}
10.7 Assumptions for infeasible start Newton method. Consider the set of assumptions given
\end{flushleft}


\begin{flushleft}
on page 536.
\end{flushleft}


\begin{flushleft}
(a) Suppose that the function f is closed. Show that this implies that the norm of the
\end{flushleft}


\begin{flushleft}
residual, r(x, $\nu$) 2 , is closed.
\end{flushleft}


\begin{flushleft}
Solution. Recall from §A.3.3 that a continuous function h with an open domain
\end{flushleft}


\begin{flushleft}
is closed if h(y) tends to infinity as y approaches the boundary of dom h. The
\end{flushleft}


\begin{flushleft}
function r 2 : Rn × Rp $\rightarrow$ R is clearly continuous (by assumption f is continuously
\end{flushleft}


\begin{flushleft}
differentiable), and its domain, dom f × Rp , is open. Now suppose f is closed.
\end{flushleft}


\begin{flushleft}
Consider a sequence of points (x(k) , $\nu$ (k) ) $\in$ dom r 2 converging to a limit (¯
\end{flushleft}


\begin{flushleft}
x, $\nu$¯) $\in$
\end{flushleft}


\begin{flushleft}
bd dom r 2 . Then x
\end{flushleft}


\begin{flushleft}
¯ $\in$ bd dom f , and since f is closed, f (x(k) ) $\rightarrow$ $\infty$, hence
\end{flushleft}


\begin{flushleft}
$\nabla$f (x(k) ) 2 $\rightarrow$ $\infty$, and r(x(k) , $\nu$ (k) ) 2 $\rightarrow$ $\infty$. We conclude that r 2 is closed.
\end{flushleft}





\begin{flushleft}
(b) Show that Dr satisfies a Lipschitz condition if and only if $\nabla$2 f does.
\end{flushleft}


\begin{flushleft}
Solution. First suppose that $\nabla$2 f satisfies the Lipschitz condition
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x) $-$ $\nabla$2 f (˜
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}





2





\begin{flushleft}
$\leq$L x$-$x
\end{flushleft}


˜





2





\begin{flushleft}
for x, x
\end{flushleft}


\begin{flushleft}
˜ $\in$ S. From this we get a Lipschitz condition on Dr: If y = (x, $\nu$) $\in$ S, and
\end{flushleft}


\begin{flushleft}
y˜ = (˜
\end{flushleft}


\begin{flushleft}
x, $\nu$˜) $\in$ S, then
\end{flushleft}


\begin{flushleft}
Dr(y) $-$ Dr(˜
\end{flushleft}


\begin{flushleft}
y)
\end{flushleft}





2





=





\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





=





\begin{flushleft}
$\nabla$2 f (x) $-$ $\nabla$2 f (˜
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


0


\begin{flushleft}
$\nabla$2 f (x) $-$ $\nabla$2 f (˜
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
L x$-$x
\end{flushleft}


˜ 2


\begin{flushleft}
L y $-$ y˜ 2 .
\end{flushleft}





=


$\leq$


$\leq$





\begin{flushleft}
$\nabla$2 f (˜
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





$-$





0


0





\begin{flushleft}
AT
\end{flushleft}


0





2





2





2





\begin{flushleft}
To show the converse, suppose that Dr satisfies a Lipschitz condition with constant
\end{flushleft}


\begin{flushleft}
L. Using the equations above this means that
\end{flushleft}


\begin{flushleft}
Dr(y) $-$ Dr(˜
\end{flushleft}


\begin{flushleft}
y)
\end{flushleft}





2





\begin{flushleft}
= $\nabla$2 f (x) $-$ $\nabla$2 f (˜
\end{flushleft}


\begin{flushleft}
x)
\end{flushleft}





2





\begin{flushleft}
$\leq$ L y $-$ y˜
\end{flushleft}





2





\begin{flushleft}
for all y and y˜. In particular, taking $\nu$ = $\nu$˜ = 0, this reduces to a Lipschitz condition
\end{flushleft}


\begin{flushleft}
for $\nabla$2 f , with constant L.
\end{flushleft}


\begin{flushleft}
10.8 Infeasible start Newton method and initially satisfied equality constraints. Suppose we use
\end{flushleft}


\begin{flushleft}
the infeasible start Newton method to minimize f (x) subject to aTi x = bi , i = 1, . . . , p.
\end{flushleft}


\begin{flushleft}
(a) Suppose the initial point x(0) satisfies the linear equality aTi x = bi . Show that the
\end{flushleft}


\begin{flushleft}
linear equality will remain satisfied for future iterates, i.e., if aTi x(k) = bi for all k.
\end{flushleft}


\begin{flushleft}
(b) Suppose that one of the equality constraints becomes satisfied at iteration k, i.e.,
\end{flushleft}


\begin{flushleft}
we have aTi x(k$-$1) = bi , aTi x(k) = bi . Show that at iteration k, all the equality
\end{flushleft}


\begin{flushleft}
constraints are satisfied.
\end{flushleft}





\newpage
10





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
Follows easily from
\end{flushleft}


\begin{flushleft}
k$-$1
\end{flushleft}





\begin{flushleft}
r(k) =
\end{flushleft}


\begin{flushleft}
i=0
\end{flushleft}





\begin{flushleft}
(1 $-$ t(i) )
\end{flushleft}





\begin{flushleft}
r(0) .
\end{flushleft}





\begin{flushleft}
10.9 Equality constrained entropy maximization. Consider the equality constrained entropy
\end{flushleft}


\begin{flushleft}
maximization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
Ax = b,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
xi log xi
\end{flushleft}





(10.42)





\begin{flushleft}
p×n
\end{flushleft}


\begin{flushleft}
with dom f = Rn
\end{flushleft}


\begin{flushleft}
. We assume the problem is feasible and that rank A =
\end{flushleft}


\begin{flushleft}
++ and A $\in$ R
\end{flushleft}


\begin{flushleft}
p $<$ n.
\end{flushleft}





\begin{flushleft}
(a) Show that the problem has a unique optimal solution x .
\end{flushleft}


\begin{flushleft}
(b) Find A, b, and feasible x(0) for which the sublevel set
\end{flushleft}


(0)


\begin{flushleft}
\{x $\in$ Rn
\end{flushleft}


)\}


\begin{flushleft}
++ | Ax = b, f (x) $\leq$ f (x
\end{flushleft}





\begin{flushleft}
is not closed. Thus, the assumptions listed in §10.2.4, page 529, are not satisfied for
\end{flushleft}


\begin{flushleft}
some feasible initial points.
\end{flushleft}


\begin{flushleft}
(c) Show that the problem (10.42) satisfies the assumptions for the infeasible start
\end{flushleft}


\begin{flushleft}
Newton method listed in §10.3.3, page 536, for any feasible starting point.
\end{flushleft}


\begin{flushleft}
(d) Derive the Lagrange dual of (10.42), and explain how to find the optimal solution
\end{flushleft}


\begin{flushleft}
of (10.42) from the optimal solution of the dual problem. Show that the dual problem
\end{flushleft}


\begin{flushleft}
satisfies the assumptions listed in §10.2.4, page 529, for any starting point.
\end{flushleft}


\begin{flushleft}
The results of part (b), (c), and (d) do not mean the standard Newton method will fail,
\end{flushleft}


\begin{flushleft}
or that the infeasible start Newton method or dual method will work better in practice.
\end{flushleft}


\begin{flushleft}
It only means our convergence analysis for the standard Newton method does not apply,
\end{flushleft}


\begin{flushleft}
while our convergence analysis does apply to the infeasible start and dual methods. (See
\end{flushleft}


\begin{flushleft}
exercise 10.15.)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) If p is not attained, then either p is attained asymptotically, as x goes to infinity,
\end{flushleft}


\begin{flushleft}
or in the limit as x goes to x , where x
\end{flushleft}


\begin{flushleft}
0 with one or more zero components.
\end{flushleft}


\begin{flushleft}
The first possibility cannot occur because the entropy goes to infinity as x goes to
\end{flushleft}


\begin{flushleft}
infinity. The second possibility can also be ruled out, because by assumption the
\end{flushleft}


\begin{flushleft}
problem is feasible. Suppose x
\end{flushleft}


\begin{flushleft}
˜ 0 and A˜
\end{flushleft}


\begin{flushleft}
x = b. Define v = x
\end{flushleft}


\begin{flushleft}
˜ $-$ x and
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
g(t) =
\end{flushleft}





\begin{flushleft}
(xi + tvi ) log(xi + tvi )
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
for t $>$ 0. The derivative is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
vi (1 + log(xi + tvi ).
\end{flushleft}





\begin{flushleft}
g (t) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Now if xi = 0 for some i, then vi $>$ 0, and hence limt$\rightarrow$0 g(t) = $-$$\infty$. This means it
\end{flushleft}


\begin{flushleft}
is impossible that limt$\rightarrow$0 g(t) = p .
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) Consider
\end{flushleft}


2


1





\begin{flushleft}
A=
\end{flushleft}





1


1





0


1





,





1


1





\begin{flushleft}
b=
\end{flushleft}





,





\begin{flushleft}
and starting point x(0) = (1/20, 9/10, 1/20). Eliminating x2 and x3 from the two
\end{flushleft}


\begin{flushleft}
equations
\end{flushleft}


\begin{flushleft}
2x1 + x2 = 1,
\end{flushleft}


\begin{flushleft}
x 1 + x2 + x3 = 1
\end{flushleft}


\begin{flushleft}
gives x2 = 1 $-$ 2x1 , x3 = x1 . For x(0) = (1/20, 9/10, 1/20), with f (x(0) ) = $-$0.3944
\end{flushleft}


\begin{flushleft}
we have f (x1 , 1 $-$ 2x1 , x1 ) $\leq$ f (x(0) ) if and only if 1/20 $\leq$ x1 $<$ 0.5, which is not
\end{flushleft}


\begin{flushleft}
closed.
\end{flushleft}


\begin{flushleft}
(c) The dual problem is
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$-$bT $\nu$ $-$
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}





\begin{flushleft}
exp($-$1 $-$ aTi $\nu$)
\end{flushleft}





\begin{flushleft}
where ai is the ith column of A. The dual objective function is closed with domain
\end{flushleft}


\begin{flushleft}
Rp .
\end{flushleft}


\begin{flushleft}
(d) We have
\end{flushleft}


\begin{flushleft}
r(x, $\nu$) = ($\nabla$f (x) + AT $\nu$, Ax $-$ b)
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
$\nabla$f (x)i = log xi + 1,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , n.
\end{flushleft}





\begin{flushleft}
We show that r 2 is a closed function.
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
Clearly r 2 is continuous on its domain, Rn
\end{flushleft}


\begin{flushleft}
++ × R .
\end{flushleft}





\begin{flushleft}
Suppose (x(k) , $\nu$ (k) ), k = 1, 2, . . . is a sequence of points converging to a point (¯
\end{flushleft}


\begin{flushleft}
x, $\nu$¯) $\in$
\end{flushleft}


\begin{flushleft}
(k)
\end{flushleft}


\begin{flushleft}
bd dom r 2 . We have x
\end{flushleft}


\begin{flushleft}
¯i = 0 for at least one i, so log xi + 1 + aTi $\nu$ (k) $\rightarrow$ $-$$\infty$.
\end{flushleft}


\begin{flushleft}
Hence r(x(k) , $\nu$ (k) 2 $\rightarrow$ $\infty$.
\end{flushleft}


\begin{flushleft}
We conclude that r satisfies the sublevel set condition for arbitrary starting points.
\end{flushleft}


\begin{flushleft}
10.10 Bounded inverse derivative condition for strongly convex-concave game. Consider a convexconcave game with payoff function f (see page 541). Suppose $\nabla$2uu f (u, v)
\end{flushleft}


\begin{flushleft}
mI and
\end{flushleft}


\begin{flushleft}
$\nabla$2vv f (u, v) $-$mI, for all (u, v) $\in$ dom f . Show that
\end{flushleft}


\begin{flushleft}
Dr(u, v)$-$1
\end{flushleft}





2





\begin{flushleft}
= $\nabla$2 f (u, v)$-$1
\end{flushleft}





2





\begin{flushleft}
$\leq$ 1/m.
\end{flushleft}





\begin{flushleft}
Solution. Let
\end{flushleft}


\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
ET
\end{flushleft}





\begin{flushleft}
H = $\nabla$2 f (u, v) =
\end{flushleft}





\begin{flushleft}
E
\end{flushleft}


\begin{flushleft}
$-$F
\end{flushleft}





\begin{flushleft}
where D $\in$ Sp , F $\in$ Sq , E $\in$ Rp×q , and assume D mI, F
\end{flushleft}


\begin{flushleft}
mI. Let D $-$1/2 EF $-$1/2 =
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
p×r
\end{flushleft}


\begin{flushleft}
U1 $\Sigma$V1 be the singular value decomposition (U1 $\in$ R
\end{flushleft}


\begin{flushleft}
, V1 $\in$ Rq×r , $\Sigma$ $\in$ Rr×r , r =
\end{flushleft}


\begin{flushleft}
p×(p$-$r)
\end{flushleft}


\begin{flushleft}
q×(q$-$r)
\end{flushleft}


\begin{flushleft}
rank E). Choose U2 $\in$ R
\end{flushleft}


\begin{flushleft}
and V2 $\in$ R
\end{flushleft}


\begin{flushleft}
, so that U2T U2 = I, U2T U1 = 0 and
\end{flushleft}


\begin{flushleft}
V2T V2 = I, V2T V1 = 0. Define
\end{flushleft}


\begin{flushleft}
U=
\end{flushleft}





\begin{flushleft}
U1
\end{flushleft}





\begin{flushleft}
U2
\end{flushleft}





\begin{flushleft}
$\in$ Rp×p ,
\end{flushleft}





\begin{flushleft}
V =
\end{flushleft}





\begin{flushleft}
V1
\end{flushleft}





\begin{flushleft}
$\in$ Rp×p ,
\end{flushleft}





\begin{flushleft}
V2
\end{flushleft}





\begin{flushleft}
S=
\end{flushleft}





\begin{flushleft}
$\Sigma$1
\end{flushleft}


0





0


0





\begin{flushleft}
$\in$ Rp×q .
\end{flushleft}





\begin{flushleft}
With these definitions we have D $-$1/2 EF $-$1/2 = U SV T = U1 $\Sigma$V1T , and
\end{flushleft}


\begin{flushleft}
H
\end{flushleft}





=





\begin{flushleft}
D1/2 U
\end{flushleft}


0





0


\begin{flushleft}
F
\end{flushleft}





1/2





\begin{flushleft}
V
\end{flushleft}





\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
ST
\end{flushleft}





\begin{flushleft}
U T D1/2
\end{flushleft}


0





\begin{flushleft}
S
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





0


\begin{flushleft}
V T F 1/2
\end{flushleft}





.





\begin{flushleft}
Therefore
\end{flushleft}


\begin{flushleft}
H $-$1
\end{flushleft}





=





\begin{flushleft}
U T D$-$1/2
\end{flushleft}


0





0


\begin{flushleft}
V T F $-$1/2
\end{flushleft}





\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
ST
\end{flushleft}





\begin{flushleft}
S
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





$-$1





\begin{flushleft}
D$-$1/2 U
\end{flushleft}


0





0


\begin{flushleft}
F $-$1/2 V
\end{flushleft}





\newpage
10


\begin{flushleft}
and H $-$1
\end{flushleft}





2





\begin{flushleft}
$\leq$ (1/m) G$-$1
\end{flushleft}





2,





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
ST
\end{flushleft}





\begin{flushleft}
G=
\end{flushleft}





\begin{flushleft}
S
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





.





\begin{flushleft}
We can permute the rows and columns of G so that it is block diagonal with max\{p, q\} $-$ r
\end{flushleft}


\begin{flushleft}
scalar diagonal blocks with value 1, max\{p, q\} $-$ r scalar diagonal blocks with value $-$1,
\end{flushleft}


\begin{flushleft}
and r diagonal blocks of the form
\end{flushleft}


1


\begin{flushleft}
$\sigma$i
\end{flushleft}





\begin{flushleft}
$\sigma$i
\end{flushleft}


$-$1





.





\begin{flushleft}
Note that
\end{flushleft}


1


\begin{flushleft}
$\sigma$i
\end{flushleft}





\begin{flushleft}
$\sigma$i
\end{flushleft}


$-$1





$-$1





=





1


\begin{flushleft}
1 + $\sigma$i2
\end{flushleft}





=





1


\begin{flushleft}
$\sigma$i
\end{flushleft}





\begin{flushleft}
$\sigma$i
\end{flushleft}


$-$1





\begin{flushleft}
1/ 1 + $\sigma$i2
\end{flushleft}


\begin{flushleft}
$\sigma$i / 1 + $\sigma$i2
\end{flushleft}





\begin{flushleft}
and therefore
\end{flushleft}


1


\begin{flushleft}
$\sigma$i
\end{flushleft}


\begin{flushleft}
If r = max\{p, q\}, then G$-$1
\end{flushleft}





\begin{flushleft}
In conclusion,
\end{flushleft}





2





\begin{flushleft}
$\sigma$i / 1 + $\sigma$i2
\end{flushleft}


\begin{flushleft}
$-$1/ 1 + $\sigma$i2
\end{flushleft}


$-$1





\begin{flushleft}
$\sigma$i
\end{flushleft}


$-$1





1





=





\begin{flushleft}
1 + $\sigma$i2
\end{flushleft}





2





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





1


\begin{flushleft}
1+$\sigma$i2
\end{flushleft}





0





0


1


\begin{flushleft}
1+$\sigma$i2
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
,
\end{flushleft}





.





\begin{flushleft}
= 1. Otherwise
\end{flushleft}





\begin{flushleft}
G$-$1
\end{flushleft}





2





\begin{flushleft}
= max(1 + $\sigma$i2 )$-$1/2 $\leq$ 1.
\end{flushleft}





\begin{flushleft}
H $-$1
\end{flushleft}





2





\begin{flushleft}
$\leq$ (1/m) G$-$1
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





2





\begin{flushleft}
$\leq$ 1/m.
\end{flushleft}





\begin{flushleft}
Implementation
\end{flushleft}


\begin{flushleft}
10.11 Consider the resource allocation problem described in example 10.1. You can assume the
\end{flushleft}


\begin{flushleft}
fi are strongly convex, i.e., fi (z) $\geq$ m $>$ 0 for all z.
\end{flushleft}





\begin{flushleft}
(a) Find the computational effort required to compute a Newton step for the reduced
\end{flushleft}


\begin{flushleft}
problem. Be sure to exploit the special structure of the Newton equations.
\end{flushleft}


\begin{flushleft}
(b) Explain how to solve the problem via the dual. You can assume that the conjugate
\end{flushleft}


\begin{flushleft}
functions fi∗ , and their derivatives, are readily computable, and that the equation
\end{flushleft}


\begin{flushleft}
fi (x) = $\nu$ is readily solved for x, given $\nu$. What is the computational complexity of
\end{flushleft}


\begin{flushleft}
finding a Newton step for the dual problem?
\end{flushleft}


\begin{flushleft}
(c) What is the computational complexity of computing a Newton step for the resource
\end{flushleft}


\begin{flushleft}
allocation problem? Be sure to exploit the special structure of the KKT equations.
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The reduced problem is
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
f˜(z) =
\end{flushleft}





\begin{flushleft}
n$-$1
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
fi (zi ) + fn (b $-$ 1T z).
\end{flushleft}





\begin{flushleft}
The Newton equation is
\end{flushleft}


\begin{flushleft}
(D + d11T )∆z = g.
\end{flushleft}


\begin{flushleft}
where D is diagonal with Dii = fi (zi ) and d = fn (b $-$ 1T z).
\end{flushleft}


\begin{flushleft}
The cost of computing ∆z is order n, if we use the matrix inversion lemma.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) The dual problem is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
g($\nu$) = $-$b$\nu$ $-$
\end{flushleft}





\begin{flushleft}
maxmize
\end{flushleft}





\begin{flushleft}
fi∗ ($-$$\nu$).
\end{flushleft}





\begin{flushleft}
From the solution of exercise 10.3,
\end{flushleft}


\begin{flushleft}
g ($\nu$) = 1T x($\nu$),
\end{flushleft}





\begin{flushleft}
g ($\nu$) = $-$1T $\nabla$2 f (x($\nu$))$-$1 1,
\end{flushleft}





\begin{flushleft}
where $\nabla$2 f (x($\nu$)) is diagonal with diagonal elements fi (xi ($\nu$)). The cost of forming
\end{flushleft}


\begin{flushleft}
g ($\nu$) is order n.
\end{flushleft}


\begin{flushleft}
(c) The KKT system is
\end{flushleft}


\begin{flushleft}
D 1
\end{flushleft}


\begin{flushleft}
∆x
\end{flushleft}


\begin{flushleft}
$-$g
\end{flushleft}


=


,


\begin{flushleft}
w
\end{flushleft}


0


\begin{flushleft}
1T 0
\end{flushleft}


\begin{flushleft}
which can be solved in order n operations by eliminating ∆x.
\end{flushleft}


\begin{flushleft}
10.12 Describe an efficient way to compute the Newton step for the problem
\end{flushleft}


\begin{flushleft}
tr(X $-$1 )
\end{flushleft}


\begin{flushleft}
tr(Ai X) = bi ,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p
\end{flushleft}





\begin{flushleft}
Sn
\end{flushleft}


++ ,





\begin{flushleft}
with domain
\end{flushleft}


\begin{flushleft}
assuming p and n have the same order of magnitude. Also derive the
\end{flushleft}


\begin{flushleft}
Lagrange dual problem and give the complexity of finding the Newton step for the dual
\end{flushleft}


\begin{flushleft}
problem.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The gradient of f0 is $\nabla$f0 (X) = $-$X $-$2 . The optimality conditions are
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
$-$X $-$2 +
\end{flushleft}





\begin{flushleft}
wi Ai = 0,
\end{flushleft}





\begin{flushleft}
tr(Ai X) = bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Linearizing around X gives
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
$-$X $-$2 + X $-$1 ∆XX $-$2 + X $-$2 ∆XX $-$1 +
\end{flushleft}





\begin{flushleft}
wi Ai
\end{flushleft}





=





0





\begin{flushleft}
tr(Ai (X + ∆X))
\end{flushleft}





=





\begin{flushleft}
bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i.e.,
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
∆XX $-$1 + X $-$1 ∆X +
\end{flushleft}





\begin{flushleft}
wi (XAi X)
\end{flushleft}





=





\begin{flushleft}
I
\end{flushleft}





\begin{flushleft}
tr(Ai ∆X)
\end{flushleft}





=





\begin{flushleft}
bi $-$ tr(Ai X),
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
We can eliminate ∆X from the first equation by solving p + 1 Lyapunov equations:
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
w i Yi
\end{flushleft}





\begin{flushleft}
∆X = Y0 +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
Y0 X $-$1 + X $-$1 Y0 = I,
\end{flushleft}





\begin{flushleft}
Yi X $-$1 + X $-$1 Yi = XAi X,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p.
\end{flushleft}





\begin{flushleft}
Substituting in the second equation gives
\end{flushleft}


\begin{flushleft}
Hw = g,
\end{flushleft}


\begin{flushleft}
with Hi = tr(Yi Yj ), i, j = 1, . . . , p.
\end{flushleft}


\begin{flushleft}
The cost is order pn3 for computing Yi , p2 n2 for constructing H and p3 for solving
\end{flushleft}


\begin{flushleft}
the equations.
\end{flushleft}





\newpage
10





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
(b) The conjugate of f0 is given in exercise 3.37:
\end{flushleft}


\begin{flushleft}
f0∗ (Y ) = $-$2 tr($-$Y )$-$1/2 ,
\end{flushleft}





\begin{flushleft}
dom f0∗ = $-$Sn
\end{flushleft}


++ .





\begin{flushleft}
The dual problem is
\end{flushleft}





\begin{flushleft}
with domain \{$\nu$ $\in$ Rp |
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
g($\nu$) = $-$bT $\nu$ + 2 tr(
\end{flushleft}





\begin{flushleft}
maximize
\end{flushleft}





\begin{flushleft}
$\nu$i Ai
\end{flushleft}





\begin{flushleft}
$\nu$i Ai )1/2
\end{flushleft}





\begin{flushleft}
0\}. The optimality conditions are
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
2 tr(Ai $\nabla$g0 (Z)) = bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
$\nu$i Ai ,
\end{flushleft}





\begin{flushleft}
Z=
\end{flushleft}





\begin{flushleft}
(10.12.A)
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where g0 (Z) = tr Z 1/2 .
\end{flushleft}


\begin{flushleft}
The gradient of g0 is $\nabla$ tr(Z 1/2 ) = (1/2)Z $-$1/2 , as can be seen as follows. Suppose
\end{flushleft}


\begin{flushleft}
Z 0. For small symmetric ∆Z, (Z + ∆Z)1/2 $\approx$ Z 1/2 + ∆Y where
\end{flushleft}


\begin{flushleft}
Z + ∆Z
\end{flushleft}





=





\begin{flushleft}
(Z 1/2 + ∆Y )2
\end{flushleft}





$\approx$





\begin{flushleft}
Z + Z 1/2 ∆Y + ∆Y Z 1/2 ,
\end{flushleft}





\begin{flushleft}
i.e., ∆Y is the solution of the Lyapunov equation ∆Z = Z 1/2 ∆Y + ∆Y Z 1/2 . In
\end{flushleft}


\begin{flushleft}
particular,
\end{flushleft}


\begin{flushleft}
tr ∆Y = tr(Z $-$1/2 ∆Z) $-$ tr(Z $-$1/2 ∆Y Z 1/2 ) = tr(Z $-$1/2 ∆Z) $-$ tr ∆Y,
\end{flushleft}


\begin{flushleft}
i.e., tr ∆Y = (1/2) tr(Z $-$1/2 ∆Z). Therefore
\end{flushleft}


\begin{flushleft}
tr(Z + ∆Z)1/2
\end{flushleft}





\begin{flushleft}
tr Z 1/2 + tr ∆Y
\end{flushleft}





$\approx$





\begin{flushleft}
tr Z 1/2 + (1/2) tr(Z $-$1/2 ∆Z),
\end{flushleft}





=





\begin{flushleft}
i.e., $\nabla$Z tr Z 1/2 = (1/2)Z $-$1/2 .
\end{flushleft}


\begin{flushleft}
We can therefore simplify the optimality conditions (10.12.A) as
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
tr(Ai Z $-$1/2 ) = bi ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p,
\end{flushleft}





\begin{flushleft}
Z=
\end{flushleft}





\begin{flushleft}
$\nu$i Ai ,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Linearizing around Z, $\nu$ gives
\end{flushleft}


\begin{flushleft}
tr(Ai Z $-$1/2 ) + tr(Ai ∆Y )
\end{flushleft}


\begin{flushleft}
Z
\end{flushleft}





1/2





=





\begin{flushleft}
bi ,
\end{flushleft}





1/2





=





\begin{flushleft}
∆Z
\end{flushleft}





\begin{flushleft}
Z + ∆Z
\end{flushleft}





=





\begin{flushleft}
∆Y + ∆Y Z
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
∆$\nu$i Ai ,
\end{flushleft}





\begin{flushleft}
$\nu$i Ai +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i.e., after a simplification
\end{flushleft}


\begin{flushleft}
tr(Ai ∆Y )
\end{flushleft}





=





\begin{flushleft}
bi $-$ tr(Ai Z $-$1/2 ),
\end{flushleft}





\begin{flushleft}
i = 1, . . . , p
\end{flushleft}





\begin{flushleft}
p
\end{flushleft}





\begin{flushleft}
Z 1/2 ∆Y + ∆Y Z 1/2 $-$
\end{flushleft}





\begin{flushleft}
∆$\nu$i Ai
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





=





\begin{flushleft}
$-$Z +
\end{flushleft}





\begin{flushleft}
$\nu$i Ai .
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
These equations have the same form as the Newton equations in part (a) (with X
\end{flushleft}


\begin{flushleft}
replaced with Z $-$1/2 ).
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
10.13 Elimination method for computing Newton step for convex-concave game. Consider a
\end{flushleft}


\begin{flushleft}
convex-concave game with payoff function f : Rp × Rq $\rightarrow$ R (see page 541). We assume
\end{flushleft}


\begin{flushleft}
that f is strongly convex-concave, i.e., for all (u, v) $\in$ dom f and some m $>$ 0, we have
\end{flushleft}


\begin{flushleft}
$\nabla$2uu f (u, v) mI and $\nabla$2vv f (u, v) $-$mI.
\end{flushleft}


\begin{flushleft}
(a) Show how to compute the Newton step using Cholesky factorizations of $\nabla$ 2uu f (u, v)
\end{flushleft}


\begin{flushleft}
and $-$$\nabla$2 fvv (u, v). Compare the cost of this method with the cost of using an LDLT
\end{flushleft}


\begin{flushleft}
factorization of $\nabla$f (u, v), assuming $\nabla$2 f (u, v) is dense.
\end{flushleft}





\begin{flushleft}
(b) Show how you can exploit diagonal or block diagonal structure in $\nabla$2uu f (u, v) and/or
\end{flushleft}


\begin{flushleft}
$\nabla$2vv f (u, v). How much do you save, if you assume $\nabla$2uv f (u, v) is dense?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We use the notation
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (u, v) =
\end{flushleft}





\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
ET
\end{flushleft}





\begin{flushleft}
E
\end{flushleft}


\begin{flushleft}
$-$F
\end{flushleft}





,





\begin{flushleft}
with D $\in$ Sp++ , E $\in$ Rp×q , F $\in$ Sp++ , and consider the cost of solving a system of
\end{flushleft}


\begin{flushleft}
the form
\end{flushleft}


\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
E
\end{flushleft}


\begin{flushleft}
v
\end{flushleft}


\begin{flushleft}
g
\end{flushleft}


=$-$


.


\begin{flushleft}
w
\end{flushleft}


\begin{flushleft}
h
\end{flushleft}


\begin{flushleft}
E T $-$F
\end{flushleft}


\begin{flushleft}
We have two equations
\end{flushleft}





\begin{flushleft}
Dv + Ew = $-$g,
\end{flushleft}





\begin{flushleft}
E T v $-$ F w = $-$h.
\end{flushleft}





\begin{flushleft}
From the first equation we solve for v to obtain
\end{flushleft}


\begin{flushleft}
v = $-$D $-$1 (g + Ew).
\end{flushleft}


\begin{flushleft}
Substituting in the other equation gives E T D$-$1 (g + Ew) + F w = h, so
\end{flushleft}


\begin{flushleft}
w = (F + E T D$-$1 E)$-$1 (h $-$ E T D$-$1 g).
\end{flushleft}


\begin{flushleft}
We can implement this method using the Cholesky factorization as follows.
\end{flushleft}


$\bullet$


$\bullet$


$\bullet$


$\bullet$





\begin{flushleft}
Factor D = L1 LT1 ((1/3)p3 flops).
\end{flushleft}


2


2


\begin{flushleft}
Compute y = D $-$1 g, and Y = L$-$1
\end{flushleft}


\begin{flushleft}
1 E (p (2 + q) $\approx$ p q flops).
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


2


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
Compute S = F + Y Y (pq flops) and d = h $-$ E y (2pq flops)
\end{flushleft}


\begin{flushleft}
Solve Sw = d via Cholesky factorization ((1/3)q 3 flops).
\end{flushleft}





\begin{flushleft}
The total number of flops (ignoring lower order terms) is
\end{flushleft}


\begin{flushleft}
(1/3)p3 + p2 q + pq 2 + (1/3)q 3 = (1/3)(p + q)3 .
\end{flushleft}


\begin{flushleft}
Eliminating w would give the same result.
\end{flushleft}


\begin{flushleft}
The cost is the same as using LDLT factorization of $\nabla$f (u, v), i.e., (1/3)(p + q)3 .
\end{flushleft}


\begin{flushleft}
A matrix of the form of $\nabla$2 f (u, v) above is called a quasidefinite matrix. It has the
\end{flushleft}


\begin{flushleft}
special property that it has an LDLT factorization with diagonal D: with the same
\end{flushleft}


\begin{flushleft}
notation as above,
\end{flushleft}


\begin{flushleft}
D
\end{flushleft}


\begin{flushleft}
ET
\end{flushleft}





\begin{flushleft}
E
\end{flushleft}


\begin{flushleft}
$-$F
\end{flushleft}





=





\begin{flushleft}
L1
\end{flushleft}


\begin{flushleft}
YT
\end{flushleft}





0


\begin{flushleft}
L2
\end{flushleft}





\begin{flushleft}
I
\end{flushleft}


0





0


\begin{flushleft}
$-$I
\end{flushleft}





\begin{flushleft}
LT1
\end{flushleft}


0





\begin{flushleft}
Y
\end{flushleft}


\begin{flushleft}
LT2
\end{flushleft}





.





\begin{flushleft}
(b) Assume f is the cost of factoring D, and s is the cost of solving a system Dx = b
\end{flushleft}


\begin{flushleft}
after factoring. Then the cost of the algorithm is
\end{flushleft}


\begin{flushleft}
f + p2 (s/2) + pq 2 + (1/3)q 3 .
\end{flushleft}





\newpage
10





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
Numerical experiments
\end{flushleft}


\begin{flushleft}
10.14 Log-optimal investment. Consider the log-optimal investment problem described in exercise 4.60. Use Newton's method to compute the solution, with the following problem
\end{flushleft}


\begin{flushleft}
data: there are n = 3 assets, and m = 4 scenarios, with returns
\end{flushleft}





\begin{flushleft}
p1 =
\end{flushleft}





2


1.3


1





,





\begin{flushleft}
p2 =
\end{flushleft}





2


0.5


1





,





\begin{flushleft}
p3 =
\end{flushleft}





0.5


1.3


1





,





\begin{flushleft}
p4 =
\end{flushleft}





0.5


0.5


1





.





\begin{flushleft}
The probabilities of the four scenarios are given by $\pi$ = (1/3, 1/6, 1/3, 1/6).
\end{flushleft}


\begin{flushleft}
Solution. Eliminating x3 using the equality constraint x1 + x2 + x3 = 1 gives the
\end{flushleft}


\begin{flushleft}
equivalent problem
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}





\begin{flushleft}
(1/3) log(1 + x1 + 0.3x2 ) + (1/6) log(1 + x1 $-$ 0.5x2 )
\end{flushleft}


\begin{flushleft}
+ (1/3) log(1 $-$ 0.5x1 + 0.3x2 ) + (1/6) log(1 $-$ 0.5x1 $-$ 0.5x2 ),
\end{flushleft}





\begin{flushleft}
with two variables x1 and x2 . The solution is
\end{flushleft}


\begin{flushleft}
x1 = 0.4973,
\end{flushleft}





\begin{flushleft}
x2 = 0.1994,
\end{flushleft}





\begin{flushleft}
x3 = 0.7021.
\end{flushleft}





\begin{flushleft}
We use Newton's method with backtracking parameters $\alpha$ = 0.01, $\beta$ = 0.5, stopping
\end{flushleft}


\begin{flushleft}
criterion $\lambda$ $<$ 10$-$8 , and initial point x = (0, 0, 1). The algorithm converges in five steps,
\end{flushleft}


\begin{flushleft}
with no backtracking necessary.
\end{flushleft}


\begin{flushleft}
10.15 Equality constrained entropy maximization. Consider the equality constrained entropy
\end{flushleft}


\begin{flushleft}
maximization problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}


\begin{flushleft}
Ax = b,
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
xi log xi
\end{flushleft}





\begin{flushleft}
p×n
\end{flushleft}


\begin{flushleft}
with dom f = Rn
\end{flushleft}


\begin{flushleft}
, with p $<$ n. (See exercise 10.9 for some relevant
\end{flushleft}


\begin{flushleft}
++ and A $\in$ R
\end{flushleft}


\begin{flushleft}
analysis.)
\end{flushleft}


\begin{flushleft}
Generate a problem instance with n = 100 and p = 30 by choosing A randomly (checking
\end{flushleft}


\begin{flushleft}
that it has full rank), choosing x
\end{flushleft}


\begin{flushleft}
ˆ as a random positive vector (e.g., with entries uniformly
\end{flushleft}


\begin{flushleft}
distributed on [0, 1]) and then setting b = Aˆ
\end{flushleft}


\begin{flushleft}
x. (Thus, x
\end{flushleft}


\begin{flushleft}
ˆ is feasible.)
\end{flushleft}


\begin{flushleft}
Compute the solution of the problem using the following methods.
\end{flushleft}





\begin{flushleft}
(a) Standard Newton method. You can use initial point x(0) = x
\end{flushleft}


ˆ.


\begin{flushleft}
(b) Infeasible start Newton method. You can use initial point x(0) = x
\end{flushleft}


\begin{flushleft}
ˆ (to compare with
\end{flushleft}


\begin{flushleft}
the standard Newton method), and also the initial point x(0) = 1.
\end{flushleft}


\begin{flushleft}
(c) Dual Newton method, i.e., the standard Newton method applied to the dual problem.
\end{flushleft}


\begin{flushleft}
Verify that the three methods compute the same optimal point (and Lagrange multiplier).
\end{flushleft}


\begin{flushleft}
Compare the computational effort per step for the three methods, assuming relevant
\end{flushleft}


\begin{flushleft}
structure is exploited. (Your implementation, however, does not need to exploit structure
\end{flushleft}


\begin{flushleft}
to compute the Newton step.)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Standard Newton method. A typical convergence plot is shown below.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


2





10





0





\begin{flushleft}
f (x(k) ) $-$ p
\end{flushleft}





10





$-$2





10





$-$4





10





$-$6





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}


$-$8





10





0





1





2





3





4





5





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
The Matlab code is as follows.
\end{flushleft}


\begin{flushleft}
MAXITERS = 100;
\end{flushleft}


\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
NTTOL = 1e-7;
\end{flushleft}


\begin{flushleft}
x = x0;
\end{flushleft}


\begin{flushleft}
for iter=1:MAXITERS
\end{flushleft}


\begin{flushleft}
val = x'*log(x);
\end{flushleft}


\begin{flushleft}
grad = 1+log(x);
\end{flushleft}


\begin{flushleft}
hess = diag(1./x);
\end{flushleft}


\begin{flushleft}
sol = -[hess A'; A zeros(p,p)] \ensuremath{\backslash} [grad; zeros(p,1)];
\end{flushleft}


\begin{flushleft}
v = sol(1:n);
\end{flushleft}


\begin{flushleft}
fprime = grad'*v;
\end{flushleft}


\begin{flushleft}
if (abs(fprime) $<$ NTTOL), break; end;
\end{flushleft}


\begin{flushleft}
t=1;
\end{flushleft}


\begin{flushleft}
while (min(x+t*v) $<$= 0), t = BETA*t; end;
\end{flushleft}


\begin{flushleft}
while ((x+t*v)'*log(x+t*v) $>$= val + t*ALPHA*fprime), t=BETA*t; end;
\end{flushleft}


\begin{flushleft}
x = x + t*v;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
(b) Infeasible start Newton method. The figure shows the norm of the residual versus
\end{flushleft}


\begin{flushleft}
($\nabla$(f (x)) + AT $\nu$, Ax $-$ b) verus iteration number for the same example. The lower
\end{flushleft}


\begin{flushleft}
curve uses starting point x(0) = 1; the other curve uses the same starting point as
\end{flushleft}


\begin{flushleft}
in part (a).
\end{flushleft}


5





10





2





\begin{flushleft}
r(x(k) , $\nu$ (k) )
\end{flushleft}





0





10





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





$-$5





\begin{flushleft}
x(0) = 1
\end{flushleft}





$-$10





10





$-$15





10





0





1





2





3





4





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
MAXITERS = 100;
\end{flushleft}


\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}





5





6





7





\newpage
10





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
RESTOL = 1e-7;
\end{flushleft}


\begin{flushleft}
x=x0; nu=zeros(p,1);
\end{flushleft}


\begin{flushleft}
for i=1:MAXITERS
\end{flushleft}


\begin{flushleft}
r = [1+log(x)+A'*nu; A*x-b]; resdls =
\end{flushleft}


\begin{flushleft}
sol = -[diag(1./x) A'; A zeros(p,p)] \ensuremath{\backslash}
\end{flushleft}


\begin{flushleft}
Dx = sol(1:n); Dnu = sol(n+[1:p]);
\end{flushleft}


\begin{flushleft}
if (norm(r) $<$ RESTOL), break; end;
\end{flushleft}


\begin{flushleft}
t=1;
\end{flushleft}


\begin{flushleft}
while (min(x+t*Dx) $<$= 0), t = BETA*t;
\end{flushleft}


\begin{flushleft}
while norm([1+log(x+t*Dx)+A'*(nu+Dnu);
\end{flushleft}


\begin{flushleft}
(1-ALPHA*t)*norm(r), t=BETA*t;
\end{flushleft}


\begin{flushleft}
x = x + t*Dx; nu = nu + t*Dnu;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}





\begin{flushleft}
[resdls, norm(r)];
\end{flushleft}


\begin{flushleft}
r;
\end{flushleft}





\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
A*(x+Dx)-b])
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}





$>$ ...





\begin{flushleft}
(c) Dual Newton method. The dual problem is
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}





\begin{flushleft}
$-$bT $\nu$ $-$
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
e$-$ai
\end{flushleft}





\begin{flushleft}
$\nu$$-$1
\end{flushleft}





\begin{flushleft}
where ai is the ith column of A. The figure shows the dual function value versus
\end{flushleft}


\begin{flushleft}
iteration number for the same example.
\end{flushleft}


2





10





0





\begin{flushleft}
p $-$ g($\nu$ (k) )
\end{flushleft}





10





$-$2





10





$-$4





10





$-$6





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}


$-$8





10





0





0.5





1





1.5





2





2.5





3





\begin{flushleft}
k
\end{flushleft}


\begin{flushleft}
MAXITERS = 100;
\end{flushleft}


\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
NTTOL = 1e-8;
\end{flushleft}


\begin{flushleft}
nu = zeros(p,1);
\end{flushleft}


\begin{flushleft}
for i=1:MAXITERS
\end{flushleft}


\begin{flushleft}
val = b'*nu + sum(exp(-A'*nu-1));
\end{flushleft}


\begin{flushleft}
grad = b - A*exp(-A'*nu-1);
\end{flushleft}


\begin{flushleft}
hess = A*diag(exp(-A'*nu-1))*A';
\end{flushleft}


\begin{flushleft}
v = -hess\ensuremath{\backslash}grad;
\end{flushleft}


\begin{flushleft}
fprime = grad'*v;
\end{flushleft}


\begin{flushleft}
if (abs(fprime) $<$ NTTOL), break; end;
\end{flushleft}


\begin{flushleft}
t=1;
\end{flushleft}


\begin{flushleft}
while (b'*(nu+t*v) + sum(exp(-A'*(nu+t*v)-1)) $>$ ...
\end{flushleft}


\begin{flushleft}
val + t*ALPHA*fprime), t = BETA*t; end;
\end{flushleft}


\begin{flushleft}
nu = nu + t*v;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
The computational effort is the same for each method. In the standard and infeasible
\end{flushleft}


\begin{flushleft}
start Newton methods, we solve equations with coefficient matrix
\end{flushleft}


\begin{flushleft}
$\nabla$2 f (x)
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


0





,





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}





\begin{flushleft}
$\nabla$2 f (x) = diag(x)$-$1 .
\end{flushleft}





\begin{flushleft}
Block elimination reduces the equation to one with coefficient matrix A diag(x)AT .
\end{flushleft}


\begin{flushleft}
In the dual method, we solve an equation with coefficient matrix
\end{flushleft}


\begin{flushleft}
$-$$\nabla$2 g($\nu$) = ADAT
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
where D is diagonal with Dii = e$-$ai $\nu$$-$1 .
\end{flushleft}


\begin{flushleft}
In all three methods, the main computation in each iteration is therefore the solution of
\end{flushleft}


\begin{flushleft}
a linear system of the form
\end{flushleft}


\begin{flushleft}
AT DAv = $-$g
\end{flushleft}


\begin{flushleft}
where D is diagonal with positive diagonal elements.
\end{flushleft}





\begin{flushleft}
10.16 Convex-concave game. Use the infeasible start Newton method to solve convex-concave
\end{flushleft}


\begin{flushleft}
games of the form (10.32), with randomly generated data. Plot the norm of the residual
\end{flushleft}


\begin{flushleft}
and step length versus iteration. Experiment with the line search parameters and initial
\end{flushleft}


\begin{flushleft}
point (which must satisfy u 2 $<$ 1, v 2 $<$ 1, however).
\end{flushleft}


\begin{flushleft}
Solution. See figure 10.5 and the two figures below.
\end{flushleft}


5





10





1





0





0.8


0.6





\begin{flushleft}
t(k)
\end{flushleft}





\begin{flushleft}
r(u(k) , v (k) )
\end{flushleft}





10





$-$5





10





0.4


$-$10





10





0.2





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}


$-$15





10





0





2





4





6





8





\begin{flushleft}
k
\end{flushleft}





0


0





2





4





\begin{flushleft}
k
\end{flushleft}





\begin{flushleft}
A Matlab implementation, using the notation
\end{flushleft}


\begin{flushleft}
f (x, y) = xT Ay + cT x + dT y $-$ log(1 $-$ xT x) + log(1 $-$ y T y),
\end{flushleft}


\begin{flushleft}
is as follows.
\end{flushleft}


\begin{flushleft}
BETA = .5;
\end{flushleft}


\begin{flushleft}
ALPHA = .01;
\end{flushleft}


\begin{flushleft}
MAXITERS = 100;
\end{flushleft}


\begin{flushleft}
x = .01*ones(n,1);
\end{flushleft}


\begin{flushleft}
y = .01*ones(n,1);
\end{flushleft}


\begin{flushleft}
for iters =1:MAXITERS
\end{flushleft}


\begin{flushleft}
r = [ A*y + (2/(1-x'*x))*x + c; A'*x - (2/(1-y'*y))*y + d];
\end{flushleft}


\begin{flushleft}
if (norm(r) $<$ 1e-8), break; end;
\end{flushleft}


\begin{flushleft}
Dr = [ ((2/(1-x'*x))*eye(n) + (4/(1-x'*x)\^{}2)*x*x')
\end{flushleft}


\begin{flushleft}
A ;
\end{flushleft}


\begin{flushleft}
A' (-(2/(1-y'*y))*eye(n) - (4/(1-y'*y)\^{}2)*y*y')];
\end{flushleft}


\begin{flushleft}
step = -Dr\ensuremath{\backslash}r; dx = step(1:n); dy = step(n+[1:n]);
\end{flushleft}


\begin{flushleft}
t = 1;
\end{flushleft}


\begin{flushleft}
newx = x+t*dx; newy = y+t*dy;
\end{flushleft}


\begin{flushleft}
while ((norm(newx) $>$= 1) | (norm(newy) $>$= 1)),
\end{flushleft}


\begin{flushleft}
t = BETA*t; newx = x+t*dx;
\end{flushleft}


\begin{flushleft}
newy = y+t*dy;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}





6





8





\newpage
10





\begin{flushleft}
Equality constrained minimization
\end{flushleft}





\begin{flushleft}
newr = [ A*newy + (2/(1-newx'*newx))*newx + c;
\end{flushleft}


\begin{flushleft}
A'*newx - (2/(1-newy'*newy))*newy + d ];
\end{flushleft}


\begin{flushleft}
while (norm(newr) $>$ (1-ALPHA*t)*norm(r))
\end{flushleft}


\begin{flushleft}
t = BETA*t; newx = x+t*dx;
\end{flushleft}


\begin{flushleft}
newy = y+t*dy;
\end{flushleft}


\begin{flushleft}
newr = [ A*newy + (2/(1-newx'*newx))*newx + c;
\end{flushleft}


\begin{flushleft}
A'*newx - (2/(1-newy'*newy))*newy + d];
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
x = x+t*dx; y = y+t*dy;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}





\begin{flushleft}
\newpage
Chapter 11
\end{flushleft}





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Exercises
\end{flushleft}


\begin{flushleft}
The barrier method
\end{flushleft}


\begin{flushleft}
11.1 Barrier method example. Consider the simple problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x2 + 1
\end{flushleft}


\begin{flushleft}
2 $\leq$ x $\leq$ 4,
\end{flushleft}





\begin{flushleft}
which has feasible set [2, 4], and optimal point x = 2. Plot f0 , and tf0 + $\phi$, for several
\end{flushleft}


\begin{flushleft}
values of t $>$ 0, versus x. Label x (t).
\end{flushleft}


\begin{flushleft}
Solution. The figure shows the function f0 + (1/t)I for f0 (x) = x2 + 1, with barrier
\end{flushleft}


\begin{flushleft}
function I(x) = $-$ log(x $-$ 2) $-$ log(4 $-$ x), for t = 10$-$1 , 10$-$0.8 , 10$-$0.6 , . . . , 100.8 , 10.
\end{flushleft}


\begin{flushleft}
The inner curve corresponds to t = 0.1, and the outer curve corresponds to t = 10. The
\end{flushleft}


\begin{flushleft}
objective function is shown as a dashed curve.
\end{flushleft}


60


50


40


30


20


10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





0


1





2





3





\begin{flushleft}
x
\end{flushleft}





4





5





\begin{flushleft}
11.2 What happens if the barrier method is applied to the LP
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x 1 $\leq$ x2 ,
\end{flushleft}





\begin{flushleft}
0 $\leq$ x2 ,
\end{flushleft}





\begin{flushleft}
with variable x $\in$ R2 ?
\end{flushleft}


\begin{flushleft}
Solution. We need to minimize
\end{flushleft}


\begin{flushleft}
tf0 (x) + $\phi$(x) = tx2 $-$ log(x2 $-$ x1 ) $-$ log x2 ,
\end{flushleft}


\begin{flushleft}
but this function is unbounded below (letting x1 $\rightarrow$ $-$$\infty$), so the first centering step never
\end{flushleft}


\begin{flushleft}
converges.
\end{flushleft}


\begin{flushleft}
11.3 Boundedness of centering problem. Suppose the sublevel sets of (11.1),
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}


\begin{flushleft}
Ax = b,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





\begin{flushleft}
are bounded. Show that the sublevel sets of the associated centering problem,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}


\begin{flushleft}
are bounded.
\end{flushleft}





\begin{flushleft}
tf0 (x) + $\phi$(x)
\end{flushleft}


\begin{flushleft}
Ax = b,
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
Solution. Suppose a sublevel set \{x | tf0 (x)+$\phi$(x) $\leq$ M \} is unbounded. Let \{x+sv | s $\geq$
\end{flushleft}


\begin{flushleft}
0\}, with with v = 0 and x strictly feasible, be a ray contained in the sublevel set. We have
\end{flushleft}


\begin{flushleft}
A(x + sv) = b for all s $\geq$ 0 (i.e., Ax = b and Av = 0), and fi (x + sv) $<$ 0, i = 1, . . . , m. By
\end{flushleft}


\begin{flushleft}
assumption, the sublevel sets of (11.1) are bounded, which is only possible if f0 (x + sv)
\end{flushleft}


\begin{flushleft}
increases with s for sufficiently large s. Without loss of generality, we can choose x such
\end{flushleft}


\begin{flushleft}
that $\nabla$f0 (x)T v $>$ 0.
\end{flushleft}


\begin{flushleft}
We have
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
M
\end{flushleft}





$\geq$





\begin{flushleft}
tf0 (x + sv) $-$
\end{flushleft}





$\geq$





\begin{flushleft}
tf0 (x) + st$\nabla$f0 (x)T v $-$
\end{flushleft}





\begin{flushleft}
log($-$fi (x + sv))
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log($-$fi (x) $-$ s$\nabla$fi (x)T v)
\end{flushleft}





\begin{flushleft}
for all s $\geq$ 0. This is impossible since $\nabla$f0 (x)T v $>$ 0.
\end{flushleft}


\begin{flushleft}
11.4 Adding a norm bound to ensure strong convexity of the centering problem. Suppose we
\end{flushleft}


\begin{flushleft}
add the constraint xT x $\leq$ R2 to the problem (11.1):
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
Ax = b
\end{flushleft}


\begin{flushleft}
xT x $\leq$ R 2 .
\end{flushleft}





\begin{flushleft}
Let $\phi$˜ denote the logarithmic barrier function for this modified problem. Find a $>$ 0 for
\end{flushleft}


˜


\begin{flushleft}
which $\nabla$2 (tf0 (x) + $\phi$(x))
\end{flushleft}


\begin{flushleft}
aI holds, for all feasible x.
\end{flushleft}


\begin{flushleft}
Solution. Let $\phi$ denote the logarithmic barrier of the original problem. The constraint
\end{flushleft}


\begin{flushleft}
xT x $\leq$ R2 adds the term $-$ log(R2 $-$ xT x) to the logarithmic barrier, so we have
\end{flushleft}


˜


\begin{flushleft}
$\nabla$2 (tf0 + $\phi$)
\end{flushleft}





=





\begin{flushleft}
$\nabla$2 (tf0 + $\phi$) +
\end{flushleft}





4


2


\begin{flushleft}
I+
\end{flushleft}


\begin{flushleft}
xxT
\end{flushleft}


\begin{flushleft}
R 2 $-$ xT x
\end{flushleft}


\begin{flushleft}
(R2 $-$ xT x)2
\end{flushleft}





\begin{flushleft}
$\nabla$2 (tf0 + $\phi$) + (2/R2 )I
\end{flushleft}





\begin{flushleft}
(2/R2 )I,
\end{flushleft}





\begin{flushleft}
so we can take m = 2/R2 .
\end{flushleft}


\begin{flushleft}
11.5 Barrier method for second-order cone programming. Consider the SOCP (without equality
\end{flushleft}


\begin{flushleft}
constraints, for simplicity)
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
fT x
\end{flushleft}


\begin{flushleft}
A i x + bi
\end{flushleft}





2





\begin{flushleft}
$\leq$ cTi x + di ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





(11.63)





\begin{flushleft}
The constraint functions in this problem are not differentiable (since the Euclidean norm
\end{flushleft}


\begin{flushleft}
u 2 is not differentiable at u = 0) so the (standard) barrier method cannot be applied.
\end{flushleft}


\begin{flushleft}
In §11.6, we saw that this SOCP can be solved by an extension of the barrier method
\end{flushleft}


\begin{flushleft}
that handles generalized inequalities. (See example 11.8, page 599, and page 601.) In this
\end{flushleft}


\begin{flushleft}
exercise, we show how the standard barrier method (with scalar constraint functions) can
\end{flushleft}


\begin{flushleft}
be used to solve the SOCP.
\end{flushleft}


\begin{flushleft}
We first reformulate the SOCP as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
fT x
\end{flushleft}


\begin{flushleft}
Ai x + bi 22 /(cTi x + di ) $\leq$ cTi x + di ,
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
ci x + di $\geq$ 0, i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
The constraint function
\end{flushleft}


\begin{flushleft}
fi (x) =
\end{flushleft}





\begin{flushleft}
Ai x + bi
\end{flushleft}


\begin{flushleft}
cTi x + di
\end{flushleft}





2


2





\begin{flushleft}
$-$ cTi x $-$ di
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m
\end{flushleft}





(11.64)





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
is the composition of a quadratic-over-linear function with an affine function, and is twice
\end{flushleft}


\begin{flushleft}
differentiable (and convex), provided we define its domain as dom fi = \{x | cTi x+di $>$ 0\}.
\end{flushleft}


\begin{flushleft}
Note that the two problems (11.63) and (11.64) are not exactly equivalent. If c Ti x +di = 0
\end{flushleft}


\begin{flushleft}
for some i, where x is the optimal solution of the SOCP (11.63), then the reformulated
\end{flushleft}


\begin{flushleft}
problem (11.64) is not solvable; x is not in its domain. Nevertheless we will see that
\end{flushleft}


\begin{flushleft}
the barrier method, applied to (11.64), produces arbitrarily accurate suboptimal solutions
\end{flushleft}


\begin{flushleft}
of (11.64), and hence also for (11.63).
\end{flushleft}


\begin{flushleft}
(a) Form the log barrier $\phi$ for the problem (11.64). Compare it to the log barrier that
\end{flushleft}


\begin{flushleft}
arises when the SOCP (11.63) is solved using the barrier method for generalized
\end{flushleft}


\begin{flushleft}
inequalities (in §11.6).
\end{flushleft}





\begin{flushleft}
(b) Show that if tf T x + $\phi$(x) is minimized, the minimizer x (t) is 2m/t-suboptimal for
\end{flushleft}


\begin{flushleft}
the problem (11.63). It follows that the standard barrier method, applied to the
\end{flushleft}


\begin{flushleft}
reformulated problem (11.64), solves the SOCP (11.63), in the sense of producing
\end{flushleft}


\begin{flushleft}
arbitrarily accurate suboptimal solutions. This is the case even though the optimal
\end{flushleft}


\begin{flushleft}
point x need not be in the domain of the reformulated problem (11.64).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The log barrier $\phi$ for the problem (11.64) is
\end{flushleft}


$-$





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





=$-$





\begin{flushleft}
log cTi x + di $-$
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log (cTi x +
\end{flushleft}





\begin{flushleft}
Ai x+bi 2
\end{flushleft}


2


$-$


\begin{flushleft}
x+di
\end{flushleft}


\begin{flushleft}
cT
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}


2


\begin{flushleft}
di ) $-$ Ai x +
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
bi 22
\end{flushleft}





\begin{flushleft}
log(cTi x + di )
\end{flushleft}





\begin{flushleft}
The log barrier for the SOCP (11.63), using the generalized logarithm for the secondorder cone given in §11.6, is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





$-$





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log (cTi x + di )2 $-$ Ai x + bi
\end{flushleft}





2


2





,





\begin{flushleft}
which is exactly the same. The log barriers are the same.
\end{flushleft}


\begin{flushleft}
(b) The centering problems are the same, and the central paths are the same. The proof
\end{flushleft}


\begin{flushleft}
is identical to the derivation in example 11.8.
\end{flushleft}


\begin{flushleft}
11.6 General barriers. The log barrier is based on the approximation $-$(1/t) log($-$u) of the
\end{flushleft}


\begin{flushleft}
indicator function I$-$ (u) (see §11.2.1, page 563). We can also construct barriers from
\end{flushleft}


\begin{flushleft}
other approximations, which in turn yield generalizations of the central path and barrier
\end{flushleft}


\begin{flushleft}
method. Let h : R $\rightarrow$ R be a twice differentiable, closed, increasing convex function,
\end{flushleft}


\begin{flushleft}
with dom h = $-$R++ . (This implies h(u) $\rightarrow$ $\infty$ as u $\rightarrow$ 0.) One such function is
\end{flushleft}


\begin{flushleft}
h(u) = $-$ log($-$u); another example is h(u) = $-$1/u (for u $<$ 0).
\end{flushleft}


\begin{flushleft}
Now consider the optimization problem (without equality constraints, for simplicity)
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
where fi are twice differentiable. We define the h-barrier for this problem as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
h(fi (x)),
\end{flushleft}





\begin{flushleft}
$\phi$h (x) =
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
with domain \{x | fi (x) $<$ 0, i = 1, . . . , m\}. When h(u) = $-$ log($-$u), this is the usual
\end{flushleft}


\begin{flushleft}
logarithmic barrier; when h(u) = $-$1/u, $\phi$h is called the inverse barrier. We define the
\end{flushleft}


\begin{flushleft}
h-central path as
\end{flushleft}


\begin{flushleft}
x (t) = argmin tf0 (x) + $\phi$h (x),
\end{flushleft}


\begin{flushleft}
where t $>$ 0 is a parameter. (We assume that for each t, the minimizer exists and is
\end{flushleft}


\begin{flushleft}
unique.)
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
(a) Explain why tf0 (x) + $\phi$h (x) is convex in x, for each t $>$ 0.
\end{flushleft}


\begin{flushleft}
(b) Show how to construct a dual feasible $\lambda$ from x (t). Find the associated duality gap.
\end{flushleft}


\begin{flushleft}
(c) For what functions h does the duality gap found in part (b) depend only on t and
\end{flushleft}


\begin{flushleft}
m (and no other problem data)?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) The composition rules show that tf0 (x) + $\phi$h (x) is convex in x, since h is increasing
\end{flushleft}


\begin{flushleft}
and convex, and fi are convex.
\end{flushleft}


\begin{flushleft}
(b) The minimizer of tf0 (x)+$\phi$h (x), z = x (t), satisfies t$\nabla$f0 (z)+$\nabla$$\phi$(z) = 0. Expanding
\end{flushleft}


\begin{flushleft}
this we get
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
h (fi (z))$\nabla$fi (z) = 0.
\end{flushleft}





\begin{flushleft}
t$\nabla$f0 (z) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
This shows that z minimizes the Lagrangian f0 (z) +
\end{flushleft}


\begin{flushleft}
$\lambda$i = h (fi (z))/t,
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (z), for
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
The associated dual function value is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
g($\lambda$) = f0 (z) +
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (z) = f0 (z) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
h (fi (z))fi (z)/t,
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
so the duality gap is
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
(1/t)
\end{flushleft}





\begin{flushleft}
h (fi (z))($-$fi (z)).
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(c) The only way the expression above does not depend on problem data (except t and
\end{flushleft}


\begin{flushleft}
m) is for h (u)($-$u) to be constant. This means h (u) = a/($-$u) for some constant
\end{flushleft}


\begin{flushleft}
a, so h(u) = $-$a log($-$u) + b, for some constant b. Since h must be convex and
\end{flushleft}


\begin{flushleft}
increasing, we need a $>$ 0. Thus, h gives rise to a scaled, offset log barrier. In
\end{flushleft}


\begin{flushleft}
particular, the central path associated with h is the same as for the standard log
\end{flushleft}


\begin{flushleft}
barrier.
\end{flushleft}


\begin{flushleft}
11.7 Tangent to central path. This problem concerns dx (t)/dt, which gives the tangent to the
\end{flushleft}


\begin{flushleft}
central path at the point x (t). For simplicity, we consider a problem without equality
\end{flushleft}


\begin{flushleft}
constraints; the results readily generalize to problems with equality constraints.
\end{flushleft}


\begin{flushleft}
(a) Find an explicit expression for dx (t)/dt. Hint. Differentiate the centrality equations (11.7) with respect to t.
\end{flushleft}


\begin{flushleft}
(b) Show that f0 (x (t)) decreases as t increases. Thus, the objective value in the barrier
\end{flushleft}


\begin{flushleft}
method decreases, as the parameter t is increased. (We already know that the duality
\end{flushleft}


\begin{flushleft}
gap, which is m/t, decreases as t increases.)
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Differentiating the centrality equation yields
\end{flushleft}


\begin{flushleft}
$\nabla$f0 (x (t)) + t$\nabla$2 f0 (x (t)) + $\nabla$2 $\phi$(x (t))
\end{flushleft}





\begin{flushleft}
dx
\end{flushleft}


= 0.


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
Thus, the tangent to the central path at x (t) is given by
\end{flushleft}


\begin{flushleft}
dx
\end{flushleft}


\begin{flushleft}
= $-$ t$\nabla$2 f0 (x (t)) + $\nabla$2 $\phi$(x (t))
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





$-$1





\begin{flushleft}
$\nabla$f0 (x (t)).
\end{flushleft}





\begin{flushleft}
(11.7.A)
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
(b) We will show that df0 (x (t))/dt $<$ 0.
\end{flushleft}


\begin{flushleft}
df0 (x (t))
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
dx (t)
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





=





\begin{flushleft}
$\nabla$f0 (x (t))T
\end{flushleft}





=


$<$





\begin{flushleft}
$-$$\nabla$f0 (x (t))T t$\nabla$2 f0 (x (t)) + $\nabla$2 $\phi$(x (t))
\end{flushleft}


0.





$-$1





\begin{flushleft}
$\nabla$f0 (x (t))
\end{flushleft}





\begin{flushleft}
11.8 Predictor-corrector method for centering problems. In the standard barrier method, x ($\mu$t)
\end{flushleft}


\begin{flushleft}
is computed using Newton's method, starting from the initial point x (t). One alternative
\end{flushleft}


\begin{flushleft}
that has been proposed is to make an approximation or prediction x of x ($\mu$t), and then
\end{flushleft}


\begin{flushleft}
start the Newton method for computing x ($\mu$t) from x. The idea is that this should
\end{flushleft}


\begin{flushleft}
reduce the number of Newton steps, since x is (presumably) a better initial point than
\end{flushleft}


\begin{flushleft}
x (t). This method of centering is called a predictor-corrector method, since it first makes
\end{flushleft}


\begin{flushleft}
a prediction of what x ($\mu$t) is, then corrects the prediction using Newton's method.
\end{flushleft}


\begin{flushleft}
The most widely used predictor is the first-order predictor, based on the tangent to the
\end{flushleft}


\begin{flushleft}
central path, explored in exercise 11.7. This predictor is given by
\end{flushleft}


\begin{flushleft}
x = x (t) +
\end{flushleft}





\begin{flushleft}
dx (t)
\end{flushleft}


\begin{flushleft}
($\mu$t $-$ t).
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





\begin{flushleft}
Derive an expression for the first-order predictor x. Compare it to the Newton update
\end{flushleft}


\begin{flushleft}
obtained, i.e., x (t) + ∆xnt , where ∆xnt is the Newton step for $\mu$tf0 (x) + $\phi$(x), at x (t).
\end{flushleft}


\begin{flushleft}
What can you say when the objective f0 is linear? (For simplicity, you can consider a
\end{flushleft}


\begin{flushleft}
problem without equality constraints.)
\end{flushleft}


\begin{flushleft}
Solution. The first-order predictor is, using the expression for dx /dt found in exercise 11.7,
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
dx (t)
\end{flushleft}


\begin{flushleft}
($\mu$t $-$ t)
\end{flushleft}


\begin{flushleft}
dt
\end{flushleft}





=





\begin{flushleft}
x (t) +
\end{flushleft}





=





\begin{flushleft}
x (t) $-$ ($\mu$ $-$ 1)t t$\nabla$2 f0 (x (t)) + $\nabla$2 $\phi$(x (t))
\end{flushleft}





$-$1





\begin{flushleft}
$\nabla$f0 (x (t)).
\end{flushleft}





\begin{flushleft}
The Newton step for $\mu$tf0 + $\phi$, at the point x (t), is given by
\end{flushleft}


\begin{flushleft}
∆xnt
\end{flushleft}





=


=





\begin{flushleft}
$-$ $\mu$t$\nabla$2 f0 (x (t)) + $\nabla$2 $\phi$(x (t))
\end{flushleft}


2





2





$-$1





\begin{flushleft}
($\mu$t$\nabla$f0 (x (t)) + $\nabla$$\phi$(x (t)))
\end{flushleft}





\begin{flushleft}
$-$($\mu$ $-$ 1)t $\mu$t$\nabla$ f0 (x (t)) + $\nabla$ $\phi$(x (t))
\end{flushleft}





$-$1





\begin{flushleft}
$\nabla$f0 (x (t)),
\end{flushleft}





\begin{flushleft}
where we use t$\nabla$f0 (x (t)) + $\nabla$$\phi$(x (t)) = 0. The Newton update is then
\end{flushleft}


\begin{flushleft}
x (t) + ∆xnt = x (t) $-$ ($\mu$ $-$ 1)t $\mu$t$\nabla$2 f0 (x (t)) + $\nabla$2 $\phi$(x (t))
\end{flushleft}





$-$1





\begin{flushleft}
$\nabla$f0 (x (t)).
\end{flushleft}





\begin{flushleft}
This is similar to, but not quite the same as, the first-order predictor.
\end{flushleft}


\begin{flushleft}
Now let's consider the special case when f0 is linear, say, f0 (x) = cT x. Then the first-order
\end{flushleft}


\begin{flushleft}
predictor is given by
\end{flushleft}


\begin{flushleft}
x = x (t) $-$ ($\mu$ $-$ 1)t$\nabla$2 $\phi$(x (t))$-$1 c.
\end{flushleft}





\begin{flushleft}
The Newton update is exactly the same. The Newton step for $\mu$tf0 + $\phi$ at x is exactly
\end{flushleft}


\begin{flushleft}
the tangent to the central path. We conclude that when the objective is linear, the fancy
\end{flushleft}


\begin{flushleft}
sounding predictor-corrector method is exactly the same as the simple method of just
\end{flushleft}


\begin{flushleft}
starting Newton's method from the current point x (t).
\end{flushleft}


\begin{flushleft}
11.9 Dual feasible points near the central path. Consider the problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m,
\end{flushleft}





\begin{flushleft}
with variable x $\in$ Rn . We assume the functions fi are convex and twice differentiable. (We
\end{flushleft}


\begin{flushleft}
assume for simplicity there are no equality constraints.) Recall (from §11.2.2, page 565)
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
that $\lambda$i = $-$1/(tfi (x (t))), i = 1, . . . , m, is dual feasible, and in fact, x (t) minimizes
\end{flushleft}


\begin{flushleft}
L(x, $\lambda$). This allows us to evaluate the dual function for $\lambda$, which turns out to be g($\lambda$) =
\end{flushleft}


\begin{flushleft}
f0 (x (t)) $-$ m/t. In particular, we conclude that x (t) is m/t-suboptimal.
\end{flushleft}


\begin{flushleft}
In this problem we consider what happens when a point x is close to x (t), but not quite
\end{flushleft}


\begin{flushleft}
centered. (This would occur if the centering steps were terminated early, or not carried
\end{flushleft}


\begin{flushleft}
out to full accuracy.) In this case, of course, we cannot claim that $\lambda$i = $-$1/(tfi (x)),
\end{flushleft}


\begin{flushleft}
i = 1, . . . , m, is dual feasible, or that x is m/t-suboptimal. However, it turns out that
\end{flushleft}


\begin{flushleft}
a slightly more complicated formula does yield a dual feasible point, provided x is close
\end{flushleft}


\begin{flushleft}
enough to centered.
\end{flushleft}


\begin{flushleft}
Let ∆xnt be the Newton step at x of the centering problem
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}





\begin{flushleft}
tf0 (x) $-$
\end{flushleft}





\begin{flushleft}
Define
\end{flushleft}


\begin{flushleft}
$\lambda$i =
\end{flushleft}





1


\begin{flushleft}
$-$tfi (x)
\end{flushleft}





1+





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log($-$fi (x)).
\end{flushleft}





\begin{flushleft}
$\nabla$fi (x)T ∆xnt
\end{flushleft}


\begin{flushleft}
$-$fi (x)
\end{flushleft}





,





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
You will show that for small ∆xnt (i.e., for x nearly centered), $\lambda$ is dual feasible (i.e.,
\end{flushleft}


\begin{flushleft}
$\lambda$ 0 and L(x, $\lambda$) is bounded below).
\end{flushleft}


\begin{flushleft}
In this case, the vector x does not minimize L(x, $\lambda$), so there is no general formula for the
\end{flushleft}


\begin{flushleft}
dual function value g($\lambda$) associated with $\lambda$. (If we have an analytical expression for the
\end{flushleft}


\begin{flushleft}
dual objective, however, we can simply evaluate g($\lambda$).)
\end{flushleft}


\begin{flushleft}
Hint. Use the results in exercise 3.41 to show that when ∆xnt is small enough, there exist
\end{flushleft}


\begin{flushleft}
x0 , x1 , . . . , xm such that
\end{flushleft}


\begin{flushleft}
$\nabla$f0 (x0 )
\end{flushleft}


\begin{flushleft}
$\nabla$fi (xi )
\end{flushleft}





=


=





\begin{flushleft}
$\nabla$f0 (x) + $\nabla$2 f0 (x)∆xnt
\end{flushleft}





\begin{flushleft}
$\nabla$fi (x) + (1/$\lambda$i )$\nabla$2 fi (x)∆xnt ,
\end{flushleft}





\begin{flushleft}
This implies that
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\nabla$f0 (x0 ) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i $\nabla$fi (xi ) = 0.
\end{flushleft}





\begin{flushleft}
Now use fi (z) $\geq$ fi (xi ) + $\nabla$fi (xi )T (z $-$ xi ), i = 0, . . . , m, to derive a lower bound on
\end{flushleft}


\begin{flushleft}
L(z, $\lambda$).
\end{flushleft}


\begin{flushleft}
Solution. It is clear that $\lambda$
\end{flushleft}


\begin{flushleft}
0 for sufficiently small ∆xnt . We need to show that
\end{flushleft}


\begin{flushleft}
f0 + i $\lambda$i fi is bounded below.
\end{flushleft}


\begin{flushleft}
The Newton equations at x are
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\nabla$f0 (x) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





1


\begin{flushleft}
$\nabla$fi (x) +
\end{flushleft}


\begin{flushleft}
$-$tfi (x)
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
+ $\nabla$2 f0 (x)∆xnt +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\nabla$fi (x)T ∆xnt
\end{flushleft}


\begin{flushleft}
$\nabla$fi (x)
\end{flushleft}


\begin{flushleft}
tfi (x)2
\end{flushleft}





1


\begin{flushleft}
$\nabla$2 fi (x)∆xnt = 0
\end{flushleft}


\begin{flushleft}
$-$tfi (x)
\end{flushleft}





\begin{flushleft}
i.e., using the above definition of $\lambda$,
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\nabla$f0 (x) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\lambda$i $\nabla$fi (x) + $\nabla$2 f0 (x)∆xnt +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





1


\begin{flushleft}
$\nabla$2 fi (x)∆xnt = 0.
\end{flushleft}


\begin{flushleft}
$-$tfi (x)
\end{flushleft}





\begin{flushleft}
Now, from the result in exercise 3.41, if ∆xnt is small enough, there exist x0 , x1 , . . . , xm
\end{flushleft}


\begin{flushleft}
such that
\end{flushleft}


\begin{flushleft}
$\nabla$f0 (x0 ) = $\nabla$f0 (x) + $\nabla$2 f0 (x)∆xnt ,
\end{flushleft}


\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
$\nabla$fi (xi ) = $\nabla$fi (x) + (1/$\lambda$i )$\nabla$2 fi (x)∆xnt ,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We can therefore write the Newton equation as
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\nabla$f0 (x0 ) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i $\nabla$fi (xi ) = 0.
\end{flushleft}





\begin{flushleft}
Returning to the question of boundedness of f0 +
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\lambda$i fi , we have
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (x)
\end{flushleft}





\begin{flushleft}
f0 (x) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





$\geq$





\begin{flushleft}
f0 (x0 ) + $\nabla$f0 (x0 ) (x $-$ x0 ) +
\end{flushleft}





=





\begin{flushleft}
f0 (x0 ) +
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i (fi (xi ) + $\nabla$fi (xi )T (x $-$ xi ))
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (xi ) +
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$-$ $\nabla$f0 (x0 )T x0 $-$
\end{flushleft}


=





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\lambda$i $\nabla$fi (xi )
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
$\lambda$i $\nabla$fi (xi )T xi
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (xi ) $-$ $\nabla$f0 (x0 )T x0 $-$
\end{flushleft}





\begin{flushleft}
f0 (x0 ) +
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
which shows that f0 +
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\nabla$f0 (x0 ) +
\end{flushleft}





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
$\lambda$i $\nabla$fi (xi )T xi ,
\end{flushleft}





\begin{flushleft}
$\lambda$i fi is bounded below.
\end{flushleft}





\begin{flushleft}
11.10 Another parametrization of the central path. We consider the problem (11.1), with central
\end{flushleft}


\begin{flushleft}
path x (t) for t $>$ 0, defined as the solution of
\end{flushleft}


\begin{flushleft}
tf0 (x) $-$
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log($-$fi (x))
\end{flushleft}





\begin{flushleft}
In this problem we explore another parametrization of the central path.
\end{flushleft}


\begin{flushleft}
For u $>$ p , let z (u) denote the solution of
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$ log(u $-$ f0 (x)) $-$
\end{flushleft}


\begin{flushleft}
Ax = b.
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log($-$fi (x))
\end{flushleft}





\begin{flushleft}
Show that the curve defined by z (u), for u $>$ p , is the central path. (In other words,
\end{flushleft}


\begin{flushleft}
for each u $>$ p , there is a t $>$ 0 for which x (t) = z (u), and conversely, for each t $>$ 0,
\end{flushleft}


\begin{flushleft}
there is an u $>$ p for which z (u) = x (t)).
\end{flushleft}


\begin{flushleft}
Solution. z (u) satisfies the optimality conditions
\end{flushleft}


1


\begin{flushleft}
$\nabla$f0 (z (u)) +
\end{flushleft}


\begin{flushleft}
u $-$ f0 (z (u))
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





1


\begin{flushleft}
$\nabla$fi (z (u)) + AT $\nu$ = 0
\end{flushleft}


\begin{flushleft}
$-$fi (z (u))
\end{flushleft}





\begin{flushleft}
for some $\nu$. We conclude that z (u) = x (t) for
\end{flushleft}


\begin{flushleft}
t=
\end{flushleft}





1


.


\begin{flushleft}
u $-$ f0 (z (u))
\end{flushleft}





\begin{flushleft}
Conversely, for each t $>$ 0, x (t) = z (u) with
\end{flushleft}


\begin{flushleft}
u=
\end{flushleft}





1


\begin{flushleft}
+ f0 (x (t)) $>$ p .
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
11.11 Method of analytic centers. In this problem we consider a variation on the barrier method,
\end{flushleft}


\begin{flushleft}
based on the parametrization of the central path described in exercise 11.10. For simplicity, we consider a problem with no equality constraints,
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (x)
\end{flushleft}


\begin{flushleft}
fi (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
i = 1, . . . , m.
\end{flushleft}





\begin{flushleft}
The method of analytic centers starts with any strictly feasible initial point x(0) , and any
\end{flushleft}


\begin{flushleft}
u(0) $>$ f0 (x(0) ). We then set
\end{flushleft}


\begin{flushleft}
u(1) = $\theta$u(0) + (1 $-$ $\theta$)f0 (x(0) ),
\end{flushleft}


\begin{flushleft}
where $\theta$ $\in$ (0, 1) is an algorithm parameter (usually chosen small), and then compute the
\end{flushleft}


\begin{flushleft}
next iterate as
\end{flushleft}


\begin{flushleft}
x(1) = z (u(1) )
\end{flushleft}


\begin{flushleft}
(using Newton's method, starting from x(0) ). Here z (s) denotes the minimizer of
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$-$ log(s $-$ f0 (x)) $-$
\end{flushleft}





\begin{flushleft}
log($-$fi (x)),
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
which we assume exists and is unique. This process is then repeated.
\end{flushleft}


\begin{flushleft}
The point z (s) is the analytic center of the inequalities
\end{flushleft}


\begin{flushleft}
f0 (x) $\leq$ s,
\end{flushleft}





\begin{flushleft}
f1 (x) $\leq$ 0, . . . , fm (x) $\leq$ 0,
\end{flushleft}





\begin{flushleft}
hence the algorithm name.
\end{flushleft}


\begin{flushleft}
Show that the method of centers works, i.e., x(k) converges to an optimal point. Find a
\end{flushleft}


\begin{flushleft}
stopping criterion that guarantees that x is -suboptimal, where $>$ 0.
\end{flushleft}


\begin{flushleft}
Hint. The points x(k) are on the central path; see exercise 11.10. Use this to show that
\end{flushleft}


\begin{flushleft}
u+ $-$ p $\leq$
\end{flushleft}





\begin{flushleft}
m+$\theta$
\end{flushleft}


\begin{flushleft}
(u $-$ p ),
\end{flushleft}


\begin{flushleft}
m+1
\end{flushleft}





\begin{flushleft}
where u and u+ are the values of u on consecutive iterations.
\end{flushleft}


\begin{flushleft}
Solution. Let x = z (u). From the duality result in exercise 11.10,
\end{flushleft}


\begin{flushleft}
p
\end{flushleft}





$\geq$


=





\begin{flushleft}
f0 (x) $-$ m(u $-$ f0 (x))
\end{flushleft}


\begin{flushleft}
(m + 1)f0 (x) $-$ mu,
\end{flushleft}





\begin{flushleft}
and therefore
\end{flushleft}


\begin{flushleft}
f0 (x) $\leq$
\end{flushleft}





\begin{flushleft}
p + mu
\end{flushleft}


.


\begin{flushleft}
m+1
\end{flushleft}





\begin{flushleft}
Let u+ = $\theta$u + (1 $-$ $\theta$)f0 (x). We have
\end{flushleft}


\begin{flushleft}
u+ $-$ p
\end{flushleft}





=


$\leq$


=


=





\begin{flushleft}
$\theta$u + (1 $-$ $\theta$)f0 (x) $-$ p
\end{flushleft}


\begin{flushleft}
p + mu
\end{flushleft}


\begin{flushleft}
+ $\theta$u $-$ p
\end{flushleft}


\begin{flushleft}
(1 $-$ $\theta$)
\end{flushleft}


\begin{flushleft}
m+1
\end{flushleft}


\begin{flushleft}
(1 $-$ $\theta$)m
\end{flushleft}


\begin{flushleft}
1$-$$\theta$
\end{flushleft}


\begin{flushleft}
$-$1 p +
\end{flushleft}


\begin{flushleft}
+$\theta$
\end{flushleft}


\begin{flushleft}
m+1
\end{flushleft}


\begin{flushleft}
m+1
\end{flushleft}


\begin{flushleft}
m+$\theta$
\end{flushleft}


\begin{flushleft}
(u $-$ p ).
\end{flushleft}


\begin{flushleft}
m+1
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
11.12 Barrier method for convex-concave games. We consider a convex-concave game with
\end{flushleft}


\begin{flushleft}
inequality constraints,
\end{flushleft}


\begin{flushleft}
minimizew maximizez
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
f0 (w, z)
\end{flushleft}


\begin{flushleft}
fi (w) $\leq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
f˜i (z) $\leq$ 0, i = 1, . . . , m.
\end{flushleft}


˜





\begin{flushleft}
Here w $\in$ Rn is the variable associated with minimizing the objective, and z $\in$ Rn˜ is
\end{flushleft}


\begin{flushleft}
the variable associated with maximizing the objective. The constraint functions f i and f˜i
\end{flushleft}


\begin{flushleft}
are convex and differentiable, and the objective function f0 is differentiable and convexconcave, i.e., convex in w, for each z, and concave in z, for each w. We assume for
\end{flushleft}


\begin{flushleft}
simplicity that dom f0 = Rn × Rn˜ .
\end{flushleft}


\begin{flushleft}
A solution or saddle-point for the game is a pair w , z , for which
\end{flushleft}


\begin{flushleft}
f0 (w , z) $\leq$ f0 (w , z ) $\leq$ f0 (w, z )
\end{flushleft}


\begin{flushleft}
holds for every feasible w and z. (For background on convex-concave games and functions,
\end{flushleft}


\begin{flushleft}
see §5.4.3, §10.3.4 and exercises 3.14, 5.24, 5.25, 10.10, and 10.13.) In this exercise we
\end{flushleft}


\begin{flushleft}
show how to solve this game using an extension of the barrier method, and the infeasible
\end{flushleft}


\begin{flushleft}
start Newton method (see §10.3).
\end{flushleft}


\begin{flushleft}
(a) Let t $>$ 0. Explain why the function
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


˜





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
tf0 (w, z) $-$
\end{flushleft}





\begin{flushleft}
log($-$f˜i (z))
\end{flushleft}





\begin{flushleft}
log($-$fi (w)) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
is convex-concave in (w, z). We will assume that it has a unique saddle-point,
\end{flushleft}


\begin{flushleft}
(w (t), z (t)), which can be found using the infeasible start Newton method.
\end{flushleft}


\begin{flushleft}
(b) As in the barrier method for solving a convex optimization problem, we can derive
\end{flushleft}


\begin{flushleft}
a simple bound on the suboptimality of (w (t), z (t)), which depends only on the
\end{flushleft}


\begin{flushleft}
problem dimensions, and decreases to zero as t increases. Let W and Z denote the
\end{flushleft}


\begin{flushleft}
feasible sets for w and z,
\end{flushleft}


\begin{flushleft}
Z = \{z | f˜i (z) $\leq$ 0, i = 1, . . . , m\}.
\end{flushleft}


˜





\begin{flushleft}
W = \{w | fi (w) $\leq$ 0, i = 1, . . . , m\},
\end{flushleft}


\begin{flushleft}
Show that
\end{flushleft}


\begin{flushleft}
f0 (w (t), z (t))
\end{flushleft}





$\leq$





\begin{flushleft}
f0 (w (t), z (t))
\end{flushleft}





$\geq$





\begin{flushleft}
m
\end{flushleft}


,


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
w$\in$W
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}


˜


\begin{flushleft}
sup f0 (w (t), z) $-$ ,
\end{flushleft}


\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
z$\in$Z
\end{flushleft}


\begin{flushleft}
inf f0 (w, z (t)) +
\end{flushleft}





\begin{flushleft}
and therefore
\end{flushleft}


\begin{flushleft}
sup f0 (w (t), z) $-$ inf f0 (w, z (t)) $\leq$
\end{flushleft}





\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
w$\in$W
\end{flushleft}





\begin{flushleft}
m+m
\end{flushleft}


˜


.


\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Follows from the convex-concave property of f0 ; convexity of $-$ log($-$fi ), and concavity of log($-$f˜i ).
\end{flushleft}


\begin{flushleft}
(b) Since (w (t), z (t)) is a saddle-point of the function
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
tf0 (w, z) $-$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


˜





\begin{flushleft}
log($-$f˜i (z)),
\end{flushleft}





\begin{flushleft}
log($-$fi (w)) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
its gradient with respect to w, and also with respect to z, vanishes there:
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
t$\nabla$w f0 (w (t), z (t)) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





1


\begin{flushleft}
$\nabla$fi (w (t))
\end{flushleft}


\begin{flushleft}
$-$fi (w (t))
\end{flushleft}





=





0





$-$1


\begin{flushleft}
$\nabla$f˜i (z (t))
\end{flushleft}


\begin{flushleft}
$-$f˜i (z (t))
\end{flushleft}





=





0.





\begin{flushleft}
m
\end{flushleft}


˜





\begin{flushleft}
t$\nabla$z f0 (w (t), z (t)) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
It follows that w (t) minimizes
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
f0 (w, z (t)) +
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (w)
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
over w, where $\lambda$i = 1/($-$tfi (w (t))), i.e., for all w, we have
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
f0 (w (t), z (t)) +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (w (t)) $\leq$ f0 (w, z (t)) +
\end{flushleft}





\begin{flushleft}
$\lambda$i fi (w).
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
The lefthand side is equal to f0 (w (t), z (t)) $-$ m/t, and for all w $\in$ W , the second
\end{flushleft}


\begin{flushleft}
term on the righthand side is nonpositive, so we have
\end{flushleft}


\begin{flushleft}
f0 (w (t), z (t)) $\leq$ inf f0 (w, z (t)) + m/t.
\end{flushleft}


\begin{flushleft}
w$\in$W
\end{flushleft}





\begin{flushleft}
A similar argument shows that
\end{flushleft}


\begin{flushleft}
f0 (w (t), z (t)) $\geq$ sup f0 (w (t), z) $-$ m/t.
\end{flushleft}


\begin{flushleft}
z$\in$Z
\end{flushleft}





\begin{flushleft}
Self-concordance and complexity analysis
\end{flushleft}


\begin{flushleft}
11.13 Self-concordance and negative entropy.
\end{flushleft}


\begin{flushleft}
(a) Show that the negative entropy function x log x (on R++ ) is not self-concordant.
\end{flushleft}


\begin{flushleft}
(b) Show that for any t $>$ 0, tx log x $-$ log x is self-concordant (on R++ ).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) First we consider f (x) = x log x, for which
\end{flushleft}


\begin{flushleft}
f (x) = 1 + log x,
\end{flushleft}


\begin{flushleft}
Thus
\end{flushleft}





\begin{flushleft}
f (x) =
\end{flushleft}





1


,


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
f (x) = $-$
\end{flushleft}





1


.


\begin{flushleft}
x2
\end{flushleft}





\begin{flushleft}
1/x2
\end{flushleft}


\begin{flushleft}
|f (x)|
\end{flushleft}


1


=


= $\surd$


3/2


\begin{flushleft}
f (x)
\end{flushleft}


\begin{flushleft}
1/x3/2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
which is unbounded above (as x $\rightarrow$ 0+ ). In particular, the self-concordance inequality |f (x)| $\leq$ 2f (x)3/2 fails for x = 1/5, so f is not self-concordant.
\end{flushleft}





\begin{flushleft}
(b) Now we consider g(x) = tx log x $-$ log x, for which
\end{flushleft}


\begin{flushleft}
g (x) = $-$
\end{flushleft}


\begin{flushleft}
Therefore
\end{flushleft}





1


\begin{flushleft}
+ t + t log x,
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
g (x) =
\end{flushleft}





1


\begin{flushleft}
t
\end{flushleft}


+ ,


\begin{flushleft}
x2
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
g (x) = $-$
\end{flushleft}





\begin{flushleft}
|g (x)|
\end{flushleft}


\begin{flushleft}
2/x3 + t/x2
\end{flushleft}


\begin{flushleft}
2 + tx
\end{flushleft}


=


=


.


3/2


2


\begin{flushleft}
g (x)3/2
\end{flushleft}


(1


\begin{flushleft}
+ tx)3/2
\end{flushleft}


\begin{flushleft}
(1/x + t/x)
\end{flushleft}





2


\begin{flushleft}
t
\end{flushleft}


$-$ 2.


\begin{flushleft}
x3
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Define
\end{flushleft}


\begin{flushleft}
h(a) =
\end{flushleft}





\begin{flushleft}
2+a
\end{flushleft}


\begin{flushleft}
(1 + a)3/2
\end{flushleft}





\begin{flushleft}
h(tx) =
\end{flushleft}





\begin{flushleft}
|g (x)|
\end{flushleft}


.


\begin{flushleft}
g (x)3/2
\end{flushleft}





\begin{flushleft}
so that
\end{flushleft}





\begin{flushleft}
We have h(0) = 2 and we will show that h (a) $<$ 0 for a $>$ 0, i.e., h is decreasing for
\end{flushleft}


\begin{flushleft}
a $>$ 0. This will prove that h(a) $\leq$ h(0) = 2, and therefore
\end{flushleft}


\begin{flushleft}
|g (x)|
\end{flushleft}


$\leq$ 2.


\begin{flushleft}
g (x)3/2
\end{flushleft}


\begin{flushleft}
We have
\end{flushleft}


\begin{flushleft}
h (a)
\end{flushleft}





=


=


=


$<$





\begin{flushleft}
(1 + a)3/2 $-$ (3/2)(1 + a)1/2 (2 + a)
\end{flushleft}


\begin{flushleft}
(1 + a)3
\end{flushleft}


\begin{flushleft}
(1 + a)1/2 ((1 + a) $-$ (3/2)(2 + a))
\end{flushleft}


\begin{flushleft}
(1 + a)3
\end{flushleft}


\begin{flushleft}
(2 + a/2)
\end{flushleft}


$-$


\begin{flushleft}
(1 + a)5/2
\end{flushleft}


0,





\begin{flushleft}
for a $>$ 0, so we are done.
\end{flushleft}


\begin{flushleft}
11.14 Self-concordance and the centering problem. Let $\phi$ be the logarithmic barrier function of
\end{flushleft}


\begin{flushleft}
problem (11.1). Suppose that the sublevel sets of (11.1) are bounded, and that tf 0 + $\phi$ is
\end{flushleft}


\begin{flushleft}
closed and self-concordant. Show that t$\nabla$2 f0 (x) + $\nabla$2 $\phi$(x) 0, for all x $\in$ dom $\phi$. Hint.
\end{flushleft}


\begin{flushleft}
See exercises 9.17 and 11.3.
\end{flushleft}


\begin{flushleft}
Solution. From exercise 11.3, the sublevel sets of tf0 + $\phi$ are bounded.
\end{flushleft}


\begin{flushleft}
From exercise 9.17, the nullspace of tf0 + $\phi$ is independent of x. So if the Hessian is not
\end{flushleft}


\begin{flushleft}
positive definite, tf0 + $\phi$ is linear along certain lines, which would contradict the fact that
\end{flushleft}


\begin{flushleft}
the sublevel sets are bounded.
\end{flushleft}





\begin{flushleft}
Barrier method for generalized inequalities
\end{flushleft}


\begin{flushleft}
11.15 Generalized logarithm is K-increasing. Let $\psi$ be a generalized logarithm for the proper
\end{flushleft}


\begin{flushleft}
cone K. Suppose y K 0.
\end{flushleft}


\begin{flushleft}
(a) Show that $\nabla$$\psi$(y) K ∗ 0, i.e., that $\psi$ is K-nondecreasing. Hint. If $\nabla$$\psi$(y) K ∗ 0,
\end{flushleft}


\begin{flushleft}
then there is some w K 0 for which w T $\nabla$$\psi$(y) $\leq$ 0. Use the inequality $\psi$(sw) $\leq$
\end{flushleft}


\begin{flushleft}
$\psi$(y) + $\nabla$$\psi$(y)T (sw $-$ y), with s $>$ 0.
\end{flushleft}


\begin{flushleft}
(b) Now show that $\nabla$$\psi$(y) K ∗ 0, i.e., that $\psi$ is K-increasing. Hint. Show that
\end{flushleft}


\begin{flushleft}
$\nabla$2 $\psi$(y) ≺ 0, $\nabla$$\psi$(y) K ∗ 0 imply $\nabla$$\psi$(y) K ∗ 0.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) If $\nabla$$\psi$(y)
\end{flushleft}


\begin{flushleft}
we have
\end{flushleft}





\begin{flushleft}
K∗
\end{flushleft}





\begin{flushleft}
0, there exists a w
\end{flushleft}


\begin{flushleft}
$\psi$(sw)
\end{flushleft}





$\leq$





=


$\leq$





\begin{flushleft}
K
\end{flushleft}





\begin{flushleft}
0 such that w T $\nabla$$\psi$(y) $\leq$ 0. By concavity of $\psi$
\end{flushleft}





\begin{flushleft}
$\psi$(y) + $\nabla$$\psi$(y)T (sw $-$ y)
\end{flushleft}


\begin{flushleft}
$\psi$(y) $-$ $\theta$ + sw T $\nabla$$\psi$(y)
\end{flushleft}


\begin{flushleft}
$\psi$(y) $-$ $\theta$
\end{flushleft}





\begin{flushleft}
for all s $>$ 0. In particular, $\psi$(sw) is bounded, for s $\geq$ 0. But we have $\psi$(sw) =
\end{flushleft}


\begin{flushleft}
$\psi$(w) + $\theta$ log s, which is unbounded as s $\rightarrow$ $\infty$. (We need w K 0 to ensure that
\end{flushleft}


\begin{flushleft}
sw $\in$ dom $\psi$.)
\end{flushleft}





\newpage
11


\begin{flushleft}
(b) We now know that $\nabla$$\psi$(y)
\end{flushleft}





\begin{flushleft}
K∗
\end{flushleft}





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
0. For small v we have
\end{flushleft}





\begin{flushleft}
$\nabla$$\psi$(y + v) $\approx$ $\nabla$$\psi$(y) + $\nabla$2 $\psi$(y)v,
\end{flushleft}


\begin{flushleft}
and by part (a) we have $\nabla$$\psi$(y +v)
\end{flushleft}


\begin{flushleft}
that we must have $\nabla$$\psi$(y) K ∗ 0.
\end{flushleft}





\begin{flushleft}
K∗
\end{flushleft}





\begin{flushleft}
0. Since $\nabla$2 $\psi$(y) is nonsingular, we conclude
\end{flushleft}





\begin{flushleft}
11.16 [NN94, page 41] Properties of a generalized logarithm. Let $\psi$ be a generalized logarithm
\end{flushleft}


\begin{flushleft}
for the proper cone K, with degree $\theta$. Prove that the following properties hold at any
\end{flushleft}


\begin{flushleft}
y K 0.
\end{flushleft}


\begin{flushleft}
(a) $\nabla$$\psi$(sy) = $\nabla$$\psi$(y)/s for all s $>$ 0.
\end{flushleft}





\begin{flushleft}
(b) $\nabla$$\psi$(y) = $-$$\nabla$2 $\psi$(y)y.
\end{flushleft}


\begin{flushleft}
(c) y T $\nabla$$\psi$ 2 (y)y = $-$$\theta$.
\end{flushleft}





\begin{flushleft}
(d) $\nabla$$\psi$(y)T $\nabla$2 $\psi$(y)$-$1 $\nabla$$\psi$(y) = $-$$\theta$.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) Differentiate $\psi$(sy) = $\psi$(y) + $\theta$ log s with respect to y to get s$\nabla$$\psi$(sy) = $\nabla$$\psi$(y).
\end{flushleft}





\begin{flushleft}
(b) Differentiating (y + tv)T $\nabla$$\psi$(y + tv) = $\theta$ with respect to t gives
\end{flushleft}





\begin{flushleft}
$\nabla$$\psi$(y + tv)T v + (y + tv)T $\nabla$2 $\psi$(y + tv)v = 0.
\end{flushleft}


\begin{flushleft}
At t = 0 we get
\end{flushleft}


\begin{flushleft}
$\nabla$$\psi$(y)T v + y T $\nabla$2 $\psi$(y)v = 0.
\end{flushleft}





\begin{flushleft}
This holds for all v, so $\nabla$$\psi$(y) = $-$$\nabla$2 $\psi$(y)y.
\end{flushleft}





\begin{flushleft}
(c) From part (b),
\end{flushleft}





\begin{flushleft}
y T $\nabla$$\psi$ 2 (y)y = $-$y T $\nabla$$\psi$(y) = $-$$\theta$.
\end{flushleft}


\begin{flushleft}
(d) From part (b),
\end{flushleft}


\begin{flushleft}
$\nabla$$\psi$(y)T $\nabla$2 $\psi$(y)$-$1 $\nabla$$\psi$(y) = $-$$\nabla$$\psi$(y)T y = $-$$\theta$.
\end{flushleft}


\begin{flushleft}
11.17 Dual generalized logarithm. Let $\psi$ be a generalized logarithm for the proper cone K, with
\end{flushleft}


\begin{flushleft}
degree $\theta$. Show that the dual generalized logarithm $\psi$, defined in (11.49), satisfies
\end{flushleft}


\begin{flushleft}
$\psi$(sv) = $\psi$(v) + $\theta$ log s,
\end{flushleft}


\begin{flushleft}
for v K ∗ 0, s $>$ 0.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}
$\psi$(sv) = inf sv T u $-$ $\psi$(u) = inf v T u
\end{flushleft}


\begin{flushleft}
˜ $-$ $\psi$(˜
\end{flushleft}


\begin{flushleft}
u/s)
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}





\begin{flushleft}
u
\end{flushleft}


˜





\begin{flushleft}
where u
\end{flushleft}


\begin{flushleft}
˜ = su. Using the logarithm property for $\psi$, we have $\psi$(˜
\end{flushleft}


\begin{flushleft}
u/s) = $\psi$(˜
\end{flushleft}


\begin{flushleft}
u) $-$ $\theta$ log s, so
\end{flushleft}


\begin{flushleft}
$\psi$(sv) = inf v T u
\end{flushleft}


\begin{flushleft}
˜ $-$ $\psi$(˜
\end{flushleft}


\begin{flushleft}
u) + $\theta$ log s = $\psi$(u) + $\theta$ log s.
\end{flushleft}


\begin{flushleft}
u
\end{flushleft}


˜





\begin{flushleft}
11.18 Is the function
\end{flushleft}


\begin{flushleft}
$\psi$(y) = log
\end{flushleft}





\begin{flushleft}
yn+1 $-$
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
yi2
\end{flushleft}





\begin{flushleft}
yn+1
\end{flushleft}





,





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
with dom $\psi$ = \{y $\in$ Rn+1 | yn+1 $>$
\end{flushleft}


\begin{flushleft}
y 2 \}, a generalized logarithm for the secondi=1 i
\end{flushleft}


\begin{flushleft}
n+1
\end{flushleft}


\begin{flushleft}
order cone in R
\end{flushleft}


?


\begin{flushleft}
Solution. It is not. It satisfies all the required properties except closedness.
\end{flushleft}


\begin{flushleft}
To see this, take any a $>$ 0, and suppose y approaches the origin along the path
\end{flushleft}


\begin{flushleft}
(y1 , . . . , yn ) =
\end{flushleft}





\begin{flushleft}
t(t $-$ a)/n,
\end{flushleft}





\begin{flushleft}
yn+1 = t
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
where t $>$ 0. We have
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
yi2 )1/2 =
\end{flushleft}





(


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
t(t $-$ a) $<$ yn+1
\end{flushleft}





\begin{flushleft}
so y $\in$ int K. However,
\end{flushleft}


\begin{flushleft}
$\psi$(y) = log(t $-$ t(t $-$ a)/t) = log a.
\end{flushleft}


\begin{flushleft}
Therefore we can find sequences of points with any arbitrary limit.
\end{flushleft}





\begin{flushleft}
Implementation
\end{flushleft}


\begin{flushleft}
11.19 Yet another method for computing the Newton step. Show that the Newton step for the
\end{flushleft}


\begin{flushleft}
barrier method, which is given by the solution of the linear equations (11.14), can be
\end{flushleft}


\begin{flushleft}
found by solving a larger set of linear equations with coefficient matrix
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
t$\nabla$2 f0 (x) +
\end{flushleft}





1


\begin{flushleft}
$\nabla$2 fi (x)
\end{flushleft}


\begin{flushleft}
i $-$fi (x)
\end{flushleft}





\begin{flushleft}
Df (x)
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
Df (x)T
\end{flushleft}


\begin{flushleft}
$-$ diag(f (x))2
\end{flushleft}


0





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


\begin{flushleft}
0 
\end{flushleft}


0





\begin{flushleft}
where f (x) = (f1 (x), . . . , fm (x)).
\end{flushleft}


\begin{flushleft}
For what types of problem structure might solving this larger system be interesting?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
t$\nabla$2 f0 (x) +
\end{flushleft}





1


\begin{flushleft}
$\nabla$2 fi (x)
\end{flushleft}


\begin{flushleft}
i $-$fi (x)
\end{flushleft}





\begin{flushleft}
Df (x)
\end{flushleft}


\begin{flushleft}
A
\end{flushleft}





\begin{flushleft}
Df (x)T
\end{flushleft}


\begin{flushleft}
$-$ diag(f (x))2
\end{flushleft}


0





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
AT
\end{flushleft}


\begin{flushleft}
0 
\end{flushleft}


0





\begin{flushleft}
∆xnt
\end{flushleft}


\begin{flushleft}
y
\end{flushleft}


\begin{flushleft}
$\nu$nt
\end{flushleft}





=$-$





\begin{flushleft}
g
\end{flushleft}


0


0





.





\begin{flushleft}
where g = t$\nabla$f0 (x) + $\nabla$$\phi$(x). From the second equation,
\end{flushleft}


\begin{flushleft}
yi =
\end{flushleft}





\begin{flushleft}
$\nabla$fi (x)T ∆xnt
\end{flushleft}


\begin{flushleft}
fi (x)2
\end{flushleft}





\begin{flushleft}
and substituting in the first equation gives (11.14).
\end{flushleft}


\begin{flushleft}
This might be useful if the big matrix is sparse, and the 2 × 2 block system (obtained by
\end{flushleft}


\begin{flushleft}
pivoting on the diag(f (x))2 block) has a dense (1,1) block. For example if the (1,1) block
\end{flushleft}


\begin{flushleft}
of the big system is block diagonal, m
\end{flushleft}


\begin{flushleft}
n is small, and Df (x) is dense.
\end{flushleft}


\begin{flushleft}
11.20 Network rate optimization via the dual problem. In this problem we examine a dual method
\end{flushleft}


\begin{flushleft}
for solving the network rate optimization problem of §11.8.4. To simplify the presentation
\end{flushleft}


\begin{flushleft}
we assume that the utility functions Ui are strictly concave, with dom Ui = R++ , and
\end{flushleft}


\begin{flushleft}
that they satisfy Ui (xi ) $\rightarrow$ $\infty$ as xi $\rightarrow$ 0 and Ui (xi ) $\rightarrow$ 0 as xi $\rightarrow$ $\infty$.
\end{flushleft}


\begin{flushleft}
(a) Express the dual problem of (11.62) in terms of the conjugate utility functions
\end{flushleft}


\begin{flushleft}
Vi = ($-$Ui )∗ , defined as
\end{flushleft}


\begin{flushleft}
Vi ($\lambda$) = sup($\lambda$x + Ui (x)).
\end{flushleft}


\begin{flushleft}
x$>$0
\end{flushleft}





\begin{flushleft}
Show that dom Vi = $-$R++ , and that for each $\lambda$ $<$ 0 there is a unique x with
\end{flushleft}


\begin{flushleft}
Ui (x) = $-$$\lambda$.
\end{flushleft}





\begin{flushleft}
(b) Describe a barrier method for the dual problem. Compare the complexity per iteration with the complexity of the method in §11.8.4. Distinguish the same two cases
\end{flushleft}


\begin{flushleft}
as in §11.8.4 (AT A is sparse and AAT is sparse).
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
(a) Suppose $\lambda$ $<$ 0. Since Ui is strictly concave and increasing, with Ui (xi ) $\rightarrow$ $\infty$ as
\end{flushleft}


\begin{flushleft}
xi $\rightarrow$ 0 and Ui (xi ) $\rightarrow$ 0 as xi $\rightarrow$ $\infty$, there is a unique x with
\end{flushleft}


\begin{flushleft}
Ui (x) = $-$$\lambda$.
\end{flushleft}


\begin{flushleft}
After changing problem (11.62) its Lagrangian is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
L(x, $\lambda$, z)
\end{flushleft}





=


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
($-$Ui (x)) + $\lambda$T (Ax $-$ c) $-$ z T x
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





=





$-$





\begin{flushleft}
Ui (x) $-$ (AT $\lambda$)i xi + zi xi $-$ cT $\lambda$.
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
The minimum over x is
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
inf L(x, $\lambda$, z)
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





=





$-$





\begin{flushleft}
inf
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
(Ui (x) $-$ (AT $\lambda$)i xi + zi xi ) $-$ cT $\lambda$
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





=





=





$-$


$-$





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
sup(Ui (x) $-$ (AT $\lambda$)i xi + zi xi ) $-$ cT $\lambda$
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}





\begin{flushleft}
Vi ($-$(AT $\lambda$)i + zi ) $-$ cT $\lambda$,
\end{flushleft}





\begin{flushleft}
so the dual problem is (after changing the sign again)
\end{flushleft}


\begin{flushleft}
cT $\lambda$ +
\end{flushleft}


\begin{flushleft}
$\lambda$ 0,
\end{flushleft}





\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
z
\end{flushleft}





\begin{flushleft}
Vi ($-$(AT $\lambda$)i + zi )
\end{flushleft}


0.





\begin{flushleft}
The function Vi is increasing on its domain $-$R++ , so z = 0 at the optimum and
\end{flushleft}


\begin{flushleft}
the dual problem simplifies to
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
cT $\lambda$ +
\end{flushleft}


\begin{flushleft}
$\lambda$ 0
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
Vi ($-$(AT $\lambda$)i )
\end{flushleft}





\begin{flushleft}
$-$$\lambda$i can be interpreted as the price on link i. $-$(AT $\lambda$)i is the sum of the prices along
\end{flushleft}


\begin{flushleft}
the path of flow i.
\end{flushleft}


\begin{flushleft}
(b) The Hessian of
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
Vi ($-$(AT $\lambda$)i )
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
c $\lambda$+
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
is
\end{flushleft}





$-$





\begin{flushleft}
log $\lambda$i
\end{flushleft}


\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
H = tA diag($-$AT $\lambda$)$-$2 AT + diag($\lambda$)$-$2 .
\end{flushleft}





\begin{flushleft}
If AAT is sparse, we solve the Newton equation H∆$\lambda$ = $-$g.
\end{flushleft}


\begin{flushleft}
If AT A is sparse, we apply the matrix inversion lemma and compute the Newton
\end{flushleft}


\begin{flushleft}
step by first solving an equation with coefficient matrix of the form D1 + AT D2 A,
\end{flushleft}


\begin{flushleft}
where D1 and D2 are diagonal (see §11.8.4).
\end{flushleft}





\begin{flushleft}
Numerical experiments
\end{flushleft}


\begin{flushleft}
11.21 Log-Chebyshev approximation with bounds. We consider an approximation problem: find
\end{flushleft}


\begin{flushleft}
x $\in$ Rn , that satisfies the variable bounds l x u, and yields Ax $\approx$ b, where b $\in$ Rm .
\end{flushleft}


\begin{flushleft}
You can assume that l ≺ u, and b 0 (for reasons we explain below). We let aTi denote
\end{flushleft}


\begin{flushleft}
the ith row of the matrix A.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
We judge the approximation Ax $\approx$ b by the maximum fractional deviation, which is
\end{flushleft}


\begin{flushleft}
max max\{(aTi x)/bi , bi /(aTi x)\} = max
\end{flushleft}





\begin{flushleft}
i=1,...,n
\end{flushleft}





\begin{flushleft}
i=1,...,n
\end{flushleft}





\begin{flushleft}
max\{aTi x, bi \}
\end{flushleft}


,


\begin{flushleft}
min\{aTi x, bi \}
\end{flushleft}





\begin{flushleft}
when Ax 0; we define the maximum fractional deviation as $\infty$ if Ax 0.
\end{flushleft}


\begin{flushleft}
The problem of minimizing the maximum fractional deviation is called the fractional
\end{flushleft}


\begin{flushleft}
Chebyshev approximation problem, or the logarithmic Chebyshev approximation problem,
\end{flushleft}


\begin{flushleft}
since it is equivalent to minimizing the objective
\end{flushleft}


\begin{flushleft}
max | log aTi x $-$ log bi |.
\end{flushleft}





\begin{flushleft}
i=1,...,n
\end{flushleft}





\begin{flushleft}
(See also exercise 6.3, part (c).)
\end{flushleft}


\begin{flushleft}
(a) Formulate the fractional Chebyshev approximation problem (with variable bounds)
\end{flushleft}


\begin{flushleft}
as a convex optimization problem with twice differentiable objective and constraint
\end{flushleft}


\begin{flushleft}
functions.
\end{flushleft}


\begin{flushleft}
(b) Implement a barrier method that solves the fractional Chebyshev approximation
\end{flushleft}


\begin{flushleft}
problem. You can assume an initial point x(0) , satisfying l ≺ x(0) ≺ u, Ax(0) 0, is
\end{flushleft}


\begin{flushleft}
known.
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We can formulate the fractional Chebyshev approximation problem with variable
\end{flushleft}


\begin{flushleft}
bounds as
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
s
\end{flushleft}


\begin{flushleft}
subject to (aTi x)/bi $\leq$ s, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
bi /(aTi x) $\leq$ s, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
aTi x $\geq$ 0, i = 1, . . . , m
\end{flushleft}


\begin{flushleft}
l x u,
\end{flushleft}


\begin{flushleft}
This is clearly a convex problem, since the inequalities are linear, except for the
\end{flushleft}


\begin{flushleft}
second group, which involves the inverse.
\end{flushleft}


\begin{flushleft}
The sublevel sets are bounded (by the last constraint).
\end{flushleft}


\begin{flushleft}
Note that we can, without loss of generality, take bi = 1, and replace ai with ai /bi .
\end{flushleft}


\begin{flushleft}
We will assume this has been done. To simplify the notation, we will use ai to
\end{flushleft}


\begin{flushleft}
denote the scaled version (i.e., ai /bi in the original problem data).
\end{flushleft}


\begin{flushleft}
(b) In the centering problems we must minimize the function
\end{flushleft}


=





\begin{flushleft}
ts $-$
\end{flushleft}


$-$





=





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
ts + $\phi$(s, x)
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(s $-$ aTi x) $-$
\end{flushleft}


\begin{flushleft}
log(ui $-$ xi ) $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log aTi x $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(s $-$ 1/aTi x)
\end{flushleft}





\begin{flushleft}
log(xi $-$ li )
\end{flushleft}





\begin{flushleft}
$\phi$1 (s, x) + $\phi$2 (s, x) + $\phi$3 (s, x)
\end{flushleft}





\begin{flushleft}
with variables x, s, where
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\phi$1 (s, x)
\end{flushleft}





=





\begin{flushleft}
ts $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
log(ui $-$ xi ) $-$
\end{flushleft}





\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
$\phi$2 (s, x)
\end{flushleft}





\begin{flushleft}
$\phi$3 (s, x)
\end{flushleft}





=





=





$-$


$-$





\begin{flushleft}
i=1
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(s $-$ aTi x)
\end{flushleft}


\begin{flushleft}
log(s(aTi x) $-$ 1).
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(xi $-$ li )
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
The gradient and Hessian of $\phi$1 are
\end{flushleft}


\begin{flushleft}
$\nabla$$\phi$1 (s, x)
\end{flushleft}





=





\begin{flushleft}
t
\end{flushleft}


\begin{flushleft}
diag(u $-$ x)$-$1 1 $-$ diag(x $-$ l)$-$1 1
\end{flushleft}





\begin{flushleft}
$\nabla$2 $\phi$1 (s, x)
\end{flushleft}





=





0


0





0


\begin{flushleft}
diag(u $-$ x)$-$2 + diag(x $-$ l)$-$2
\end{flushleft}





.





\begin{flushleft}
The gradient and Hessian of $\phi$2 are
\end{flushleft}


\begin{flushleft}
$\nabla$$\phi$2 (x)
\end{flushleft}





=





\begin{flushleft}
$-$1T
\end{flushleft}


\begin{flushleft}
AT
\end{flushleft}





\begin{flushleft}
diag(s $-$ Ax)$-$1 1
\end{flushleft}





\begin{flushleft}
$\nabla$2 $\phi$2 (x)
\end{flushleft}





=





\begin{flushleft}
$-$1T
\end{flushleft}


\begin{flushleft}
AT
\end{flushleft}





\begin{flushleft}
diag(s $-$ Ax)$-$2
\end{flushleft}





$-$1





\begin{flushleft}
A
\end{flushleft}





.





\begin{flushleft}
We can find the gradient and Hessian of $\phi$3 by expressing it as $\phi$3 (s, x) = h(s, Ax)
\end{flushleft}


\begin{flushleft}
where
\end{flushleft}


\begin{flushleft}
m
\end{flushleft}





\begin{flushleft}
h(s, y) = $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(syi $-$ 1),
\end{flushleft}





\begin{flushleft}
and then applying the chain rule. The gradient and Hesian of h are
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
yi /(syi $-$ 1)
\end{flushleft}


\begin{flushleft}
s/(sy1 $-$ 1)
\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
=$-$
\end{flushleft}


..


\begin{flushleft}

\end{flushleft}


.


\begin{flushleft}
s/(sym $-$ 1)
\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
$\nabla$h(s, y) = $-$ 
\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
and
\end{flushleft}





\begin{flushleft}
$\nabla$2 h(s, y)
\end{flushleft}





=





=





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}

\end{flushleft}





\begin{flushleft}
m
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
yi2 /(syi $-$ 1)2
\end{flushleft}


\begin{flushleft}
1/(sy1 $-$ 1)2
\end{flushleft}


\begin{flushleft}
1/(sy2 $-$ 1)2
\end{flushleft}


..


.


\begin{flushleft}
1/(sym $-$ 1)2
\end{flushleft}





\begin{flushleft}
1/(sy1 $-$ 1)2
\end{flushleft}


\begin{flushleft}
s2 /(sy1 $-$ 1)2
\end{flushleft}


0


..


.


0





\begin{flushleft}
i
\end{flushleft}





\begin{flushleft}
y T diag(sy $-$ 1)$-$2 y
\end{flushleft}


\begin{flushleft}
diag(sy $-$ 1)$-$2 1
\end{flushleft}





\begin{flushleft}
y T diag(sy $-$ 1)$-$1 1
\end{flushleft}


\begin{flushleft}
s diag(sy $-$ 1)$-$1 1
\end{flushleft}





\begin{flushleft}
1/(sy2 $-$ 1)2
\end{flushleft}


0


\begin{flushleft}
s2 /(sy2 $-$ 1)2
\end{flushleft}


..


.


0





\begin{flushleft}
1T diag(sy $-$ 1)$-$2
\end{flushleft}


\begin{flushleft}
s2 diag(sy $-$ 1)$-$2
\end{flushleft}





···


···


···


..


.


···





\begin{flushleft}
1/(sym $-$ 1)2
\end{flushleft}


0


0


..


.


\begin{flushleft}
s2 /(sym $-$ 1)2
\end{flushleft}





.





\begin{flushleft}
We therefore obtain
\end{flushleft}


\begin{flushleft}
$\nabla$$\phi$3 (s, x)
\end{flushleft}





=


\begin{flushleft}
$\nabla$2 $\phi$3 (s, x)
\end{flushleft}





1


0





=





0


\begin{flushleft}
AT
\end{flushleft}


\begin{flushleft}
yT
\end{flushleft}


\begin{flushleft}
sAT
\end{flushleft}





$-$





0


\begin{flushleft}
AT
\end{flushleft}





\begin{flushleft}
$\nabla$h(s, Ax)
\end{flushleft}


\begin{flushleft}
diag(sAx $-$ 1)$-$1 1
\end{flushleft}





=





1


0





=





\begin{flushleft}
xT A diag(sAx $-$ 1)$-$2 Ax
\end{flushleft}


\begin{flushleft}
AT diag(sAx $-$ 1)$-$2 1
\end{flushleft}





\begin{flushleft}
$\nabla$2 h(s, Ax)
\end{flushleft}





\begin{flushleft}
A Matlab implementation is given below.
\end{flushleft}





1


0





0


\begin{flushleft}
A
\end{flushleft}


\begin{flushleft}
1T diag(sAx $-$ 1)$-$2 A
\end{flushleft}


\begin{flushleft}
s AT diag(sAx $-$ 1)$-$2 A
\end{flushleft}


2





.





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
MAXITERS = 200;
\end{flushleft}


\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
NTTOL = 1e-8;
\end{flushleft}


\begin{flushleft}
\% terminate Newton iterations if lambda\^{}2 $<$ NTTOL
\end{flushleft}


\begin{flushleft}
MU = 20;
\end{flushleft}


\begin{flushleft}
TOL = 1e-4;
\end{flushleft}


\begin{flushleft}
\% terminate if duality gap less than TOL
\end{flushleft}


\begin{flushleft}
x = x0; y = A*x; s = 1.1*max([max(A*x), max(1./y)]);
\end{flushleft}


\begin{flushleft}
t = 1;
\end{flushleft}


\begin{flushleft}
for iter = 1:MAXITERS
\end{flushleft}


\begin{flushleft}
val = t*s - sum(log(u-x)) - sum(log(x-l)) - sum(log(s-y)) - ...
\end{flushleft}


\begin{flushleft}
sum(log(s*y-1));
\end{flushleft}


\begin{flushleft}
grad = [t-sum(1./(s-y))-sum(y./(s*y-1));
\end{flushleft}


\begin{flushleft}
1./(u-x)-1./(x-l)+A'*(1./(s-y)-s./(s*y-1))];
\end{flushleft}


\begin{flushleft}
hess = [sum((s-y).\^{}(-2)+(y./(s*y-1)).\^{}2) ...
\end{flushleft}


\begin{flushleft}
(-(s-y).\^{}(-2) + (s*y-1).\^{}(-2))'*A;
\end{flushleft}


\begin{flushleft}
A'*(-(s-y).\^{}(-2) + (s*y-1).\^{}(-2)) ...
\end{flushleft}


\begin{flushleft}
diag((u-x).\^{}(-2) + (x-l).\^{}(-2)) + ...
\end{flushleft}


\begin{flushleft}
A'*(diag((s-y).\^{}(-2)+(s./(s*y-1)).\^{}2))*A];
\end{flushleft}


\begin{flushleft}
step = -hess\ensuremath{\backslash}grad; fprime = grad'*step;
\end{flushleft}


\begin{flushleft}
if (abs(fprime) $<$ NTTOL),
\end{flushleft}


\begin{flushleft}
gap = (3*m+2*n)/t;
\end{flushleft}


\begin{flushleft}
if (gap$<$TOL); break; end;
\end{flushleft}


\begin{flushleft}
t = MU*t;
\end{flushleft}


\begin{flushleft}
else
\end{flushleft}


\begin{flushleft}
ds = step(1); dx = step(1+[1:n]); dy = A*dx;
\end{flushleft}


\begin{flushleft}
tls = 1;
\end{flushleft}


\begin{flushleft}
news = s+tls*ds; newx = x+tls*dx; newy = y+tls*dy;
\end{flushleft}


\begin{flushleft}
while (min([news-newy; news-1./newy; newy; newx-l; u-newx]) $<$= 0),
\end{flushleft}


\begin{flushleft}
tls = BETA*tls;
\end{flushleft}


\begin{flushleft}
news = s+tls*ds; newx = x+tls*dx; newy = y+tls*dy;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
newval = t*news - sum(log(u-newx)) - sum(log(newx-l)) ...
\end{flushleft}


\begin{flushleft}
- sum(log(news-newy)) - sum(log(news*newy-1));
\end{flushleft}


\begin{flushleft}
while (newval $>$= val + tls*ALPHA*fprime),
\end{flushleft}


\begin{flushleft}
tls = BETA*tls;
\end{flushleft}


\begin{flushleft}
news = s+tls*ds; newx = x+tls*dx; newy = y+tls*dy;
\end{flushleft}


\begin{flushleft}
newval = t*news - sum(log(u-newx)) - sum(log(newx-l)) ...
\end{flushleft}


\begin{flushleft}
- sum(log(news-newy)) - sum(log(news*newy-1));
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
x = x+tls*dx; y = A*x; s = s+tls*ds;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}





\begin{flushleft}
11.22 Maximum volume rectangle inside a polyhedron. Consider the problem described in exercise 8.16, i.e., finding the maximum volume rectangle R = \{x | l
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
u\} that lies in
\end{flushleft}


\begin{flushleft}
a polyhedron described by a set of linear inequalities, P = \{x | Ax
\end{flushleft}


\begin{flushleft}
b\}. Implement a
\end{flushleft}


\begin{flushleft}
barrier method for solving this problem. You can assume that b 0, which means that
\end{flushleft}


\begin{flushleft}
for small l ≺ 0 and u 0, the rectangle R lies inside P.
\end{flushleft}


\begin{flushleft}
Test your implementation on several simple examples. Find the maximum volume rect-
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
angle that lies in the polyhedron defined by
\end{flushleft}





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
A=
\end{flushleft}





0


2


2


$-$4


$-$4





\begin{flushleft}

\end{flushleft}





$-$1


$-$4


1


4


0





\begin{flushleft}

\end{flushleft}


\begin{flushleft}

\end{flushleft}


\begin{flushleft}
,
\end{flushleft}


\begin{flushleft}

\end{flushleft}





\begin{flushleft}
b = 1.
\end{flushleft}





\begin{flushleft}
Plot this polyhedron, and the maximum volume rectangle that lies inside it.
\end{flushleft}


\begin{flushleft}
Solution. We use the formulation
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$-$ i=1 log(ui $-$ li )
\end{flushleft}


\begin{flushleft}
A+ u $-$ A$-$ l b,
\end{flushleft}





\begin{flushleft}
(with implicit constraint u
\end{flushleft}


\begin{flushleft}
l) worked out in exercise 8.16. Here a+
\end{flushleft}


\begin{flushleft}
ij = max\{aij , 0\},
\end{flushleft}


$-$


\begin{flushleft}
aij = max\{$-$aij , 0\}.
\end{flushleft}


\begin{flushleft}
The gradient and Hessian of the function
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\psi$(l, u) = $-$t
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log(ui $-$ li ) $-$
\end{flushleft}





\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
log((b $-$ A+ u + A$-$ l)i )
\end{flushleft}





\begin{flushleft}
are
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
$\nabla$$\psi$(l, u)
\end{flushleft}





=





\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





\begin{flushleft}
diag(u $-$ l)$-$1 1 +
\end{flushleft}





\begin{flushleft}
$-$A$-$
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
A+
\end{flushleft}





\begin{flushleft}
$\nabla$2 $\psi$(l, u)
\end{flushleft}





=





\begin{flushleft}
t
\end{flushleft}





\begin{flushleft}
I
\end{flushleft}


\begin{flushleft}
$-$I
\end{flushleft}





\begin{flushleft}
diag(u $-$ l)$-$2
\end{flushleft}





\begin{flushleft}
$-$I
\end{flushleft}





\begin{flushleft}
T
\end{flushleft}





\begin{flushleft}
$-$A$-$
\end{flushleft}


\begin{flushleft}
T
\end{flushleft}


\begin{flushleft}
A+
\end{flushleft}





+





\begin{flushleft}
I
\end{flushleft}





\begin{flushleft}
diag(b $-$ A+ u + A$-$ l)$-$1 1
\end{flushleft}





\begin{flushleft}
diag(b $-$ A+ u + A$-$ l)$-$2
\end{flushleft}





\begin{flushleft}
$-$A$-$
\end{flushleft}





\begin{flushleft}
A+
\end{flushleft}





.





\begin{flushleft}
A plot of the particular polyhedron and the maximum volume box is given below.
\end{flushleft}





\begin{flushleft}
x2
\end{flushleft}





0.5





0





\begin{flushleft}
PSfrag replacements
\end{flushleft}


$-$0.5


$-$0.5





0





\begin{flushleft}
x1
\end{flushleft}





0.5





\begin{flushleft}
An implementation in Matlab is given below.
\end{flushleft}


\begin{flushleft}
MAXITERS = 200;
\end{flushleft}


\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
NTTOL = 1e-8;
\end{flushleft}


\begin{flushleft}
\% terminate Newton iterations if lambda\^{}2 $<$ NTTOL
\end{flushleft}


\begin{flushleft}
MU = 20;
\end{flushleft}


\begin{flushleft}
TOL = 1e-4;
\end{flushleft}


\begin{flushleft}
\% terminate if duality gap less than TOL
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}





\begin{flushleft}
Ap = max(A,0); Am = max(-A,0);
\end{flushleft}


\begin{flushleft}
r = max(Ap*ones(n,1) + Am*ones(n,1));
\end{flushleft}


\begin{flushleft}
u = (.5/r)*ones(n,1); l = -(.5/r)*ones(n,1);
\end{flushleft}


\begin{flushleft}
t = 1;
\end{flushleft}


\begin{flushleft}
for iter = 1:MAXITERS
\end{flushleft}


\begin{flushleft}
y = b+Am*l-Ap*u;
\end{flushleft}


\begin{flushleft}
val = -t*sum(log(u-l)) - sum(log(y));
\end{flushleft}


\begin{flushleft}
grad = t*[1./(u-l); -1./(u-l)] + [-Am'; Ap']*(1./y);
\end{flushleft}


\begin{flushleft}
hess = t*[diag(1./(u-l).\^{}2), -diag(1./(u-l).\^{}2);
\end{flushleft}


\begin{flushleft}
-diag(1./(u-l).\^{}2), diag(1./(u-l).\^{}2)] + ...
\end{flushleft}


\begin{flushleft}
[-Am'; Ap']*diag(1./y.\^{}2)*[-Am Ap];
\end{flushleft}


\begin{flushleft}
step = -hess\ensuremath{\backslash}grad; fprime = grad'*step;
\end{flushleft}


\begin{flushleft}
if (abs(fprime) $<$ NTTOL),
\end{flushleft}


\begin{flushleft}
gap = (2*m)/t;
\end{flushleft}


\begin{flushleft}
disp(['iter ', int2str(iter), '; gap = ', num2str(gap)]);
\end{flushleft}


\begin{flushleft}
if (gap$<$TOL); break; end;
\end{flushleft}


\begin{flushleft}
t = MU*t;
\end{flushleft}


\begin{flushleft}
else
\end{flushleft}


\begin{flushleft}
dl = step(1:n); du = step(n+[1:n]);
\end{flushleft}


\begin{flushleft}
dy = Am*dl-Ap*du;
\end{flushleft}


\begin{flushleft}
tls = 1;
\end{flushleft}


\begin{flushleft}
while (min([u-l+tls*(du-dl); y+tls*dy]) $<$= 0)
\end{flushleft}


\begin{flushleft}
tls = BETA*tls;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
while (-t*sum(log(u-l+tls*(du-dl))) - sum(log(y+tls*dy)) $>$= ...
\end{flushleft}


\begin{flushleft}
val + tls*ALPHA*fprime),
\end{flushleft}


\begin{flushleft}
tls = BETA*tls;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
l = l+tls*dl; u = u+tls*du;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
11.23 SDP bounds and heuristics for the two-way partitioning problem. In this exercise we
\end{flushleft}


\begin{flushleft}
consider the two-way partitioning problem (5.7), described on page 219, and also in exercise 5.39:
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
xT W x
\end{flushleft}


(11.65)


\begin{flushleft}
subject to x2i = 1, i = 1, . . . , n,
\end{flushleft}


\begin{flushleft}
with variable x $\in$ Rn . We assume, without loss of generality, that W $\in$ Sn satisfies
\end{flushleft}


\begin{flushleft}
Wii = 0. We denote the optimal value of the partitioning problem as p , and x will
\end{flushleft}


\begin{flushleft}
denote an optimal partition. (Note that $-$x is also an optimal partition.)
\end{flushleft}


\begin{flushleft}
The Lagrange dual of the two-way partitioning problem (11.65) is given by the SDP
\end{flushleft}


\begin{flushleft}
maximize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
$-$1T $\nu$
\end{flushleft}


\begin{flushleft}
W + diag($\nu$)
\end{flushleft}





0,





(11.66)





\begin{flushleft}
with variable $\nu$ $\in$ Rn . The dual of this SDP is
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
subject to
\end{flushleft}





\begin{flushleft}
tr(W X)
\end{flushleft}


\begin{flushleft}
X 0
\end{flushleft}


\begin{flushleft}
Xii = 1,
\end{flushleft}





(11.67)


\begin{flushleft}
i = 1, . . . , n,
\end{flushleft}





\begin{flushleft}
with variable X $\in$ Sn . (This SDP can be interpreted as a relaxation of the two-way
\end{flushleft}


\begin{flushleft}
partitioning problem (11.65); see exercise 5.39.) The optimal values of these two SDPs
\end{flushleft}


\begin{flushleft}
are equal, and give a lower bound, which we denote d , on the optimal value p . Let $\nu$
\end{flushleft}


\begin{flushleft}
and X denote optimal points for the two SDPs.
\end{flushleft}





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
(a) Implement a barrier method that solves the SDP (11.66) and its dual (11.67), given
\end{flushleft}


\begin{flushleft}
the weight matrix W . Explain how you obtain nearly optimal $\nu$ and X, give formulas for any Hessians and gradients that your method requires, and explain how
\end{flushleft}


\begin{flushleft}
you compute the Newton step. Test your implementation on some small problem
\end{flushleft}


\begin{flushleft}
instances, comparing the bound you find with the optimal value (which can be found
\end{flushleft}


\begin{flushleft}
by checking the objective value of all 2n partitions). Try your implementation on a
\end{flushleft}


\begin{flushleft}
randomly chosen problem instance large enough that you cannot find the optimal
\end{flushleft}


\begin{flushleft}
partition by exhaustive search (e.g., n = 100).
\end{flushleft}


\begin{flushleft}
(b) A heuristic for partitioning. In exercise 5.39, you found that if X has rank one,
\end{flushleft}


\begin{flushleft}
then it must have the form X = x (x )T , where x is optimal for the two-way
\end{flushleft}


\begin{flushleft}
partitioning problem. This suggests the following simple heuristic for finding a good
\end{flushleft}


\begin{flushleft}
partition (if not the best): solve the SDPs above, to find X (and the bound d ).
\end{flushleft}


\begin{flushleft}
Let v denote an eigenvector of X associated with its largest eigenvalue, and let
\end{flushleft}


\begin{flushleft}
x
\end{flushleft}


\begin{flushleft}
ˆ = sign(v). The vector x
\end{flushleft}


\begin{flushleft}
ˆ is our guess for a good partition.
\end{flushleft}


\begin{flushleft}
Try this heuristic on some small problem instances, and the large problem instance
\end{flushleft}


\begin{flushleft}
you used in part (a). Compare the objective value of your heuristic partition, x
\end{flushleft}


\begin{flushleft}
ˆT W x
\end{flushleft}


ˆ,


\begin{flushleft}
with the lower bound d .
\end{flushleft}


\begin{flushleft}
(c) A randomized method. Another heuristic technique for finding a good partition,
\end{flushleft}


\begin{flushleft}
given the solution X of the SDP (11.67), is based on randomization. The method
\end{flushleft}


\begin{flushleft}
is simple: we generate independent samples x(1) , . . . , x(K) from a normal distribution
\end{flushleft}


\begin{flushleft}
on Rn , with zero mean and covariance X . For each sample we consider the heuristic
\end{flushleft}


\begin{flushleft}
approximate solution x
\end{flushleft}


\begin{flushleft}
ˆ(k) = sign(x(k) ). We then take the best among these, i.e.,
\end{flushleft}


\begin{flushleft}
the one with lowest cost. Try out this procedure on some small problem instances,
\end{flushleft}


\begin{flushleft}
and the large problem instance you considered in part (a).
\end{flushleft}


\begin{flushleft}
(d) A greedy heuristic refinement. Suppose you are given a partition x, i.e., xi $\in$ \{$-$1, 1\},
\end{flushleft}


\begin{flushleft}
i = 1, . . . , n. How does the objective value change if we move element i from one
\end{flushleft}


\begin{flushleft}
set to the other, i.e., change xi to $-$xi ? Now consider the following simple greedy
\end{flushleft}


\begin{flushleft}
algorithm: given a starting partition x, move the element that gives the largest
\end{flushleft}


\begin{flushleft}
reduction in the objective. Repeat this procedure until no reduction in objective
\end{flushleft}


\begin{flushleft}
can be obtained by moving an element from one set to the other.
\end{flushleft}


\begin{flushleft}
Try this heuristic on some problem instances, including the large one, starting from
\end{flushleft}


\begin{flushleft}
various initial partitions, including x = 1, the heuristic approximate solution found
\end{flushleft}


\begin{flushleft}
in part (b), and the randomly generated approximate solutions found in part (c).
\end{flushleft}


\begin{flushleft}
How much does this greedy refinement improve your approximate solutions from
\end{flushleft}


\begin{flushleft}
parts (b) and (c)?
\end{flushleft}


\begin{flushleft}
Solution.
\end{flushleft}


\begin{flushleft}
(a) We implement a barrier method to solve the SDP (11.66). The only constraint in
\end{flushleft}


\begin{flushleft}
the problem is the LMI W + diag($\nu$)
\end{flushleft}


\begin{flushleft}
0, for which we will use the log barrier
\end{flushleft}


\begin{flushleft}
$-$ log det(W + diag($\nu$)). To start the barrier method, we need a strictly feasible
\end{flushleft}


\begin{flushleft}
point, but this is easily found. If $\lambda$min (W ) is the smallest eigenvalue of the matrix
\end{flushleft}


\begin{flushleft}
W , then W +($-$$\lambda$min (W )+1)I has smallest eigenvalue one, and so is positive definite.
\end{flushleft}


\begin{flushleft}
Thus, $\nu$ = ($-$$\lambda$min (W ) + 1)1 is a strictly feasible starting point.
\end{flushleft}


\begin{flushleft}
At each outer iteration, we use Newton's method to minimize
\end{flushleft}


\begin{flushleft}
f ($\nu$) = t1T $\nu$ $-$ log det(W + diag($\nu$)).
\end{flushleft}





\begin{flushleft}
(11.23.A)
\end{flushleft}





\begin{flushleft}
We can start with t = 1, and at the end of each outer iteration increase t by a factor
\end{flushleft}


\begin{flushleft}
$\mu$ = 10 (say) until the desired accuracy is reached. At the end of each iteration, the
\end{flushleft}


\begin{flushleft}
duality gap is exactly n/t, with dual feasible point
\end{flushleft}


\begin{flushleft}
Z = (n/t)(W + diag($\nu$))$-$1 .
\end{flushleft}


\begin{flushleft}
We will return $\nu$ and Z, at the end of the first outer iteration to satisfy n/t $\leq$ ,
\end{flushleft}


\begin{flushleft}
where is the required tolerance.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


\begin{flushleft}
Now we turn to the question of how to compute the gradient and Hessian of f . We
\end{flushleft}


\begin{flushleft}
know that for X $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ , the gradient of the function g(X) = log det(X) at X is
\end{flushleft}


\begin{flushleft}
given by
\end{flushleft}


\begin{flushleft}
$\nabla$g(X) = X $-$1 .
\end{flushleft}


\begin{flushleft}
We use the chain rule, with
\end{flushleft}





\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
$\nu$i Eii ,
\end{flushleft}





\begin{flushleft}
X = W + diag($\nu$) = W +
\end{flushleft}


\begin{flushleft}
i=1
\end{flushleft}





\begin{flushleft}
where Eii is the matrix with a one in the i, i entry and zeros elsewhere, to obtain
\end{flushleft}


\begin{flushleft}
$\nabla$f ($\nu$)i
\end{flushleft}





=


=





\begin{flushleft}
t $-$ tr((W + diag($\nu$))$-$1 Eii )
\end{flushleft}





\begin{flushleft}
t $-$ (W + diag($\nu$))$-$1
\end{flushleft}





\begin{flushleft}
ii
\end{flushleft}





\begin{flushleft}
for i = 1, . . . , n. Thus we have the simple formula
\end{flushleft}


\begin{flushleft}
$\nabla$f ($\nu$) = t1 $-$ diag((W + diag($\nu$))$-$1 ).
\end{flushleft}


\begin{flushleft}
The second derivative of log det X, at X $\in$ Sn
\end{flushleft}


\begin{flushleft}
++ , is given by the bilinear form
\end{flushleft}


\begin{flushleft}
$\nabla$2 g(X)[Y, Z] = $-$ tr(X $-$1 Y X $-$1 Z).
\end{flushleft}


\begin{flushleft}
Applying this to our function f yields, with X = W + diag($\nu$),
\end{flushleft}


\begin{flushleft}
$\nabla$2 f ($\nu$)ij = tr(X $-$1 Eii X $-$1 Ejj ) = X $-$1
\end{flushleft}





2


\begin{flushleft}
ij
\end{flushleft}





,





\begin{flushleft}
for i, j = 1, . . . , n. Thus we have the very simple formula for the Hessian:
\end{flushleft}


\begin{flushleft}
$\nabla$2 f ($\nu$) = (W + diag($\nu$))$-$1 ◦ (W + diag($\nu$))$-$1 ,
\end{flushleft}


\begin{flushleft}
where for U, V $\in$ Sn , the Schur (or Hadamard, or elementwise) product of U and
\end{flushleft}


\begin{flushleft}
V , denoted W = U ◦ V , is defined by Wij = Uij Vij .
\end{flushleft}


\begin{flushleft}
We first test the method on some small problems. We generate random symmetric
\end{flushleft}


\begin{flushleft}
matrices W $\in$ S10 , with off-diagonal elements generated from independent N (0, 1)
\end{flushleft}


\begin{flushleft}
distributions, and zero diagonal elements. The figure shows the distribution of the
\end{flushleft}


\begin{flushleft}
relative error
\end{flushleft}


\begin{flushleft}
$-$1T $\nu$ $-$ p
\end{flushleft}


\begin{flushleft}
|p |
\end{flushleft}


\begin{flushleft}
for 100 randomly generated matrices.
\end{flushleft}





\begin{flushleft}
number of samples
\end{flushleft}





10


8


6


4


2





\begin{flushleft}
PSfrag replacements
\end{flushleft}


0


$-$0.2





$-$0.1





\begin{flushleft}
($-$1T $\nu$ $-$ p )/|p |
\end{flushleft}





0





\newpage
11





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
We notice that the lower bound is equal (or very close) to p in 10 cases, and never
\end{flushleft}


\begin{flushleft}
less than about 15\% below p .
\end{flushleft}


\begin{flushleft}
We also generate a larger problem instance, with n = 100. The optimal value of the
\end{flushleft}


\begin{flushleft}
relaxation is $-$1687.5. The lower bound from the eigenvalue decomposition of W
\end{flushleft}


\begin{flushleft}
(see remark 5.1) is n$\lambda$min (W ) = $-$1898.4.
\end{flushleft}





\begin{flushleft}
(b) We first try the heuristic on the family of 100 problems with n = 10. The heuristic
\end{flushleft}


\begin{flushleft}
gave the correct solution in 70 instances. For the larger problem, the heuristic gives
\end{flushleft}


\begin{flushleft}
the upper bound $-$1336.5. At this point we can say that the optimal value of the
\end{flushleft}


\begin{flushleft}
larger problem lies between $-$1336.5 and $-$1687.5.
\end{flushleft}


\begin{flushleft}
(c) We first try this heuristic, with K = 10, on the family of 100 problems with n = 10.
\end{flushleft}


\begin{flushleft}
The heuristic gave the correct solution in 88 instances.
\end{flushleft}


\begin{flushleft}
We plot below a histogram of the objective obtained by the randomized heuristic,
\end{flushleft}


\begin{flushleft}
over 1000 samples.
\end{flushleft}


30





\begin{flushleft}
number of samples
\end{flushleft}





25


20


15


10


5





\begin{flushleft}
PSfrag replacements
\end{flushleft}





0


$-$1500





$-$1400





$-$1300





$-$1200





$-$1100





$-$1000





$-$900





\begin{flushleft}
objective value
\end{flushleft}


\begin{flushleft}
Many of these samples have an objective value larger than the one found in part (b)
\end{flushleft}


\begin{flushleft}
above, but some have a lower cost. The minimum value is $-$1421.7, so p lies between
\end{flushleft}


\begin{flushleft}
$-$1421.7 and $-$1687.5.
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}





\begin{flushleft}
(d) The contribution of xj to the cost is ( i=1 Wij xi )xj . If this number is positive,
\end{flushleft}


\begin{flushleft}
n
\end{flushleft}


\begin{flushleft}
then switching the sign of xj will decrease the objective by 2 i=1 Wij xi .
\end{flushleft}


\begin{flushleft}
We apply the greedy heuristic to the larger problem instance. For x = 1, the cost
\end{flushleft}


\begin{flushleft}
is reduced from 13.6 to $-$1344.8. For the solution from part (b), the cost is reduced
\end{flushleft}


\begin{flushleft}
from $-$1336.5 to $-$1440.6. For the solution from part (b), the cost is reduced from
\end{flushleft}


\begin{flushleft}
$-$1421.7 to $-$1440.6.
\end{flushleft}


\begin{flushleft}
11.24 Barrier and primal-dual interior-point methods for quadratic programming. Implement
\end{flushleft}


\begin{flushleft}
a barrier method, and a primal-dual method, for solving the QP (without equality constraints, for simplicity)
\end{flushleft}


\begin{flushleft}
minimize
\end{flushleft}


\begin{flushleft}
(1/2)xT P x + q T x
\end{flushleft}


\begin{flushleft}
subject to Ax b,
\end{flushleft}


\begin{flushleft}
with A $\in$ Rm×n . You can assume a strictly feasible initial point is given. Test your codes
\end{flushleft}


\begin{flushleft}
on several examples. For the barrier method, plot the duality gap versus Newton steps.
\end{flushleft}


\begin{flushleft}
For the primal-dual interior-point method, plot the surrogate duality gap and the norm
\end{flushleft}


\begin{flushleft}
of the dual residual versus iteration number.
\end{flushleft}


\begin{flushleft}
Solution. The first figure shows the progress (duality gap) versus Newton iterations for
\end{flushleft}


\begin{flushleft}
the barrier method, applied to a randomly generated instance with n = 100 variables and
\end{flushleft}


\begin{flushleft}
m = 200 constraints. We use $\mu$ = 20, $\alpha$ = 0.01, $\beta$ = 0.5, and t(0) = 1. We choose b 0,
\end{flushleft}


\begin{flushleft}
and use x(0) = 0.
\end{flushleft}





\begin{flushleft}
\newpage
Exercises
\end{flushleft}


4





10





2





\begin{flushleft}
duality gap
\end{flushleft}





10





0





10





$-$2





10





$-$4





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





$-$6





10





0





10





20





30





40





50





60





\begin{flushleft}
iteration number
\end{flushleft}


\begin{flushleft}
The next two figure show the progress (surrogate duality gap $\eta$ and dual residual norm
\end{flushleft}


\begin{flushleft}
rdual 2 versus iteration number) of the primal-dual method applied to the same problem
\end{flushleft}


(0)


\begin{flushleft}
instance. We use $\mu$ = 10, $\alpha$ = 0.01, $\beta$ = 0.5, x(0) = 1, and $\lambda$i = 1/bi .
\end{flushleft}


4





10





5





10





2





10





0





10





0





$-$2





\begin{flushleft}
rdual
\end{flushleft}





\begin{flushleft}
$\eta$ˆ
\end{flushleft}





2





10


10





$-$5





10





$-$4





10





$-$10





10





$-$6





10





\begin{flushleft}
PSfrag replacements
\end{flushleft}





\begin{flushleft}
PSfrag replacements
\end{flushleft}


$-$8





10





0





5





10





15





20





$-$15





10





0





\begin{flushleft}
iteration number
\end{flushleft}





5





10





15





\begin{flushleft}
iteration number
\end{flushleft}





\begin{flushleft}
The Matlab code for the barrier method is as follows.
\end{flushleft}


\begin{flushleft}
MAXITERS = 200;
\end{flushleft}


\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
MU = 20;
\end{flushleft}


\begin{flushleft}
TOL = 1e-3;
\end{flushleft}


\begin{flushleft}
NTTOL = 1e-6;
\end{flushleft}


\begin{flushleft}
x = zeros(n,1); t = 1;
\end{flushleft}


\begin{flushleft}
for iter = 1:MAXITERS
\end{flushleft}


\begin{flushleft}
y = b-A*x;
\end{flushleft}


\begin{flushleft}
val = t*(.5*x'*P*x + q'*x) - sum(log(y));
\end{flushleft}


\begin{flushleft}
grad = t*(P*x+q) + A'*(1./y);
\end{flushleft}


\begin{flushleft}
hess = t*P + A'*diag(1./y.\^{}2)*A;
\end{flushleft}


\begin{flushleft}
v = -hess\ensuremath{\backslash}grad; fprime = grad'*v;
\end{flushleft}


\begin{flushleft}
s = 1; dy = -A*v;
\end{flushleft}


\begin{flushleft}
while (min(y+s*dy) $<$= 0), s = BETA*s; end;
\end{flushleft}


\begin{flushleft}
while (t*(.5*(x+s*v)'*P*(x+s*v) + q'*(x+s*v)) - ...
\end{flushleft}


\begin{flushleft}
sum(log(y+s*dy)) $>$= val + ALPHA*s*fprime), s=BETA*s;end;
\end{flushleft}


\begin{flushleft}
x = x+s*v;
\end{flushleft}


\begin{flushleft}
if (-fprime $<$ NTTOL),
\end{flushleft}





20





\newpage
11


\begin{flushleft}
gap = m/t;
\end{flushleft}


\begin{flushleft}
if (gap $<$ TOL),
\end{flushleft}


\begin{flushleft}
t = MU*t;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}





\begin{flushleft}
break;
\end{flushleft}





\begin{flushleft}
Interior-point methods
\end{flushleft}





\begin{flushleft}
end;
\end{flushleft}





\begin{flushleft}
The Matlab code for the primal-dual method is as follows.
\end{flushleft}


\begin{flushleft}
MAXITERS = 200;
\end{flushleft}


\begin{flushleft}
TOL = 1e-6;
\end{flushleft}


\begin{flushleft}
RESTOL = 1e-8;
\end{flushleft}


\begin{flushleft}
MU = 10;
\end{flushleft}


\begin{flushleft}
ALPHA = 0.01;
\end{flushleft}


\begin{flushleft}
BETA = 0.5;
\end{flushleft}


\begin{flushleft}
x = zeros(n,1); s = b-A*x; z = 1./s;
\end{flushleft}


\begin{flushleft}
for iters = 1:MAXITERS
\end{flushleft}


\begin{flushleft}
gap = s'*z;
\end{flushleft}


\begin{flushleft}
res = P*x + q + A'*z ;
\end{flushleft}


\begin{flushleft}
if ((gap $<$ TOL) \& (norm(res) $<$ RESTOL)), break; end;
\end{flushleft}


\begin{flushleft}
tinv = gap/(m*MU);
\end{flushleft}


\begin{flushleft}
sol = -[ P
\end{flushleft}


\begin{flushleft}
A'; A
\end{flushleft}


\begin{flushleft}
diag(-s./z) ] \ensuremath{\backslash} ...
\end{flushleft}


\begin{flushleft}
[ P*x+q+A'*z; -s + tinv*(1./z) ];
\end{flushleft}


\begin{flushleft}
dx = sol(1:n); dz = sol(n+[1:m]); ds = -A*dx;
\end{flushleft}


\begin{flushleft}
r = [P*x+q+A'*z; z.*s-tinv];
\end{flushleft}


\begin{flushleft}
step = min(1.0, 0.99/max(-dz./z));
\end{flushleft}


\begin{flushleft}
while (min(s+step*ds) $<$= 0), step = BETA*step; end;
\end{flushleft}


\begin{flushleft}
newz = z+step*dz; newx = x+step*dx; news = s+step*ds;
\end{flushleft}


\begin{flushleft}
newr = [P*newx+q+A'*newz; newz.*news-tinv];
\end{flushleft}


\begin{flushleft}
while (norm(newr) $>$ (1-ALPHA*step)*norm(r))
\end{flushleft}


\begin{flushleft}
step = BETA*step;
\end{flushleft}


\begin{flushleft}
newz = z+step*dz; newx = x+step*dx; news = s+step*ds;
\end{flushleft}


\begin{flushleft}
newr = [P*newx+q+A'*newz; newz.*news-tinv];
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}


\begin{flushleft}
x = x+step*dx; z = z +step*dz; s = b-A*x;
\end{flushleft}


\begin{flushleft}
end;
\end{flushleft}





\newpage



\end{document}
